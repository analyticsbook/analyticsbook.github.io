<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Exercises | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Exercises | Data Analytics">

<title>Exercises | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="exercises-6" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-hw-solutionpath"></span>
<img src="graphics/8_hw_solutionpath.png" alt="The path trajectory of a LASSO model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 160: The path trajectory of a LASSO model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Figure <a href="exercises-6.html#fig:f8-hw-solutionpath">160</a> shows the path trajectory generated by applying <code>glmnet()</code> on a dataset with <span class="math inline">\(10\)</span> predictors. Which two variables are the top two significant variables (note the index of the variables is shown in the right end of the figure)?</p></li>
<li><p> Consider the dataset shown in Table <a href="exercises-6.html#tab:t8-hw-lasso">42</a>. Set <span class="math inline">\(\lambda = 1\)</span> and initial values for <span class="math inline">\(\beta_1 = 0\)</span>, and <span class="math inline">\(\beta_2 = 1\)</span>. Implement the Shooting algorithm by manual operation. Do one iteration. Report <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-hw-lasso">Table 42: </span>Dataset for Q2</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-0.15\)</span></td>
<td align="left"><span class="math inline">\(-0.48\)</span></td>
<td align="left"><span class="math inline">\(0.46\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-0.72\)</span></td>
<td align="left"><span class="math inline">\(-0.54\)</span></td>
<td align="left"><span class="math inline">\(-0.37\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.91\)</span></td>
<td align="left"><span class="math inline">\(-0.27\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.61\)</span></td>
<td align="left"><span class="math inline">\(1.59\)</span></td>
<td align="left"><span class="math inline">\(1.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-1.11\)</span></td>
<td align="left"><span class="math inline">\(0.34\)</span></td>
<td align="left"><span class="math inline">\(-0.11\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Follow up on the dataset in Q2. Use the R pipeline for LASSO on this data. Compare the result from R and the result by your manual calculation.</p></li>
<li><p> Conduct a principal component analysis for the dataset shown in Table <a href="exercises-6.html#tab:t8-hw-pca">43</a>. Show details of the process.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-hw-pca">Table 43: </span>Dataset for Q4</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1.8\)</span></td>
<td align="left"><span class="math inline">\(2.08\)</span></td>
<td align="left"><span class="math inline">\(-0.28\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3.6\)</span></td>
<td align="left"><span class="math inline">\(-0.78\)</span></td>
<td align="left"><span class="math inline">\(0.79\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.2\)</span></td>
<td align="left"><span class="math inline">\(-0.08\)</span></td>
<td align="left"><span class="math inline">\(-0.52\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4.3\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left"><span class="math inline">\(-0.47\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.1\)</span></td>
<td align="left"><span class="math inline">\(0.71\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3.6\)</span></td>
<td align="left"><span class="math inline">\(1.29\)</span></td>
<td align="left"><span class="math inline">\(0.67\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.2\)</span></td>
<td align="left"><span class="math inline">\(0.57\)</span></td>
<td align="left"><span class="math inline">\(0.15\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4.0\)</span></td>
<td align="left"><span class="math inline">\(1.12\)</span></td>
<td align="left"><span class="math inline">\(1.18\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<ol style="list-style-type: lower-alpha">
<li>Standardize the dataset (i.e., by making the means of the variables to be zero, and the standard derivations of the variables to be <span class="math inline">\(1\)</span>). (b) Calculate the sample covariance matrix (i.e., <span class="math inline">\(\boldsymbol{S} =(\boldsymbol{X}^T\boldsymbol{X})/(N-1))\)</span>. (c) Conduct eigenvalue decomposition on the sample covariance matrix, obtain the four eigenvectors and their eigenvalues. (d) Report the percentage of variances that could be explained by the four PCs, respectively. Draw the screeplot. How many PCs are sufficient to represent the dataset? In other words, which PCs are significant? (e) Interpret the PCs you have selected, i.e., which variables define which PCs? (f) Convert the original data into the space spanned by the four PCs, by filling in Table <a href="exercises-6.html#tab:t8-hw-pca2">44</a>.</li>
</ol>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-hw-pca2">Table 44: </span>Dataset for Q4</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(\text{PC}_1\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_2\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_3\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Follow up on the dataset in Q2 from Chapter 7. (a) Conduct the PCA analysis on the three predictors to identify the three principal components and their contributions on explaining the variance in data; and (b) use the R pipeline for PCA to do the PCA analysis and compare with your manual calculation.</p></li>
<li><p> Suppose that we have an outcome variable that could be augmented into the dataset in Q4, as shown in Table <a href="exercises-6.html#tab:t8-hw-pca-lr">45</a>. Apply the shooting algorithm for LASSO on this dataset to identify important variables. Use the following initial values for the parameters, <span class="math inline">\(\lambda=1, \beta_1=0, \beta_2=1, \beta_3=1, \beta_4=1\)</span>, and just do one iteration of the shooting algorithm. Show details of manual calculation.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-hw-pca-lr">Table 45: </span>Dataset for Q6</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1.8\)</span></td>
<td align="left"><span class="math inline">\(2.08\)</span></td>
<td align="left"><span class="math inline">\(-0.28\)</span></td>
<td align="left"><span class="math inline">\(1.2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3.6\)</span></td>
<td align="left"><span class="math inline">\(-0.78\)</span></td>
<td align="left"><span class="math inline">\(0.79\)</span></td>
<td align="left"><span class="math inline">\(2.1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.2\)</span></td>
<td align="left"><span class="math inline">\(-0.08\)</span></td>
<td align="left"><span class="math inline">\(-0.52\)</span></td>
<td align="left"><span class="math inline">\(0.8\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4.3\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left"><span class="math inline">\(-0.47\)</span></td>
<td align="left"><span class="math inline">\(1.5\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.1\)</span></td>
<td align="left"><span class="math inline">\(0.71\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.8\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3.6\)</span></td>
<td align="left"><span class="math inline">\(1.29\)</span></td>
<td align="left"><span class="math inline">\(0.67\)</span></td>
<td align="left"><span class="math inline">\(1.6\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.2\)</span></td>
<td align="left"><span class="math inline">\(0.57\)</span></td>
<td align="left"><span class="math inline">\(0.15\)</span></td>
<td align="left"><span class="math inline">\(1.2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4.0\)</span></td>
<td align="left"><span class="math inline">\(1.12\)</span></td>
<td align="left"><span class="math inline">\(1.18\)</span></td>
<td align="left"><span class="math inline">\(1.6\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> After extraction of the four PCs from Q4, use <code>lm()</code> in R to build a linear regression model with the outcome variable (as shown in Table <a href="exercises-6.html#tab:t8-hw-pca-lr">45</a>) and the four PCs as the predictors. (a) Report the summary of your linear regression model with the four PCs; and (b) which PCs significantly affect the outcome variable?</p></li>
<li><p> Revisit Q1 in <strong>Chapter 3</strong>. Derive the shooting algorithm for weighted least squares regression with <span class="math inline">\(L_1\)</span> norm penalty.</p></li>
<li><p> Design a simulated experiment to evaluate the effectiveness of the <code>glmet()</code> in the R package <code>glmnet</code>. (a) For instance, you can simulate <span class="math inline">\(20\)</span> samples from a linear regression model with <span class="math inline">\(10\)</span> variables, where only <span class="math inline">\(2\)</span> out of the <span class="math inline">\(10\)</span> variables are truly significant, e.g., the true model is
<span class="math display">\[
  y = \beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
  \]</span>
where <span class="math inline">\(\beta_{1} = 1\)</span>, <span class="math inline">\(\beta_{2} = 1\)</span>, and
<span class="math display">\[
  \epsilon \sim N\left(0, 1\right).
  \]</span>
You can simulate <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> using the standard normal distribution <span class="math inline">\(N\left(0, 1\right)\)</span>. For the other <span class="math inline">\(8\)</span> variables, <span class="math inline">\(x_3\)</span> to <span class="math inline">\(x_{10}\)</span>, you can simulate each from <span class="math inline">\(N\left(0, 1\right)\)</span>. In data analysis, we will use all <span class="math inline">\(10\)</span> variables as predictors, since we won’t know the true model. (b) Run <code>lm()</code> on the simulated data and comment on the results. (c) Run <code>glmnet()</code> on the simulated data, and check the path trajectory plot to see if the true significant variables could be detected. (d) Use the cross-validation process integrated into the <code>glmnet</code> package to see if the true significant variables could be detected. (e) Use <code>rpart()</code> to build a decision tree and extract the variable importance score to see if the true significant variables could be detected. (f) Use <code>randomforest()</code> to build a random forest model and extract the variable importance score, to see if the true significant variables could be detected.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="remarks-6.html"><button class="btn btn-default">Previous</button></a>
<a href="chapter-9-pragmatism-experience-experimental.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
