<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Clustering | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Clustering | Data Analytics">

<title>Clustering | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="clustering" class="section level2 unnumbered">
<h2>Clustering</h2>
<div id="rationale-and-formulation-9" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><em>Clustering</em> takes the idea of <em>diagnosis</em> to a different level. If the <em>residual analysis</em> is like a tailor working out the perfect outfit for a client, clustering is … well, it is better to see Figure <a href="clustering.html#fig:f6-twocluster-nd">106</a> first.</p>
<p></p>
<div class="figure"><span id="fig:f6-twocluster-nd"></span>
<p class="caption marginnote shownote">
Figure 106: A tailor tries to (left) make an outfit (i.e., the normal curve) for a client (i.e., the data, represented as a histogram) vs. (right) then the tailor realizes the form of the outfit should be two normal curves
</p>
<img src="graphics/6_twocluster_nd.png" alt="A tailor tries to (left) make an outfit (i.e., the normal curve) for a client (i.e., the data, represented as a histogram) vs. (right) then the tailor realizes the form of the outfit should be two normal curves" width="100%"  />
</div>
<p></p>
<p>Figure <a href="clustering.html#fig:f6-twocluster-nd">106</a> demonstrates one meaning of clustering: a dataset is heterogeneous and is probably collected from a few different populations (sometimes we call them <em>sub</em>populations). Understanding the clustering structure of a dataset not only benefits the statistical modeling, as shown in Figure <a href="clustering.html#fig:f6-twocluster-nd">106</a> where we will use two normal distributions to model the data, but also reveals insights about the problem under study. For example, the dataset shown in Figure <a href="clustering.html#fig:f6-twocluster-nd">106</a> was collected from a disease study of young children. It suggests that there are two disease mechanisms (we often call them two <em>phenotypes</em>). Phenotypes discovery is important for disease treatment, since patients with different disease mechanisms respond to treatments differently. A typical approach for phenotypes discovery is to collect an abundance of data from many patients. Then, we employ a range of algorithms to discover clusters of the data points. These clustering algorithms, differ from each other in their premises of what a cluster looks like, more or less bear the same conceptual framework as shown in Figure <a href="clustering.html#fig:f6-twocluster-nd">106</a>.</p>
<p></p>
<div class="figure"><span id="fig:f6-twocluster-lr"></span>
<p class="caption marginnote shownote">
Figure 107: Another example of clustering: if the clustering structure is ignored, the fitted model (left) may show the opposite direction of the true model (right)
</p>
<img src="graphics/6_twocluster_lr.png" alt="Another example of clustering: if the clustering structure is ignored, the fitted model (left) may show the opposite direction of the true model (right) " width="100%"  />
</div>
<p></p>
<p>Clustering is a flexible concept that could be applied in other scenarios as well. Figure <a href="clustering.html#fig:f6-twocluster-lr">107</a> demonstrates another meaning of clustering. It is less commonly perceived, but in practice it is not uncommon. The “moral of the story” shown in Figure <a href="clustering.html#fig:f6-twocluster-lr">107</a> tells us that, when you have a dataset, you may want to conduct EDA and check the clustering structure first before imposing a model that may only fit the <em>data format</em> but not the <em>statistical structure</em><label for="tufte-sn-144" class="margin-toggle sidenote-number">144</label><input type="checkbox" id="tufte-sn-144" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">144</span> E.g., in Figure <a href="clustering.html#fig:f6-twocluster-lr">107</a>: <em>data format</em>: we have predictors and outcome, so it seems natural to fit a linear regression model; <em>statistical structure</em>: however, it is a mix of two subpopulations that demand two models.</span>.</p>
</div>
<div id="theory-and-method-6" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>Given a dataset, how do we know there is a clustering structure? Consider the dataset shown in Table <a href="clustering.html#tab:t6-example">25</a>. Are there <em>sub</em>populations as shown in Figure <a href="clustering.html#fig:f6-twocluster-nd">106</a>?</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example">Table 25: </span>Example of a dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">$ $</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Value</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Cluster</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p>A visual check of the <span class="math inline">\(7\)</span> data points suggests there are probably two clusters. If each cluster can be modeled as a Gaussian distribution, this would be a two-component <strong>Gaussian Mixture Model</strong> (<strong>GMM</strong>)<label for="tufte-sn-145" class="margin-toggle sidenote-number">145</label><input type="checkbox" id="tufte-sn-145" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">145</span> A GMM consists of multiple Gaussian distributions. Figure <a href="clustering.html#fig:f6-twocluster-nd">106</a> shows one example of two univariate Gaussian distributions mixed together. Generally, the parameters of a GMM are denoted as <span class="math inline">\(\boldsymbol{\Theta}\)</span>, which include the parameters of each Gaussian distribution: <span class="math inline">\(\mu_{i}\)</span> and <span class="math inline">\(\sigma_{i}\)</span> are the mean and variance of the <span class="math inline">\(i^{th}\)</span> Gaussian distribution, respectively, and <span class="math inline">\(\pi_{i}\)</span> is the proportion of the data points that were sampled from the <span class="math inline">\(i^{th}\)</span> Gaussian distribution. </span>.</p>
<p>In this particular dataset, clustering could be done by learning the parameters of the two-component (<strong>GMM</strong>), (i.e., to address the question marks in the last row of Table <a href="clustering.html#tab:t6-example">25</a>). If we have known the parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span>, we could probabilistically infer which cluster each data point belongs to (i.e., to address the question marks in the second row of Table <a href="clustering.html#tab:t6-example">25</a>). On the other hand, if we have known which cluster each data point belongs to, we can collect the data points of each cluster to estimate the parameters of the Gaussian distribution that characterizes each cluster. This “locked” relation between the two tasks is shown in Figure <a href="clustering.html#fig:f6-cluster-cycle">108</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-cluster-cycle"></span>
<img src="graphics/6_cluster_cycle.png" alt="The locked relation between parameter estimation (M-step, i.e., last row of Table \@ref(tab:t6-example)) and data point inference (E-step, i.e., second row of Table \@ref(tab:t6-example)) in GMM" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 108: The locked relation between parameter estimation (M-step, i.e., last row of Table <a href="clustering.html#tab:t6-example">25</a>) and data point inference (E-step, i.e., second row of Table <a href="clustering.html#tab:t6-example">25</a>) in GMM<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The two interdependent tasks hold the key for each other. What is needed is <em>initialization</em>. As there are two blocks in Figure <a href="clustering.html#fig:f6-cluster-cycle">108</a>, we have two locations to initialize the process of unlocking.</p>
<p><em>Initialization.</em> Let’s initialize the values in the second row of Table <a href="clustering.html#tab:t6-example">25</a> for an example. We assign (i.e., <em>randomly</em>) labels on the data points as shown in Table <a href="clustering.html#tab:t6-example-init">26</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-init">Table 26: </span>Initialization on the dataset example</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>M-step.</em> Then, we estimate <span class="math inline">\(\mu_{1}=1.75\)</span> and <span class="math inline">\(\sigma_{1}^{2}=2.83\)</span> based on the data points <span class="math inline">\(\{1.13, 4.76, 0.87, 1.03, 0.98\}\)</span>.<label for="tufte-sn-146" class="margin-toggle sidenote-number">146</label><input type="checkbox" id="tufte-sn-146" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">146</span> These <span class="math inline">\(5\)</span> data instances are initially assigned to <span class="math inline">\(C1\)</span>. Note that <span class="math inline">\(4.76\)</span> is different from the rest of the data points in the same cluster. This is an error introduced by the initialization. Later we will see that this error could be automatically fixed by the algorithm.</span></p>
<p>Similarly, we could estimate <span class="math inline">\(\mu_{2}=3.81\)</span> and <span class="math inline">\(\sigma_{2}^{2}=0.47\)</span> based on the data points <span class="math inline">\(\{3.32, 4.29\}\)</span>.</p>
<p>It is straightforward to estimate <span class="math inline">\(\pi_{1}=5/7 = 0.714\)</span> and <span class="math inline">\(\pi_{2}=2/7 = 0.286\)</span>.</p>
<p>Table <a href="clustering.html#tab:t6-example-init">26</a> is updated.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-thetaupdated">Table 27: </span><span class="math inline">\(\boldsymbol{\Theta}\)</span> updated</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>E-step.</em> Since the labels of the data points were randomly initialized, they need to be updated given the latest estimation of <span class="math inline">\(\boldsymbol{\Theta}\)</span>. We continue to update the labels of the data points. To facilitate the presentation, we invent a binary indicator variable, denoted as <span class="math inline">\(z_{n m}\)</span>: <span class="math inline">\(z_{n m}=1\)</span> indicates that the data point <span class="math inline">\(x_{n}\)</span> was <em>assumed to be</em> sampled from the <span class="math inline">\(m^{th}\)</span> cluster; otherwise, <span class="math inline">\(z_{n m}=0\)</span>.</p>
<p>For example, if the first data point was sampled from the first cluster, the probability that <span class="math inline">\(x_1 = 1.13\)</span> is<label for="tufte-sn-147" class="margin-toggle sidenote-number">147</label><input type="checkbox" id="tufte-sn-147" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">147</span> In R, we could use the function <code>dnorm</code> to calculate it. For example, for <span class="math inline">\(p\left(x_{1} = 1.13| z_{11}=1 \right)\)</span>, we use <code>dnorm(1.13, mean = 1.75, sd = sqrt(2.83))</code> since <span class="math inline">\(\mu_{1}=1.75, \sigma_{1}^{2}=2.83\)</span>.</span></p>
<p><span class="math display">\[
    p\left(x_{1} = 1.13| z_{11}=1 \right)=0.22.
\]</span></p>
<p>And if the first data point was sampled from the second cluster, the probability that <span class="math inline">\(x_1 = 1.13\)</span> is</p>
<p><span class="math display">\[
    p\left(x_{1} = 1.13 | z_{12}=1 \right)=0.0003.
\]</span></p>
<p>Repeat it for all the other data points, we have:</p>
<p><!-- % \setlength{\belowdisplayskip}{0pt} % %%\setlength{\belowdisplayshortskip}{0pt} -->
<!-- % \setlength{\abovedisplayskip}{0pt} % %%\setlength{\abovedisplayshortskip}{0pt} --></p>
<p><span class="math display">\[
p\left(x_{2}=4.76 | z_{21}=1 \right)=0.05, p\left(x_{2}=4.76 | z_{22}=1 \right)=0.22;
\]</span></p>
<p><span class="math display">\[
p\left(x_{3}=0.87 | z_{31}=1 \right)=0.02, p\left(x_{3}=0.87 | z_{32}=1 \right)=0;
\]</span></p>
<p><span class="math display">\[
p\left(x_{4}=3.32 | z_{41}=1 \right)=0.15, p\left(x_{4}=3.32 | z_{42}=1 \right)=0.45;
\]</span></p>
<p><span class="math display">\[
p\left(x_{5}=4.29 | z_{51}=1 \right)=0.08, p\left(x_{5}=4.29 | z_{52}=1 \right)=0.45;
\]</span></p>
<p><span class="math display">\[
p\left(x_{6}=1.03 | z_{61}=1 \right)=0.22, p\left(x_{6}=1.03 | z_{62}=1 \right)=0.0001;
\]</span></p>
<p><span class="math display">\[
p\left(x_{7}=0.98 | z_{71}=1 \right)=0.21, p\left(x_{7}=0.98 | z_{72}=1 \right)=0.0001.
\]</span></p>
<!-- % \vspace{6pt} -->
<p>Note that we need to calculate “the probability of <em>which cluster</em> a data point was sampled from”<label for="tufte-sn-148" class="margin-toggle sidenote-number">148</label><input type="checkbox" id="tufte-sn-148" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">148</span> I.e., <span class="math inline">\(p\left(z_{11}=1 | x_1 = 1.13\right)\)</span>.</span>. This is different from the probabilities we have calculated as shown above, which concerns“if a data point was sampled from a cluster, then the probability of the <em>specific value</em> the data point took on”<label for="tufte-sn-149" class="margin-toggle sidenote-number">149</label><input type="checkbox" id="tufte-sn-149" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">149</span> I.e., <span class="math inline">\(p\left(x_{1} = 1.13| z_{11}=1 \right)\)</span>.</span>.</p>
<p>Thus, we further calculate the conditional probabilities of <span class="math inline">\(p\left(z_{i1} | x_i\right)\)</span></p>
<p><span class="math display">\[
p\left(z_{11}=1 | x_1 = 1.13 \right)=\frac{0.22 \times 0.714}{0.22 \times 0.714+0.0003 \times 0.286}=0.99; \text{ thus } x_1 \in C_1.
\]</span></p>
<p><span class="math display">\[
p\left(z_{21}=1 | x_2 = 4.76 \right)=\frac{0.05 \times 0.714}{0.05 \times 0.714+0.22 \times 0.286}=0.37; \text{ thus } x_2 \in C_2.
\]</span></p>
<p><span class="math display">\[
p\left(z_{31}=1 | x_3 = 0.87 \right)=\frac{0.02 \times 0.714}{0.02 \times 0.714+0.00 \times 0.286}=1; \text{ thus } x_3 \in C_1.
\]</span></p>
<p><span class="math display">\[
p\left(z_{41}=1 | x_4 = 3.32 \right)=\frac{0.15 \times 0.714}{0.15 \times 0.714+0.45 \times 0.286}=0.44; \text{ thus } x_4 \in C_2.
\]</span></p>
<p><span class="math display">\[
p\left(z_{51}=1 | x_5 = 4.29 \right)=\frac{0.08 \times 0.714}{0.08 \times 0.714+0.45 \times 0.286}=0.29; \text{ thus } x_5 \in C_2.
\]</span></p>
<p><span class="math display">\[
p\left(z_{61}=1 | x_6 = 1.03 \right)=\frac{0.22 \times 0.714}{0.22 \times 0.714+0.0001 \times 0.286}=0.99; \text{ thus } x_6 \in C_1.
\]</span></p>
<p><span class="math display">\[
p\left(z_{71}=1 | x_7 = 0.98 \right)=\frac{0.21 \times 0.714}{0.21 \times 0.714+0.0001 \times 0.286}=0.99; \text{ thus } x_7 \in C_1.
\]</span></p>
<!-- %\vspace{6pt} -->
<p>Table <a href="clustering.html#tab:t6-example-thetaupdated">27</a> can be updated to Table <a href="clustering.html#tab:t6-example-final">28</a>.</p>
<p>We can repeat this process and cycle through the two steps as shown in Figure <a href="clustering.html#fig:f6-cluster-cycle">108</a>, until the process converges, i.e., <span class="math inline">\(\boldsymbol{\Theta}\)</span> remains the same (or its change is very small), or the labels of the data points remain the same. In this example, we actually only need one more iteration to reach convergence. This algorithm is a basic version of the so-called <strong>EM algorithm</strong> . Interested readers could find a complete derivation process in the <strong>Remarks</strong> section.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-final">Table 28: </span>Cluster labels updated</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
</div>
<div id="formal-definition-of-the-gmm" class="section level3 unnumbered">
<h3>Formal definition of the GMM</h3>
<p>As a <em>data modeling</em> approach, the GMM implies a <em>data-generating mechanism</em> , that is summarized in below.</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> [1.] Suppose that there are <span class="math inline">\(M\)</span> distributions mixed together.</p></li>
<li><p> [2.] In GMM, we assume that all the distributions are Gaussian distributions, i.e., the parameters of the <span class="math inline">\(m^{\text{th}}\)</span> distribution are <span class="math inline">\(\left\{\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m}\right\}\)</span>, and <span class="math inline">\(m=1,2, \ldots, M\)</span>.<label for="tufte-sn-150" class="margin-toggle sidenote-number">150</label><input type="checkbox" id="tufte-sn-150" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">150</span> <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span> is the mean vector; <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> is the covariance matrix.</span></p></li>
<li><p> [3.] For any data point <span class="math inline">\(\boldsymbol{x}\)</span>, without knowing its specific value, the prior probability that it comes from the <span class="math inline">\(m^{\text{th}}\)</span> distribution is denoted as <span class="math inline">\(\pi_m\)</span>.<label for="tufte-sn-151" class="margin-toggle sidenote-number">151</label><input type="checkbox" id="tufte-sn-151" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">151</span> In other words, this is the percentage of the data points in the whole mix that come from the <span class="math inline">\(m^{th}\)</span> distribution.</span> Note that <span class="math inline">\(\sum_{m=1}^{M} \pi_m=1\)</span>.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>The final distribution form of <span class="math inline">\(\boldsymbol{x}\)</span> is a mixed distribution with <span class="math inline">\(m\)</span> components</p>
<p><span class="math display">\[
\boldsymbol{x} \sim \pi_{1} N\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1}\right) + \pi_{2} N\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2}\right) + {\ldots} + \pi_{m} N\left(\boldsymbol{\mu}_{m}, \boldsymbol{\Sigma}_{m}\right).
\]</span></p>
<p>To learn the parameters from data, the EM algorithm is used. A basic walk-through of the EM algorithm has been given, i.e., see the example using Table <a href="clustering.html#tab:t6-example">25</a>.</p>
</div>
<div id="r-lab-8" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We simulate a dataset with <span class="math inline">\(4\)</span> clusters as shown in Figure <a href="clustering.html#fig:f6-4clusters">109</a>.</p>
<p></p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="clustering.html#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a clustering structure</span></span>
<span id="cb138-2"><a href="clustering.html#cb138-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">10</span>,<span class="dv">2</span>), </span>
<span id="cb138-3"><a href="clustering.html#cb138-3" aria-hidden="true" tabindex="-1"></a>       <span class="fu">rnorm</span>(<span class="dv">200</span>,<span class="dv">20</span>,<span class="dv">1</span>), <span class="fu">rnorm</span>(<span class="dv">200</span>,<span class="dv">40</span>, <span class="dv">2</span>))</span>
<span id="cb138-4"><a href="clustering.html#cb138-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(<span class="dv">800</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb138-5"><a href="clustering.html#cb138-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X,Y, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;gray25&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-4clusters"></span>
<img src="graphics/6_13.png" alt="A mixture of $4$ Gaussian distributions" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 109: A mixture of <span class="math inline">\(4\)</span> Gaussian distributions<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We use the R package <code>Mclust</code> to implement the GMM model using the EM algorithm.</p>
<p></p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="clustering.html#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use GMM to identify the clusters</span></span>
<span id="cb139-2"><a href="clustering.html#cb139-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(mclust)</span>
<span id="cb139-3"><a href="clustering.html#cb139-3" aria-hidden="true" tabindex="-1"></a>XY.clust <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(<span class="fu">data.frame</span>(X,Y))</span>
<span id="cb139-4"><a href="clustering.html#cb139-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(XY.clust)</span>
<span id="cb139-5"><a href="clustering.html#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(XY.clust)</span></code></pre></div>
<p></p>
<p>We obtain the following result. Visualization of the identified clusters is shown in Figure <a href="clustering.html#fig:f6-14">110</a>. Note that we didn’t specify the number of clusters in the analysis. <code>Mclust</code> used BIC and correctly identified the <span class="math inline">\(4\)</span> clusters. For each cluster, the data points are about <span class="math inline">\(200\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="clustering.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb140-2"><a href="clustering.html#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Gaussian finite mixture model fitted by EM algorithm </span></span>
<span id="cb140-3"><a href="clustering.html#cb140-3" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb140-4"><a href="clustering.html#cb140-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb140-5"><a href="clustering.html#cb140-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Mclust VVI (diagonal, varying volume and shape) model with</span></span>
<span id="cb140-6"><a href="clustering.html#cb140-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 components:</span></span>
<span id="cb140-7"><a href="clustering.html#cb140-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb140-8"><a href="clustering.html#cb140-8" aria-hidden="true" tabindex="-1"></a><span class="do">##  log.likelihood   n df       BIC       ICL</span></span>
<span id="cb140-9"><a href="clustering.html#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="do">##        -3666.07 800 19 -7459.147 -7459.539</span></span>
<span id="cb140-10"><a href="clustering.html#cb140-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb140-11"><a href="clustering.html#cb140-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Clustering table:</span></span>
<span id="cb140-12"><a href="clustering.html#cb140-12" aria-hidden="true" tabindex="-1"></a><span class="do">##   1   2   3   4 </span></span>
<span id="cb140-13"><a href="clustering.html#cb140-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 199 201 200 200</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-14"></span>
<img src="graphics/6_14.png" alt="Clustering results of the simulated data" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 110: Clustering results of the simulated data<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now let’s implement GMM on the AD data. Result is shown in Figure <a href="clustering.html#fig:f6-15">111</a>.</p>
<p></p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="clustering.html#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;mclust&quot;)</span></span>
<span id="cb141-2"><a href="clustering.html#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(mclust)</span>
<span id="cb141-3"><a href="clustering.html#cb141-3" aria-hidden="true" tabindex="-1"></a>AD.Mclust <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(AD[,<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">10</span>,<span class="dv">12</span>,<span class="dv">14</span>,<span class="dv">15</span>)])</span>
<span id="cb141-4"><a href="clustering.html#cb141-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(AD.Mclust)</span>
<span id="cb141-5"><a href="clustering.html#cb141-5" aria-hidden="true" tabindex="-1"></a>AD.Mclust<span class="sc">$</span>data <span class="ot">=</span> AD.Mclust<span class="sc">$</span>data[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)]</span>
<span id="cb141-6"><a href="clustering.html#cb141-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(AD.Mclust)</span></span>
<span id="cb141-7"><a href="clustering.html#cb141-7" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb141-8"><a href="clustering.html#cb141-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Gaussian finite mixture model fitted by EM algorithm </span></span>
<span id="cb141-9"><a href="clustering.html#cb141-9" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb141-10"><a href="clustering.html#cb141-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-11"><a href="clustering.html#cb141-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Mclust EEI (diagonal, equal volume and shape) model </span></span>
<span id="cb141-12"><a href="clustering.html#cb141-12" aria-hidden="true" tabindex="-1"></a><span class="do">## with 4 components:</span></span>
<span id="cb141-13"><a href="clustering.html#cb141-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-14"><a href="clustering.html#cb141-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  log.likelihood   n df       BIC       ICL</span></span>
<span id="cb141-15"><a href="clustering.html#cb141-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       -3235.874 517 43 -6740.414 -6899.077</span></span>
<span id="cb141-16"><a href="clustering.html#cb141-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-17"><a href="clustering.html#cb141-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Clustering table:</span></span>
<span id="cb141-18"><a href="clustering.html#cb141-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   1   2   3   4 </span></span>
<span id="cb141-19"><a href="clustering.html#cb141-19" aria-hidden="true" tabindex="-1"></a><span class="do">##  43 253  92 129</span></span></code></pre></div>
<p></p>
<p></p>
<div class="figure"><span id="fig:f6-15"></span>
<p class="caption marginnote shownote">
Figure 111: Clustering results of the AD dataset
</p>
<img src="graphics/6_15.png" alt="Clustering results of the AD dataset" width="100%"  />
</div>
<p></p>
<p>Four clusters are identified as well. Figure <a href="clustering.html#fig:f6-15">111</a> shows the boundaries between clusters are not as distinct as the boundaries in Figure <a href="clustering.html#fig:f6-14">110</a>. In real applications, particularly for those applications of which we haven’t known enough, clustering is an exploration tool that could generate suggestive results but may not provide confirmatory conclusions.</p>
</div>
</div>
<p style="text-align: center;">
<a href="diagnosis-in-random-forests.html"><button class="btn btn-default">Previous</button></a>
<a href="remarks-4.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
