<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Chapter 3. Recognition: Logistic Regression &amp; Ranking | Data Analytics: A Small Data Approach" />
<meta property="og:type" content="book" />


<meta property="og:description" content="This book is suitable for an introductory course of data analytics to help students understand some main statistical learning models, such as linear regression, logistic regression, tree models and random forests, ensemble learning, sparse learning, principal component analysis, kernel methods including the support vector machine and kernel regression, etc. Data science practice is a process that should be told as a story, rather than a one-time implementation of one single model. This process is a main focus of this book, with many course materials about exploratory data analysis, residual analysis, and flowcharts to develop and validate models and data pipelines." />


<meta name="author" content="Shuai Huang &amp; Houtao Deng" />

<meta name="date" content="2021-04-16" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="This book is suitable for an introductory course of data analytics to help students understand some main statistical learning models, such as linear regression, logistic regression, tree models and random forests, ensemble learning, sparse learning, principal component analysis, kernel methods including the support vector machine and kernel regression, etc. Data science practice is a process that should be told as a story, rather than a one-time implementation of one single model. This process is a main focus of this book, with many course materials about exploratory data analysis, residual analysis, and flowcharts to develop and validate models and data pipelines.">

<title>Chapter 3. Recognition: Logistic Regression &amp; Ranking | Data Analytics: A Small Data Approach</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<meta name="description" content="My awesome presentation"/>
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#cover">Cover</a></li>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="chapter-3.-recognition-logistic-regression-ranking" class="section level1 unnumbered">
<h1>Chapter 3. Recognition: Logistic Regression &amp; Ranking</h1>
<div id="overview-1" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 3 is about <em>Recognition</em>. This is an important skill in real-world practices of data analytics. It is to recognize <em>the same</em> abstracted form embedded in different real-world problems. No matter how different the problem looks, we hope to leverage existing models and solutions that have been proven effective for the forms that are recognizable in the problem. This is why we say the same model/theory could be applied in multiple areas<label for="tufte-sn-52" class="margin-toggle sidenote-number">52</label><input type="checkbox" id="tufte-sn-52" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">52</span> Another practical metaphor is: <em>a model is a hammer, and applications are nails</em>.</span>.</p>
<p>This is not to say that a real-world problem is equivalent to an abstracted problem. A dialectic thinking is needed here to understand the relationship between a real-world problem and its reduced form, an abstracted formulation. On one hand, for a real-world problem to be <em>real-world</em>, it always has something that exceeds the boundary of a reduced form. On the other hand, for a real-world problem to be solvable, it has to have some kinds of forms.</p>
<p>Many operations researchers believe that being able to recognize these abstracted forms holds the key to solve real-world problems effectively<label for="tufte-sn-53" class="margin-toggle sidenote-number">53</label><input type="checkbox" id="tufte-sn-53" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">53</span> Some said, <em>formulation is an art; and a good formulation contributes more than <span class="math inline">\(50\%\)</span> in solving the problem.</em></span>. For some abstracted forms, indeed we have studied them well and are confident to provide a sense of “closure.” It takes a sense of closure to conclude that we have solved a real-world problem, or at least we have reached the best solution as far as our knowledge permits. And we have established criteria to evaluate how well we have solved these abstract forms. Those are the territories where we have surveyed in detail and in depth. If to solve a real-world problem is to battle a dragon in its lair, <em>recognition</em> is all about paving the way for the dragon to follow the bread crumbs so that we can battle it in a familiar battlefield.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="logistic-regression-model" class="section level2 unnumbered">
<h2>Logistic regression model</h2>
<div id="rationale-and-formulation-2" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-lrgoal1"></span>
<img src="graphics/3_lrgoal1.png" alt="Direct application of linear regression on binary outcome, i.e., illustration of Eq. \@ref(eq:3-goal1) on a one-predictor problem where $x$ is the dose of a treatment and $y$ is the binary outcome variable." width="100%"  />
<!--
<p class="caption marginnote">-->Figure 27: Direct application of linear regression on binary outcome, i.e., illustration of Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-goal1">(23)</a> on a one-predictor problem where <span class="math inline">\(x\)</span> is the dose of a treatment and <span class="math inline">\(y\)</span> is the binary outcome variable.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Linear regression models are introduced in <strong>Chapter 2</strong> as a tool to predict a continuous response using a few input variables. In some applications, the response variable is a binary variable that denotes two classes. For example, in the AD dataset, we have a variable called <code>DX_bl</code> that encodes the diagnosis information of the subjects, i.e., <code>0</code> denotes <em>normal</em>, while <code>1</code> denotes <em>diseased</em>.</p>
<p>We have learned about linear regression models to connect the input variables with the outcome variable. It is natural to wonder if the linear regression framework could still be useful here. If we write the regression equation</p>
<p><span class="math display" id="eq:3-goal1">\[\begin{equation}
\small
    \text{The goal: } \underbrace{y}_{\text{Binary}}=\underbrace{\beta_{0}+\sum_{i=1}^{p} \beta_{i} x_{i}+\varepsilon.}_{\text{Continuous and unbounded}}
\tag{23}
\end{equation}\]</span></p>
<p>Something doesn’t make sense. The reason is obvious: the right-hand side of Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-goal1">(23)</a> is continuous without bounds, while the left hand side of the equation is a binary variable. A graphical illustration is shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal1">27</a>. So we have to modify this equation, either the right-hand side or the left-hand side.</p>
<p>Since we want it to be a linear model, it is better not to modify the right-hand side. So look at the left-hand side. Why we have to stick with the natural scale of <span class="math inline">\(y\)</span>? We could certainly work out a more linear-model-friendly scale. For example, instead of predicting <span class="math inline">\(y\)</span>, how about predicting the probability <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span>? If we know <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span>, we can certainly convert it to the scale of <span class="math inline">\(y\)</span>.<label for="tufte-sn-54" class="margin-toggle sidenote-number">54</label><input type="checkbox" id="tufte-sn-54" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">54</span> I.e., if <span class="math inline">\(Pr(y=1|\boldsymbol{x}) \geq 0.5\)</span>, we conclude <span class="math inline">\(y = 1\)</span>; otherwise, <span class="math inline">\(y=0\)</span>.</span></p>
<p>Thus, we consider the following revised goal</p>
<p><span class="math display" id="eq:3-goal2">\[\begin{equation}
\small
    \text{Revised goal: } \underbrace{Pr(y=1|\boldsymbol{x})}_{\text{Continuous but bounded}}=\underbrace{\beta_{0}+\sum_{i=1}^{p} \beta_{i} x_{i}+\varepsilon.}_{\text{Continuous and unbounded}}
\tag{24}
\end{equation}\]</span></p>
<p>Changing our outcome variable from <span class="math inline">\(y\)</span> to <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span> is a good move, since <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span> is on a continuous scale. However, as it is a probability, it has to be in the range of <span class="math inline">\([0,1]\)</span>. We need more modifications to make things work.</p>
<p>If we make a lot of modifications and things barely work, we may have lost the essence. What is the essence of the linear model that we would like to leverage in this binary prediction problem? Interpretability—sure, the linear form seems easy to understand, but as we have pointed out in <strong>Chapter 2</strong>, this interpretability comes with a price, and we need to be cautious when we draw conclusions about the linear model, although there are easy conventions for us to follow. On the other hand, it would sound absurd if we dig into the literature and found there had been no <em>linear model</em> for binary classification problems. Linear model is the baseline of the data analytics enterprise. It is the starting point of our data analytics adventure. That is how important it is.</p>
<p>Back to the business to modify the linear formalism for a binary classification problem. Now our outcome variable is <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span>, and we realize it still doesn’t match with the linear form <span class="math inline">\(\beta_0+\sum_{i=1}^p \beta_i x_i\)</span>. What is the essential task here? If we put the puzzle in a context, it may give us some hints. For example, if our goal is to predict the risk of Alzheimer’s disease for subjects who are aged <span class="math inline">\(65\)</span> years or older, we have known the average risk from recent national statistics is <span class="math inline">\(8.8\%\)</span>. Now if we have a group of individuals who are aged 65 years or older, we could make a risk prediction for them <em>as a group</em>, i.e., 8.8%. But this is not the best we could do for each <em>individual</em>. We could examine an individual’s characteristics such as the gene <em>APOE</em><label for="tufte-sn-55" class="margin-toggle sidenote-number">55</label><input type="checkbox" id="tufte-sn-55" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">55</span> <em>APOE</em> polymorphic alleles play a major role in determining the risk of Alzheimer’s disease (AD): individuals carrying the <span class="math inline">\(\epsilon4\)</span> allele are at increased risk of AD compared with those carrying the more common <span class="math inline">\(\epsilon3\)</span> allele, whereas the <span class="math inline">\(\epsilon2\)</span> allele decreases risk.</span> and see if an individual has higher (or lower) risk than the average. Now comes the inspiration: what if we can <em>rank</em> the risk of the individuals based on their characteristics, can it help with the final goal that is to predict the outcome variable <span class="math inline">\(y\)</span>?</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-lrgoal2"></span>
<img src="graphics/3_lrgoal2.png" alt="Application of the **logistic function** on binary outcome" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 28: Application of the <strong>logistic function</strong> on binary outcome<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now we look closer into the idea of a linear form, and we realize it is more useful in <em>ranking</em> the possibilities rather than directly being eligible probabilities.</p>
<p><span class="math display" id="eq:3-goal3">\[\begin{equation}
\small
    \text{Revised goal: } Pr(y=1|\boldsymbol{x})\propto\beta_0+\sum_{i=1}^p\, \beta_i x_i.
\tag{25}
\end{equation}\]</span></p>
<p>In other words, a linear form can make a comparison of two inputs, say, <span class="math inline">\(\boldsymbol{x}_i\)</span> and <span class="math inline">\(\boldsymbol{x}_j\)</span>, and evaluates which one leads to a higher probability of <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span>.</p>
<p>It is fine that we use the linear form to generate numerical values that rank the subjects. We just need one more step to transform those ranks into probabilities. Statisticians have found that the <strong>logistic function</strong> is suitable here for the transformation</p>
<p><span class="math display" id="eq:3-pry1">\[\begin{equation}
\small
    Pr(y=1|\boldsymbol{x}) = \frac{1}{1+e^{-\left(\beta_0+\sum\nolimits_{i=1}\nolimits^{p}\, \beta_i x_i\right)}}.
\tag{26}
\end{equation}\]</span></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal2">28</a> shows that the <em>logistic function</em> indeed provides a better fit of the data than the linear function as shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal1">27</a>.</p>
<p>Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-pry1">(26)</a> can be rewritten as Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-logitR">(27)</a></p>
<p><span class="math display" id="eq:3-logitR">\[\begin{equation}
\small
    \log {\frac{Pr(y=1|\boldsymbol{x})}{1-Pr(y=1|\boldsymbol{x})}}=\beta_0+\sum\nolimits_{i=1}\nolimits^{p}\beta_i x_i.
\tag{27}
\end{equation}\]</span></p>
<p>This is the so-called <strong>logistic regression</strong> model. The name stems from the transformation of <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span> used here, i.e., the <span class="math inline">\(\log \frac{Pr(y=1|\boldsymbol{x})}{1-Pr(y=1|\boldsymbol{x})}\)</span>, which is the logistic transformation that has been widely used in many areas such as physics and signal processing.</p>
<p>Note that we have mentioned that we can predict <span class="math inline">\(y=1\)</span> if <span class="math inline">\(Pr(y=1|\boldsymbol{x})\geq0.5\)</span>, and <span class="math inline">\(y=0\)</span> if <span class="math inline">\(Pr(y=1|\boldsymbol{x})&lt;0.5\)</span>. While <span class="math inline">\(0.5\)</span> seems naturally a cut-off value here, it is not necessarily optimal in every application. We could use the techniques discussed in <strong>Chapter 5</strong> such as cross-validation to decide what is the optimal cut-off value in practice.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-discretize"></span>
<img src="graphics/3_discretize.png" alt="Illustration of the discretization process, e.g., two categories ($0.0-0.1$ and $0.1-0.2$) of $x$ are shown" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 29: Illustration of the discretization process, e.g., two categories (<span class="math inline">\(0.0-0.1\)</span> and <span class="math inline">\(0.1-0.2\)</span>) of <span class="math inline">\(x\)</span> are shown<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Visual inspection of data.</em> How do we know that our data could be characterized using a logistic function?</p>
<p>We can <em>discretize</em> the predictor <span class="math inline">\(x\)</span> in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal2">28</a> into a few categories, compute the empirical estimate of <span class="math inline">\(Pr(y=1|x)\)</span> in each category, and create a new data table. This procedure is illustrated in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-discretize">29</a>.</p>
<p>Suppose that we discretize the data in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal2">28</a> and obtain the result as shown in Table <a href="chapter-3-recognition-logistic-regression-ranking.html#tab:t3-goal3">6</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t3-goal3">Table 6: </span>Example of a result after discretization</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Level of <span class="math inline">\(x\)</span></th>
<th align="left"><span class="math inline">\(1\)</span></th>
<th align="left"><span class="math inline">\(2\)</span></th>
<th align="left"><span class="math inline">\(3\)</span></th>
<th align="left"><span class="math inline">\(4\)</span></th>
<th align="left"><span class="math inline">\(5\)</span></th>
<th align="left"><span class="math inline">\(6\)</span></th>
<th align="left"><span class="math inline">\(7\)</span></th>
<th align="left"><span class="math inline">\(8\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(Pr(y=1\)</span>|<span class="math inline">\(x)\)</span></td>
<td align="left"><span class="math inline">\(0.00\)</span></td>
<td align="left"><span class="math inline">\(0.04\)</span></td>
<td align="left"><span class="math inline">\(0.09\)</span></td>
<td align="left"><span class="math inline">\(0.20\)</span></td>
<td align="left"><span class="math inline">\(0.59\)</span></td>
<td align="left"><span class="math inline">\(0.89\)</span></td>
<td align="left"><span class="math inline">\(0.92\)</span></td>
<td align="left"><span class="math inline">\(0.99\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Then, we revise the scale of the <span class="math inline">\(y\)</span>-axis of Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal2">28</a> to be <span class="math inline">\(Pr(y=1|x)\)</span>, and create Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal3">30</a>. It could be seen that the empirical curve does fit the form of Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-pry1">(26)</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-lrgoal3"></span>
<img src="graphics/3_lrgoal3.png" alt="Revised scale of the $y$-axis of Figure \@ref(fig:f3-lrgoal2), i.e., illustration of Eq. \@ref(eq:3-pry1) " width="100%"  />
<!--
<p class="caption marginnote">-->Figure 30: Revised scale of the <span class="math inline">\(y\)</span>-axis of Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal2">28</a>, i.e., illustration of Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-pry1">(26)</a> <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
</div>
<div id="theory-and-method-1" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>We collect data to estimate the regression parameters of the logistic regression in Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-logitR">(27)</a>. Denote the sample size as <span class="math inline">\(N\)</span>. <span class="math inline">\(\boldsymbol{y} \in R^{N \times 1}\)</span> denotes the <span class="math inline">\(N\)</span> measurements of the outcome variable, and <span class="math inline">\(\boldsymbol{X} \in R^{N \times (p+1)}\)</span> denotes the data matrix that includes the <span class="math inline">\(N\)</span> measurements of the <span class="math inline">\(p\)</span> input variables plus the dummy variable for the intercept coefficient <span class="math inline">\(\beta_0\)</span>. As in a linear regression model, <span class="math inline">\(\boldsymbol{\beta}\)</span> is the column vector form of the regression parameters.</p>
<p><em>The likelihood function.</em> The <strong>likelihood function</strong> evaluates how well a given set of parameters fit the data<label for="tufte-sn-56" class="margin-toggle sidenote-number">56</label><input type="checkbox" id="tufte-sn-56" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">56</span> The <em>least squares</em> loss function we derived in <strong>Chapter 2</strong> could also be derived based on the likelihood function of a linear regression model.</span>. The likelihood function has a specific definition, i.e., the conditional probability of the data conditional on the given set of parameters. Here, the dataset is <span class="math inline">\(D = \left \{\boldsymbol{X}, \boldsymbol{y} \right\}\)</span>, so the likelihood function is defined as <span class="math inline">\(Pr(D | \boldsymbol{\beta})\)</span>. It could be broken down into <span class="math inline">\(N\)</span> components<label for="tufte-sn-57" class="margin-toggle sidenote-number">57</label><input type="checkbox" id="tufte-sn-57" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">57</span> Note that, it is assumed that <span class="math inline">\(D\)</span> consists of <span class="math inline">\(N\)</span> independent data points.</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
Pr(D | \boldsymbol{\beta}) = \prod\nolimits_{n=1}\nolimits^{N}Pr(\boldsymbol{x}_n, {y_n} | \boldsymbol{\beta}).
 
\end{equation*}\]</span></p>
<p>For data point <span class="math inline">\((\boldsymbol{x}_n, {y_n})\)</span>, the conditional probability <span class="math inline">\(Pr(\boldsymbol{x}_n, {y_n} | \boldsymbol{\beta})\)</span> is</p>
<div style="page-break-after: always;"></div>
<p><span class="math display">\[\begin{equation}
    Pr(\boldsymbol{x}_n, {y_n} | \boldsymbol{\beta})=\begin{cases}
    p(\boldsymbol{x}_n), &amp; if \, y_n = 1 \\
    1-p(\boldsymbol{x}_n), &amp; if \, y_n = 0. \\
    \end{cases}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(p(\boldsymbol{x}_n) = Pr(y=1|\boldsymbol{x})\)</span>.</p>
<p>A succinct form to represent these two scenarios together is</p>
<p><span class="math display">\[\begin{equation*}
\small
  Pr(\boldsymbol{x}_n, {y_n} | \boldsymbol{\beta}) = p(\boldsymbol{x}_n)^{y_n}\left[1-p(\boldsymbol{x}_n)\right]^{1-y_n}. 
\end{equation*}\]</span></p>
<p>Then we can generalize this to all the <span class="math inline">\(N\)</span> data points, and derive the complete likelihood function as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
Pr(D | \boldsymbol{\beta})=\prod\nolimits_{n=1}\nolimits^{N}p(\boldsymbol{x}_n)^{y_n}\left[1-p(\boldsymbol{x}_n)\right]^{1-y_n}.
 
\end{equation*}\]</span></p>
<p>It is common to write up its log-likelihood function, defined as <span class="math inline">\(l(\boldsymbol \beta) = \log Pr(D | \boldsymbol{\beta})\)</span>, to turn products into sums</p>
<p><span class="math display">\[\begin{equation*}
\small
  l(\boldsymbol \beta)=\sum\nolimits_{n=1}\nolimits^N\, \left \{ y_n \log p(\boldsymbol{x}_n)+(1-y_n)\log [1-p(\boldsymbol{x}_n)]\right\}. 
\end{equation*}\]</span></p>
<p>By plugging in the definition of <span class="math inline">\(p(\boldsymbol{x}_n)\)</span>, this could be further transformed into</p>
<p><span class="math display" id="eq:3-likelihood">\[\begin{equation}
\small
\begin{split}
    l(\boldsymbol \beta) = \sum\nolimits_{n=1}\nolimits^N -\log \left(1+e^{\beta_0+\sum\nolimits_{i=1}\nolimits^p\, \beta_i x_{ni}} \right) - \\ \sum\nolimits_{n=1}\nolimits^N y_n(\beta_0+\sum\nolimits_{i=1}\nolimits^p\, \beta_i x_{ni}).
\end{split}
\tag{28}
\end{equation}\]</span></p>
<p>Note that, for any probabilistic model<label for="tufte-sn-58" class="margin-toggle sidenote-number">58</label><input type="checkbox" id="tufte-sn-58" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">58</span> A probabilistic model has a joint distribution for all the random variables concerned in the model. Interested readers can read this comprehensive book: Koller, D. and Friedman, N., <em>Probabilistic Graphical Models: Principles and Techniques</em>, The MIT Press, 2009.</span>, we could derive the likelihood function in one way or another, in a similar fashion as we have done for the logistic regression model.</p>
<p><em>Algorithm.</em> Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-likelihood">(28)</a> provides the objective function of a maximization problem, i.e., the parameter that maximizes <span class="math inline">\(l(\boldsymbol \beta)\)</span> is the best parameter. Theoretically, we could use the First Derivative Test to find the optimal solution. The problem here is that there is no closed-form solution found if we directly apply the First Derivative Test.</p>
<p>Instead, the <strong>Newton-Raphson algorithm</strong> is commonly used to optimize the log-likelihood function of the logistic regression model. It is an iterative algorithm that starts from an <strong>initial solution</strong>, continues to seek updates of the current solution using the following formula</p>
<p><span class="math display" id="eq:3-newtonmethod">\[\begin{equation}
\small
\boldsymbol \beta^{new} = \boldsymbol \beta^{old} - (\frac{\partial^2 l(\boldsymbol \beta)}{\partial \boldsymbol \beta \partial \boldsymbol \beta^T})^{-1} \frac{\partial l(\boldsymbol \beta)}{\partial \boldsymbol \beta}.
\tag{29}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\frac{\partial l(\boldsymbol \beta)}{\partial \boldsymbol \beta}\)</span> is the <strong>gradient</strong> of the current solution, that points to the <strong>direction</strong> following which we should increment the current solution to improve on the objective function. On the other hand, how far we should go along this direction is decided by the <strong>step size</strong> factor, defined as <span class="math inline">\((\frac{\partial^2 l(\boldsymbol \beta)}{\partial \boldsymbol \beta \partial \boldsymbol \beta^T})^{-1}\)</span>. Theoretical results have shown that this formula could converge to the optimal solution. An illustration is given in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-RWalgor">31</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-RWalgor"></span>
<p class="caption marginnote shownote">
Figure 31: Illustration of the gradient-based optimization algorithms that include the Newton-Raphson algorithm as an example. An algorithm starts from an initial solution (e.g., <span class="math inline">\(x_0\)</span> and <span class="math inline">\(x_0&#39;\)</span> are two examples of initial solutions in the figure), uses the gradient to find the <strong>direction</strong>, and moves the solution along that direction with the computed <strong>step size</strong>, until it finds the optimal solution <span class="math inline">\(x^*\)</span>.
</p>
<img src="graphics/3_RWalgor.png" alt="Illustration of the gradient-based optimization algorithms that include the Newton-Raphson algorithm as an example. An algorithm starts from an initial solution (e.g., $x_0$ and $x_0'$ are two examples of initial solutions in the figure), uses the gradient to find the **direction**, and moves the solution along that direction with the computed **step size**, until it finds the optimal solution $x^*$." width="80%"  />
</div>
<p></p>
<p>The Newton-Raphson algorithm presented in Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-newtonmethod">(29)</a> is general. To apply it in a logistic regression model, since we have an explicit form of <span class="math inline">\(l(\boldsymbol \beta)\)</span>, we can derive the gradient and step size as shown below</p>
<p><span class="math display">\[\begin{align*}
    \frac{\partial l(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &amp;= \sum\nolimits_{n=1}^{N}\boldsymbol{x}_n\left[y_n -p(\boldsymbol{x}_n)\right], \\
    \frac{\partial^2 l(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^T} &amp;= -\sum\nolimits_{n=1}^N \boldsymbol{x}_n\boldsymbol{x}_n^T p(\boldsymbol{x}_n)\left[1-p(\boldsymbol{x}_n)\right].
\end{align*}\]</span></p>
<p>A certain structure can be revealed if we rewrite it in matrix form<label for="tufte-sn-59" class="margin-toggle sidenote-number">59</label><input type="checkbox" id="tufte-sn-59" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">59</span> <span class="math inline">\(\boldsymbol{p}(\boldsymbol{x})\)</span> is a <span class="math inline">\(N\times1\)</span> column vector of <span class="math inline">\(p(\boldsymbol{x}_n)\)</span>, and <span class="math inline">\(\boldsymbol{W}\)</span> is a <span class="math inline">\(N\times N\)</span> diagonal matrix with the <span class="math inline">\(n^{th}\)</span> diagonal element as <span class="math inline">\(p(\boldsymbol{x}_n )\left[1-p(\boldsymbol{x}_n)\right]\)</span>.</span></p>
<p><span class="math display" id="eq:3-gradient">\[\begin{align}
    \frac{\partial l(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = \boldsymbol{X}^T\left[\boldsymbol{y}-\boldsymbol{p}(\boldsymbol{x})\right],  \\
    \frac{\partial^2 l(\boldsymbol{\beta})}{\boldsymbol{\beta} \boldsymbol{\beta}^T} = -\boldsymbol{X}^T\boldsymbol{W}\boldsymbol{X}.
    \tag{30}
\end{align}\]</span></p>
<p>Plugging Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-gradient">(30)</a> into the updating formula as shown in Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-newtonmethod">(29)</a>, we can derive a specific formula for logistic regression</p>
<p><span class="math display" id="eq:3-betaupdate">\[\begin{align}
    &amp;\boldsymbol{\beta}^{new} = \boldsymbol{\beta}^{old} + (\boldsymbol{X}^T\boldsymbol{WX})^{-1}\boldsymbol{X}^T\left[ \boldsymbol{y}-\boldsymbol{p}(\boldsymbol{x}) \right], \\
    &amp; = (\boldsymbol{X}^T\boldsymbol{WX})^{-1}\boldsymbol{X}^T\boldsymbol{W} \left(\boldsymbol{X}\boldsymbol{\beta}^{old}+\boldsymbol{W}^{-1} \left[ \boldsymbol{y} - \boldsymbol{p}(\boldsymbol{x})\right] \right), \\
    &amp; = (\boldsymbol{X}^T\boldsymbol{WX})^{-1}\boldsymbol{X}^T\boldsymbol{Wz}.
\tag{31}
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol{z}=\boldsymbol{X}\boldsymbol{\beta}^{old} + \boldsymbol{W}^{-1}(\boldsymbol{y} - \boldsymbol{p}(\boldsymbol{x}))\)</span>.</p>
<p>Putting all these together, a complete flow of the algorithm is shown below</p>
<p><!-- begin{enumerate} --></p>
<p>1. Initialize <span class="math inline">\(\boldsymbol{\beta}.\)</span><label for="tufte-sn-60" class="margin-toggle sidenote-number">60</label><input type="checkbox" id="tufte-sn-60" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">60</span> I.e., use random values for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</span></p>
<p>2. Compute <span class="math inline">\(\boldsymbol{p}(\boldsymbol{x}_n)\)</span> by its definition: <span class="math inline">\(\boldsymbol{p}(\boldsymbol{x}_n )=\frac{1}{1+e^{-(\beta_0+\sum_{i=1}^p\, \beta_i x_{ni})}}\)</span> for <span class="math inline">\(n=1,2,\ldots,N\)</span>.</p>
<p>3. Compute the diagonal matrix <span class="math inline">\(\boldsymbol{W}\)</span>, with the <span class="math inline">\(n^{th}\)</span> diagonal element as <span class="math inline">\(\boldsymbol{p}\left(\boldsymbol{x}_{n}\right)\left[1-\boldsymbol{p}\left(\boldsymbol{x}_{n}\right)\right]\)</span> for <span class="math inline">\(n=1,2,…,N\)</span>.</p>
<p>4. Set <span class="math inline">\(\boldsymbol{z}\)</span> as <span class="math inline">\(= \boldsymbol{X} \boldsymbol{\beta}+\boldsymbol{W}^{-1}[\boldsymbol{y}-\boldsymbol{p}(\boldsymbol{x})]\)</span>.</p>
<p>5. Set <span class="math inline">\(\boldsymbol{\beta} = \left(\boldsymbol{X}^{T} \boldsymbol{W X}\right)^{-1} \boldsymbol{X}^{T} \boldsymbol{W} \boldsymbol{z}\)</span>.</p>
<p>6. If the stopping criteria<label for="tufte-sn-61" class="margin-toggle sidenote-number">61</label><input type="checkbox" id="tufte-sn-61" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">61</span> A common stopping criteria is to evaluate the difference between two consecutive solutions, i.e., if the Euclidean distance between the two vectors, <span class="math inline">\(\boldsymbol{\beta}^{new}\)</span> and <span class="math inline">\(\boldsymbol{\beta}^{old}\)</span>, is less than <span class="math inline">\(10^{-4}\)</span>, then it is considered no difference and the algorithm stops.</span> is met, stop; otherwise, go back to step 2.</p>
<p><!-- end{enumerate} --></p>
<p><em>Generalized least squares estimator.</em> The estimation formula as shown in Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-betaupdate">(31)</a> resembles the generalized least squares (GLS) estimator of a regression model, where each data point <span class="math inline">\((\boldsymbol{x}_n,y_n)\)</span> is associated with a weight <span class="math inline">\(w_n\)</span>. This insight revealed by the Newton-Raphson algorithm suggests a new perspective to look at the logistic regression model. The updating formula shown in Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-betaupdate">(31)</a> suggests that, in each iteration of parameter updating, we actually solve a weighted regression model as</p>
<p><span class="math display">\[\begin{equation*}
\small
  \boldsymbol{\beta}^{new} \leftarrow \mathop{\arg\min}_{\boldsymbol{\beta}} (\boldsymbol{z}-\boldsymbol{X}\boldsymbol \beta)^T\boldsymbol{W}(\boldsymbol{z}-\boldsymbol{X}\boldsymbol{\beta}). 
\end{equation*}\]</span></p>
<p>For this reason, the algorithm we just introduced is also called the <strong>Iteratively Reweighted Least Squares</strong> (<strong>IRLS</strong>) algorithm. <span class="math inline">\(\boldsymbol{z}\)</span> is referred to as the <strong>adjusted response</strong>.</p>
</div>
<div id="r-lab-2" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>In the AD dataset, the variable <code>DX_bl</code> encodes the diagnosis information, i.e.,<code>0</code> denotes <em>normal</em> while <code>1</code> denotes <em>diseased</em>. We build a logistic regression model using <code>DX_bl</code> as the outcome variable.</p>
<!-- % ^[Using numbers to encode categorical variables is not the best way, but it is not uncommon. You may consider encoding them in a safer way, i.e., convert "1" to be "c1" and "0" to be "c0". Is this an act of paranoid? No. There could be millions of small problems like this, and if you don't seal these little devils in bottles when you build up your analytics pipeline --- you will witness their enormous power. \\ Whatever can go wrong, will go wrong --- Murphy's law.] -->
<p><em>The 7-Step R Pipeline.</em>
<strong>Step 1</strong> is to import data into R.</p>
<p></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb31-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb31-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb31-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb31-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb31-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb31-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb31-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb31-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb31-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(AD)</span></span></code></pre></div>
<p></p>
<p><strong>Step 2</strong> is for data preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb32-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb32-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb32-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>DX_bl</span>
<span id="cb32-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The following code makes sure the variable &quot;DX_bl&quot; is a &quot;factor&quot;. </span></span>
<span id="cb32-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># It denotes &quot;0&quot; as &quot;c0&quot; and &quot;1&quot; as &quot;c1&quot;, to highlight the fact </span></span>
<span id="cb32-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># that &quot;DX_bl&quot; is a factor variable, not a numerical variable.</span></span>
<span id="cb32-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-10" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, Y)</span>
<span id="cb32-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co"># as.factor is to convert any variable into the </span></span>
<span id="cb32-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="co"># format as &quot;factor&quot; variable.</span></span>
<span id="cb32-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-13" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Y) </span>
<span id="cb32-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb32-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-16" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb32-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb32-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-19" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># generate the same random sequence</span></span>
<span id="cb32-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb32-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-21" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb32-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-22" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb32-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb32-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb32-24" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> is to use the function <code>glm()</code> to build a logistic regression model<label for="tufte-sn-62" class="margin-toggle sidenote-number">62</label><input type="checkbox" id="tufte-sn-62" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">62</span> Type<code>help(glm)</code> in R Console to learn more of the function.</span>.</p>
<p></p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; Use glm() function to build a full model </span></span>
<span id="cb33-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with all predictors</span></span>
<span id="cb33-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb33-3" aria-hidden="true" tabindex="-1"></a>logit.AD.full <span class="ot">&lt;-</span> <span class="fu">glm</span>(DX_bl<span class="sc">~</span>., <span class="at">data =</span> data.train,</span>
<span id="cb33-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb33-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb33-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.AD.full)</span></code></pre></div>
<p></p>
<p>And the result is shown below</p>
<p></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb34-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = DX_bl ~ ., family = &quot;binomial&quot;, data = data.train)</span></span>
<span id="cb34-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Deviance Residuals: </span></span>
<span id="cb34-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min       1Q   Median       3Q      Max  </span></span>
<span id="cb34-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.4250  -0.3645  -0.0704   0.2074   3.1707  </span></span>
<span id="cb34-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb34-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb34-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  43.97098    7.83797   5.610 2.02e-08 ***</span></span>
<span id="cb34-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          -0.07304    0.03875  -1.885  0.05945 .  </span></span>
<span id="cb34-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER      0.48668    0.46682   1.043  0.29716    </span></span>
<span id="cb34-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     -0.24907    0.08714  -2.858  0.00426 ** </span></span>
<span id="cb34-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          -3.28887    0.59927  -5.488 4.06e-08 ***</span></span>
<span id="cb34-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45          2.09311    1.36020   1.539  0.12385    </span></span>
<span id="cb34-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV     -38.03422    6.16738  -6.167 6.96e-10 ***</span></span>
<span id="cb34-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="do">## e2_1          0.90115    0.85564   1.053  0.29225    </span></span>
<span id="cb34-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="do">## e4_1          0.56917    0.54502   1.044  0.29634    </span></span>
<span id="cb34-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3818361    -0.47249    0.45309  -1.043  0.29703    </span></span>
<span id="cb34-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="do">## rs744373      0.02681    0.44235   0.061  0.95166    </span></span>
<span id="cb34-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-21" aria-hidden="true" tabindex="-1"></a><span class="do">## rs11136000   -0.31382    0.46274  -0.678  0.49766    </span></span>
<span id="cb34-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="do">## rs610932      0.55388    0.49832   1.112  0.26635    </span></span>
<span id="cb34-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3851179    -0.18635    0.44872  -0.415  0.67793    </span></span>
<span id="cb34-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3764650    -0.48152    0.54982  -0.876  0.38115    </span></span>
<span id="cb34-25"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444     0.74252    0.45761   1.623  0.10467    </span></span>
<span id="cb34-26"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-26" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb34-27"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span>
<span id="cb34-28"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-29"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb34-30"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-31"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 349.42  on 257  degrees of freedom</span></span>
<span id="cb34-32"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 139.58  on 242  degrees of freedom</span></span>
<span id="cb34-33"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-33" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 171.58</span></span>
<span id="cb34-34"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-35"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb34-35" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 7</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> is to use the <code>step()</code> function for model selection.</p>
<!-- % ^[1) direction="backward" was used in the example of linear regression model; here, we use  direction="both", which means that we start with the full model, then sequentially both remove insignificant variables and also recruit new variables (from the ones that have been removed previously - why? remember that the fact that a variable is significant also depends on what are other variables on the model already); 2) trace = 0 is to disable the presentation of showing all the models that have been evaluated along the process. If you want to see the process, simply set trace = 1 or not specify this argument (by default, trace = 1 in the \textcolor[rgb]{0,0,1}{step}() function).] -->
<p></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; use step() to automatically delete </span></span>
<span id="cb35-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co"># all the insignificant</span></span>
<span id="cb35-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># variables</span></span>
<span id="cb35-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Also means, automatic model selection</span></span>
<span id="cb35-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb35-5" aria-hidden="true" tabindex="-1"></a>logit.AD.reduced <span class="ot">&lt;-</span> <span class="fu">step</span>(logit.AD.full, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>,</span>
<span id="cb35-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb35-6" aria-hidden="true" tabindex="-1"></a>                         <span class="at">trace =</span> <span class="dv">0</span>)</span>
<span id="cb35-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.AD.reduced)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb36-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = DX_bl ~ AGE + PTEDUCAT + FDG + AV45 + HippoNV + </span></span>
<span id="cb36-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="do">##     rs3865444, family = &quot;binomial&quot;, data = data.train)</span></span>
<span id="cb36-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Deviance Residuals: </span></span>
<span id="cb36-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="do">##      Min        1Q    Median        3Q       Max  </span></span>
<span id="cb36-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.38957  -0.42407  -0.09268   0.25092   2.73658  </span></span>
<span id="cb36-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb36-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb36-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  42.68795    7.07058   6.037 1.57e-09 ***</span></span>
<span id="cb36-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          -0.07993    0.03650  -2.190  0.02853 *  </span></span>
<span id="cb36-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     -0.22195    0.08242  -2.693  0.00708 ** </span></span>
<span id="cb36-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          -3.16994    0.55129  -5.750 8.92e-09 ***</span></span>
<span id="cb36-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45          2.62670    1.18420   2.218  0.02655 *  </span></span>
<span id="cb36-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV     -36.22215    5.53083  -6.549 5.79e-11 ***</span></span>
<span id="cb36-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444     0.71373    0.44290   1.612  0.10707    </span></span>
<span id="cb36-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb36-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span>
<span id="cb36-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb36-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 349.42  on 257  degrees of freedom</span></span>
<span id="cb36-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 144.62  on 251  degrees of freedom</span></span>
<span id="cb36-25"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 158.62</span></span>
<span id="cb36-26"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-26" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-27"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 7</span></span></code></pre></div>
<p></p>
<p>You may have noticed that some variables included in this model are actually not significant.</p>
<p><strong>Step 4</strong> compares the final model selected by the <code>step()</code> function with the full model.</p>
<p></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 continued</span></span>
<span id="cb37-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(logit.AD.reduced,logit.AD.full,<span class="at">test =</span> <span class="st">&quot;LRT&quot;</span>)</span>
<span id="cb37-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The argument, test = &quot;LRT&quot;, means that the p-value </span></span>
<span id="cb37-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># is derived via the Likelihood Ratio Test (LRT).</span></span></code></pre></div>
<p></p>
<p>And we can see that the two models are not statistically different, i.e., <em>p-value</em> is <span class="math inline">\(0.8305\)</span>.</p>
<p><strong>Step 5</strong> is to evaluate the overall significance of the final model<label for="tufte-sn-63" class="margin-toggle sidenote-number">63</label><input type="checkbox" id="tufte-sn-63" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">63</span> <strong>Step 4</strong> compares two models. <strong>Step 5</strong> tests if a model has a lack-of-fit with data. A model could be better than another, but it is possible that both of them fit the data poorly.</span>.</p>
<p></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; test the significance of the logistic model</span></span>
<span id="cb38-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Test residual deviance for lack-of-fit </span></span>
<span id="cb38-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (if &gt; 0.10, little-to-no lack-of-fit)</span></span>
<span id="cb38-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb38-4" aria-hidden="true" tabindex="-1"></a>dev.p.val <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(logit.AD.reduced<span class="sc">$</span>deviance,</span>
<span id="cb38-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb38-5" aria-hidden="true" tabindex="-1"></a>                        logit.AD.reduced<span class="sc">$</span>df.residual)</span></code></pre></div>
<p></p>
<p>And it can be seen that the model shows no lack-of-fit as the <em>p-value</em> is <span class="math inline">\(1\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb39-1" aria-hidden="true" tabindex="-1"></a>dev.p.val</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> is to use your final model for prediction. We can do so using the <code>predict()</code> function.</p>
<!-- % ^[A few comments: 1) \textcolor[rgb]{0,0,1}{predict}() is a function that you can find in many R packages. R package developers usually write such a function, in the form as predict(obj, data), where "obj" is the object of the created model by that package, and "data" is the data points you want to predict on. 2) While in many cases this is not needed, sometimes you do need to specify the argument "type". Here, you can type \textcolor[rgb]{0,0,1}{help(predict)} in R Console and see it has several options, e.g., type = c("link", "response", "terms"). We use the default here, i.e., type = "link", which means that we expect the outcome "y\_hat" are the values from the linear equation part of the logistic regression model. In this way, "y\_hat" are the intermediate values that are not the final prediction. To convert it into final prediction, which should be binary, we can use "0" as the cut-off value, i.e., if y\_hat < 0, we name it as one class, and if y\_hat > 0, it is another class. Notice that the cut-off value as 0 is default, but not necessary an optimal value in every application.] -->
<p></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict on test data using your </span></span>
<span id="cb41-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic regression model</span></span>
<span id="cb41-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb41-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit.AD.reduced, data.test)</span></code></pre></div>
<p></p>
<p><strong>Step 7</strong> is to evaluate the prediction performance of the final model.</p>
<p></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7 -&gt; Evaluate the prediction performance of </span></span>
<span id="cb42-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># your logistic regression model</span></span>
<span id="cb42-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (1) Three main metrics for classification: Accuracy, </span></span>
<span id="cb42-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sensitivity (1- False Positive), </span></span>
<span id="cb42-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Specificity (1 - False Negative)</span></span>
<span id="cb42-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-6" aria-hidden="true" tabindex="-1"></a>y_hat2 <span class="ot">&lt;-</span> y_hat</span>
<span id="cb42-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-7" aria-hidden="true" tabindex="-1"></a>y_hat2[<span class="fu">which</span>(y_hat <span class="sc">&gt;</span> <span class="dv">0</span>)] <span class="ot">=</span> <span class="st">&quot;c1&quot;</span> </span>
<span id="cb42-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Since y_hat here is the values from the linear equation </span></span>
<span id="cb42-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># part of the logistic regression model, by default, </span></span>
<span id="cb42-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co"># we should use 0 as a cut-off value (only by default, </span></span>
<span id="cb42-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co"># not optimal though), i.e., if y_hat &lt; 0, we name it </span></span>
<span id="cb42-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="co"># as one class, and if y_hat &gt; 0, it is another class.</span></span>
<span id="cb42-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-13" aria-hidden="true" tabindex="-1"></a>y_hat2[<span class="fu">which</span>(y_hat <span class="sc">&lt;</span> <span class="dv">0</span>)] <span class="ot">=</span> <span class="st">&quot;c0&quot;</span></span>
<span id="cb42-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret) </span>
<span id="cb42-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co"># confusionMatrix() in the package &quot;caret&quot; is a powerful </span></span>
<span id="cb42-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="co"># function to summarize the prediction performance of a </span></span>
<span id="cb42-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="co"># classification model, reporting metrics such as Accuracy, </span></span>
<span id="cb42-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Sensitivity (1- False Positive), </span></span>
<span id="cb42-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Specificity (1 - False Negative), to name a few.</span></span>
<span id="cb42-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-21" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb42-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">table</span>(y_hat2, data.test<span class="sc">$</span>DX_bl))</span>
<span id="cb42-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-24" aria-hidden="true" tabindex="-1"></a><span class="co"># (2) ROC curve is another commonly reported metric for </span></span>
<span id="cb42-25"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="co"># classification models</span></span>
<span id="cb42-26"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-26" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC) </span>
<span id="cb42-27"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-27" aria-hidden="true" tabindex="-1"></a><span class="co"># pROC has the roc() function that is very useful here</span></span>
<span id="cb42-28"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-28" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">roc</span>(data.test<span class="sc">$</span>DX_bl, y_hat),</span>
<span id="cb42-29"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb42-29" aria-hidden="true" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">main=</span><span class="st">&quot;ROC Curve&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-Step7-ROC"></span>
<img src="graphics/3_Step7_ROC.png" alt="The ROC curve of the final model" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 32: The ROC curve of the final model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Results are shown below. We haven’t discussed the <strong>ROC curve</strong> yet, which will be a main topic in <strong>Chapter 5</strong>. At this moment, remember that a model with a ROC curve that has a larger <strong>Area Under the Curve</strong> (<strong>AUC</strong>) is a better model. And a model whose ROC curve ties with the diagonal straight line (as shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-Step7-ROC">32</a>) is equivalent with random guess.</p>
<p></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="do">## y_hat2  c0  c1</span></span>
<span id="cb43-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="do">##     c0 117  29</span></span>
<span id="cb43-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="do">##     c1  16  97</span></span>
<span id="cb43-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="do">##                Accuracy : 0.8263          </span></span>
<span id="cb43-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="do">##                  95% CI : (0.7745, 0.8704)</span></span>
<span id="cb43-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5135          </span></span>
<span id="cb43-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : &lt; 2e-16         </span></span>
<span id="cb43-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Kappa : 0.6513          </span></span>
<span id="cb43-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.07364         </span></span>
<span id="cb43-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="do">##             Sensitivity : 0.8797          </span></span>
<span id="cb43-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="do">##             Specificity : 0.7698          </span></span>
<span id="cb43-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8014          </span></span>
<span id="cb43-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.8584          </span></span>
<span id="cb43-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-18" aria-hidden="true" tabindex="-1"></a><span class="do">##              Prevalence : 0.5135          </span></span>
<span id="cb43-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-19" aria-hidden="true" tabindex="-1"></a><span class="do">##          Detection Rate : 0.4517          </span></span>
<span id="cb43-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-20" aria-hidden="true" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5637          </span></span>
<span id="cb43-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8248          </span></span>
<span id="cb43-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb43-23" aria-hidden="true" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : c0 </span></span></code></pre></div>
<p></p>
<p><em>Model uncertainty.</em> The <span class="math inline">\(95 \%\)</span> confidence interval (CI) of the regression coefficients can be derived, as shown below</p>
<p></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="do">## coefficients and 95% CI</span></span>
<span id="cb44-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(<span class="at">coef =</span> <span class="fu">coef</span>(logit.AD.reduced), <span class="fu">confint</span>(logit.AD.reduced))</span></code></pre></div>
<p></p>
<p>Results are</p>
<p></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="do">##                     coef       2.5 %       97.5 %</span></span>
<span id="cb45-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  42.68794758  29.9745022  57.88659748</span></span>
<span id="cb45-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          -0.07993473  -0.1547680  -0.01059348</span></span>
<span id="cb45-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     -0.22195425  -0.3905105  -0.06537066</span></span>
<span id="cb45-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          -3.16994212  -4.3519800  -2.17636447</span></span>
<span id="cb45-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45          2.62670085   0.3736259   5.04703489</span></span>
<span id="cb45-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV     -36.22214822 -48.1671093 -26.35100122</span></span>
<span id="cb45-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444     0.71373441  -0.1348687   1.61273264</span></span></code></pre></div>
<p></p>
<p><em>Prediction uncertainty.</em> As in linear regression, we could derive the variance of the estimated regression coefficients <span class="math inline">\(\operatorname{var}(\hat{\boldsymbol{\beta}})\)</span>; then, since <span class="math inline">\(\boldsymbol{\hat{y}} = \boldsymbol{X} \hat{\boldsymbol{\beta}}\)</span>, we can derive <span class="math inline">\(\operatorname{var}(\boldsymbol{\hat{y}})\)</span><label for="tufte-sn-64" class="margin-toggle sidenote-number">64</label><input type="checkbox" id="tufte-sn-64" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">64</span> The linearity assumption between <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(y\)</span> enables the explicit characterization of this chain of uncertainty propagation.</span>. Skipping the technical details, the <span class="math inline">\(95\%\)</span> CI of the predictions are obtained using the R code below</p>
<p></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remark: how to obtain the 95% CI of the predictions</span></span>
<span id="cb46-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit.AD.reduced, data.test, <span class="at">type =</span> <span class="st">&quot;link&quot;</span>, </span>
<span id="cb46-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">se.fit =</span> <span class="cn">TRUE</span>) </span>
<span id="cb46-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># se.fit = TRUE, is to get the standard error in the predictions, </span></span>
<span id="cb46-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co"># which is necessary information for us to construct </span></span>
<span id="cb46-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="co"># the 95% CI of the predictions</span></span>
<span id="cb46-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-7" aria-hidden="true" tabindex="-1"></a>data.test<span class="sc">$</span>fit    <span class="ot">&lt;-</span> y_hat<span class="sc">$</span>fit</span>
<span id="cb46-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-8" aria-hidden="true" tabindex="-1"></a>data.test<span class="sc">$</span>se.fit <span class="ot">&lt;-</span> y_hat<span class="sc">$</span>se.fit</span>
<span id="cb46-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co"># We can readily convert this information into the 95% CIs </span></span>
<span id="cb46-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co"># of the predictions (the way these 95% CIs are </span></span>
<span id="cb46-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co"># derived are again, only in approximated sense).</span></span>
<span id="cb46-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co"># CI for fitted values</span></span>
<span id="cb46-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-13" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> <span class="fu">within</span>(data.test, {</span>
<span id="cb46-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co"># added &quot;fitted&quot; to make predictions at appended temp values</span></span>
<span id="cb46-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-15" aria-hidden="true" tabindex="-1"></a>fitted    <span class="ot">=</span> <span class="fu">exp</span>(fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(fit))</span>
<span id="cb46-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-16" aria-hidden="true" tabindex="-1"></a>fit.lower <span class="ot">=</span> <span class="fu">exp</span>(fit <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> </span>
<span id="cb46-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-17" aria-hidden="true" tabindex="-1"></a>                                          <span class="fu">exp</span>(fit <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit))</span>
<span id="cb46-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-18" aria-hidden="true" tabindex="-1"></a>fit.upper <span class="ot">=</span> <span class="fu">exp</span>(fit <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> </span>
<span id="cb46-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-19" aria-hidden="true" tabindex="-1"></a>                                          <span class="fu">exp</span>(fit <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit))</span>
<span id="cb46-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb46-20" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div>
<p></p>
<p><em>Odds ratio.</em> The <strong>odds ratio</strong> (<strong>OR</strong>) quantifies the strength of the association between two events, <em>A</em> and <em>B</em>. It is defined as the ratio of the odds of <em>A</em> in the presence of <em>B</em> and the odds of <em>A</em> in the absence of <em>B</em>, or equivalently due to symmetry.</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p>If the <em>OR</em> equals <span class="math inline">\(1\)</span>, <em>A</em> and <em>B</em> are independent;</p></li>
<li><p>If the <em>OR</em> is greater than <span class="math inline">\(1\)</span>, the presence of one event increases the odds of the other event;</p></li>
<li><p>If the <em>OR</em> is less than <span class="math inline">\(1\)</span>, the presence of one event reduces the odds of the other event.</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>A regression coefficient of a logistic regression model can be converted into an <em>odds ratio</em>, as done in the following codes.</p>
<p></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="do">## odds ratios and 95% CI</span></span>
<span id="cb47-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(logit.AD.reduced), </span>
<span id="cb47-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb47-3" aria-hidden="true" tabindex="-1"></a>          <span class="fu">confint</span>(logit.AD.reduced)))</span></code></pre></div>
<p></p>
<p>The odds ratios and their <span class="math inline">\(95\%\)</span> CIs are</p>
<p></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="do">##                       OR        2.5 %       97.5 %</span></span>
<span id="cb48-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 3.460510e+18 1.041744e+13 1.379844e+25</span></span>
<span id="cb48-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         9.231766e-01 8.566139e-01 9.894624e-01</span></span>
<span id="cb48-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT    8.009520e-01 6.767113e-01 9.367202e-01</span></span>
<span id="cb48-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG         4.200603e-02 1.288128e-02 1.134532e-01</span></span>
<span id="cb48-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45        1.382807e+01 1.452993e+00 1.555605e+02</span></span>
<span id="cb48-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV     1.857466e-16 1.205842e-21 3.596711e-12</span></span>
<span id="cb48-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444   2.041601e+00 8.738306e-01 5.016501e+00</span></span></code></pre></div>
<p></p>
<p><em>Exploratory Data Analysis (EDA).</em> EDA essentially conceptualizes the analysis process as a dynamic one, sometimes with a playful tone<label for="tufte-sn-65" class="margin-toggle sidenote-number">65</label><input type="checkbox" id="tufte-sn-65" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">65</span> And it is probably because of this conceptual framework, EDA happens to use a lot of figures to explore the data. Figures are rich in information, some are not easily generalized into abstract numbers.</span> EDA could start with something simple. For example, we can start with a smaller model rather than throw everything into the analysis.</p>
<p>Let’s revisit the data analysis done in the <em>7-step R pipeline</em> and examine a simple logistic regression model with only one predictor, <code>FDG</code>.</p>
<p></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression model with FDG</span></span>
<span id="cb49-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb49-2" aria-hidden="true" tabindex="-1"></a>logit.AD.FDG <span class="ot">&lt;-</span> <span class="fu">glm</span>(DX_bl <span class="sc">~</span> FDG, <span class="at">data =</span> AD, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb49-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.AD.FDG)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb50-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = DX_bl   FDG, family = &quot;binomial&quot;, data = AD)</span></span>
<span id="cb50-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Deviance Residuals:</span></span>
<span id="cb50-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min       1Q   Median       3Q      Max</span></span>
<span id="cb50-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.4686  -0.8166  -0.2758   0.7679   2.7812</span></span>
<span id="cb50-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb50-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error z value Pr(&gt;|z|)</span></span>
<span id="cb50-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  18.3300     1.7676   10.37   &lt;2e-16 ***</span></span>
<span id="cb50-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          -2.9370     0.2798  -10.50   &lt;2e-16 ***</span></span>
<span id="cb50-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb50-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb50-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb50-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-17" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-18" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 711.27  on 516  degrees of freedom</span></span>
<span id="cb50-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 499.00  on 515  degrees of freedom</span></span>
<span id="cb50-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-20" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 503</span></span>
<span id="cb50-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p></p>
<p>It can be seen that the predictor <code>FDG</code> is significant, as the <em>p-value</em> is <span class="math inline">\(&lt;2e-16\)</span> that is far less than <span class="math inline">\(0.05\)</span>. On the other hand, although there is no <em>R-Squared</em> in the logistic regression model, we could observe that, out of the total deviance of <span class="math inline">\(711.27\)</span>, <span class="math inline">\(711.27 - 499.00 = 212.27\)</span> could be explained by <code>FDG</code>.</p>
<p>This process could be repeated for every variable in order to have a sense of what are their <em>marginal</em> contributions in explaining away the variation in the outcome variable. This practice, which seems dull, is not always associated with an immediate reward. But it is not uncommon in practice, particularly when we have seen in <strong>Chapter 2</strong> that, in regression models, the regression coefficients are interdependent, the regression models are not causal models, and, when you throw variables into the model, they may generate interactions just like chemicals, etc. Looking at your data from every possible angle is useful to conduct data <em>analytics</em>.</p>
<p>Back to the simple model that only uses one variable, <code>FDG</code>. To understand better how well it predicts the outcome, we can draw figures to visualize the predictions. First, let’s get the predictions and their <span class="math inline">\(95\%\)</span> CI values.</p>
<p></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-1" aria-hidden="true" tabindex="-1"></a>logit.AD.FDG <span class="ot">&lt;-</span> <span class="fu">glm</span>(DX_bl <span class="sc">~</span>   FDG, <span class="at">data =</span> data.train,</span>
<span id="cb51-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb51-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit.AD.FDG, data.test, <span class="at">type =</span> <span class="st">&quot;link&quot;</span>,</span>
<span id="cb51-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb51-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-5" aria-hidden="true" tabindex="-1"></a>data.test<span class="sc">$</span>fit    <span class="ot">&lt;-</span> y_hat<span class="sc">$</span>fit</span>
<span id="cb51-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-6" aria-hidden="true" tabindex="-1"></a>data.test<span class="sc">$</span>se.fit <span class="ot">&lt;-</span> y_hat<span class="sc">$</span>se.fit</span>
<span id="cb51-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># CI for fitted values</span></span>
<span id="cb51-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-9" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> <span class="fu">within</span>(data.test, {</span>
<span id="cb51-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co"># added &quot;fitted&quot; to make predictions at appended temp values</span></span>
<span id="cb51-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-11" aria-hidden="true" tabindex="-1"></a>  fitted    <span class="ot">=</span> <span class="fu">exp</span>(fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(fit))</span>
<span id="cb51-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-12" aria-hidden="true" tabindex="-1"></a>  fit.lower <span class="ot">=</span> <span class="fu">exp</span>(fit <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(fit <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span></span>
<span id="cb51-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-13" aria-hidden="true" tabindex="-1"></a>                                                    se.fit))</span>
<span id="cb51-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-14" aria-hidden="true" tabindex="-1"></a>  fit.upper <span class="ot">=</span> <span class="fu">exp</span>(fit <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(fit <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span></span>
<span id="cb51-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-15" aria-hidden="true" tabindex="-1"></a>                                                    se.fit))</span>
<span id="cb51-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb51-16" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-pred-FDG-boxplot"></span>
<img src="graphics/3_pred_FDG_boxplot.png" alt="Boxplots of the *predicted probabilities of diseased*, i.e., the $Pr(y=1|\boldsymbol{x})$" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 33: Boxplots of the <em>predicted probabilities of diseased</em>, i.e., the <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We then draw Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-pred-FDG-boxplot">33</a> using the following script.</p>
<p></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Boxplot to evaluate the prediction performance</span></span>
<span id="cb52-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb52-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb52-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">qplot</span>(<span class="fu">factor</span>(data.test<span class="sc">$</span>DX_bl), data.test<span class="sc">$</span>fit, <span class="at">data =</span> data.test,</span>
<span id="cb52-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb52-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">geom=</span><span class="fu">c</span>(<span class="st">&quot;boxplot&quot;</span>), <span class="at">fill =</span> <span class="fu">factor</span>(data.test<span class="sc">$</span>DX_bl)) <span class="sc">+</span> </span>
<span id="cb52-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb52-5" aria-hidden="true" tabindex="-1"></a>      <span class="fu">labs</span>(<span class="at">fill=</span><span class="st">&quot;Dx_bl&quot;</span>) <span class="sc">+</span></span>
<span id="cb52-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb52-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">25</span>))</span></code></pre></div>
<p></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-pred-FDG-boxplot">33</a> indicates that the model can separate the two classes significantly (while not being good enough). It gives us a global presentation of the prediction. We can draw another figure, Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-1">34</a>, to examine more details, i.e., look into the “local” parts of the predictions to see where we can improve.</p>
<p></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb53-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-2" aria-hidden="true" tabindex="-1"></a>newData <span class="ot">&lt;-</span> data.test[<span class="fu">order</span>(data.test<span class="sc">$</span>FDG),]</span>
<span id="cb53-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-3" aria-hidden="true" tabindex="-1"></a>newData<span class="sc">$</span>DX_bl <span class="ot">=</span> <span class="fu">as.numeric</span>(newData<span class="sc">$</span>DX_bl)</span>
<span id="cb53-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-4" aria-hidden="true" tabindex="-1"></a>newData<span class="sc">$</span>DX_bl[<span class="fu">which</span>(newData<span class="sc">$</span>DX_bl<span class="sc">==</span><span class="dv">1</span>)] <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb53-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-5" aria-hidden="true" tabindex="-1"></a>newData<span class="sc">$</span>DX_bl[<span class="fu">which</span>(newData<span class="sc">$</span>DX_bl<span class="sc">==</span><span class="dv">2</span>)] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb53-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-6" aria-hidden="true" tabindex="-1"></a>newData<span class="sc">$</span>DX_bl <span class="ot">=</span> <span class="fu">as.numeric</span>(newData<span class="sc">$</span>DX_bl)</span>
<span id="cb53-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(newData, <span class="fu">aes</span>(<span class="at">x =</span> FDG, <span class="at">y =</span> DX_bl))</span>
<span id="cb53-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="co"># predicted curve and point-wise 95\% CI</span></span>
<span id="cb53-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-9" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> FDG, <span class="at">ymin =</span> fit.lower,</span>
<span id="cb53-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-10" aria-hidden="true" tabindex="-1"></a>                         <span class="at">ymax =</span> fit.upper), <span class="at">alpha =</span> <span class="fl">0.2</span>)</span>
<span id="cb53-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> FDG, <span class="at">y =</span> fitted), <span class="at">colour=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb53-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="co"># fitted values</span></span>
<span id="cb53-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> fitted), <span class="at">size=</span><span class="dv">2</span>, <span class="at">colour=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb53-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-14" aria-hidden="true" tabindex="-1"></a><span class="co"># observed values</span></span>
<span id="cb53-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-15" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb53-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-16" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Probability&quot;</span>) <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">18</span>))</span>
<span id="cb53-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-17" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span></span>
<span id="cb53-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-18" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;Observed and predicted probability of disease&quot;</span>)</span>
<span id="cb53-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb53-19" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-1"></span>
<img src="graphics/3_1.png" alt="Predicted probabilities (the red curve) with their 95\% CIs (the gray area) versus observed outcomes in data (the dots above and below)" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 34: Predicted probabilities (the red curve) with their 95% CIs (the gray area) versus observed outcomes in data (the dots above and below)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-1">34</a> shows that the model captures the relationship between <code>FDG</code> with <code>DX_bl</code> with a smooth logit curve, and the prediction confidences are fairly small (evidenced by the tight 95% CIs). On the other hand, it is also obvious that the single-predictor model does well on the two ends of the probability range (i.e., close to <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>), but not in the middle range where data points from the two classes could not be clearly separated.</p>
<p>We can add more predictors to enhance its prediction power. To decide on which predictors we should include, we can visualize the relationships between the predictors with the outcome variable. For example, continuous predictors could be presented in <strong>Boxplot</strong> to see if the distribution of the continuous predictor is different across the two classes, i.e., if it is different, it means the predictor could help separate the two classes. The following R codes generate Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-2">35</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-2"></span>
<p class="caption marginnote shownote">
Figure 35: Boxplots of the continuous predictors in the two classes
</p>
<img src="graphics/3_2.png" alt="Boxplots of the continuous predictors in the two classes" width="80%"  />
</div>
<p></p>
<p></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;reshape2&quot;)</span></span>
<span id="cb54-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(reshape2)</span>
<span id="cb54-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-3" aria-hidden="true" tabindex="-1"></a>data.train<span class="sc">$</span>ID <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(data.train)[<span class="dv">1</span>])</span>
<span id="cb54-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-4" aria-hidden="true" tabindex="-1"></a>AD.long <span class="ot">&lt;-</span> <span class="fu">melt</span>(data.train[,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">16</span>,<span class="dv">17</span>)],</span>
<span id="cb54-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">id.vars =</span> <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;DX_bl&quot;</span>))</span>
<span id="cb54-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data using ggplot</span></span>
<span id="cb54-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb54-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(AD.long, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(DX_bl), <span class="at">y =</span> value))</span>
<span id="cb54-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co"># boxplot, size=.75 to stand out behind CI</span></span>
<span id="cb54-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-10" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">size =</span> <span class="fl">0.75</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb54-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co"># points for observed data</span></span>
<span id="cb54-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-12" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">position =</span> <span class="fu">position_jitter</span>(<span class="at">w =</span> <span class="fl">0.05</span>, <span class="at">h =</span> <span class="dv">0</span>),</span>
<span id="cb54-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-13" aria-hidden="true" tabindex="-1"></a>                    <span class="at">alpha =</span> <span class="fl">0.1</span>)</span>
<span id="cb54-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="co"># diamond at mean for each group</span></span>
<span id="cb54-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-15" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">stat_summary</span>(<span class="at">fun =</span> mean, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">shape =</span> <span class="dv">18</span>,</span>
<span id="cb54-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-16" aria-hidden="true" tabindex="-1"></a>                      <span class="at">size =</span> <span class="dv">6</span>, <span class="at">alpha =</span> <span class="fl">0.75</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb54-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence limits based on normal distribution</span></span>
<span id="cb54-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-18" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">stat_summary</span>(<span class="at">fun.data =</span> <span class="st">&quot;mean_cl_normal&quot;</span>,</span>
<span id="cb54-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-19" aria-hidden="true" tabindex="-1"></a>                      <span class="at">geom =</span> <span class="st">&quot;errorbar&quot;</span>, <span class="at">width =</span> .<span class="dv">2</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>)</span>
<span id="cb54-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-20" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">facet_wrap</span>(<span class="sc">~</span> variable, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb54-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-21" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span></span>
<span id="cb54-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Boxplots of variables by diagnosis (0 - normal; 1 - patient)&quot;</span>)</span>
<span id="cb54-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb54-23" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-2">35</a> shows that some variables, e.g., <code>FDG</code> and <code>HippoNV</code>, could separate the two classes significantly. Some variables, such as <code>AV45</code> and <code>AGE</code>, have less prediction power, but still look promising. Note these observations cautiously, since these figures only show <em>marginal</em> relationship among variables<label for="tufte-sn-66" class="margin-toggle sidenote-number">66</label><input type="checkbox" id="tufte-sn-66" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">66</span> Boxplot is nice but it cannot show synergistic effects among the variables.</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-3"></span>
<img src="graphics/3_3.png" alt="Boxplots of the *predicted probabilities of diseased*, i.e., the $Pr(y=1|\boldsymbol{x})$" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 36: Boxplots of the <em>predicted probabilities of diseased</em>, i.e., the <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-3">36</a> shows the boxplot of the predicted probabilities of diseased made by the final model identified in <strong>Step 4</strong> of the <em>7-step R pipeline</em>. This figure is to be compared with Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-pred-FDG-boxplot">33</a>. It indicates that the final model is much better than the model that only uses the predictor <code>FDG</code> alone.</p>
</div>
</div>
<div id="ranking-problem-by-pairwise-comparison" class="section level2 unnumbered">
<h2>Ranking problem by pairwise comparison</h2>
<div id="rationale-and-formulation-3" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>In recent years, we have witnessed a growing interest in estimating the ranks of a list of items. This same problem could be found in a variety of applications, such as the online advertisement of products on Amazon or movie recommendation by Netflix. These problems could be analytically summarized as: given a list of items denoted by <span class="math inline">\(\boldsymbol{M}=\left\{M_{1}, M_{2}, \ldots, M_{p}\right\}\)</span>, what is the rank of the items (denoted by <span class="math inline">\(\boldsymbol{\phi}=\left\{\phi_{1}, \phi_{2}, \ldots, \phi_{p}\right\}\)</span>)?<label for="tufte-sn-67" class="margin-toggle sidenote-number">67</label><input type="checkbox" id="tufte-sn-67" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">67</span> Here, <span class="math inline">\(\boldsymbol{\phi}\)</span> is a vector of real values, i.e., the larger the <span class="math inline">\(\phi_i\)</span>, the higher the rank of <span class="math inline">\(M_i\)</span>.</span></p>
<p>To obtain ranking of items, comparison data (either by domain expert or users) is often collected, e.g., a pair of items in <span class="math inline">\(M\)</span>, let’s say, <span class="math inline">\(M_i\)</span> and <span class="math inline">\(M_j\)</span>, will be pushed to the expert/user who conducts the comparison to see if <span class="math inline">\(M_i\)</span> is better than <span class="math inline">\(M_j\)</span>; then, a score, denoted as <span class="math inline">\(y_k\)</span>, will be returned, i.e., a positive <span class="math inline">\(y_k\)</span> indicates that the expert/user supports that <span class="math inline">\(M_i\)</span> is better than <span class="math inline">\(M_j\)</span>, while a negative <span class="math inline">\(y_k\)</span> indicates the opposite. Note that the larger the <span class="math inline">\(y_k\)</span>, the stronger the support.</p>
<p>Denote the expert/user data as <span class="math inline">\(\boldsymbol y\)</span>, which is a vector and consists of the set of pairwise comparisons. The question is to estimate the ranking <span class="math inline">\(\boldsymbol \phi\)</span> based on <span class="math inline">\(\boldsymbol y\)</span>.</p>
</div>
<div id="theory-and-method-2" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>It looks like an unfamiliar problem, but a surprise recognition was made in the paper<label for="tufte-sn-68" class="margin-toggle sidenote-number">68</label><input type="checkbox" id="tufte-sn-68" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">68</span> Osting, B., Brune, C. and Osher, S. <em>Enhanced statistical rankings via targeted data collection</em>. Proceedings of the 30 International Conference on Machine Learning (ICML), 2013.</span> that the underlying statistical model is a linear regression model. This indicates that we can use the rich array of methods in linear regression framework to solve many problems in ranking.</p>
<p>To see that, first, we need to make explicit the relationship between the parameter to be estimated (<span class="math inline">\(\boldsymbol \phi\)</span>) and the data (<span class="math inline">\(\boldsymbol y\)</span>). For the <span class="math inline">\(k^{th}\)</span> comparison that involves items <span class="math inline">\(M_i\)</span> and <span class="math inline">\(M_j\)</span>, we could assume that <span class="math inline">\(y_k\)</span> is distributed as</p>
<p><span class="math display" id="eq:3-rank-y">\[\begin{equation}
\small
y_{k} \sim N\left(\phi_{i}-\phi_{j}, \sigma^{2} / w_{k}\right).
\tag{32}
\end{equation}\]</span></p>
<p>This assumes that if the item <span class="math inline">\(M_i\)</span> is more (or less) important than the item <span class="math inline">\(M_j\)</span>, we will expect to see positive (or negative) values of <span class="math inline">\(y_k\)</span>. <span class="math inline">\(\sigma^2\)</span> encodes the overall accuracy level of the expert/user knowledge<label for="tufte-sn-69" class="margin-toggle sidenote-number">69</label><input type="checkbox" id="tufte-sn-69" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">69</span> More knowledgeable expert/user will have smaller <span class="math inline">\(\sigma^2\)</span>.</span>. Expert/user could also provide their confidence level on a particular comparison, encoded in <span class="math inline">\(w_k\)</span><label for="tufte-sn-70" class="margin-toggle sidenote-number">70</label><input type="checkbox" id="tufte-sn-70" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">70</span> When this information is lacking, we could simply assume <span class="math inline">\(w_k=1\)</span> for all the comparison data.</span>.</p>
<p>Following this line, we illustrate how we could represent the comparison data in a more compact matrix form. This is shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-5">37</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-5"></span>
<p class="caption marginnote shownote">
Figure 37: The data structure and its analytic formulation underlying the pairwise comparison. Each node is an item in <span class="math inline">\(M\)</span>, while each arc represents a comparison of two items
</p>
<img src="graphics/3_5.png" alt="The data structure and its analytic formulation underlying the pairwise comparison. Each node is an item in $M$, while each arc represents a comparison of two items" width="80%"  />
</div>
<p></p>
<p>The matrix <span class="math inline">\(\boldsymbol B\)</span> shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-5">37</a> is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  \boldsymbol{B}_{k j}=\left\{\begin{array}{cc}{1} &amp; {\text { if } j=h e a d(k)} \\ {-1} &amp; {\text { if } j=\operatorname{tail}(k)} \\ {0} &amp; {\text { otherwise }}\end{array}\right. 
\end{equation*}\]</span></p>
<p>Here, <span class="math inline">\(j=tail(k)\)</span> if the <span class="math inline">\(k^{th}\)</span> comparison is asked in the form as “if <span class="math inline">\(M_i\)</span> is better than <span class="math inline">\(M_j\)</span>” (i.e., denoted as <span class="math inline">\(M_i\rightarrow M_j\)</span>); otherwise, <span class="math inline">\(j=head(k)\)</span> for a question asked in the form as <span class="math inline">\(M_j\rightarrow M_i\)</span>.</p>
<p>Based on the definition of <span class="math inline">\(\boldsymbol B\)</span>, we rewrite Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-rank-y">(32)</a> as</p>
<p><span class="math display" id="eq:3-rank-y-reg">\[\begin{equation}
\small
    y_k = \sum_{i=1}^{p} \phi_{i} \boldsymbol{B}_{ki} + \varepsilon_k,
\tag{33}
\end{equation}\]</span></p>
<p>where the distribution of <span class="math inline">\(\epsilon_k\)</span> is</p>
<p><span class="math display" id="eq:3-rank-y-reg-eps">\[\begin{equation}
\small
\epsilon_k \sim N\left(0, \sigma^{2}/w_k \right).
\tag{34}
\end{equation}\]</span></p>
<p>Putting Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-rank-y-reg">(33)</a> in matrix form, we can derive that
<span class="math display">\[\begin{equation*}
\small
  \boldsymbol{y} \sim N\left(\boldsymbol{B} \boldsymbol{\phi}, \sigma^{2} \boldsymbol{W}^{-1}\right). 
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol W\)</span> is the diagonal matrix of elements <span class="math inline">\(w_k\)</span> for <span class="math inline">\(k=1,2,…,K\)</span>.</p>
<p>Using the framework developed in <strong>Chapter 2</strong>,<label for="tufte-sn-71" class="margin-toggle sidenote-number">71</label><input type="checkbox" id="tufte-sn-71" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">71</span> Here, the estimation of <span class="math inline">\(\boldsymbol \phi\)</span> is a generalized least squares problem.</span> we could derive the estimator of <span class="math inline">\(\boldsymbol \phi\)</span> as</p>
<p><span class="math display">\[\begin{equation*}
\small
  \widehat{\boldsymbol{\phi}}=\left(\boldsymbol{B}^{T} \boldsymbol{W} \boldsymbol{B}\right)^{-1} \boldsymbol{B}^{T} \boldsymbol{W} \boldsymbol{y}. 
\end{equation*}\]</span></p>
<!-- % The recognition of the linear regression formulation underling the ranking problem brings more insights and operational possibilities to solve the problem better. For example, as many design of experiments techniques have been developed for optimal data collection, while most are based on the linear regression framework, these techniques could find relevance in this ranking problem, e.g., what new comparison data we should collect to optimize statistical accuracy and efficiency given limited budget? As shown in the paper, an E-optimal design method could be introduced here to optimally decide on which new comparison should be conducted. As this process involves Bayesian statistics, optimal design, and optimization, interested readers are encouraged to read the paper. -->
</div>
</div>
<div id="statistical-process-control-using-decision-tree" class="section level2 unnumbered">
<h2>Statistical process control using decision tree</h2>
<p>A fundamental problem in <strong>statistical process control</strong> (<strong>SPC</strong>) is illustrated in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-spcintro">38</a>: given a sequence of observations of a variable that <em>represents the temporal variability of a process</em>, is this process <em>stable</em>?</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-spcintro"></span>
<img src="graphics/3_spcintro.png" alt="A fundamental problem in statistical process control" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 38: A fundamental problem in statistical process control<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>SPC is built on a creative application of the statistical distribution theory. A distribution model represents a <em>stable process</em>—that is the main premise of SPC—while also allots a calculated proportion to <em>chance outliers</em>. An illustration is shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-spcintro2">39</a> (left).</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-spcintro2"></span>
<p class="caption marginnote shownote">
Figure 39: (Left) The use of a distribution model to represent a <em>stable process</em>; and (right) the basic idea of a control chart
</p>
<img src="graphics/3_spcintro2.png" alt="(Left) The use of a distribution model to represent a *stable process*; and (right) the basic idea of a control chart" width="80%"  />
</div>
<p></p>
<p>A further invention of SPC is to convert a distribution model, a static object, into a temporal chart, the so-called <strong>control chart</strong>, as shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-spcintro2">39</a> (right). A <em>control chart</em> has the upper and lower <em>control limits</em>, and a <em>center line</em>. It is interesting to note that Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-spcintro2">39</a> (left) also provides a graphical illustration of how <em>hypothesis testing</em> works, and Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-spcintro2">39</a> (right) illustrates the concept of <em>control chart</em>. The two build on the same foundation and differ in perspectives: one is horizontal and the other vertical.</p>
<p>A control chart is used to monitor a process. A reference data is collected to draw the control limits and the center line. Then, new data will be continuously collected over time and drawn in the chart, as shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-spcintro4">40</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-spcintro4"></span>
<p class="caption marginnote shownote">
Figure 40:  A control chart, built on a reference data (i.e., <span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_8\)</span>), is used to monitor future data (i.e., <span class="math inline">\(x_9\)</span>-<span class="math inline">\(x_{12}\)</span>). An alarm is issued when <span class="math inline">\(x_{12}\)</span> is found to be <em>out of the control limit</em>.
</p>
<img src="graphics/3_spcintro4.png" alt=" A control chart, built on a reference data (i.e., $x_1$-$x_8$), is used to monitor future data (i.e., $x_9$-$x_{12}$). An alarm is issued when $x_{12}$ is found to be *out of the control limit*." width="80%"  />
</div>
<p></p>
<p>Because of this dependency of SPC on distribution models, a considerable amount of interest has been focused on extending it for applications where the data could not be characterized by a distribution. Along this endeavor, how to leverage decision tree models<label for="tufte-sn-72" class="margin-toggle sidenote-number">72</label><input type="checkbox" id="tufte-sn-72" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">72</span> Remember that the decision tree models can deal with complex datasets such as mixed types of variables, as discussed in <strong>Chapter 2</strong>.</span> for SPC purposes has been an interesting research problem.</p>
<div id="rationale-and-formulation-4" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>One such interesting framework<label for="tufte-sn-73" class="margin-toggle sidenote-number">73</label><input type="checkbox" id="tufte-sn-73" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">73</span> Deng, H., Runger, G. and Tuv, E., <em>System monitoring with real-time contrasts,</em> Journal of Quality Technology, Volume 44, Issue 1, Pages 9-27, 2012.</span> is proposed to cast the process monitoring problem shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-spcintro4">40</a> as a classification problem: the <em>reference data</em> presumably collected from a stable process represents one class, while the <em>online data</em> collected after the reference data represents another class. If the two classes could be significantly separated, an alarm should be issued. Otherwise, if there is no change in the stable process, the two data sets must come from the same distribution, then it will be difficult to classify the two data sets. This will result in a large classification error.</p>
<p>In other words, the classification error is an indicator that we can monitor<label for="tufte-sn-74" class="margin-toggle sidenote-number">74</label><input type="checkbox" id="tufte-sn-74" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">74</span> While process monitoring sounds straightforward, the real challenge sometimes lies in the question about what to monitor, and how.</span>.</p>
<p>Here we introduce the <strong>real-time contrasts</strong> method (<strong>RTC</strong>). The key idea of <em>RTC</em> is to have a <strong>sliding window</strong>, with length of <span class="math inline">\(L\)</span>, that includes the most recent data points to be compared with the reference data. We label the <em>reference data</em> as one class, and the data points in the <em>sliding window</em> as another class. We track the classification error to monitor the process.</p>
<p>We illustrate the <em>RTC</em> method through a simple problem. The collected data for monitoring is shown in Table <a href="chapter-3-recognition-logistic-regression-ranking.html#tab:t9-2">7</a>. The reference data is <span class="math inline">\(\{1,2\}\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-2">Table 7: </span>Example of an online dataset with <span class="math inline">\(4\)</span> time points</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><strong>Data ID</strong></th>
<th align="left"><span class="math inline">\(1\)</span></th>
<th align="left"><span class="math inline">\(2\)</span></th>
<th align="left"><span class="math inline">\(3\)</span></th>
<th align="left"><span class="math inline">\(4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Value</strong></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>To monitor the process, we use a window size of <span class="math inline">\(2\)</span>. This means the first monitoring action takes place at the time when the <span class="math inline">\(2^{nd}\)</span> data point is collected. The reference dataset, <span class="math inline">\(\{1,2\}\)</span>, is labeled as class <span class="math inline">\(0\)</span>, and the two online data points, <span class="math inline">\(\{2,1\}\)</span>, are labeled as class <span class="math inline">\(1\)</span>. As these two datasets are identical, the classification error rate is as large as 0.5. No alarm is issued.</p>
<p>At the next time point, the sliding window now includes data points <span class="math inline">\(\{1,3\}\)</span>. A classification rule “<em>if value <span class="math inline">\(\leq 2\)</span>, class <span class="math inline">\(0\)</span>; else, class <span class="math inline">\(1\)</span></em>” would achieve the best classification error rate as <span class="math inline">\(0.25\)</span>. An alarm probably should be issued.</p>
<p>At the next time point, the sliding window includes data points <span class="math inline">\(\{3,3\}\)</span>. The same classification rule “<em>if value <span class="math inline">\(\leq 2\)</span>, class <span class="math inline">\(0\)</span>; else, class <span class="math inline">\(1\)</span></em>” can classify all examples correctly with error rate of <span class="math inline">\(0\)</span>. An alarm should be issued.</p>
<p>We see that the classification error rate is a <strong>monitoring statistic</strong> to guide the triggering of alerts. It is also useful to use the probability estimates of the data points as the <em>monitoring statistic</em>. In other words, the sum of the probability estimates from all data points in the sliding window can be used for monitoring, which is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  p_{t}=\frac{\sum_{i=1}^{w} \hat{p}_{1}\left(x_{i}\right)}{w}. 
\end{equation*}\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol x_i\)</span> is the <span class="math inline">\(i^{th}\)</span> data point in the sliding window, <span class="math inline">\(w\)</span> is the window size, and <span class="math inline">\(\hat{p}_1(\boldsymbol x_i)\)</span> is the probability estimate of <span class="math inline">\(\boldsymbol x_i\)</span> belonging to class <span class="math inline">\(1\)</span>. At each time point in monitoring, we can obtain a <span class="math inline">\(p_t\)</span>. Following the tradition of control chart, we could chart the time series of <span class="math inline">\(p_t\)</span> and observe the patterns to see if alerts should be triggered.</p>
<!-- % Actually, through in-depth research into this idea of directly using classification error rate as the monitoring statistic, a limitation soon reveals itself. Considering the number of data points in the monitoring window, which is $ L$. The number of possible distinct classification error rate values are actually limited to be $L+1$. This suggests that, while the monitoring statistic should be a continuum, the resolution of the classification error rate to reflect the continuum maybe limited if the window size is too small. This will result in gaps between the monitoring statistics, failing to capture changes that happen in the gaps which are blind zones. -->
<!-- % As a remedy, the probability estimates of the data points can be used to replace the errors of the data points. The probability estimates are continuous indicators, while the errors are binary indicators. Then, the sum of the probability estimates from all data points in the sliding window can be used for monitoring, which is defined as: -->
<!-- % \begin{equation*}
\small
  p_{t}=\frac{\sum_{i=1}^{w} \hat{p}_{1}\left(x_{i}\right)}{w}. 
\end{equation*} -->
<!-- % Here, $\boldsymbol x_i$ is the $i^{th}$data point in the sliding window, $w$ is the window size, and $\hat{p}_1(\boldsymbol x_i)$ is the probability estimate of $\boldsymbol x_i$ belonging to $f_1(\boldsymbol x)$. At each time point in monitoring, we can obtain a $p_t$. Following the tradition of control chart, we could chart the time series of $p_t$ and observe the patterns to see if alerts should be triggered. -->
<!-- % Besides this monitoring capacity, on the other hand, we could also use the classification model for fault diagnosis. Specifically, when the random forest is used for classification, the importance scores from random forests can be used for fault diagnosis. When process is under normal conditions and the classification errors are expected to be high, the importance scores are expected to be equal among process variables as none of them contribute to the classification problem. When process is abnormal, classification errors should be reduced, and the variables responsible for the process abnormality should now have larger importance scores. This gives us the foundation for using random forest for fault diagnosis. -->
<!-- % Note that, under the RTC framework, the size of the sliding window is an important parameter. When the window is too long, the method requires a large number of real-time data in each monitoring epoch, which can delay the identification of abnormal patterns. In contrast, if the window is too short, the classifiers built on the small data sets may be unstable and are prone to more false positives. In our R lab, we will explore this phenomenon further. -->
</div>
<div id="r-lab-3" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We have coded the RTC method into a R function, <code>Monitoring()</code>, as shown below, to give an example about how to write self-defined function in R. This function takes two datasets as input: the first is the reference data, <code>data0</code>, and the second is the online data points, <code>data.real.time</code>. The window size should also be provided in <code>wsz</code>. And we use a classification method named random forest<label for="tufte-sn-75" class="margin-toggle sidenote-number">75</label><input type="checkbox" id="tufte-sn-75" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">75</span> More details are in <strong>Chapter 4</strong>.</span> to build a classifier. The <code>Monitoring()</code> function returns a few monitoring statistics for each online data point, and a score of each variable that represents how likely the variable is responsible for the process change.</p>
<p></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb55-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb55-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb55-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb55-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_gray</span>(<span class="at">base_size =</span> <span class="dv">15</span>) )</span>
<span id="cb55-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co"># define monitoring function. data0: reference data;</span></span>
<span id="cb55-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co"># data.real.time: real-time data; wsz: window size</span></span>
<span id="cb55-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-10" aria-hidden="true" tabindex="-1"></a>Monitoring <span class="ot">&lt;-</span> <span class="cf">function</span>( data0, data.real.time, wsz ){</span>
<span id="cb55-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-11" aria-hidden="true" tabindex="-1"></a>num.data.points <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data.real.time)</span>
<span id="cb55-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-12" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb55-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-13" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb55-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>( i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.data.points  ){</span>
<span id="cb55-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-16" aria-hidden="true" tabindex="-1"></a><span class="co"># at the start of monitoring, when real-time data size is </span></span>
<span id="cb55-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-17" aria-hidden="true" tabindex="-1"></a><span class="co"># smaller than the window size, combine the real-time</span></span>
<span id="cb55-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-18" aria-hidden="true" tabindex="-1"></a><span class="co"># data points and random samples from the reference data</span></span>
<span id="cb55-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-19" aria-hidden="true" tabindex="-1"></a><span class="co"># to form a data set of wsz</span></span>
<span id="cb55-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(i<span class="sc">&lt;</span>wsz){</span>
<span id="cb55-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-21" aria-hidden="true" tabindex="-1"></a>  ssfr <span class="ot">&lt;-</span> wsz <span class="sc">-</span> i</span>
<span id="cb55-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-22" aria-hidden="true" tabindex="-1"></a>  sample.reference <span class="ot">&lt;-</span> data0[<span class="fu">sample</span>(<span class="fu">nrow</span>(data0),                                   ssfr,<span class="at">replace =</span> <span class="cn">TRUE</span>), ]</span>
<span id="cb55-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-23" aria-hidden="true" tabindex="-1"></a>  current.real.time.data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(sample.reference,</span>
<span id="cb55-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-24" aria-hidden="true" tabindex="-1"></a>                            data.real.time[<span class="dv">1</span><span class="sc">:</span>i,,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb55-25"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-25" aria-hidden="true" tabindex="-1"></a>}<span class="cf">else</span>{</span>
<span id="cb55-26"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-26" aria-hidden="true" tabindex="-1"></a>  current.real.time.data <span class="ot">&lt;-</span>  data.real.time[(i<span class="sc">-</span>wsz<span class="sc">+</span>                                       <span class="dv">1</span>)<span class="sc">:</span>i,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb55-27"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb55-28"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-28" aria-hidden="true" tabindex="-1"></a>current.real.time.data<span class="sc">$</span>class <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb55-29"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-29" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rbind</span>( data0, current.real.time.data )</span>
<span id="cb55-30"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-30" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(data) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="dv">1</span><span class="sc">:</span>(<span class="fu">ncol</span>(data)<span class="sc">-</span><span class="dv">1</span>)),</span>
<span id="cb55-31"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-31" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;Class&quot;</span>)</span>
<span id="cb55-32"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-32" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>Class <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>Class)</span>
<span id="cb55-33"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-34"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-34" aria-hidden="true" tabindex="-1"></a><span class="co"># apply random forests to the data</span></span>
<span id="cb55-35"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-35" aria-hidden="true" tabindex="-1"></a>my.rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Class <span class="sc">~</span> .,<span class="at">sampsize=</span><span class="fu">c</span>(wsz,wsz), <span class="at">data=</span>data)</span>
<span id="cb55-36"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-37"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-37" aria-hidden="true" tabindex="-1"></a><span class="co"># get importance score</span></span>
<span id="cb55-38"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-38" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(importance.mat, <span class="fu">t</span>(my.rf<span class="sc">$</span>importance))</span>
<span id="cb55-39"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-39" aria-hidden="true" tabindex="-1"></a><span class="co"># get monitoring statistics</span></span>
<span id="cb55-40"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-40" aria-hidden="true" tabindex="-1"></a>ooblist <span class="ot">&lt;-</span> my.rf[<span class="dv">5</span>]</span>
<span id="cb55-41"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-41" aria-hidden="true" tabindex="-1"></a>oobcolumn<span class="ot">=</span><span class="fu">matrix</span>(<span class="fu">c</span>(ooblist[[<span class="dv">1</span>]]),<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb55-42"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-42" aria-hidden="true" tabindex="-1"></a>ooberrornormal<span class="ot">=</span> (oobcolumn[,<span class="dv">3</span>])[<span class="dv">1</span>]</span>
<span id="cb55-43"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-43" aria-hidden="true" tabindex="-1"></a>ooberrorabnormal<span class="ot">=</span>(oobcolumn[,<span class="dv">3</span>])[<span class="dv">2</span>]</span>
<span id="cb55-44"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-45"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-45" aria-hidden="true" tabindex="-1"></a>temp<span class="ot">=</span>my.rf[<span class="dv">6</span>]</span>
<span id="cb55-46"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-46" aria-hidden="true" tabindex="-1"></a>p1vote <span class="ot">&lt;-</span> <span class="fu">mean</span>(temp<span class="sc">$</span>votes[,<span class="dv">2</span>][(<span class="fu">nrow</span>(data0)<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="fu">nrow</span>(data)])</span>
<span id="cb55-47"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-48"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-48" aria-hidden="true" tabindex="-1"></a>this.stat <span class="ot">&lt;-</span> <span class="fu">c</span>(ooberrornormal,ooberrorabnormal,p1vote)</span>
<span id="cb55-49"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-49" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(stat.mat, this.stat)</span>
<span id="cb55-50"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-50" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb55-51"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-51" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">importance.mat =</span> importance.mat,</span>
<span id="cb55-52"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-52" aria-hidden="true" tabindex="-1"></a>               <span class="at">stat.mat =</span> stat.mat)</span>
<span id="cb55-53"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-53" aria-hidden="true" tabindex="-1"></a><span class="fu">return</span>(result)</span>
<span id="cb55-54"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb55-54" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p>To demonstrate how to use <code>Monitoring()</code>, let’s consider a <span class="math inline">\(2\)</span>-dimensional process with two variables, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. We simulate the reference data that follow a normal distribution with mean of <span class="math inline">\(0\)</span> and standard deviation of <span class="math inline">\(1\)</span>. The online data come from two distributions: the first <span class="math inline">\(100\)</span> data points are sampled from the same distribution as the reference data, while the second <span class="math inline">\(100\)</span> data points are sampled from another distribution (i.e., the mean of <span class="math inline">\(x_2\)</span> changes to <span class="math inline">\(2\)</span>). We label the reference data with class <span class="math inline">\(0\)</span> and the online data with class <span class="math inline">\(1\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data generation</span></span>
<span id="cb56-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># sizes of reference data, real-time data without change, </span></span>
<span id="cb56-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and real-time data with changes</span></span>
<span id="cb56-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-4" aria-hidden="true" tabindex="-1"></a>length0 <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb56-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-5" aria-hidden="true" tabindex="-1"></a>length1 <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb56-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-6" aria-hidden="true" tabindex="-1"></a>length2 <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb56-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2-dimension</span></span>
<span id="cb56-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-9" aria-hidden="true" tabindex="-1"></a>dimension <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb56-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="co"># reference data</span></span>
<span id="cb56-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-12" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( dimension <span class="sc">*</span> length0, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb56-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data with no change</span></span>
<span id="cb56-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-14" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( dimension <span class="sc">*</span> length2, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb56-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data different from the reference data in the </span></span>
<span id="cb56-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="co"># second the variable</span></span>
<span id="cb56-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-17" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>( <span class="at">V1 =</span> <span class="fu">rnorm</span>( <span class="dv">1</span> <span class="sc">*</span> length1, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb56-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-18" aria-hidden="true" tabindex="-1"></a>                <span class="at">V2 =</span> <span class="fu">rnorm</span>( <span class="dv">1</span> <span class="sc">*</span> length1, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>) )</span>
<span id="cb56-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-20" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to data frame</span></span>
<span id="cb56-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-21" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data0, <span class="at">nrow =</span> length0, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb56-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb56-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-23" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data1, <span class="at">nrow =</span> length2, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb56-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb56-25"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-25" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> data2 <span class="sc">%&gt;%</span> <span class="fu">as.data.frame</span>()</span>
<span id="cb56-26"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-27"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-27" aria-hidden="true" tabindex="-1"></a><span class="co"># assign variable names</span></span>
<span id="cb56-28"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-28" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>( data0 ) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(data0))</span>
<span id="cb56-29"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-29" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>( data1 ) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(data1))</span>
<span id="cb56-30"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-30" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>( data2 ) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(data2))</span>
<span id="cb56-31"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-32"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-32" aria-hidden="true" tabindex="-1"></a><span class="co"># assign reference data with class 0 and real-time data with class 1</span></span>
<span id="cb56-33"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-33" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> data0 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">0</span>)</span>
<span id="cb56-34"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-34" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> data1 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">1</span>)</span>
<span id="cb56-35"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-35" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> data2 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">1</span>)</span>
<span id="cb56-36"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-37"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-37" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data consists of normal data and abnormal data</span></span>
<span id="cb56-38"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb56-38" aria-hidden="true" tabindex="-1"></a>data.real.time <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data1,data2)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-11"></span>
<img src="graphics/9_11.png" alt="Scatterplot of the reference dataset and the first $100$ online data points; both data come from the process under normal condition " width="100%"  />
<!--
<p class="caption marginnote">-->Figure 41: Scatterplot of the reference dataset and the first <span class="math inline">\(100\)</span> online data points; both data come from the process under normal condition <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-11">41</a> shows the scatterplot of the reference dataset and the first <span class="math inline">\(100\)</span> online data points. It can be seen that the two sets of data points are similar.</p>
<p></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb57-1" aria-hidden="true" tabindex="-1"></a>data.plot <span class="ot">&lt;-</span> <span class="fu">rbind</span>( data0, data1 ) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="fu">factor</span>(class))</span>
<span id="cb57-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data.plot, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">shape =</span> class, <span class="at">color=</span>class)) <span class="sc">+</span> </span>
<span id="cb57-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb57-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-12">42</a> shows the scatterplot of the reference dataset and the second <span class="math inline">\(100\)</span> online data points.</p>
<p></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb58-1" aria-hidden="true" tabindex="-1"></a>data.plot <span class="ot">&lt;-</span> <span class="fu">rbind</span>( data0, data2 ) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="fu">factor</span>(class))</span>
<span id="cb58-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data.plot, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">shape =</span> class,</span>
<span id="cb58-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb58-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">color=</span>class)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-12"></span>
<img src="graphics/9_12.png" alt="Scatterplot of the reference dataset and the second $100$ online data points that come from the process under abnormal condition" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 42: Scatterplot of the reference dataset and the second <span class="math inline">\(100\)</span> online data points that come from the process under abnormal condition<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now we apply the <em>RTC</em> method. A <em>window size</em> of <span class="math inline">\(10\)</span> is used. The error rates of the two classes and the probability estimates of the data points over time are shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-13">43</a> drawn by the following R code.</p>
<p></p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-1" aria-hidden="true" tabindex="-1"></a>wsz <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb59-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-2" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">Monitoring</span>( data0, data.real.time, wsz )</span>
<span id="cb59-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-3" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> result<span class="sc">$</span>stat.mat</span>
<span id="cb59-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-4" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> result<span class="sc">$</span>importance.mat</span>
<span id="cb59-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot different monitor statistics</span></span>
<span id="cb59-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-7" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(stat.mat)</span>
<span id="cb59-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-8" aria-hidden="true" tabindex="-1"></a>stat.mat<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(stat.mat)</span>
<span id="cb59-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(stat.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;error0&quot;</span>,<span class="st">&quot;error1&quot;</span>,<span class="st">&quot;prob&quot;</span>,<span class="st">&quot;id&quot;</span>)</span>
<span id="cb59-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-10" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> stat.mat <span class="sc">%&gt;%</span> <span class="fu">gather</span>(type, statistics, error0,</span>
<span id="cb59-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-11" aria-hidden="true" tabindex="-1"></a>                                error1,prob)</span>
<span id="cb59-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(stat.mat,<span class="fu">aes</span>(<span class="at">x=</span>id,<span class="at">y=</span>statistics,<span class="at">color=</span>type)) <span class="sc">+</span> </span>
<span id="cb59-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb59-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb59-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-13"></span>
<p class="caption marginnote shownote">
Figure 43: (Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: <em>error0</em> denotes the error rate in Class <span class="math inline">\(0\)</span>, <em>error1</em> denotes the error rate in Class <span class="math inline">\(1\)</span>, and <em>prob</em> denotes the probability estimates of the data points; (right) chart of the importance score of the two variables
</p>
<img src="graphics/9_13.png" alt="(Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: *error0* denotes the error rate in Class $0$, *error1* denotes the error rate in Class $1$, and *prob* denotes the probability estimates of the data points; (right) chart of the importance score of the two variables" width="49%" height="49%"  /><img src="graphics/9_14.png" alt="(Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: *error0* denotes the error rate in Class $0$, *error1* denotes the error rate in Class $1$, and *prob* denotes the probability estimates of the data points; (right) chart of the importance score of the two variables" width="49%" height="49%"  />
</div>
<p></p>
<p>We have known that the process shift happened on <span class="math inline">\(x_2\)</span> after the <span class="math inline">\(100^{th}\)</span> data point—and that is exactly when a good monitoring statistic should signal the process change. Check Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-13">43</a> (left) and draw your observation.</p>
<p>As the two classes are separated, we could check which variables are significant. The importance scores of the two variables obtained by the random forest model are shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-13">43</a> (right) drawn by the following R code.</p>
<p></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot importance scores for diagnosis</span></span>
<span id="cb60-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-2" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(importance.mat)</span>
<span id="cb60-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-3" aria-hidden="true" tabindex="-1"></a>importance.mat<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(importance.mat)</span>
<span id="cb60-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(importance.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>,<span class="st">&quot;X2&quot;</span>,<span class="st">&quot;id&quot;</span>)</span>
<span id="cb60-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-5" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> importance.mat <span class="sc">%&gt;%</span> </span>
<span id="cb60-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(variable, importance,X1,X2)</span>
<span id="cb60-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(importance.mat,<span class="fu">aes</span>(<span class="at">x=</span>id,<span class="at">y=</span>importance,</span>
<span id="cb60-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">color=</span>variable)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb60-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-13">43</a> (right) shows that the scores of <span class="math inline">\(x_2\)</span> significantly increase after the <span class="math inline">\(100^{th}\)</span> data point. This indicates that <span class="math inline">\(x_2\)</span> is responsible for the process change, which is true.</p>
<p>Let’s consider a <span class="math inline">\(10\)</span>-dimensional dataset with <span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_{10}\)</span>. We still simulate <span class="math inline">\(100\)</span> reference data points of each variable from a normal distribution with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. We use the same distribution to draw the first <span class="math inline">\(100\)</span> online data points. Then, we draw the second <span class="math inline">\(100\)</span> online data points with two variables, <span class="math inline">\(x_9\)</span> and <span class="math inline">\(x_{10}\)</span>, whose means changed from <span class="math inline">\(0\)</span> to <span class="math inline">\(2\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 10-dimensions, with 2 variables being changed from </span></span>
<span id="cb61-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the normal condition</span></span>
<span id="cb61-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-3" aria-hidden="true" tabindex="-1"></a>dimension <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb61-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-4" aria-hidden="true" tabindex="-1"></a>wsz <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb61-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co"># reference data</span></span>
<span id="cb61-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-6" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( dimension <span class="sc">*</span> length0, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb61-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data with no change</span></span>
<span id="cb61-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-8" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( dimension <span class="sc">*</span> length1, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb61-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data different from the reference data in the </span></span>
<span id="cb61-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="co"># second the variable</span></span>
<span id="cb61-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-11" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="fu">rnorm</span>( (dimension <span class="sc">-</span> <span class="dv">2</span>) <span class="sc">*</span> length2, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb61-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-12" aria-hidden="true" tabindex="-1"></a>            <span class="fu">rnorm</span>( (<span class="dv">2</span>) <span class="sc">*</span> length2, <span class="at">mean =</span> <span class="dv">20</span>, <span class="at">sd =</span> <span class="dv">1</span>))</span>
<span id="cb61-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to data frame</span></span>
<span id="cb61-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-16" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data0, <span class="at">nrow =</span> length0, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb61-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb61-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-18" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data1, <span class="at">nrow =</span> length1, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb61-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb61-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-20" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data2, <span class="at">ncol =</span> <span class="dv">10</span>, <span class="at">byrow =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb61-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb61-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-23" aria-hidden="true" tabindex="-1"></a><span class="co"># assign reference data with class 0 and real-time data </span></span>
<span id="cb61-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-24" aria-hidden="true" tabindex="-1"></a><span class="co"># with class 1</span></span>
<span id="cb61-25"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-25" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> data0 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">0</span>)</span>
<span id="cb61-26"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-26" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> data1 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">1</span>)</span>
<span id="cb61-27"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-27" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> data2 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">1</span>)</span>
<span id="cb61-28"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-29"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-29" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data consists of normal data and abnormal data</span></span>
<span id="cb61-30"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb61-30" aria-hidden="true" tabindex="-1"></a>data.real.time <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data1,data2)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-14"></span>
<p class="caption marginnote shownote">
Figure 44: (Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: <em>error0</em> denotes the error rate in Class <span class="math inline">\(0\)</span>, <em>error1</em> denotes the error rate in Class <span class="math inline">\(1\)</span>, and <em>prob</em> denotes the probability estimates of the data points; (right) chart of the importance score of the variables
</p>
<img src="graphics/9_19.png" alt="(Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: *error0* denotes the error rate in Class $0$, *error1* denotes the error rate in Class $1$, and *prob* denotes the probability estimates of the data points; (right) chart of the importance score of the variables" width="49%" height="49%"  /><img src="graphics/9_20.png" alt="(Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: *error0* denotes the error rate in Class $0$, *error1* denotes the error rate in Class $1$, and *prob* denotes the probability estimates of the data points; (right) chart of the importance score of the variables" width="49%" height="49%"  />
</div>
<p></p>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-14">44</a> (left) shows that all the monitoring statistics change after the <span class="math inline">\(101^{th}\)</span> time point, and the variables’ scores in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-14">44</a> (right) indicate the change is due to <span class="math inline">\(x_9\)</span> and <span class="math inline">\(x_{10}\)</span>, which is true. The following R codes generated Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-14">44</a> (left).</p>
<p></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-1" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">Monitoring</span>( data0, data.real.time, wsz )</span>
<span id="cb62-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-2" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> result<span class="sc">$</span>stat.mat</span>
<span id="cb62-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-3" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> result<span class="sc">$</span>importance.mat</span>
<span id="cb62-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot different monitor statistics</span></span>
<span id="cb62-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-6" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(stat.mat)</span>
<span id="cb62-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-7" aria-hidden="true" tabindex="-1"></a>stat.mat<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(stat.mat)</span>
<span id="cb62-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(stat.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;error0&quot;</span>,<span class="st">&quot;error1&quot;</span>,<span class="st">&quot;prob&quot;</span>,<span class="st">&quot;id&quot;</span>)</span>
<span id="cb62-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-9" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> stat.mat <span class="sc">%&gt;%</span> <span class="fu">gather</span>(type, statistics, error0,</span>
<span id="cb62-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-10" aria-hidden="true" tabindex="-1"></a>                                error1,prob)</span>
<span id="cb62-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(stat.mat,<span class="fu">aes</span>(<span class="at">x=</span>id,<span class="at">y=</span>statistics,<span class="at">color=</span>type))<span class="sc">+</span></span>
<span id="cb62-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb62-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb62-13" aria-hidden="true" tabindex="-1"></a>                                      <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p>The following R codes generated Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f9-14">44</a> (right).</p>
<p></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot importance scores for diagnosis</span></span>
<span id="cb63-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-2" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(importance.mat)</span>
<span id="cb63-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-3" aria-hidden="true" tabindex="-1"></a>importance.mat<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(importance.mat)</span>
<span id="cb63-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># colnames(importance.mat) &lt;- c(&quot;X1&quot;,&quot;X2&quot;,&quot;id&quot;)</span></span>
<span id="cb63-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-5" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> importance.mat <span class="sc">%&gt;%</span> </span>
<span id="cb63-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(variable, importance,X1<span class="sc">:</span>X10)</span>
<span id="cb63-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-7" aria-hidden="true" tabindex="-1"></a>importance.mat<span class="sc">$</span>variable <span class="ot">&lt;-</span> <span class="fu">factor</span>( importance.mat<span class="sc">$</span>variable,</span>
<span id="cb63-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-8" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">levels =</span> <span class="fu">paste0</span>( <span class="st">&quot;X&quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>))</span>
<span id="cb63-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="co"># levels(importance.mat$variable) &lt;- paste0( &quot;X&quot;, 1:10  )</span></span>
<span id="cb63-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(importance.mat,<span class="fu">aes</span>(<span class="at">x=</span>id,<span class="at">y=</span>importance,<span class="at">color=</span></span>
<span id="cb63-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-11" aria-hidden="true" tabindex="-1"></a>          variable)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb63-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb63-12" aria-hidden="true" tabindex="-1"></a>          <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span></code></pre></div>
<p></p>
</div>
</div>
<div id="remarks-1" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="more-about-the-logistic-function" class="section level3 unnumbered">
<h3>More about the logistic function</h3>
<p>Like the linear regression model, Eq. <a href="chapter-3-recognition-logistic-regression-ranking.html#eq:3-logitR">(27)</a> seems like <em>one</em> model that explains all the data points<label for="tufte-sn-76" class="margin-toggle sidenote-number">76</label><input type="checkbox" id="tufte-sn-76" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">76</span> We have mentioned that a model with this trait is called a <em>global</em> model.</span>. This observation is good, but we may easily overlook its subtle complexity. As shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lr3regions">45</a>, the logistic regression model is able to encapsulate a complex relationships between <span class="math inline">\(x\)</span> (the dose) with <span class="math inline">\(y\)</span> (the response to treatment) as one succinct mathematical form. This is remarkable, probably unusual, and unmistakably beautiful.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-lr3regions"></span>
<img src="graphics/3_lr3regions.png" alt="The three regions of the logistic function" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 45: The three regions of the logistic function<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>And the regression coefficients flexibly tune the exact shape of the logistic function for each dataset, as shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-difflogit">46</a>.</p>
<p>On the other hand, the logistic function is not the only choice. There are some other options, i.e., Chester Ittner Bliss used the <em>cumulative normal distribution function</em> to perform the transformation and called his model the <strong>probit regression</strong> model. There is an interesting discussion of this piece of history in statistics in Chapter 9 of the book<label for="tufte-sn-77" class="margin-toggle sidenote-number">77</label><input type="checkbox" id="tufte-sn-77" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">77</span> Cramer, J.S., <em>Logit Models from Economics and Other Fields</em>, Cambridge University Press, 2003.</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-difflogit"></span>
<img src="graphics/3_difflogit.png" alt="Three examples of the logistic function" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 46: Three examples of the logistic function<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
</div>
<div id="does-the-logistic-function-make-sense-an-eda-approach" class="section level3 unnumbered">
<h3>Does the logistic function make sense? — An EDA approach</h3>
<p>Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal3">30</a> outlines the main premise of the logistic regression model. It remains unknown whether or not this is a practical assumption. Here, we show how we could evaluate this assumption in a specific dataset. Let’s use the AD dataset and pick up the predictor, <code>HippoNV</code>, and the outcome variable <code>DX_bl</code>.</p>
<p>First, we create a data table like the one shown in Table <a href="chapter-3-recognition-logistic-regression-ranking.html#tab:t3-goal3">6</a>. We discretize the continuous variable <code>HippoNV</code> into distinct levels, and compute the prevalence of AD incidences within each level (i.e., the <span class="math inline">\(Pr(y=1|x)\)</span>). The following R code serves this data processing purpose.</p>
<p></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the frequency table in accordance of categorization</span></span>
<span id="cb64-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="co"># of HippoNV</span></span>
<span id="cb64-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-3" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">=</span> <span class="fu">quantile</span>(AD<span class="sc">$</span>HippoNV,<span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.05</span>, <span class="at">to =</span> <span class="fl">0.95</span>,</span>
<span id="cb64-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-4" aria-hidden="true" tabindex="-1"></a>                                          <span class="at">by =</span> <span class="fl">0.05</span>))</span>
<span id="cb64-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-5" aria-hidden="true" tabindex="-1"></a>AD<span class="sc">$</span>HippoNV.category <span class="ot">&lt;-</span> <span class="fu">cut</span>(AD<span class="sc">$</span>HippoNV, <span class="at">breaks=</span><span class="fu">c</span>(<span class="sc">-</span><span class="cn">Inf</span>,</span>
<span id="cb64-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-6" aria-hidden="true" tabindex="-1"></a>                                              temp, <span class="cn">Inf</span>))</span>
<span id="cb64-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-7" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">xtabs</span>(<span class="sc">~</span>DX_bl <span class="sc">+</span> HippoNV.category, </span>
<span id="cb64-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-8" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data =</span> AD))</span>
<span id="cb64-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-9" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> tempData[<span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">2</span>, <span class="at">to =</span>                      <span class="dv">2</span><span class="sc">*</span><span class="fu">length</span>(<span class="fu">unique</span>(AD<span class="sc">$</span>HippoNV.category)),                      <span class="at">by =</span> <span class="dv">2</span>),]</span>
<span id="cb64-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">xtabs</span>(<span class="sc">~</span>DX_bl <span class="sc">+</span> HippoNV.category, <span class="at">data =</span> AD))</span>
<span id="cb64-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-11" aria-hidden="true" tabindex="-1"></a>tempData<span class="sc">$</span>Total <span class="ot">&lt;-</span> <span class="fu">colSums</span>(<span class="fu">as.matrix</span>(<span class="fu">xtabs</span>(<span class="sc">~</span>DX_bl <span class="sc">+</span></span>
<span id="cb64-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-12" aria-hidden="true" tabindex="-1"></a>                    HippoNV.category,<span class="at">data =</span> AD)))</span>
<span id="cb64-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-13" aria-hidden="true" tabindex="-1"></a>tempData<span class="sc">$</span>p.hat <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> tempData<span class="sc">$</span>Freq<span class="sc">/</span>tempData<span class="sc">$</span>Total</span>
<span id="cb64-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-14" aria-hidden="true" tabindex="-1"></a>tempData<span class="sc">$</span>HippoNV.category <span class="ot">=</span> <span class="fu">as.numeric</span>(tempData<span class="sc">$</span>HippoNV.category)</span>
<span id="cb64-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(tempData)</span></code></pre></div>
<p></p>
<p>We use the <code>str()</code> function to visualize the data we have converted: <span class="math inline">\(20\)</span> levels of <code>HippoNV</code> have been created, denoted by the variable <code>HippoNV.category</code>; <code>Total</code> denotes the total number of subjects within each level; and <code>p.hat</code> denotes the proportion of the diseased subjects within each level (i.e., the <span class="math inline">\(Pr(y=1|x)\)</span>).</p>
<p></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(tempData)</span>
<span id="cb65-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="do">## &#39;data.frame&#39;:    20 obs. of  5 variables:</span></span>
<span id="cb65-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ DX_bl           : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 ...</span></span>
<span id="cb65-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ HippoNV.category: num  1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span id="cb65-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ Freq            : int  24 25 25 21 22 15 17 17 19 11 ...</span></span>
<span id="cb65-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ Total           : num  26 26 26 26 26 25 26 26 26 34 ...</span></span>
<span id="cb65-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb65-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ p.hat           : num  0.0769 0.0385 0.0385 0.1923 0.1538</span></span></code></pre></div>
<p></p>
<p>We draw a scatterplot of <code>HippoNV.category</code> versus <code>p.hat</code>, as shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-4">47</a>. We also use the <code>loess</code> method, which is a <em>nonparametric smoothing</em> method<label for="tufte-sn-78" class="margin-toggle sidenote-number">78</label><input type="checkbox" id="tufte-sn-78" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">78</span> Related methods will be introduced in <strong>Chapter 9</strong>.</span>, to fit a smooth curve of the scatter data points. Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-4">47</a> exhibits a similar pattern as Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-lrgoal3">30</a>. This provides an empirical justification of the use of the logistic regression model in this dataset.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-4"></span>
<img src="graphics/3_4.png" alt="The empirical relationship between `HippoNV` and `DX_bl` takes a shape as the logistic function" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 47: The empirical relationship between <code>HippoNV</code> and <code>DX_bl</code> takes a shape as the logistic function<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the scatterplot of HippoNV.category </span></span>
<span id="cb66-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="co"># versus the probability of normal</span></span>
<span id="cb66-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb66-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tempData, <span class="fu">aes</span>(<span class="at">x =</span> HippoNV.category, <span class="at">y =</span> p.hat))</span>
<span id="cb66-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>)</span>
<span id="cb66-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>)</span>
<span id="cb66-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span><span class="st">&quot;Empirically observed probability of normal&quot;</span></span>
<span id="cb66-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-8" aria-hidden="true" tabindex="-1"></a>              , <span class="at">xlab =</span> <span class="st">&quot;HippoNV&quot;</span>)</span>
<span id="cb66-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb66-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
</div>
<div id="regression-vs.-tree-models" class="section level3 unnumbered">
<h3>Regression vs. tree models</h3>
<p>A decision tree model draws a distinct type of <strong>decision boundary</strong>, as illustrated in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-tree-boundary">48</a>. Think about how a tree is built: at each node, a split is implemented based on <em>one single variable</em>, and in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-tree-boundary">48</a> the classification boundary is either parallel or perpendicular to one axis.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-tree-boundary"></span>
<p class="caption marginnote shownote">
Figure 48: Illustration of a decision tree model for a binary classification problem (i.e., the solid circles and empty squares represent data points from two classes), built on two predictors (i.e., <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>); (left) is the scatterplot of the data overlaid with the decision boundary of the decision tree model, which is shown in the (right)
</p>
<img src="graphics/5_simple_tree.png" alt="Illustration of a decision tree model for a binary classification problem (i.e., the solid circles and empty squares represent data points from two classes), built on two predictors (i.e., $x_1$ and $x_2$); (left) is the scatterplot of the data overlaid with the decision boundary of the decision tree model, which is shown in the (right)" width="80%"  />
</div>
<p></p>
<p>This implies that, when applying a decision tree to a dataset with linear relationship between predictors and outcome variables, it may not be an optimal choice. In the following example, we simulate a dataset and apply a decision tree and a logistics regression model to the data, respectively. The training data, and the predicted classes for each data point from the logistic regression and decision models are shown in Figures <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f2-22">49</a>, <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f2-23">50</a> and <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f2-24">51</a>, respectively. It can be seen that the classification boundary from the logistics regression model is linear, while the one from the decision tree is parallel to the axis. Decision tree is not able to capture the linear relationship in the data. The R code for this experiment is shown in below.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-22"></span>
<img src="graphics/2_22.png" alt="Scatterplot of the generated dataset" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 49: Scatterplot of the generated dataset<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-23"></span>
<img src="graphics/2_23.png" alt="Decision boundary captured by a logistic regression model" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 50: Decision boundary captured by a logistic regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(rpart)</span>
<span id="cb67-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-2" aria-hidden="true" tabindex="-1"></a>ndata <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb67-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-3" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(ndata, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)</span>
<span id="cb67-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-4" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(ndata, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)</span>
<span id="cb67-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X1,X2)</span>
<span id="cb67-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">mutate</span>( <span class="at">X12 =</span> <span class="fl">0.5</span> <span class="sc">*</span> (X1 <span class="sc">-</span> X2), <span class="at">Y =</span></span>
<span id="cb67-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-7" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">ifelse</span>(X12<span class="sc">&gt;=</span><span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>))</span>
<span id="cb67-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-8" aria-hidden="true" tabindex="-1"></a>ix <span class="ot">&lt;-</span> <span class="fu">which</span>( <span class="fu">abs</span>(data<span class="sc">$</span>X12) <span class="sc">&lt;=</span> <span class="fl">0.05</span>)</span>
<span id="cb67-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-9" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>Y[ix] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">runif</span>( <span class="fu">length</span>(ix)) <span class="sc">&lt;</span> <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb67-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-10" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data  <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>X12) <span class="sc">%&gt;%</span>  <span class="fu">mutate</span>(<span class="at">Y =</span></span>
<span id="cb67-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-11" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">as.factor</span>(<span class="fu">as.character</span>(Y)))</span>
<span id="cb67-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data,<span class="fu">aes</span>(<span class="at">x=</span>X1,<span class="at">y=</span>X2,<span class="at">color=</span>Y))<span class="sc">+</span><span class="fu">geom_point</span>()</span>
<span id="cb67-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-13" aria-hidden="true" tabindex="-1"></a>linear_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y <span class="sc">~</span>  ., <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb67-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-14" aria-hidden="true" tabindex="-1"></a>                                                  <span class="at">data =</span> data)</span>
<span id="cb67-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-15" aria-hidden="true" tabindex="-1"></a>tree_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>( Y <span class="sc">~</span>  ., <span class="at">data =</span> data)</span>
<span id="cb67-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-16" aria-hidden="true" tabindex="-1"></a>pred_linear <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear_model, data,<span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb67-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-17" aria-hidden="true" tabindex="-1"></a>pred_tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_model, data,<span class="at">type=</span><span class="st">&quot;prob&quot;</span>)[,<span class="dv">1</span>]</span>
<span id="cb67-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-18" aria-hidden="true" tabindex="-1"></a>data_pred <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">pred_linear_class =</span></span>
<span id="cb67-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ifelse</span>(pred_linear <span class="sc">&lt;</span><span class="fl">0.5</span>,<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">%&gt;%</span><span class="fu">mutate</span>(<span class="at">pred_linear_class =</span></span>
<span id="cb67-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.factor</span>(<span class="fu">as.character</span>(pred_linear_class)))<span class="sc">%&gt;%</span></span>
<span id="cb67-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">pred_tree_class =</span> <span class="fu">ifelse</span>( pred_tree <span class="sc">&lt;</span><span class="fl">0.5</span>,<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb67-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>( <span class="at">pred_tree_class =</span></span>
<span id="cb67-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-23" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">as.factor</span>(<span class="fu">as.character</span>(pred_tree_class)))</span>
<span id="cb67-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-24" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_pred,<span class="fu">aes</span>(<span class="at">x=</span>X1,<span class="at">y=</span>X2,<span class="at">color=</span>pred_linear_class))<span class="sc">+</span></span>
<span id="cb67-25"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb67-26"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-26" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_pred,<span class="fu">aes</span>(<span class="at">x=</span>X1,<span class="at">y=</span>X2,<span class="at">color=</span>pred_tree_class))<span class="sc">+</span></span>
<span id="cb67-27"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb67-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-24"></span>
<img src="graphics/2_24.png" alt="Decision boundary captured by the tree model" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 51: Decision boundary captured by the tree model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
</div>
<div id="can-we-use-a-tree-model-for-regression" class="section level3 unnumbered">
<h3>Can we use a tree model for regression?</h3>
<p>The answer is yes. There is nothing preventing us from modifying the tree-learning process as we have presented in <strong>Chapter 2</strong> for predicting continuous outcome. You only need to modify the IG, i.e., to create a similar counterpart for continuous outcomes.</p>
<p>Without going into further technical details, we present the modified 6-step R pipeline for a regression tree.</p>
<p></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># AGE, PTGENDER and PTEDUCAT are used as the </span></span>
<span id="cb68-2"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictor variables. </span></span>
<span id="cb68-3"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="co"># MMSCORE (a numeric value) is the outcome.</span></span>
<span id="cb68-4"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-5"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: read data into R</span></span>
<span id="cb68-6"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-6" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb68-7"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb68-8"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-8" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb68-9"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: data preprocessing</span></span>
<span id="cb68-10"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb68-11"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-11" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span>
<span id="cb68-12"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-12" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb68-13"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span>
<span id="cb68-14"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-15"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb68-16"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-16" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb68-17"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-17" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb68-18"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb68-19"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-19" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span>
<span id="cb68-20"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-21"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: build the tree</span></span>
<span id="cb68-22"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-22" aria-hidden="true" tabindex="-1"></a><span class="co"># for regression problems, use method=&quot;anova&quot;</span></span>
<span id="cb68-23"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-23" aria-hidden="true" tabindex="-1"></a>tree_reg <span class="ot">&lt;-</span> <span class="fu">rpart</span>( MMSCORE <span class="sc">~</span>  ., data.train, <span class="at">method=</span><span class="st">&quot;anova&quot;</span>) </span>
<span id="cb68-24"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-25"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: draw the tree</span></span>
<span id="cb68-26"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-26" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(rpart.plot)</span>
<span id="cb68-27"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-27" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree_reg, <span class="at">nn.cex=</span><span class="dv">1</span>)</span>
<span id="cb68-28"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-29"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; prune the tree</span></span>
<span id="cb68-30"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-30" aria-hidden="true" tabindex="-1"></a>tree_reg <span class="ot">&lt;-</span> <span class="fu">prune</span>(tree_reg,<span class="at">cp=</span><span class="fl">0.03</span>)</span>
<span id="cb68-31"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-31" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree_reg,<span class="at">nn.cex=</span><span class="dv">1</span>)</span>
<span id="cb68-32"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-33"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict using your tree model</span></span>
<span id="cb68-34"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-34" aria-hidden="true" tabindex="-1"></a>pred.tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_reg, data.test)</span>
<span id="cb68-35"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-35" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(pred.tree, data.test<span class="sc">$</span>MMSCORE)</span>
<span id="cb68-36"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-36" aria-hidden="true" tabindex="-1"></a><span class="co">#For regression model, you can use correlation </span></span>
<span id="cb68-37"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-37" aria-hidden="true" tabindex="-1"></a><span class="co"># to measure how close are your predictions </span></span>
<span id="cb68-38"><a href="chapter-3-recognition-logistic-regression-ranking.html#cb68-38" aria-hidden="true" tabindex="-1"></a><span class="co"># with the true outcome values of the data points</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-tree-interaction"></span>
<img src="graphics/3_tree_interaction.png" alt="Decision tree to predict `MMSCORE` using `PTEDUCAT` and `AGE`" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 52: Decision tree to predict <code>MMSCORE</code> using <code>PTEDUCAT</code> and <code>AGE</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The learned tree is shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-tree-interaction">52</a>. In the EDA analysis shown in <strong>Chapter 2</strong>, it has been shown that the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code> changes substantially according to different levels of <code>AGE</code>. Here shows the decision tree can also capture the interaction between <code>PTEDUCAT</code>, <code>AGE</code> and <code>MMSCORE</code>.</p>
<!-- % ^[Some advanced examples for interested readers: Neal, R. *Bayesian learning for neural networks*, Springer Verlag 1996. \\ Lee, K. and Kim, J. *On the equivalence of linear discriminant analysis and least squares*, AAAI 2005. \\ Ye, J. *Least squares linear discriminant analysis*, ICML 2007. \\ Li, F., Yang, Y. and Xing, E. *From LASSO regression to feature vector machine*, NIPS 2005.] -->
</div>
</div>
<div id="exercises-1" class="section level2 unnumbered">
<h2>Exercises</h2>
<div id="data-analysis" class="section level3 unnumbered">
<h3>Data analysis</h3>
<p><!-- begin{enumerate} --></p>
<p>1. Consider the case that, in building linear regression models, there is a concern that some data points may be more important (or more trustable). For these cases, it is not uncommon to assign a weight to each data point. Denote the weight for the <span class="math inline">\(i^{th}\)</span> data point as <span class="math inline">\(w_i\)</span>. An example is shown in Table <a href="chapter-3-recognition-logistic-regression-ranking.html#tab:t3-hw-wls">8</a>, as the last column, e.g., <span class="math inline">\(w_1=1\)</span>, <span class="math inline">\(w_2=2\)</span>, <span class="math inline">\(w_5=3\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t3-hw-wls">Table 8: </span>Dataset for building a weighted linear regression model</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
<th align="left"><span class="math inline">\(w\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-0.15\)</span></td>
<td align="left"><span class="math inline">\(-0.48\)</span></td>
<td align="left"><span class="math inline">\(0.46\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-0.72\)</span></td>
<td align="left"><span class="math inline">\(-0.54\)</span></td>
<td align="left"><span class="math inline">\(-0.37\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.91\)</span></td>
<td align="left"><span class="math inline">\(-0.27\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0.61\)</span></td>
<td align="left"><span class="math inline">\(1.59\)</span></td>
<td align="left"><span class="math inline">\(1.35\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(-1.11\)</span></td>
<td align="left"><span class="math inline">\(0.34\)</span></td>
<td align="left"><span class="math inline">\(-0.11\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>We still want to estimate the regression parameters in the least-squares framework. Follow the process of the derivation of the least-squares estimator as shown in <strong>Chapter 2</strong>, and propose your new estimator of the regression parameters.</p>
<p>2. Follow up the weighted least squares estimator derived in Q1, please calculate the regression parameters of the regression model using the data shown in Table <a href="chapter-3-recognition-logistic-regression-ranking.html#tab:t3-hw-wls">8</a>.</p>
<p>3. Follow up the dataset in Q1. Use the R pipeline for linear regression on this data (set up the weights in the <code>lm()</code> function). Compare the result from R and the result by your manual calculation.</p>
<p>4. Consider the dataset in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-hw-dt">5</a>. Use the R pipeline for building a logistic regression model on this data.</p>
<p>5. Consider the model fitted in Q4. Suppose that now there are two new data points as shown in Table <a href="chapter-3-recognition-logistic-regression-ranking.html#tab:t3-hw-lr-pred">9</a>. Please use the fitted model to predict on these two data points and fill in the table.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t3-hw-lr-pred">Table 9: </span>Two test data points</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(9\)</span></td>
<td align="left"><span class="math inline">\(0.25\)</span></td>
<td align="left"><span class="math inline">\(0.18\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(10\)</span></td>
<td align="left"><span class="math inline">\(0.08\)</span></td>
<td align="left"><span class="math inline">\(1.12\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p>6. Use the dataset <code>PimaIndiansDiabetes2</code> in the <code>mlbench</code> R package, run the R pipeline for logistic regression on it, and summarize your findings.</p>
<p>7. Follow up on the simulation experiment in Q12 in <strong>Chapter 2</strong>. Apply <code>glm()</code> on the simulated data to build a logistic regression model, and comment on the result.</p>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
</div>
<p style="text-align: center;">
<a href="chapter-2-abstraction-regression-tree-models.html"><button class="btn btn-default">Previous</button></a>
<a href="chapter-4-resonance-bootstrap-random-forests.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
