<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Remarks | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Remarks | Data Analytics">

<title>Remarks | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>â˜° Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="remarks-2" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="the-gini-index-versus-the-entropy" class="section level3 unnumbered">
<h3>The Gini index versus the entropy</h3>
<p>The <em>Gini index</em> plays the same role as the <em>entropy</em>. To see that, the following R code plots the Gini index and the entropy in a binary class problem<label for="tufte-sn-103" class="margin-toggle sidenote-number">103</label><input type="checkbox" id="tufte-sn-103" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">103</span> A binary class problem has two nominal parameters, <span class="math inline">\(p_{1}\)</span> and <span class="math inline">\(p_{2}\)</span>. Because <span class="math inline">\(p_{2}\)</span> equals <span class="math inline">\(1-p_{1}\)</span>, it is actually a one-parameter system, such that we can visualize how the Gini index and the entropy changes according to <span class="math inline">\(p_{1}\)</span>, as shown in Figure <a href="remarks-2.html#fig:f4-15">73</a>.</span>. Their similarity is evident as shown in Figure <a href="remarks-2.html#fig:f4-15">73</a>.</p>
<p>The following R scripts package the Gini index and entropy as two functions.</p>
<p></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="remarks-2.html#cb100-1" aria-hidden="true" tabindex="-1"></a>entropy <span class="ot">&lt;-</span> <span class="cf">function</span>(p_v) {</span>
<span id="cb100-2"><a href="remarks-2.html#cb100-2" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb100-3"><a href="remarks-2.html#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (p <span class="cf">in</span> p_v) {</span>
<span id="cb100-4"><a href="remarks-2.html#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (p <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb100-5"><a href="remarks-2.html#cb100-5" aria-hidden="true" tabindex="-1"></a>this_term <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb100-6"><a href="remarks-2.html#cb100-6" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb100-7"><a href="remarks-2.html#cb100-7" aria-hidden="true" tabindex="-1"></a>this_term <span class="ot">&lt;-</span> <span class="sc">-</span>p <span class="sc">*</span> <span class="fu">log2</span>(p)</span>
<span id="cb100-8"><a href="remarks-2.html#cb100-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb100-9"><a href="remarks-2.html#cb100-9" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> e <span class="sc">+</span> this_term</span>
<span id="cb100-10"><a href="remarks-2.html#cb100-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb100-11"><a href="remarks-2.html#cb100-11" aria-hidden="true" tabindex="-1"></a><span class="fu">return</span>(e)</span>
<span id="cb100-12"><a href="remarks-2.html#cb100-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="remarks-2.html#cb101-1" aria-hidden="true" tabindex="-1"></a>gini <span class="ot">&lt;-</span> <span class="cf">function</span>(p_v) {</span>
<span id="cb101-2"><a href="remarks-2.html#cb101-2" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb101-3"><a href="remarks-2.html#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (p <span class="cf">in</span> p_v) {</span>
<span id="cb101-4"><a href="remarks-2.html#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (p <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb101-5"><a href="remarks-2.html#cb101-5" aria-hidden="true" tabindex="-1"></a>this.term <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb101-6"><a href="remarks-2.html#cb101-6" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb101-7"><a href="remarks-2.html#cb101-7" aria-hidden="true" tabindex="-1"></a>this.term <span class="ot">&lt;-</span> p <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p)</span>
<span id="cb101-8"><a href="remarks-2.html#cb101-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb101-9"><a href="remarks-2.html#cb101-9" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> e <span class="sc">+</span> this.term</span>
<span id="cb101-10"><a href="remarks-2.html#cb101-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb101-11"><a href="remarks-2.html#cb101-11" aria-hidden="true" tabindex="-1"></a><span class="fu">return</span>(e)</span>
<span id="cb101-12"><a href="remarks-2.html#cb101-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p>The following R script draws Figure <a href="remarks-2.html#fig:f4-15">73</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-15"></span>
<img src="graphics/4_15.png" alt="Gini index vs. entropy" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 73: Gini index vs.Â entropy<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="remarks-2.html#cb102-1" aria-hidden="true" tabindex="-1"></a>entropy.v <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb102-2"><a href="remarks-2.html#cb102-2" aria-hidden="true" tabindex="-1"></a>gini.v <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb102-3"><a href="remarks-2.html#cb102-3" aria-hidden="true" tabindex="-1"></a>p.v <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb102-4"><a href="remarks-2.html#cb102-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (p <span class="cf">in</span> p.v) {</span>
<span id="cb102-5"><a href="remarks-2.html#cb102-5" aria-hidden="true" tabindex="-1"></a>entropy.v <span class="ot">&lt;-</span> <span class="fu">c</span>(entropy.v, (<span class="fu">entropy</span>(<span class="fu">c</span>(p, <span class="dv">1</span> <span class="sc">-</span> p))))</span>
<span id="cb102-6"><a href="remarks-2.html#cb102-6" aria-hidden="true" tabindex="-1"></a>gini.v <span class="ot">&lt;-</span> <span class="fu">c</span>(gini.v, (<span class="fu">gini</span>(<span class="fu">c</span>(p, <span class="dv">1</span> <span class="sc">-</span> p))))</span>
<span id="cb102-7"><a href="remarks-2.html#cb102-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb102-8"><a href="remarks-2.html#cb102-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p.v, gini.v, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb102-9"><a href="remarks-2.html#cb102-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;percentage of class 1&quot;</span>,<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb102-10"><a href="remarks-2.html#cb102-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;impurity measure&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>,</span>
<span id="cb102-11"><a href="remarks-2.html#cb102-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">cex.axis =</span> <span class="fl">1.5</span>, <span class="at">cex.main =</span> <span class="fl">1.5</span>,<span class="at">cex.sub =</span> <span class="fl">1.5</span>)</span>
<span id="cb102-12"><a href="remarks-2.html#cb102-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(p.v, entropy.v, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb102-13"><a href="remarks-2.html#cb102-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Entropy&quot;</span>, <span class="st">&quot;Gini index&quot;</span>),</span>
<span id="cb102-14"><a href="remarks-2.html#cb102-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="at">cex =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<p></p>
<p>It can be seen in Figure <a href="remarks-2.html#fig:f4-15">73</a> that the two impurity measures are similar. Both reach minimum, i.e., <span class="math inline">\(0\)</span>, when all the data instances belong to the same class, and they reach maximum when there are equal numbers of data instances for the two classes. In practice, they produce similar trees.</p>
</div>
<div id="why-random-forests-work" class="section level3 unnumbered">
<h3>Why random forests work</h3>
<p>A random forest model is inspirational because it shows that <em>randomness</em>, usually considered as a troublemaker, has a productive dimension. This seems to be counterintuitive. An explanation has been pointed out in numerous literature that the random forests, together with other models that are called <strong>ensemble learning</strong> models, could make a group of <strong>weak models</strong> come together to form a strong model.</p>
<p>For example, consider a random forest model with <span class="math inline">\(100\)</span> trees. Each tree is a <em>weak model</em> and its accuracy is <span class="math inline">\(0.6\)</span>. Assume that the trees are independent<label for="tufte-sn-104" class="margin-toggle sidenote-number">104</label><input type="checkbox" id="tufte-sn-104" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">104</span> I.e., the predictions of one tree provide no hint to guess the predictions of another tree.</span>, with <span class="math inline">\(100\)</span> trees the probability of the random forest model to predict correctly on any data point is <span class="math inline">\(0.97\)</span>, i.e.,</p>
<p><span class="math display">\[ 
\sum_{k=51}^{100} C(n, k) \times 0.6^{k} \times 0.4^{100-k} = 0.97. 
\]</span></p>
<p>This result is impressive, but donâ€™t forget the <em>assumption of the independence</em> between the trees. This assumption does not hold in reality in a strict sense; i.e., ideally, we hope to have an algorithm that can find many good models that perform well and are all different; but, if we build many models using one dataset, these models would more or less resemble each other. Particularly, when we solely focus on models that can achieve optimal performance, it is often that the identified models end up more or less the same. Responding to this dilemma, randomness (i.e., the use of Bootstrap to randomize choices of data instances and the use of random feature selection for building trees) is introduced into the model-building process to create diversity of the models. The dynamics between the degree of randomness, the performance of each individual model, and their difference, should be handled well. To develop the craft, you may have many practices and focus on driving this dynamics towards a collective good.</p>
</div>
<div id="variable-importance-by-random-forests" class="section level3 unnumbered">
<h3>Variable importance by random forests</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-18"></span>
<img src="graphics/8_18.png" alt="Tree \#1" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 74: Tree #1<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Recall that a random forest model consists of decision trees that are defined by splits on some variables. The splits provide information about variablesâ€™ importance. To explain this, consider the data example shown in Table <a href="remarks-2.html#tab:t8-3">12</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-3">Table 12: </span>Example of a dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Assume that a random forest model with two trees (i.e., shown in Figures <a href="remarks-2.html#fig:f8-18">74</a> and <a href="remarks-2.html#fig:f8-19">75</a>) is built on the dataset.</p>
<p>At split 1, the Gini gain for <span class="math inline">\(x_1\)</span> is:</p>
<p><span class="math display">\[0.375-0.5\times0-0.5\times0.5 = 0.125.\]</span></p>
<p>At split 2, the Gini gain for <span class="math inline">\(x_3\)</span> is:</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-19"></span>
<img src="graphics/8_19.png" alt="Tree \#2" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 75: Tree #2<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display">\[0.5-0.5\times0-0.5\times0 = 0.5.\]</span></p>
<p>At split 3, the Gini gain for <span class="math inline">\(x_2\)</span> is</p>
<p><span class="math display">\[0.5-0.25\times0 + 0.75\times0.44 = 0.17.\]</span></p>
<p>At split 4, the Gini gain for <span class="math inline">\(x_3\)</span> is</p>
<p><span class="math display">\[0.44-0.5\times0 + 0.5\times0 = 0.44.\]</span></p>
<p>Table <a href="remarks-2.html#tab:t8-scoresheet">13</a> summarizes the contributions of the variables in the splits. The total contribution can be used as the variableâ€™s importance score.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-scoresheet">Table 13: </span>Contributions of the variables in the splits</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID of splits</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.125\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.5\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.17\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.44\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(Total\)</span></td>
<td align="left"><span class="math inline">\(0.125\)</span></td>
<td align="left"><span class="math inline">\(0.17\)</span></td>
<td align="left"><span class="math inline">\(0.94\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The variableâ€™s importance score helps us identify the variables that have strong predictive values. The approach to obtain the variableâ€™s importance score is simple and effective, but it is not perfect. For example, if we revisit the example shown in Table <a href="remarks-2.html#tab:t8-3">12</a>, we may notice that <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are identical. What does this suggest to you<label for="tufte-sn-105" class="margin-toggle sidenote-number">105</label><input type="checkbox" id="tufte-sn-105" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">105</span> Interested readers may read this article: Deng, H. and Runger, G., <em>Gene selection with guided regularized random forest</em>, Pattern Recognition, Volume 46, Issue 12, Pages 3483-3489, 2013.</span>?</p>
</div>
<div id="partial-dependency-plot" class="section level3 unnumbered">
<h3>Partial dependency plot</h3>
<p>Variable importance scores indicate whether a variable is informative in predicting the outcome variable. It does not provide information about <em>how</em> the outcome variable is influenced by the variables. <strong>Partial dependency plot</strong> can be used to visualize the relationship between a predictor and the outcome variable, averaged on other predictors.</p>
<p></p>
<div class="figure"><span id="fig:f6-7"></span>
<p class="caption marginnote shownote">
Figure 76: Partial dependency plots of variables in random forests
</p>
<img src="graphics/6_7.png" alt="Partial dependency plots of variables in random forests" width="100%"  />
</div>
<p></p>
<p>We draw in Figure <a href="remarks-2.html#fig:f6-7">76</a> the partial dependency plots of two variables on the AD dataset. It is clear that the relationships between the outcome variable with both predictors are significant. And the orientation of both relationships is visualized by the plots.</p>
<p></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="remarks-2.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the partial dependency plots of variables in random forests </span></span>
<span id="cb103-2"><a href="remarks-2.html#cb103-2" aria-hidden="true" tabindex="-1"></a>randomForest<span class="sc">::</span><span class="fu">partialPlot</span>(rf, data, HippoNV, <span class="st">&quot;1&quot;</span>)</span>
<span id="cb103-3"><a href="remarks-2.html#cb103-3" aria-hidden="true" tabindex="-1"></a>randomForest<span class="sc">::</span><span class="fu">partialPlot</span>(rf, data, FDG, <span class="st">&quot;1&quot;</span>)</span></code></pre></div>
<p></p>
</div>
</div>
<p style="text-align: center;">
<a href="random-forests.html"><button class="btn btn-default">Previous</button></a>
<a href="exercises-2.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
