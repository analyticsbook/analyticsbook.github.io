<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Chapter 2. Abstraction: Regression &amp; Tree Models | Data Analytics: A Small Data Approach" />
<meta property="og:type" content="book" />


<meta property="og:description" content="This book is suitable for an introductory course of data analytics to help students understand some main statistical learning models, such as linear regression, logistic regression, tree models and random forests, ensemble learning, sparse learning, principal component analysis, kernel methods including the support vector machine and kernel regression, etc. Data science practice is a process that should be told as a story, rather than a one-time implementation of one single model. This process is a main focus of this book, with many course materials about exploratory data analysis, residual analysis, and flowcharts to develop and validate models and data pipelines." />


<meta name="author" content="Shuai Huang &amp; Houtao Deng" />

<meta name="date" content="2021-04-16" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="This book is suitable for an introductory course of data analytics to help students understand some main statistical learning models, such as linear regression, logistic regression, tree models and random forests, ensemble learning, sparse learning, principal component analysis, kernel methods including the support vector machine and kernel regression, etc. Data science practice is a process that should be told as a story, rather than a one-time implementation of one single model. This process is a main focus of this book, with many course materials about exploratory data analysis, residual analysis, and flowcharts to develop and validate models and data pipelines.">

<title>Chapter 2. Abstraction: Regression &amp; Tree Models | Data Analytics: A Small Data Approach</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<meta name="description" content="My awesome presentation"/>
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#cover">Cover</a></li>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="chapter-2.-abstraction-regression-tree-models" class="section level1 unnumbered">
<h1>Chapter 2. Abstraction: Regression &amp; Tree Models</h1>
<div id="overview" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 2 is about <em>Abstraction</em>. It concerns how we model and formulate a problem using <em>specific mathematical models</em>. Abstraction is powerful. It begins with identification of a few main entities from the problem, and continues to characterize their relationships. Then we focus on the study of these interconnected entities as a pure mathematical system. Consequences can be analytically established within this abstracted framework, while a phenomenon in a concerned context could be identified as special instances, or manifestations, of the abstracted model. In other words, by making abstraction of a real-world problem, we free ourselves from the application context that is usually unbounded and not well defined.</p>
<p>People often adopt a blackbox view of a real-world problem, as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-1">3</a>. There is one (or more) key performance metrics of the system, called the output variable<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> Denoted as <span class="math inline">\(y\)</span>, e.g., the yield of a chemical process, the mortality rate of an ICU, the GDP of a nation, etc.</span>, and there is a set of input variables<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> Denoted as <span class="math inline">\(x_{1}, x_{2}, \ldots, x_{p}\)</span>; also called predictors, covariates, features, and, sometimes, factors.</span> that may help us predict the output variable. These variables are the <em>few main entities</em> identified from the problem, and how the input variables impact the output variable is <em>one</em> main type of relationship we develop models to characterize.</p>
<!--

 \small

<div class="figure">
<p class="caption">(\#fig:unnamed-chunk-4)\label{fig:2-1} The blackbox nature of many data science problems</p><img src="graphics/2_1.png" alt="\label{fig:2-1} The blackbox nature of many data science problems"  /></div>

 \normalsize
-->
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-1"></span>
<p class="caption marginnote shownote">
Figure 3: The blackbox nature of many data science problems
</p>
<img src="graphics/2_1.png" alt="The blackbox nature of many data science problems" width="80%"  />
</div>
<p></p>
<p>These relationships are usually unknown, due to our lack of understanding of the system. It is not always plausible or economically feasible to develop a Newtonian style characterization of the system<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> I.e., using differential equations.</span>. Thus, statistical models are needed. They collect data from this blackbox system and build models to characterize the relationship between the input variables and the output variable. Generally, there are two cultures for statistical modeling<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> Breiman, L., * Statistical Modeling: The Two Cultures,* Statistical Science, Volume 16, Issue 3, 199-231, 2001.</span>: One is the <strong>data modeling</strong> culture, while another is the <strong>algorithmic modeling</strong> culture. Linear regression models are examples of the <em>data modeling</em> culture; decision tree models are examples of the <em>algorithmic modeling</em> culture.</p>
<p>Two goals are shared by the two cultures: (1) to understand the relationships between the predictors and the output, and (2) to predict the output based on the predictors. The two also share some common criteria to evaluate the success of their models, such as the prediction performance. Another commonality they share is a generic form of their models</p>
<p><span class="math display" id="eq:ch2-genericmodel">\[\begin{equation}
\small
    y=f(\boldsymbol{x})+\epsilon,
\tag{1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f(\boldsymbol{x})\)</span> reflects the <em>signal</em> part of <span class="math inline">\(y\)</span> that can be ascertained by knowing <span class="math inline">\(\boldsymbol{x}\)</span>, and <span class="math inline">\(\epsilon\)</span> reflects the <em>noise</em> part of <span class="math inline">\(y\)</span> that remains uncertain even when we know <span class="math inline">\(x\)</span>. To better illustrate this, we could annotate the model form in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:ch2-genericmodel">(1)</a> as<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> An interesting book about the antagonism between signal and noise: Silver, N., <em>The Signal and the Noise: Why So Many Predictions Fail–but Some Don’t</em>, Penguin Books, 2015. The author’s prediction model, however, failed to predict Donald Trump’s victory of the 2016 US Election.</span></p>
<p><span class="math display" id="eq:2-genericmodel">\[\begin{equation}
\small
    \underbrace{y}_{data} = \underbrace{f(\boldsymbol{x})}_{signal} + \underbrace{\epsilon}_{noise},
\tag{2}
\end{equation}\]</span></p>
<p>The two cultures differ in their ideas about how to model these two parts. A brief illustration is shown in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-1">1</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-1">Table 1: </span>Comparison between the two cultures of models</span><!--</caption>--></p>
<table>
<colgroup>
<col width="12%" />
<col width="19%" />
<col width="33%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left"><span class="math inline">\(f(\boldsymbol{x})\)</span></th>
<th align="left"><span class="math inline">\(\epsilon\)</span></th>
<th align="left">Ideology</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Data Modeling</strong></td>
<td align="left">Explicit form (e.g., linear regression).</td>
<td align="left">Statistical distribution (e.g., Gaussian).</td>
<td align="left">Imply <em>Cause</em> and <em>effect</em>; uncertainty has a structure.</td>
</tr>
<tr class="even">
<td align="left"><strong>Algorithmic Modeling</strong></td>
<td align="left">Implicit form (e.g., tree model).</td>
<td align="left">Rarely modeled as structured uncertainty; taken as meaningless noise.</td>
<td align="left">More focus on prediction; to <em>fit</em> data rather than to <em>explain</em> data.</td>
</tr>
</tbody>
</table>
<p></p>
<p>An illustration of the <em>data modeling</em>, using linear regression model, is shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-datamodel">4</a>. To develop such a model, we need efforts in two endeavors: the modeling of the signal, and the modeling of the noise (also called errors). It was probably the modeling of the errors, rather than the modeling of the signal, that eventually established a science: Statistics<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> Errors, as the name suggests, are embarrassment to a theory that claims to be rational. Errors are irrational, like a crack on the smooth surface of rationality. But rationally, if we could find <em>a law of errors</em>, we then find the law of irrationality. With that, once again rationality trumps irrationality, and the crack is sealed.</span>.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f2-datamodel"></span>
<img src="graphics/2_datamodel.png" alt="Illustration of the *ideology* of data modeling, i.e., data is used to calibrate, or, estimate, the parameters of a pre-specified mathematical structure" width="80%"  />
<p class="caption marginnote shownote">
Figure 4: Illustration of the <em>ideology</em> of data modeling, i.e., data is used to calibrate, or, estimate, the parameters of a pre-specified mathematical structure
</p>
</div>
<p></p>
<p>One only needs to take a look at the beautiful form of the normal distribution (and notice the name as well) to have an impression of its grand status as the law of errors. Comparing with other candidate forms that historically were its competitors, this concentrated, symmetrical, round and smooth form seems a more rational form that a law should take, i.e., see Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-errorlaws">5</a>.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f2-errorlaws"></span>
<img src="graphics/2_errorlaws.png" alt="Hypothesized laws of errors, including the normal distribution (also called the Gaussian distribution, developed by Gauss in 1809) and some of its old rivalries" width="80%"  />
<p class="caption marginnote shownote">
Figure 5: Hypothesized laws of errors, including the normal distribution (also called the Gaussian distribution, developed by Gauss in 1809) and some of its old rivalries
</p>
</div>
<p></p>
<p>The <span class="math inline">\(\epsilon\)</span> in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:ch2-genericmodel">(1)</a> is often called the <strong>error term</strong>, noise term, or residual term. <span class="math inline">\(\epsilon\)</span> is usually modeled as a Gaussian distribution with mean as <span class="math inline">\(0\)</span>. The mean has to be <span class="math inline">\(0\)</span>; otherwise, it contradicts with the name <em>error</em>. <span class="math inline">\(f(\boldsymbol{x})\)</span> is also called the model of the mean structure<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> To see that, notice that <span class="math inline">\(\mathrm{E}{(y)} = \mathrm{E}{[f(\boldsymbol{x}) + \epsilon]} = \mathrm{E}{[f(\boldsymbol{x})]} + \mathrm{E}{[\epsilon]}\)</span>. Since <span class="math inline">\(\mathrm{E}{(\epsilon)} = 0\)</span> and <span class="math inline">\(f(\boldsymbol{x})\)</span> is not a random variable, we have <span class="math inline">\(\mathrm{E}{(y)} = f(\boldsymbol{x})\)</span>. Thus, <span class="math inline">\(f(\boldsymbol{x})\)</span> essentially predicts the mean of the output variable.</span>.</p>
</div>
<div id="regression-models" class="section level2 unnumbered">
<h2>Regression models</h2>
<div id="rationale-and-formulation" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>Let’s consider a simple regression model, where there is only one predictor <span class="math inline">\(x\)</span> to predict the outcome <span class="math inline">\(y\)</span>. Linear regression model assumes a linear form of <span class="math inline">\(f(x)\)</span></p>
<p><span class="math display" id="eq:2-simLR-fx">\[\begin{equation}
\small
f(x)=\beta_{0}+\beta_{1} x ,
\tag{3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is called the <strong>intercept</strong>, and <span class="math inline">\(\beta_1\)</span> is called the <strong>slope</strong>. Both are also called <strong>regression coefficients</strong>, or more generally, <strong>parameters</strong>.</p>
<p>And <span class="math inline">\(\epsilon\)</span> is modeled as a normal distribution<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> I.e., could be other types of distributions, but normal distribution is the norm.</span> with mean <span class="math inline">\(0\)</span>,</p>
<p><span class="math display" id="eq:2-simLR-eps">\[\begin{equation}
\small
\epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right),
\tag{4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> is the <strong>variance</strong> of the error.</p>
<p>For any given value of <span class="math inline">\(x\)</span>, we know the model of <span class="math inline">\(y\)</span> is</p>
<p><span class="math display" id="eq:2-simLR-y">\[\begin{equation}
\small
y = \beta_{0}+\beta_{1}x + \epsilon.
\tag{5}
\end{equation}\]</span></p>
<p>As Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-lrpred">6</a> reveals, in linear regression model, <span class="math inline">\(y\)</span> is not modeled as a numerical value, but as a distribution. In other words, <span class="math inline">\(y\)</span> itself is treated as a random variable. Its distribution’s mean is modeled by <span class="math inline">\(x\)</span> and the variance is <em>inherited</em> from <span class="math inline">\(\epsilon\)</span>. Knowing the value of <span class="math inline">\(x\)</span> helps us to determine the <em>location</em> of this distribution, but not the <em>shape</em>—the shape is always fixed.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-lrpred"></span>
<p class="caption marginnote shownote">
Figure 6: In a linear regression model, <span class="math inline">\(y\)</span> is modeled as a distribution as well
</p>
<img src="graphics/2_lrpred.png" alt="In a linear regression model, $y$ is modeled as a distribution as well" width="80%"  />
</div>
<p></p>
<p>To make a prediction of <span class="math inline">\(y\)</span> for any given <span class="math inline">\(x\)</span>, <span class="math inline">\(\beta_{0}+\beta_{1}x\)</span> comes as a natural choice. It is too natural that it is often unnoticed or unquestioned. Nonetheless, to predict a random variable, using its mean is the “best” choice, but it is not the only possibility, as Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-lrpred">6</a> reveals that <span class="math inline">\(y\)</span> itself is a random variable, and to predict a random variable, we could also use a confidence interval instead of a point estimate. It depends on what you’d like to predict. If the goal is to predict what is the most likely value for <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>, then the best guess is <span class="math inline">\(\beta_{0}+\beta_{1}x\)</span>.<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> An important job for statisticians is to prove some ideas are our best choices, i.e., by showing that these choices are optimal decisions under some specific conditions (accurately defined by mathematical terms). It is often that intuitions come before proofs, so many theories are actually developed retrospectively.</span></p>
<!-- \begin{figure} -->
<!--     \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width=0.95\textwidth]{graphics/2_lrpred.png} -->
<!--    \caption{} -->
<!--    \label{fig:2-lrpred} -->
<!-- \end{figure} -->
<p>There are more assumptions that have been made to enable the model in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-y">(5)</a>.</p>
<p><!-- begin{itemize} --></p>
<ul>
<li>There is a linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. And this linear relationship remains the same for all the values of <span class="math inline">\(x\)</span>. This is often referred to as a <em>global</em> relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Sometimes this assumption is considered strong, e.g., as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-2">7</a>, in drug research it is often found that the dose (<span class="math inline">\(x\)</span>) is related to the effect of the drug (<span class="math inline">\(y\)</span>) in a varying manner that depends on the value of <span class="math inline">\(x\)</span>. Still, from Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-2">7</a> we can see that the linear line captures an essential component in the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, providing a good statistical approximation. Regression models that capture <em>locality</em> in the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are introduced in <strong>Chapter 9</strong>.</li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-2"></span>
<img src="graphics/2_2.png" alt="Complex relationship between dose ($x$) and drug response ($y$), while the linear line does provide a good statistical approximation" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 7: Complex relationship between dose (<span class="math inline">\(x\)</span>) and drug response (<span class="math inline">\(y\)</span>), while the linear line does provide a good statistical approximation<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<ul>
<li>The model acknowledges a degree of unpredictability of <span class="math inline">\(y\)</span>. Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-y">(5)</a> indicates that <span class="math inline">\(y\)</span> is generated by a combination of the signal (i.e., <span class="math inline">\(\beta_{0}+\beta_{1}x\)</span>) and the noise (i.e., <span class="math inline">\(\epsilon\)</span>). Since we could never predict noise, we compute a metric called <strong>R-squared</strong> to quantify the predictability of a model</li>
</ul>
<p><span class="math display" id="eq:2-R2">\[\begin{equation}
\small
        \text{R-squared} = \frac{\sigma_{y}^{2}-\sigma_{\varepsilon}^{2}}{\sigma_{y}^{2}}.
\tag{6}
    \end{equation}\]</span>
Here, <span class="math inline">\(\sigma_{y}^{2}\)</span> is the variance of <span class="math inline">\(y\)</span>. The <em>R-squared</em> ranges from <span class="math inline">\(0\)</span> (zero predictability) to <span class="math inline">\(1\)</span> (perfect predictability).</p>
<ul>
<li>The <em>significance</em> of <span class="math inline">\(x\)</span> in predicting <span class="math inline">\(y\)</span>, and the <em>accuracy</em> of <span class="math inline">\(x\)</span> in predicting <span class="math inline">\(y\)</span>, are two different concepts. A predictor <span class="math inline">\(x\)</span> could be inadequate in predicting <span class="math inline">\(y\)</span>, i.e., the R-squared could be as low as <span class="math inline">\(0.1\)</span>, but it still could be statistically significant. In other words, the relation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is not strong, but it is not spurious either. This often happens in social science research and education research projects. Some scenarios are shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-signvsaccu">8</a>.</li>
</ul>
<p></p>
<div class="figure fullwidth"><span id="fig:f2-signvsaccu"></span>
<img src="graphics/2_fourtypes.png" alt="Significance vs. accuracy" width="80%"  />
<p class="caption marginnote shownote">
Figure 8: Significance vs. accuracy
</p>
</div>
<p></p>
<ul>
<li>The noise is usually modeled as a normal distribution, but this assumption could be relaxed. A detailed discussion about how to check the normality assumption in data analysis can be found in <strong>Chapter 5</strong>.</li>
</ul>
<p><!-- end{itemize} --></p>
</div>
<div id="theory-and-method" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p><em>Parameter estimation.</em> To estimate a model is to estimate its parameters, i.e., for the model shown in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-y">(5)</a>, unknown parameters include <span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span>, and <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>. Usually, we estimate the regression coefficients first. Then, as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-datamodel">4</a>, errors could be computed, and further, <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> could be estimated<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> I.e., as a standard practice of sample variance estimation by taking the residuals (i.e., <span class="math inline">\(\epsilon_1\)</span>, <span class="math inline">\(\epsilon_2\)</span> and <span class="math inline">\(\epsilon_3\)</span>) as <em>samples</em> of the population of <em>error</em>.</span>.</p>
<p>A training dataset is collected to estimate the parameters. The basic idea is that the best estimate should lead to a line, as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-datamodel">4</a>, that fits the training data as close as possible. To quantify this quality of fitness of a line, two principles are shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-3">9</a>: one based on perpendicular offset (left), while another one based on vertical offset (right). History of statistics has chosen the vertical offset as a more favorable approach, since it leads to tractability in analytic forms<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> When there were no computers yet, analytic tractability was, and still is, held as a sacred quality of a model.</span>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-3"></span>
<p class="caption marginnote shownote">
Figure 9: Two principles to fit a linear regression model: (left) perpendicular offsets; (right) vertical offsets. The distances between the dots (the training data) with the line (the trained model) provide a quantitative metric of how well the model fits the data.
</p>
<img src="graphics/2_3.png" alt="Two principles to fit a linear regression model: (left) perpendicular offsets; (right) vertical offsets. The distances between the dots (the training data) with the line (the trained model) provide a quantitative metric of how well the model fits the data." width="60%"  />
</div>
<p></p>
<p>The principle of minimizing vertical offsets leads to the <strong>least-squares estimation</strong> of linear regression models. We can exercise the least squares estimation using the simple regression model shown in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-y">(5)</a>. The objective, based on the principle suggested in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-3">9</a> (right), is to find the line that <strong>minimizes</strong> the <strong>sum of the squared</strong> of the vertical derivations of the observed data points from the line.</p>
<p>Suppose that we have collected <span class="math inline">\(N\)</span> data points, denoted as, <span class="math inline">\(\left(x_{n}, y_{n}\right)\)</span> for <span class="math inline">\(n=1,2, \dots, N\)</span>.<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> Data is paired, i.e., <span class="math inline">\(y_{n}\)</span> corresponds to <span class="math inline">\(x_{n}\)</span>.</span> For each data point, i.e., the <span class="math inline">\(n_{th}\)</span> data point, the residual <span class="math inline">\(\epsilon_{n}\)</span> is defined as</p>
<p><span class="math display" id="eq:2-simLR-res">\[\begin{equation}
\small
\epsilon_{n} = y_{n}-\left(\beta_{0}+\beta_{1} x_{n}\right).
\tag{7}
\end{equation}\]</span></p>
<p>Then, we define the sum of the squared of the vertical derivations of the observed data points from the line as</p>
<p><span class="math display" id="eq:2-simLR-LS">\[\begin{equation}
\small
l\left(\beta_{0}, \beta_{1}\right)=\sum_{n=1}^{N}\epsilon_{n}^2.
\tag{8}
\end{equation}\]</span></p>
<p>Plugging Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-res">(7)</a> in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-LS">(8)</a> we have</p>
<p><span class="math display" id="eq:2-simLR-LS-2">\[\begin{equation}
\small
l\left(\beta_{0}, \beta_{1}\right)=\sum_{n=1}^{N}\left[y_{n}-\left(\beta_{0}+\beta_{1} x_{n}\right)\right]^{2}.
\tag{9}
\end{equation}\]</span></p>
<p>To estimate <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> is to minimize this least squares <strong>loss function</strong> <span class="math inline">\(l\left(\beta_{0}, \beta_{1}\right)\)</span>. This is an <strong>unconstrained continuous optimization</strong> problem. We take derivatives of <span class="math inline">\(l\left(\beta_{0}, \beta_{1}\right)\)</span> regarding the two parameters and set them to be zero, to derive the estimation equations—this is a common practice of the <strong>First Derivative Test</strong>, illustrated in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-1stderivativetest">10</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-1stderivativetest"></span>
<img src="graphics/2_1stderivativetest.png" alt="Illustration of the **First Derivative Test** in optimization, i.e., the optimal solution would lead the first derivative to be zero. It is widely used in statistics and machine learning to find optimal solutions of some model formulations. More applications of this technique can be found in later chapters." width="100%"  />
<!--
<p class="caption marginnote">-->Figure 10: Illustration of the <strong>First Derivative Test</strong> in optimization, i.e., the optimal solution would lead the first derivative to be zero. It is widely used in statistics and machine learning to find optimal solutions of some model formulations. More applications of this technique can be found in later chapters.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\frac{\partial l\left(\beta_{0}, \beta_{1}\right)}{\partial \beta_{0}}=-2 \sum_{n=1}^{N}\left[y_{n}-\left(\beta_{0}+\beta_{1} x_{n}\right)\right]=0,
 
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
\small
  
\frac{\partial l\left(\beta_{0}, \beta_{1}\right)}{\partial \beta_{1}}=-2 \sum_{n=1}^{N} x_{n}\left[y_{n}-\left(\beta_{0}+\beta_{1} x_{n}\right)\right]=0.
 
\end{equation*}\]</span></p>
<p>These two could be rewritten in a more succinct way</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\left[ \begin{array}{cc}{N} &amp; {\sum_{n=1}^{N} x_{n}} \\ {\sum_{n=1}^{N} x_{n}} &amp; {\sum_{n=1}^{N} x_{n}^{2}}\end{array}\right] \left[ \begin{array}{c}{\beta_{0}} \\ {\beta_{1}}\end{array}\right]=\left[ \begin{array}{c}{\sum_{n=1}^{N} y_{n}} \\ {\sum_{n=1}^{N} x_{n} y_{n}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p>We solve these two equations and derive the estimators of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>, denoted as <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>, respectively, as</p>
<p><span class="math display" id="eq:2-beta-hat-scalar">\[\begin{equation}
\small
    \begin{aligned}
    &amp;\hat{\beta}_{1}=\frac{\sum_{n=1}^{N}\left(x_{n}-\overline{x}\right)\left(y_{n}-\overline{y}\right)}{\sum_{n=1}^{N} x_{n}^{2}-N \overline{x}^{2}}, \\
    &amp;\hat{\beta}_{0}= \overline{y} - \hat{\beta}_{1} \overline{x}.
    \end{aligned}
\tag{10}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\overline{x}\)</span> and <span class="math inline">\(\overline{y}\)</span> are the sample mean of the two variables, respectively.</p>
<p>There is a structure hidden inside Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-beta-hat-scalar">(10)</a>. Note that the estimator <span class="math inline">\(\hat{\beta}_{1}\)</span> can be rewritten as</p>
<p><span class="math display" id="eq:2-beta1hat">\[\begin{equation}
\small
\hat{\beta}_{1}=\frac{\sum_{n=1}^{N}\left(x_{n}-\overline{x}\right)\left(y_{n}-\overline{y}\right)}{N-1} \Big/ \frac{\sum_{n=1}^{N} x_{n}^{2}-N \overline{x}^{2}}{N-1},
\tag{11}
\end{equation}\]</span></p>
<p>and note that the sample variance of <span class="math inline">\(x\)</span> is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\operatorname{var}(x)=\frac{\sum_{n=1}^{N} x_{n}^{2}-N \overline{x}^{2}}{N-1},
 
\end{equation*}\]</span></p>
<p>while the numerator in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-beta1hat">(11)</a> is called the <strong>sample covariance</strong><label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> The covariance is a measure of the joint variability of two random variables. Denoted as <span class="math inline">\(\operatorname{cov}(x, y)\)</span>, the larger the covariance, the stronger the two variables interact.</span>.</p>
<p>Thus, we can <em>re</em>write the estimators of <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{0}\)</span> as</p>
<p><span class="math display" id="eq:2-simLR-LSE">\[\begin{equation}
\small
    \begin{aligned}
    &amp;\hat{\beta}_{1}=\frac{\operatorname{cov}(x, y)}{\operatorname{var}(x)}, \\
    &amp;\hat{\beta}_{0} = \overline{y} - \hat{\beta}_{1} \overline{x}.
    \end{aligned}
\tag{12}
\end{equation}\]</span></p>
<p><em>A small data example.</em> Let’s practice the estimation method using a simple example. The dataset is shown in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-1ex">2</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-1ex">Table 2: </span>An example dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x\)</span></th>
<th align="left"><span class="math inline">\(1\)</span></th>
<th align="left"><span class="math inline">\(3\)</span></th>
<th align="left"><span class="math inline">\(3\)</span></th>
<th align="left"><span class="math inline">\(5\)</span></th>
<th align="left"><span class="math inline">\(5\)</span></th>
<th align="left"><span class="math inline">\(6\)</span></th>
<th align="left"><span class="math inline">\(8\)</span></th>
<th align="left"><span class="math inline">\(9\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(y\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(8\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Following Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-beta-hat-scalar">(10)</a> we can get <span class="math inline">\(\beta_0 = -1.0714\)</span> and <span class="math inline">\(\beta_1 = 1.2143\)</span>. The R codes to verify your calculation are shown below.</p>
<p></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple example of regression with one predictor</span></span>
<span id="cb1-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb1-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>),<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>),</span>
<span id="cb1-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb1-3" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">4</span>),<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>),<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">5</span>),</span>
<span id="cb1-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb1-4" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">7</span>),<span class="fu">c</span>(<span class="dv">9</span>,<span class="dv">8</span>)))</span>
<span id="cb1-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(data) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;Y&quot;</span>,<span class="st">&quot;X&quot;</span>)</span>
<span id="cb1-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(data)</span>
<span id="cb1-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb1-7" aria-hidden="true" tabindex="-1"></a>lm.YX <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data =</span> data)</span>
<span id="cb1-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.YX)</span></code></pre></div>
<p></p>
<p><em>Extension to multivariate regression model.</em> Consider a more general case where there are more than one predictor</p>
<p><span class="math display" id="eq:2-multiLR">\[\begin{equation}
\small
    y=\beta_{0}+\sum_{i=1}^{p} \beta_{i} x_{i}+\varepsilon.
\tag{13}
\end{equation}\]</span></p>
<p>To fit this multivariate linear regression model with <span class="math inline">\(p\)</span> predictors, we collect <span class="math inline">\(N\)</span> data points, denoted as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{y}=\left[ \begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{N}}\end{array}\right], \text {     }  \boldsymbol{X}=\left[ \begin{array}{ccccc}{1} &amp; {x_{11}} &amp; {x_{21}} &amp; {\cdots} &amp; {x_{p 1}} \\ {1} &amp; {x_{12}} &amp; {x_{22}} &amp; {\cdots} &amp; {x_{p 2}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {1} &amp; {x_{1 N}} &amp; {x_{2 N}} &amp; {\cdots} &amp; {x_{p N}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{y} \in R^{N \times 1}\)</span> denotes for the <span class="math inline">\(N\)</span> measurements of the outcome variable, and <span class="math inline">\(\boldsymbol{X} \in R^{N \times(p+1)}\)</span> denotes for the data matrix that includes the <span class="math inline">\(N\)</span> measurements of the <span class="math inline">\(p\)</span> input variables and the intercept term, <span class="math inline">\(\beta_{0}\)</span>, i.e., the first column of <span class="math inline">\(\boldsymbol{X}\)</span> corresponds to <span class="math inline">\(\beta_{0}\)</span>.<label for="tufte-sn-18" class="margin-toggle sidenote-number">18</label><input type="checkbox" id="tufte-sn-18" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">18</span> Again, the data is paired, i.e., <span class="math inline">\(y_{n}\)</span> corresponds to <span class="math inline">\(\boldsymbol{x}_n\)</span> that is the <span class="math inline">\(n_{th}\)</span> row of the matrix <span class="math inline">\(\boldsymbol{X}\)</span>.</span></p>
<p>To estimate the regression coefficients in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR">(13)</a>, again, we use the least squares estimation method. The first step is to calculate the sum of the squared of the vertical derivations of the observed data points from “the line”<label for="tufte-sn-19" class="margin-toggle sidenote-number">19</label><input type="checkbox" id="tufte-sn-19" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">19</span> Here, actually, a hyperplane.</span>. Following Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-res">(7)</a>, we can define the residual as</p>
<p><span class="math display" id="eq:2-multiLR-res">\[\begin{equation}
\small
\epsilon_{n} = y_n - \left(\beta_{0}+\sum_{i=1}^{p} \beta_{i} x_{in}\right).
\tag{14}
\end{equation}\]</span></p>
<p>Then, following Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-LS">(8)</a>, the sum of the squared of the vertical derivations of the observed data points from “the line” is</p>
<p><span class="math display" id="eq:2-multiLR-LS">\[\begin{equation}
\small
l\left(\beta_{0}, ...,  \beta_{p}\right)=\sum_{n=1}^{N}\epsilon_{n}^2.
\tag{15}
\end{equation}\]</span></p>
<p>This is again an unconstrained continuous optimization problem, that could be solved by the same procedure we have done for the simple linear regression model. Here, we show how a vector-/matrix-based representation of this derivation process could make things easier.</p>
<p>Let’s write up the regression coefficients and residuals in vector forms as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\beta}=\left[ \begin{array}{c}{\beta_{0}} \\ {\beta_{1}} \\ {\vdots} \\ {\beta_{p}}\end{array}\right], \text { and } \boldsymbol{\varepsilon}=\left[ \begin{array}{c}{\varepsilon_{1}} \\ {\varepsilon_{2}} \\ {\vdots} \\ {\varepsilon_{N}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol{\beta} \in R^{(p+1) \times 1}\)</span> denotes for the regression parameters and <span class="math inline">\(\boldsymbol{\varepsilon} \in R^{N \times 1}\)</span> denotes for the <span class="math inline">\(N\)</span> residuals which are assumed to follow a normal distribution with mean as zero and variance as <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>.</p>
<p>Then, based on Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR-res">(14)</a>, we rewrite <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> as
<span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\varepsilon} = \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}.
 
\end{equation*}\]</span></p>
<p>Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR-LS">(15)</a> could be rewritten as</p>
<p><span class="math display" id="eq:2-multiLR-LS-matrix">\[\begin{equation}
\small
l(\boldsymbol{\beta})=(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta}).
\tag{16}
\end{equation}\]</span></p>
<p>To estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> is to solve the optimization problem</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\min _{\boldsymbol{\beta}}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta}).
 
\end{equation*}\]</span></p>
<p>To solve this problem, we can take the gradients of the objective function regarding <span class="math inline">\(\boldsymbol{\beta}\)</span> and set them to be zero</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\frac{\partial(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=0,
 
\end{equation*}\]</span></p>
<p>which gives rise to the equation</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{X}^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})=0.
 
\end{equation*}\]</span></p>
<p>This leads to the <strong>least squares estimator</strong> of <span class="math inline">\(\boldsymbol{\beta}\)</span> as</p>
<p><span class="math display" id="eq:2-multiLR-LSE">\[\begin{equation}
\small
  \widehat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y}.
\tag{17}
\end{equation}\]</span></p>
<p>A resemblance can be easily detected between the estimator in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR-LSE">(17)</a> with Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-LSE">(12)</a>, by noticing that <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{y}\)</span> reflects the correlation<label for="tufte-sn-20" class="margin-toggle sidenote-number">20</label><input type="checkbox" id="tufte-sn-20" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">20</span> I.e., corresponds to <span class="math inline">\(\operatorname{cov}(x, y)\)</span>.</span> between predictors and output, and <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> reflects the variability<label for="tufte-sn-21" class="margin-toggle sidenote-number">21</label><input type="checkbox" id="tufte-sn-21" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">21</span> I.e., corresponds to <span class="math inline">\(\operatorname{var}(x)\)</span>.</span> of the predictors.</p>
<p>Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR-LSE">(17)</a> may come as a surprise to some readers. The regression coefficients, <span class="math inline">\(\boldsymbol{\beta}\)</span>, by their definition, are supposed to only characterize the relationship between <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(y\)</span>. However, from Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR-LSE">(17)</a>, it is clear that the variability of <span class="math inline">\(\boldsymbol{x}\)</span> matters. This is not a contradiction. <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> are <em>two</em> different entities: <span class="math inline">\(\boldsymbol{\beta}\)</span> is a theoretical concept, while <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is a statistical estimate. Statisticians have established theories<label for="tufte-sn-22" class="margin-toggle sidenote-number">22</label><input type="checkbox" id="tufte-sn-22" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">22</span> E.g, interested readers may read this book: Ravishanker, N. and Dey, D.K., <em>A First Course in Linear Model Theory</em>, Chapman &amp; Hall/CRC, 2001.</span> to study how well <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> estimates <span class="math inline">\(\boldsymbol{\beta}\)</span>. From Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR-LSE">(17)</a>, it is clear that where we observe the linear system<label for="tufte-sn-23" class="margin-toggle sidenote-number">23</label><input type="checkbox" id="tufte-sn-23" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">23</span> I.e., from which <span class="math inline">\(\boldsymbol{x}\)</span> we take measurement of <span class="math inline">\(y\)</span>’s.</span> matters to the modeling of the system. This is one main motivation of the area called the <strong>Design of Experiments</strong> that aims to identify the best locations of <span class="math inline">\(\boldsymbol{x}\)</span> from which we collect observations of the outcome variable, in order to achieve the best parameter estimation results.</p>
<p>By generalizing the result in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-lrpred">6</a> on the multivariate regression, we can see that <span class="math inline">\(\boldsymbol{y}\)</span> is a random vector<label for="tufte-sn-24" class="margin-toggle sidenote-number">24</label><input type="checkbox" id="tufte-sn-24" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">24</span> “MVN” stands for Multivariate Normal Distribution. See <strong>Appendix</strong> for background knowledge on MVN.</span>,</p>
<p><span class="math display">\[\begin{equation}
    \boldsymbol{y} \sim \text{MVN}\left(\boldsymbol{X}^{T}\boldsymbol{\beta},\sigma_{\varepsilon}^{2} \boldsymbol{I}\right).
\end{equation}\]</span></p>
<p>And <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, as shown in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR-LSE">(17)</a>, is essentially a <em>function</em> of <span class="math inline">\(\boldsymbol{y}\)</span>. Thus, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is a random vector as well. In other words, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> has a distribution. Because of the normality of <span class="math inline">\(\boldsymbol{y}\)</span>, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is also distributed as a normal distribution.</p>
<p>The mean of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is <span class="math inline">\(\boldsymbol{\beta}\)</span>, because</p>
<p>And the covariance matrix of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is</p>
<p>Because</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\operatorname{cov}(\boldsymbol{y}) = \sigma_{\varepsilon}^{2} \boldsymbol{I},
 
\end{equation*}\]</span></p>
<p>we have</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\operatorname{cov}(\widehat{\boldsymbol{\beta}}) =
\sigma_{\varepsilon}^{2}\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1}.
 
\end{equation*}\]</span></p>
<p>Thus, we have derived that</p>
<p><span class="math display" id="eq:2-betaDist-matrix">\[\begin{equation}
\small
    \boldsymbol{y} \sim \text{MVN}\left(\boldsymbol{X}^{T}\boldsymbol{\beta},\sigma_{\varepsilon}^{2} \boldsymbol{I}\right)  \Rightarrow \widehat{\boldsymbol{\beta}} \sim \text{MVN}\left[\boldsymbol{\beta},\sigma_{\varepsilon}^{2} \left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1}\right].
\tag{18}
\end{equation}\]</span></p>
<!-- \begin{equation} -->
<!--     \boldsymbol{y} \sim \text{MVN}\left(\boldsymbol{X}^{T}\boldsymbol{\beta}},\sigma_{\varepsilon}^{2} \boldsymbol{I}\right)  \Rightarrow \widehat{\boldsymbol{\beta}} \sim \text{MVN}\left(\boldsymbol{\beta}},\sigma_{\varepsilon}^{2}\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1}\right). -->
<!-- \end{equation} -->
<p>For each individual parameter <span class="math inline">\(\beta_i\)</span>, we can infer that</p>
<p><span class="math display" id="eq:2-betaDist">\[\begin{equation}
\small
    \hat{\beta}_{i} \sim N\left(\beta_{i}, \frac{\sigma_{\varepsilon}^{2}}{\boldsymbol{x}_{i}^T \boldsymbol{x}_{i}}\right)
\tag{19}
\end{equation}\]</span></p>
<p><em>Hypothesis testing of regression parameters.</em> Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-betaDist">(19)</a> lays the foundation for developing hypothesis testing of the regression parameters.</p>
<p>A hypothesis testing begins with a null hypothesis, e.g.,</p>
<p><span class="math display">\[\begin{equation*}
\small
  
H_{0} : \beta_{i}=0.
 
\end{equation*}\]</span></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-4"></span>
<img src="graphics/2_4.png" alt="The distribution of $\hat{\beta}_{i}$ " width="100%"  />
<!--
<p class="caption marginnote">-->Figure 11: The distribution of <span class="math inline">\(\hat{\beta}_{i}\)</span> <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>If the null hypothesis is true, then based on Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-betaDist">(19)</a>, we have</p>
<p><span class="math display" id="eq:2-betaDist0">\[\begin{equation}
\small
    \hat{\beta}_{i} \sim N\left(0, \frac{\sigma_{\varepsilon}^{2}}{\boldsymbol{x}_{i}^T \boldsymbol{x}_{i}}\right).
\tag{20}
\end{equation}\]</span></p>
<p>This distribution is shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-4">11</a>. It is a graphical display of the possibilities of the values of <span class="math inline">\(\hat{\beta}_{i}\)</span> that we may observe, <em>if</em> <span class="math inline">\(H_{0}\)</span> is true.</p>
<p>Then we can derive further implications. Based on Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-4">11</a>, we could define a range of <span class="math inline">\(\hat{\beta}_{i}\)</span> that we believe as most plausible<label for="tufte-sn-25" class="margin-toggle sidenote-number">25</label><input type="checkbox" id="tufte-sn-25" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">25</span> Note that I use the word “plausible” instead of “possible.” Any value is always <em>possible</em>, according to Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-betaDist0">(20)</a>. But the <em>possibility</em> is not equally distributed, as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-4">11</a>. Some values are more possible than others.</span>. In other words, if the null hypothesis is true, then it is normal to see <span class="math inline">\(\hat{\beta}_{i}\)</span> in this range. This thought leads to Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-5">12</a>. This is <em>what is supposed to be</em>, if the null hypothesis is true. And any value outside of this range is considered as a result of rare chance, noise, or abnormality. We define a level of probability that represents our threshold of rare chance. We coin this threshold level as <span class="math inline">\(\alpha\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-5"></span>
<img src="graphics/2_5.png" alt="The framework of hypothesis testing" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 12: The framework of hypothesis testing<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>With the threshold level <span class="math inline">\(\alpha\)</span>, we conclude that any value of <span class="math inline">\(\hat{\beta}_{i}\)</span> that falls outside of the range is unlikely. If we see <span class="math inline">\(\hat{\beta}_{i}\)</span> falls outside of the range, we reject the null hypothesis <span class="math inline">\(H_{0}\)</span>, based on the conflict between “<em>what is supposed to be</em>” and “<em>what happened to be</em>.”<label for="tufte-sn-26" class="margin-toggle sidenote-number">26</label><input type="checkbox" id="tufte-sn-26" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">26</span> I.e., what we have assumed in <span class="math inline">\(H_{0}\)</span> is <em>what is supposed to be</em>, and what we have observed in data is <em>what happened to be</em>.</span> This framework is shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-5">12</a>.</p>
<p>Hypothesis testing is a decision made with risks. We may be wrong: even if the null hypothesis is true, there is still a small probability, <span class="math inline">\(\alpha\)</span>, that we may observe <span class="math inline">\(\hat{\beta}_{i}\)</span> falls outside of the range. But this is not a blind risk. It is a <em>different kind of risk</em>: we have scientifically derived the risk, understood it well, and accepted the risk as a cost.</p>
</div>
<div id="r-lab" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>In this section, we illustrate step-by-step a pipeline of R codes to use the linear regression model in real-world data analysis. Real-world data analysis is challenging. The <em>real-world</em> means objectivity, but the <em>real-worldliness</em> suggests subjectivity. The purpose of the R codes in this book serves a similar function as a diving coach who dives into the water to show how the action should be done, but the <em>real-worldliness</em> can only be felt if you also dive into the water and feel the thrill by yourself. Our data analysis examples try to preserve a certain degree of the <em>real-worldliness</em> that embodies both statistical regularities and realistic irregularities<label for="tufte-sn-27" class="margin-toggle sidenote-number">27</label><input type="checkbox" id="tufte-sn-27" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">27</span> Prof. George Box once said, “<em>all models are wrong, some are useful</em>.”</span>. Only the challenge in many real applications is that the boundary between the statistical regularities and realistic irregularities is unclear and undefined.</p>
<p>Having said that, making informed decisions by drawing from rigorous theories, while at the same time, maintaining a critical attitude about theory, are both needed in practices of data analytics.</p>
<p>Here, our data is from a study of Alzheimer’s disease<label for="tufte-sn-28" class="margin-toggle sidenote-number">28</label><input type="checkbox" id="tufte-sn-28" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">28</span> Data were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (<a href="http://adni.loni.usc.edu">http://adni.loni.usc.edu</a>). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer’s disease (AD).</span> that collected some demographics, genetic, and neuroimaging variables from hundreds of subjects. The goal of this dataset is to use these predictors to predict some outcome variables, e.g., one is called the Mini-Mental State Examination (<code>MMSCORE</code>), which is a clinical score for determining Alzheimer’s disease. It ranges from <span class="math inline">\(1\)</span> to <span class="math inline">\(30\)</span>, while <span class="math inline">\(25\)</span> to <span class="math inline">\(30\)</span> is normal, <span class="math inline">\(20\)</span> to <span class="math inline">\(24\)</span> suggests mild dementia, <span class="math inline">\(13\)</span> to <span class="math inline">\(20\)</span> suggests moderate dementia, and less than <span class="math inline">\(12\)</span> indicates severe dementia.</p>
<p><em>The 5-Step R Pipeline.</em> We start with a pipeline of conducting linear regression analysis in R with 5 steps. Please keep in mind that these 5 steps are not a fixed formula: it is a selection of the authors to make it simple.</p>
<p><strong>Step 1</strong> loads the data into the R work environment.</p>
<p></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb2-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb2-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb2-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb2-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb2-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb2-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb2-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb2-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb2-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(AD)</span></span></code></pre></div>
<p></p>
<p><strong>Step 2</strong> is for data preprocessing. This is a standard chunk of code, and it will be used again in future chapters. As this is the first time we see it, here, let’s break it into several pieces. The first piece is to create your <code>X</code> matrix (predictors) and <code>Y</code> vector (outcome variable). The use of <code>X</code> for predictors and <code>Y</code> for outcome are common practice.</p>
<p></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing.</span></span>
<span id="cb3-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove variable DX_bl</span></span>
<span id="cb3-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb3-3" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> AD[ , <span class="sc">-</span><span class="fu">which</span>(<span class="fu">names</span>(AD) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>))] </span>
<span id="cb3-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick up the first 15 variables for predictors</span></span>
<span id="cb3-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb3-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>]</span>
<span id="cb3-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick up the variable MMSCORE for outcome</span></span>
<span id="cb3-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb3-7" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span></code></pre></div>
<p></p>
<p>Then, we make a <code>data.frame</code> to enclose both the predictors and outcome variable together. Many R functions presume the data are <em>packaged</em> in this way.</p>
<p></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb4-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb4-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span></code></pre></div>
<p></p>
<p>Then, we split the data into two parts<label for="tufte-sn-29" class="margin-toggle sidenote-number">29</label><input type="checkbox" id="tufte-sn-29" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">29</span> Usually, there is a client who splits the data for you, sends you the training data only, and withholds the testing data. When you submit your model trained on the training data, the client could verify your model using the testing data. Here, even the dataset we are working on is already the training data, we still split this nominal training data into halves and use one half as the actual training data and the other half as the testing data. Why do we do so? Please see <strong>Chapter 5</strong>.</span>. We name the two parts as <em>training data</em> and <em>testing data</em>, respectively. The training data is to fit the model. The testing data is excluded from the model training: it will be used to test the model after the final model has been selected using the training data solely.</p>
<p></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># generate the same random sequence</span></span>
<span id="cb5-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb5-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb5-3" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb5-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb5-4" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb5-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb5-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb5-6" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> builds up a linear regression model. We use the <code>lm()</code> function to fit the regression model<label for="tufte-sn-30" class="margin-toggle sidenote-number">30</label><input type="checkbox" id="tufte-sn-30" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">30</span> Use <code>lm()</code> for more information.</span>.</p>
<p></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; Use lm() function to build a full </span></span>
<span id="cb6-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model with all predictors</span></span>
<span id="cb6-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb6-3" aria-hidden="true" tabindex="-1"></a>lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> ., <span class="at">data =</span> data.train)</span>
<span id="cb6-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD)</span></code></pre></div>
<p></p>
<p>The result is shown in below</p>
<p></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb7-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~ ., data = data.train)</span></span>
<span id="cb7-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb7-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb7-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb7-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="do">## -6.3662 -0.8555  0.1540  1.1241  4.2517 </span></span>
<span id="cb7-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb7-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb7-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb7-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 17.93920    2.38980   7.507 1.16e-12 ***</span></span>
<span id="cb7-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          0.02212    0.01664   1.329 0.185036    </span></span>
<span id="cb7-12"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER    -0.11141    0.22077  -0.505 0.614280    </span></span>
<span id="cb7-13"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     0.16943    0.03980   4.257 2.96e-05 ***</span></span>
<span id="cb7-14"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          0.65003    0.17836   3.645 0.000328 ***</span></span>
<span id="cb7-15"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45        -1.10136    0.62510  -1.762 0.079348 .  </span></span>
<span id="cb7-16"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV      7.66067    1.68395   4.549 8.52e-06 ***</span></span>
<span id="cb7-17"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="do">## e2_1        -0.26059    0.36036  -0.723 0.470291    </span></span>
<span id="cb7-18"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="do">## e4_1        -0.42123    0.24192  -1.741 0.082925 .  </span></span>
<span id="cb7-19"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3818361    0.24991    0.21449   1.165 0.245120    </span></span>
<span id="cb7-20"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="do">## rs744373    -0.25192    0.20787  -1.212 0.226727    </span></span>
<span id="cb7-21"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="do">## rs11136000  -0.23207    0.21836  -1.063 0.288926    </span></span>
<span id="cb7-22"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="do">## rs610932    -0.11403    0.21906  -0.521 0.603179    </span></span>
<span id="cb7-23"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3851179    0.16251    0.21402   0.759 0.448408    </span></span>
<span id="cb7-24"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3764650    0.47607    0.24428   1.949 0.052470 .  </span></span>
<span id="cb7-25"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444   -0.34550    0.20559  -1.681 0.094149 .  </span></span>
<span id="cb7-26"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb7-27"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span>
<span id="cb7-28"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb7-29"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.63 on 242 degrees of freedom</span></span>
<span id="cb7-30"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.3395, Adjusted R-squared:  0.2986 </span></span>
<span id="cb7-31"><a href="chapter-2-abstraction-regression-tree-models.html#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 8.293 on 15 and 242 DF,  p-value: 3.575e-15</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> is model selection. There are many variables that are not significant, i.e., their <em>p-values</em> are larger than <span class="math inline">\(0.05\)</span>. The <code>step()</code> function is used for automatic model selection<label for="tufte-sn-31" class="margin-toggle sidenote-number">31</label><input type="checkbox" id="tufte-sn-31" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">31</span> Use <code>help(step)</code> for more information.</span>, i.e., it implements a brute-force approach to identify the best combinations of variables in a linear regression model.</p>
<p></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; use step() to automatically delete </span></span>
<span id="cb8-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># all the insignificant variables</span></span>
<span id="cb8-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic model selection</span></span>
<span id="cb8-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb8-4" aria-hidden="true" tabindex="-1"></a>lm.AD.reduced <span class="ot">&lt;-</span> <span class="fu">step</span>(lm.AD, <span class="at">direction=</span><span class="st">&quot;backward&quot;</span>, <span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<p></p>
<p>And the final model the <code>step()</code> function identifies is</p>
<p></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Step:  AIC=259.92</span></span>
<span id="cb9-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="do">## MMSCORE ~ PTEDUCAT + FDG + AV45 + HippoNV + e4_1 + rs744373 + </span></span>
<span id="cb9-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="do">##     rs3764650 + rs3865444</span></span>
<span id="cb9-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb9-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="do">##             Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    </span></span>
<span id="cb9-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;                   658.95 259.92                      </span></span>
<span id="cb9-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs744373   1     6.015 664.96 260.27  2.2728  0.132934    </span></span>
<span id="cb9-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="do">## - AV45       1     7.192 666.14 260.72  2.7176  0.100511    </span></span>
<span id="cb9-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="do">## - e4_1       1     8.409 667.36 261.19  3.1774  0.075882 .  </span></span>
<span id="cb9-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3865444  1     8.428 667.38 261.20  3.1848  0.075544 .  </span></span>
<span id="cb9-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3764650  1    10.228 669.18 261.90  3.8649  0.050417 .  </span></span>
<span id="cb9-12"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="do">## - FDG        1    40.285 699.24 273.23 15.2226  0.000123 ***</span></span>
<span id="cb9-13"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="do">## - PTEDUCAT   1    44.191 703.14 274.67 16.6988 5.913e-05 ***</span></span>
<span id="cb9-14"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="do">## - HippoNV    1    53.445 712.40 278.04 20.1954 1.072e-05 ***</span></span>
<span id="cb9-15"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb9-16"><a href="chapter-2-abstraction-regression-tree-models.html#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span></code></pre></div>
<p></p>
<p>It can be seen that the predictors that are kept in the <em>final model</em> are all significant. Also, the <code>R-squared</code> is <span class="math inline">\(0.3228\)</span> using the <span class="math inline">\(8\)</span> selected predictors. This is not bad comparing with the <code>R-squared</code>, <span class="math inline">\(0.3395\)</span>, when all the <span class="math inline">\(15\)</span> predictors are used (we call this model the <em>full model</em>).</p>
<p>We compare the full model with the final model using the F-test that is implemented in <code>anova()</code>.</p>
<p></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm.AD.reduced,lm.AD)</span></code></pre></div>
<p></p>
<p>The returned result, shown below, implies that it is statistically indistinguishable between the two models (<em>p-value</em> of the F-test is <span class="math inline">\(0.529\)</span>). The model <code>lm.AD.reduced</code> provides an equally good explanation of the data as the full model does, but <code>lm.AD.reduced</code> is more economic. The principle of <strong>Occam’s razor</strong><label for="tufte-sn-32" class="margin-toggle sidenote-number">32</label><input type="checkbox" id="tufte-sn-32" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">32</span> “<em>Other things being equal, simpler explanations are generally better than more complex ones</em>,” is the basic idea of Occam’s razor. Albert Einstein was also quoted with a similar expression: “<em>Everything should be made as simple as possible, but no simpler</em>.”</span> would consider the model <code>lm.AD.reduced</code> more in favor.</p>
<p></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Analysis of Variance Table</span></span>
<span id="cb11-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb11-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Model 1: MMSCORE ~ PTEDUCAT + FDG + AV45 + HippoNV +  </span></span>
<span id="cb11-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="do">##     e4_1 + rs744373 + rs3764650 + rs3865444</span></span>
<span id="cb11-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Model 2: MMSCORE ~ AGE + PTGENDER + PTEDUCAT + FDG + AV45 +  </span></span>
<span id="cb11-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     HippoNV + e2_1 + e4_1 + rs3818361 + rs744373 + rs11136000 +  </span></span>
<span id="cb11-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     rs610932 + rs3851179 + rs3764650 + rs3865444</span></span>
<span id="cb11-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="do">##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)</span></span>
<span id="cb11-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 1    249 658.95                           </span></span>
<span id="cb11-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 2    242 642.73  7    16.218 0.8723  0.529</span></span></code></pre></div>
<p></p>
<p><strong>Step 5</strong> makes prediction. We can use the function <code>predict()</code><label for="tufte-sn-33" class="margin-toggle sidenote-number">33</label><input type="checkbox" id="tufte-sn-33" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">33</span> <code>predict(obj, data)</code></span> which is a function you can find in many R packages. It usually has two main arguments: <code>obj</code> is the model, and <code>data</code> is the data points you want to predict on. Note that, here, we test the model (that was trained on training data) on the testing data. After gathering the predictions, we use the function <code>cor()</code> to measure how close are the predictions with the true outcome values of the testing data. The higher the correlation, the better the predictions.</p>
<p></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; Predict using your linear regession model</span></span>
<span id="cb12-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb12-2" aria-hidden="true" tabindex="-1"></a>pred.lm <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm.AD.reduced, data.test)</span>
<span id="cb12-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For regression model, you can use correlation to measure </span></span>
<span id="cb12-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># how close your predictions with the true outcome </span></span>
<span id="cb12-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># values of the data points</span></span>
<span id="cb12-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(pred.lm, data.test<span class="sc">$</span>MMSCORE)</span></code></pre></div>
<p></p>
<p><em>Beyond the 5-Step Pipeline.</em> The <strong>Exploratory Data Analysis</strong> (<strong>EDA</strong>) is a practical toolbox that consists of many interesting and insightful methods and tools, mostly empirical and graphical. The idea of EDA was promoted by some statisticians<label for="tufte-sn-34" class="margin-toggle sidenote-number">34</label><input type="checkbox" id="tufte-sn-34" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">34</span> E.g., John W. Tukey was a statistician whose career was known to be an advocate of EDA. See his book: <em>Exploratory Data Analysis</em>, Addison-Wesley Publishing Co., 1977.</span>. The EDA could be used before and after we have built the model. For example, a common practice of EDA is to draw the scatterplots to see how potentially the predictors can predict the outcome variable.</p>
<p></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatterplot matrix to visualize the relationship</span></span>
<span id="cb13-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># between outcome variable with continuous predictors</span></span>
<span id="cb13-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;GGally&quot;)</span></span>
<span id="cb13-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb13-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># draw the scatterplots and also empirical</span></span>
<span id="cb13-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># shapes of the distributions of the variables</span></span>
<span id="cb13-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggpairs</span>(AD[,<span class="fu">c</span>(<span class="dv">16</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)],</span>
<span id="cb13-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">upper =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="st">&quot;points&quot;</span>),</span>
<span id="cb13-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">lower =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="st">&quot;cor&quot;</span>))</span>
<span id="cb13-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span>
<span id="cb13-12"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-14"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(<span class="fu">factor</span>(PTGENDER),</span>
<span id="cb13-15"><a href="chapter-2-abstraction-regression-tree-models.html#cb13-15" aria-hidden="true" tabindex="-1"></a>      MMSCORE, <span class="at">data =</span> AD,<span class="at">geom=</span><span class="fu">c</span>(<span class="st">&quot;boxplot&quot;</span>), <span class="at">fill =</span> <span class="fu">factor</span>(PTGENDER))</span></code></pre></div>
<p></p>
<p>Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-7">13</a> presents the continuous predictors.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-7"></span>
<p class="caption marginnote shownote">
Figure 13: Scatterplots of the continuous predictors versus outcome variable
</p>
<img src="graphics/2_7.png" alt="Scatterplots of the continuous predictors versus outcome variable" width="80%"  />
</div>
<p></p>
<p>For the other predictors which are binary, we can use a boxplot, which is shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-8">14</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-8"></span>
<p class="caption marginnote shownote">
Figure 14: Boxplots of the binary predictors versus outcome variable
</p>
<img src="graphics/2_8.png" alt="Boxplots of the binary predictors versus outcome variable" width="80%"  />
</div>
<p></p>
<p>In what follows we show another case of EDA.</p>
<p>Consider the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code>, and find a graphical way to investigate if the predictor, <code>AGE</code>, mediates the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code>. One way to do so is to color the data points in the scatterplot (i.e., the color corresponds to the numerical scale of <code>AGE</code>). The following R codes generate Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-9">15</a>.</p>
<p></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># How to detect interaction terms</span></span>
<span id="cb14-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># by exploratory data analysis (EDA)</span></span>
<span id="cb14-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb14-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb14-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(AD, <span class="fu">aes</span>(<span class="at">x =</span> PTEDUCAT, <span class="at">y =</span> MMSCORE))</span>
<span id="cb14-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb14-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">colour=</span>AGE), <span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb14-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># p &lt;- p + geom_smooth(method = &quot;auto&quot;)</span></span>
<span id="cb14-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb14-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;MMSE versus PTEDUCAT&quot;</span>)</span>
<span id="cb14-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-9"></span>
<img src="graphics/2_9.png" alt="Scatterplots of `MMSCORE` versus `PTEDUCAT`" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 15: Scatterplots of <code>MMSCORE</code> versus <code>PTEDUCAT</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>It looks like that the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code> indeed changes according to different levels of <code>AGE</code>. While this is subtle, we change the strategy and draw two more figures, i.e., we draw the same scatterplot on two levels of <code>AGE</code>, i.e., <code>AGE &lt; 60</code> and <code>AGE &gt; 80</code>. The following R codes generate Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-10">16</a>.</p>
<p></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb15-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(AD[<span class="fu">which</span>(AD<span class="sc">$</span>AGE <span class="sc">&lt;</span> <span class="dv">60</span>),], </span>
<span id="cb15-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb15-2" aria-hidden="true" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">x =</span> PTEDUCAT, <span class="at">y =</span> MMSCORE))</span>
<span id="cb15-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb15-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb15-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb15-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm)</span>
<span id="cb15-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb15-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;MMSE versus PTEDUCAT when AGE &lt; 60&quot;</span>)</span>
<span id="cb15-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb16-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(AD[<span class="fu">which</span>(AD<span class="sc">$</span>AGE <span class="sc">&gt;</span> <span class="dv">80</span>),], </span>
<span id="cb16-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb16-2" aria-hidden="true" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">x =</span> PTEDUCAT, <span class="at">y =</span> MMSCORE))</span>
<span id="cb16-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb16-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb16-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb16-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm)</span>
<span id="cb16-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb16-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;MMSE versus PTEDUCAT when AGE &gt; 80&quot;</span>)</span>
<span id="cb16-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-10"></span>
<p class="caption marginnote shownote">
Figure 16: Scatterplots of <code>MMSCORE</code> versus <code>PTEDUCAT</code> when (left) <code>AGE &lt; 60</code> or (right) <code>AGE &gt; 80</code>
</p>
<img src="graphics/2_10_1.png" alt="Scatterplots of `MMSCORE` versus `PTEDUCAT` when (left) `AGE &lt; 60`  or (right) `AGE &gt; 80`" width="49%" height="49%"  /><img src="graphics/2_10_2.png" alt="Scatterplots of `MMSCORE` versus `PTEDUCAT` when (left) `AGE &lt; 60`  or (right) `AGE &gt; 80`" width="49%" height="49%"  />
</div>
<p></p>
<p>Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-10">16</a> shows that the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code> changes dramatically according to different levels of <code>AGE</code>. In other words, it means that the way the predictor <code>PTEDUCAT</code> impacts the outcome <code>MMSCORE</code> is not simply additive as a regular linear regression model would suggest. Rather, the relationship between the two is modified by <code>AGE</code>. This discovery suggests a different mechanism underlying the three variables, as demonstrated in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-lr-interact">17</a>.</p>
<p>We then add an interaction term into the regression model</p>
<p></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the multiple linear regression model </span></span>
<span id="cb17-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with an interaction term: AGE*PTEDUCAT</span></span>
<span id="cb17-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb17-3" aria-hidden="true" tabindex="-1"></a>lm.AD.int <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> AGE <span class="sc">+</span> PTGENDER <span class="sc">+</span> PTEDUCAT </span>
<span id="cb17-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb17-4" aria-hidden="true" tabindex="-1"></a>                  <span class="sc">+</span> AGE<span class="sc">*</span>PTEDUCAT, <span class="at">data =</span> AD)</span>
<span id="cb17-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.int)</span></code></pre></div>
<p></p>
<p>We can see that this interaction term is significant.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-lr-interact"></span>
<img src="graphics/2_lr_interact.png" alt="Different data-generating mechanisms: (left) additive relationships between predictors and outcome; (right) additive relationships and interaction" width="60%"  />
<!--
<p class="caption marginnote">-->Figure 17: Different data-generating mechanisms: (left) additive relationships between predictors and outcome; (right) additive relationships and interaction<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb18-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~ AGE + PTGENDER </span></span>
<span id="cb18-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="do">##     + PTEDUCAT + AGE * PTEDUCAT,</span></span>
<span id="cb18-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     data = AD)</span></span>
<span id="cb18-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb18-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max</span></span>
<span id="cb18-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.2571 -0.9204  0.5156  1.4219  4.2975</span></span>
<span id="cb18-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb18-12"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb18-13"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  40.809411   5.500441   7.419 4.93e-13 ***</span></span>
<span id="cb18-14"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          -0.202043   0.074087  -2.727  0.00661 **</span></span>
<span id="cb18-15"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER     -0.470951   0.187143  -2.517  0.01216 *</span></span>
<span id="cb18-16"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     -0.642352   0.336212  -1.911  0.05662 .</span></span>
<span id="cb18-17"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE:PTEDUCAT  0.011083   0.004557   2.432  0.01534 *</span></span>
<span id="cb18-18"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb18-19"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb18-20"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-21"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.052 on 512 degrees of freedom</span></span>
<span id="cb18-22"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.07193,    Adjusted R-squared:  0.06468</span></span>
<span id="cb18-23"><a href="chapter-2-abstraction-regression-tree-models.html#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  9.92 on 4 and 512 DF,  p-value: 9.748e-08</span></span></code></pre></div>
<p></p>
</div>
</div>
<div id="tree-models" class="section level2 unnumbered">
<h2>Tree models</h2>
<p></p>
<div id="rationale-and-formulation-1" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>While the linear regression model is a typical data modeling method, the decision tree model represents a typical method in the category of algorithmic modeling<label for="tufte-sn-35" class="margin-toggle sidenote-number">35</label><input type="checkbox" id="tufte-sn-35" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">35</span> The two types of modeling cultures are discussed in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-1">1</a>.</span>. The linear regression model, given its many origins and implications, builds a model based on a mathematical characterization of the <em>data-generating mechanism</em>, which emphasizes an analytic understanding of the underlying system and how the data is generated from this system<label for="tufte-sn-36" class="margin-toggle sidenote-number">36</label><input type="checkbox" id="tufte-sn-36" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">36</span> I.e., Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-fx">(3)</a> explains how <span class="math inline">\(y\)</span> is impacted by <span class="math inline">\(x\)</span>, and Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-simLR-eps">(4)</a> explains how the rest of <span class="math inline">\(y\)</span> is impacted by a random force. This is illustrated in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-lr-datamodel">18</a>.</span>. This pursuit of “mechanism” is sometimes too much to ask for if we know little about the physics but only the data, since understanding the mechanism of a problem needs experimental science and profound insights. And this pursuit of “mechanism” limits the applicability of a data modeling method when the data don’t seem to follow the data-generating mechanism <em>prescribed</em> by the model.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-lr-datamodel"></span>
<img src="graphics/2_lr_datamodel.png" alt="The *data-generating mechanism* of a simple linear regression model" width="60%"  />
<!--
<p class="caption marginnote">-->Figure 18: The <em>data-generating mechanism</em> of a simple linear regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>For example, Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-3">3</a> shows a dataset that has <span class="math inline">\(6\)</span> observations, with two predictors, <em>Weather</em> and <em>Day of week (Dow)</em>, and an outcome variable, <em>Play</em>. Assume that this is a dataset collected by a causal dog walker whose routine includes a sports field.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-3">Table 3: </span>Example of a dataset where a decision tree has a home game</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Weather</th>
<th align="left">Dow (day of weak)</th>
<th align="left">Play</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">Rainy</td>
<td align="left">Saturday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">Sunny</td>
<td align="left">Saturday</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">Windy</td>
<td align="left">Tuesday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">Sunny</td>
<td align="left">Saturday</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">Sunny</td>
<td align="left">Monday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">Windy</td>
<td align="left">Saturday</td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p></p>
<p>It is hard to imagine that, for this dataset, how we can denote the two predictors as <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and connect it with the outcome variable <span class="math inline">\(y\)</span> in the form of Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR">(13)</a>, i.e.,</p>
<!-- \ArrowBetweenLines [\Downarrow] -->
<p><span class="math display">\[\begin{equation*}
    \begin{aligned}
    &amp;\text{*Yes*} = \beta_0 + \beta_1 \text{*Rainy*} + \beta_2 \text{*Tuesday*} + \epsilon?
    \end{aligned}
\end{equation*}\]</span></p>
<p>For this dataset, decision tree is a natural fit. As shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-11">19</a>, a decision tree contains a <strong>root node</strong>, <strong>inner nodes</strong>, and <strong>decision nodes</strong> (i.e., the shaded leaf nodes of the tree in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-11">19</a>). For any data point to reach its prediction, it starts from the root node, follows the <strong>splitting rules</strong> alongside the arcs to travel through inner nodes, then finally reaches a decision node. For example, consider the data point “<em>Weather = Sunny, Dow = Saturday</em>,” it starts with the root node, “<em>Weather = Sunny?</em>” then goes to inner node “<em>Dow = Saturday?</em>” then reaches the decision node as the left child node of the inner node “<em>Dow = Saturday?</em>” So the decision is “<em>Play = Yes</em>.”</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-11"></span>
<img src="graphics/2_11.png" alt="Example of a decision tree model" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 19: Example of a decision tree model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Compare with data modeling methods that hope to build a characterization of the data-generating mechanism, algorithmic modeling methods such as the decision tree mimic <em>heuristics</em> in human reasoning. It is challenging, while unnecessary, to write up a model of algorithmic modeling in mathematical forms as the one shown in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-multiLR">(13)</a>. Algorithmic modeling methods are more <em>semantics-oriented</em>, and more focused on patterns detection and description.</p>
</div>
<div id="theorymethod" class="section level3 unnumbered">
<h3>Theory/Method</h3>
<p>Decision trees could be generated by manual inspection of the data. The one shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-11">19</a> could be easily drawn with a few inspection of the 6 data points in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-3">3</a>. Automatic algorithms have been developed that can take a dataset as input and generate a decision tree as output. We can see from Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-11">19</a> that a key element of a decision tree is the <em>splitting rules</em> that guide a data point to travel through the inner nodes to reach a final decision node (i.e., to reach a decision).</p>
<p>A splitting rule is defined by a variable and the set of values the variable is allowed to take, e.g., in “<em>Weather = Sunny?</em>” “Weather” is the variable and “Sunny” is the set of value. The variable used for splitting is referred to as the <strong>splitting variable</strong>, and the set of value is referred to as the <strong>splitting value</strong>.</p>
<p>We start with the root node. Possible splitting rules are</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p>“<em>Weather = Sunny?</em>”</p></li>
<li><p>“<em>Dow = Saturday?</em>”</p></li>
<li><p>“<em>Dow = Monday?</em>”</p></li>
<li><p>“<em>Dow = Tuesday?</em>”</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>Each of the splitting rules will lead to a different root node. Two examples are shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-12">20</a>. Which one should we use?</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-12"></span>
<p class="caption marginnote shownote">
Figure 20: Example of two root nodes
</p>
<img src="graphics/2_12.png" alt="Example of two root nodes" width="80%"  />
</div>
<p></p>
<p>To help us decide on which splitting rule is the best, the concepts <strong>entropy</strong> of data and <strong>information gain</strong> (<strong>IG</strong>) are needed.</p>
<p><em>Entropy and information gain (IG).</em> We can use the concept <strong>entropy</strong> to measure the homogeneity of the data points in a node of the decision tree. It is defined as</p>
<p><span class="math display" id="eq:2-entropy">\[\begin{equation}
\small
e = \sum\nolimits_{i=1,\cdots,K}-P_i\log _{2} P_i.
\tag{21}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(K\)</span> represents the number of classes of the data points in the node<label for="tufte-sn-37" class="margin-toggle sidenote-number">37</label><input type="checkbox" id="tufte-sn-37" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">37</span> E.g., in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-3">3</a>, there are <span class="math inline">\(K=2\)</span> classes, <em>Yes</em> and <em>No</em>.</span>, and <span class="math inline">\(P_i\)</span> is the proportion of data points that belong to the class <span class="math inline">\(i\)</span>. The entropy <span class="math inline">\(e\)</span> is defined as zero when the data points in the node all belong to one single class<label for="tufte-sn-38" class="margin-toggle sidenote-number">38</label><input type="checkbox" id="tufte-sn-38" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">38</span> What is more deterministic than this case?</span>. And <span class="math inline">\(e = 1\)</span> is the maximum value for the entropy of a dataset, i.e., try an example with two classes, where <span class="math inline">\(P_1 = 0.5\)</span> and <span class="math inline">\(P_2 = 0.5\)</span>.<label for="tufte-sn-39" class="margin-toggle sidenote-number">39</label><input type="checkbox" id="tufte-sn-39" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">39</span> What is more uncertain than this case?</span></p>
<p>A node that consists of data points that are dominated by one class (i.e., entropy is small) is ready to be made a decision node. If it still has a large entropy, splitting it into two child nodes could help reduce the entropy. Thus, to further split a node, we look for the best splitting rule that can maximize the entropy reduction. This entropy reduction can be measured by <strong>IG</strong>, which is the difference of entropy of the parent node and the average entropy of the two child nodes weighted by their number of data points. It is defined as</p>
<p><span class="math display" id="eq:2-IG">\[\begin{equation}
\small
IG = e_s - \sum\nolimits_{i=1,2} w_i e_i.
\tag{22}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(e_s\)</span> is the entropy of the parent node, <span class="math inline">\(e_i\)</span> is the entropy of the child node <span class="math inline">\(i\)</span>, and <span class="math inline">\(w_i\)</span> is the number of data points in the child node <span class="math inline">\(i\)</span> divided by the number of data points in the parent node.</p>
<p>For example, for the left tree in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-12">20</a>, using the definition of entropy in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-entropy">(21)</a>, the entropy of the root node is calculated as</p>
<p><span class="math display">\[\begin{equation*}
\small
  -\frac{4}{6} \log _2 \frac{4}{6} - \frac{2}{6}\log _2 \frac{2}{6}=0.92. 
\end{equation*}\]</span></p>
<p>The entropy of the left child node (“<em>Weather = Sunny</em>”) is</p>
<p><span class="math display">\[\begin{equation*}
\small
  -\frac{2}{3} \log _2 \frac{2}{3} - \frac{1}{3} \log _2 \frac{1}{3}=0.92. 
\end{equation*}\]</span></p>
<p>The entropy of the right child node (“<em>Weather != Sunny</em>”) is <span class="math inline">\(0\)</span> since all three data points (ID = <span class="math inline">\(1,3,6\)</span>) belong to the same class.</p>
<p>Then, using the definition of IG in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-IG">(22)</a>, the IG for the splitting rule “<em>Weather = Sunny</em>” is</p>
<p><span class="math display">\[\begin{equation*}
\small
  IG = 0.92 - \frac{3}{6} \times 0.92 - \frac{3}{6} \times 0=0.46. 
\end{equation*}\]</span></p>
<p>For the tree in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-12">20</a> (right), the entropy of the left child node (“<em>Dow = Saturday</em>”) is</p>
<p><span class="math display">\[\begin{equation*}
\small
  -\frac{2}{4} \log _2 \frac{2}{4} - \frac{2}{4} \log _2 \frac{2}{4} = 1. 
\end{equation*}\]</span></p>
<p>The entropy of the right child node (“<em>Dow != Saturday</em>”) is <span class="math inline">\(0\)</span> since the two data points (ID = 3,5) belong to the same class.</p>
<p>Thus, the IG for the splitting rule “<em>Dow = Saturday</em>” is</p>
<p><span class="math display">\[\begin{equation*}
\small
  IF=0.92-\frac{4}{6} \times 1 - \frac{2}{6} \times 0=0.25. 
\end{equation*}\]</span></p>
<p>As the IG for the splitting rule “<em>Weather = Sunny</em>” is higher, the left tree in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-12">20</a> is a better choice to start the tree.</p>
<p><em>Recursive partitioning.</em> The splitting process discussed above could be repeatedly used, until there is no further need to split a node, i.e., the node contains data points from one single class, which is ideal and almost would never happen in reality; or the node has reached the minimum number of data points<label for="tufte-sn-40" class="margin-toggle sidenote-number">40</label><input type="checkbox" id="tufte-sn-40" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">40</span> It is common to assign a minimum number of data points to prevent the tree-growing algorithm to generate too tiny leaf nodes. This is to prevent “<strong>overfitting</strong>.” Elaborated discussion of overfitting will be provided in <strong>Chapter 5</strong>.</span>. This repetitive splitting process is called <strong>recursive partitioning</strong>.</p>
<p>For instance, the left child node in the tree shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-12">20</a> (left) with data points (ID = <span class="math inline">\(2,4,5\)</span>) still has two classes, and can be further split by selecting the next best splitting rule. The right child node has only one class and becomes a decision node labeled with the decision “<em>Play = No</em>.”</p>
<p>This greedy approach, like other greedy optimization approaches, is easy to use. One limitation of greedy approches is that they may find <strong>local optimal</strong> solutions instead of <strong>global optimal</strong> solutions. The optimal choice we made in choosing between the two alternatives in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-12">20</a> is a <em>local</em> optimal choice, and all later nodes of the final tree model are impacted by our decision made on the root node. The <em>optimal root node</em> doesn’t necessarily lead to the <em>optimal tree</em><label for="tufte-sn-41" class="margin-toggle sidenote-number">41</label><input type="checkbox" id="tufte-sn-41" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">41</span> In other words, an optimal tree is the optimal one among all the possible trees, so an optimal root node won’t necessarily lead to an optimal tree.</span>.</p>
<p>An illustration of the risk of getting stuck in a local optimal solution of greedy optimization approaches is shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-localoptimal">21</a>. Where the algorithm gets started matters to where it ends up. For this reason, decision tree algorithms are often sensitive to data, i.e., it is not uncommon that a slight change of the dataset may cause a considerable change of the topology of the decision tree.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-localoptimal"></span>
<img src="graphics/2_localoptimal.png" alt="A greedy optimization approach starts its adventure from an **initial solution**. Here, $x_1$, $x_2$, $x_3$ are different initial solutions of $3$ usages of the optimization approach, and $3$ *optimal* solutions are found, while only one of them is *globally optimal*." width="100%"  />
<!--
<p class="caption marginnote">-->Figure 21: A greedy optimization approach starts its adventure from an <strong>initial solution</strong>. Here, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span> are different initial solutions of <span class="math inline">\(3\)</span> usages of the optimization approach, and <span class="math inline">\(3\)</span> <em>optimal</em> solutions are found, while only one of them is <em>globally optimal</em>.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Tree pruning.</em> To enhance the robustness of the decision tree learned by data-driven approaches such as the recursive partitioning, <strong>pruning</strong> methods could be used to cut down some unstable or insignificant branches. There are <strong>pre-pruning</strong> and <strong>post-pruning</strong> methods. Pre-pruning stops growing a tree when a pre-defined criterion is met. For example, one can set the <strong>depth of a tree</strong> (i.e., the depth of a node is the number of edges from the node to the tree’s root node; the depth of a tree is the maximum depth of its leaf nodes), or the minimum number of data points at the leaf nodes. These approaches need prior knowledge, and they may not necessarily reflect the characteristics of the particular dataset. More data-dependent approaches can be used. For example, we may set a minimum IG threshold to stop growing a tree when the IG is below the threshold. This may cause another problem, i.e., a small IG at an internal node does not necessarily mean its potential child nodes can only have smaller IG values. Therefore, pre-pruning can cause over-simplified trees and thus <strong>underfitted</strong> tree models. In other words, it may be too cautious.</p>
<p>In contrast, post-pruning prunes a tree after it is fully grown. A fully grown model aggressively spans the tree, i.e., by setting the depth of the tree as a large number. To pursue a fully grown tree is to mitigate the risk of underfit. The cost is that it may overfit the data, so post-pruning is needed. Post-pruning starts from the bottom of the tree. If removing an inner node (together with all the descendant nodes) does not increase the error <em>significantly</em>, then it should be pruned. The question is how to evaluate the significance of the increase of error<label for="tufte-sn-42" class="margin-toggle sidenote-number">42</label><input type="checkbox" id="tufte-sn-42" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">42</span> Interested readers may find the discussion in the Remarks section useful.</span>.</p>
<p>We will refer readers to <strong>Chapter 5</strong> for understanding more about concepts such as <strong>empirical error</strong> and <strong>generalization error</strong>. Understanding the difference between them is a key step towards maturity in data analytics. Like the difference between <em>money</em> and <em>currency</em>, the difference will be obvious to you as long as you have seen the difference.</p>
<p><em>Extensions and other considerations.</em></p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p>In our data example in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-3">3</a> we only have categorical variables, so candidate splitting rules could be defined relatively easier. For a continuous variable, one approach to identify candidate splitting rules is to order the observed values first, and then, use the average of each pair of consecutive values for splitting.</p></li>
<li><p>If the outcome variable is continuous, we can use the variance of the outcome variable to measure the “entropy” of a node, i.e.,</p></li>
</ul>
<p><span class="math display">\[\begin{equation*}
\small
  v= \sum\nolimits_{n=1}\nolimits^N \left(\bar y - y_n\right)^2 , 
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(y_{n=1,\cdots,N}\)</span> are the values of the outcome variable in the node, and <span class="math inline">\(\bar y\)</span> is the average of the outcome variable. And the information gain can be calculated similarly.</p>
<ul>
<li>Both pre-pruning and post-pruning are useful in practices, and it is hard to say which one is better than the other. There is a belief that post-pruning can often outperform pre-pruning. A better procedure is to use <strong>cross-validation</strong><label for="tufte-sn-43" class="margin-toggle sidenote-number">43</label><input type="checkbox" id="tufte-sn-43" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">43</span> Details are given in <strong>Chapter 5</strong>.</span>. A popular pre-pruning parameter used in the R package <code>rpart</code> is <code>cp</code>, i.e., it sets a value such that all splits need to improve the IG by at least a factor of <code>cp</code> to be approved. This pre-pruning strategy works well in many applications.</li>
</ul>
<p><!-- end{itemize} --></p>
</div>
<div id="r-lab-1" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline.</em> We use <code>DX_bl</code> as the outcome variable that is binary<label for="tufte-sn-44" class="margin-toggle sidenote-number">44</label><input type="checkbox" id="tufte-sn-44" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">44</span> In <code>DX_bl</code>, <code>0</code> denotes normal subjects; <code>1</code> denotes diseased subjects.</span>. We use other variables (except <code>ID</code>, <code>TOTAL13</code> and <code>MMSCORE</code>) to predict <code>DX_bl</code>.</p>
<p><strong>Step 1</strong> loads the needed R packages and data into the workspace.</p>
<p></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Key package for decision tree in R: </span></span>
<span id="cb19-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rpart (for building the tree); </span></span>
<span id="cb19-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># rpart.plot (for drawing the tree)</span></span>
<span id="cb19-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb19-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb19-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb19-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb19-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-9" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb19-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb19-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb19-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span></code></pre></div>
<p>
</p>
<p><strong>Step 2</strong> is about data preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb20-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and </span></span>
<span id="cb20-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Y vector (outcome variable)</span></span>
<span id="cb20-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb20-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl</span>
<span id="cb20-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The following code makes sure the variable &quot;DX_bl&quot; </span></span>
<span id="cb20-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># is a &quot;factor&quot;.</span></span>
<span id="cb20-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-9" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, Y) </span>
<span id="cb20-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This line is to &quot;factorize&quot; the variable &quot;DX_bl&quot;.</span></span>
<span id="cb20-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># It denotes &quot;0&quot; as &quot;c0&quot; and &quot;1&quot; as &quot;c1&quot;,</span></span>
<span id="cb20-12"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co"># to highlight the fact that</span></span>
<span id="cb20-13"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;DX_bl&quot; is a factor variable, not a numerical variable</span></span>
<span id="cb20-14"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-14" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Y) <span class="co"># as.factor is to convert any variable</span></span>
<span id="cb20-15"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-15" aria-hidden="true" tabindex="-1"></a>                  <span class="co"># into the format as &quot;factor&quot; variable.</span></span>
<span id="cb20-16"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb20-18"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-18" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb20-19"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb20-20"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># generate the same random sequence</span></span>
<span id="cb20-22"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb20-23"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-23" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb20-24"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-24" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb20-25"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb20-26"><a href="chapter-2-abstraction-regression-tree-models.html#cb20-26" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-15"></span>
<img src="graphics/2_15.png" alt="The unpruned decision tree to predict `DX_bl`" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 22: The unpruned decision tree to predict <code>DX_bl</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 3</strong> is to use the <code>rpart()</code> function in the R package <code>rpart</code> to build the decision tree.</p>
<p></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; use rpart to build the decision tree.</span></span>
<span id="cb21-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb21-2" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data.train)</span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> is to use the <code>prp()</code> function to plot the decision tree<label for="tufte-sn-45" class="margin-toggle sidenote-number">45</label><input type="checkbox" id="tufte-sn-45" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">45</span> <code>prp()</code> is a capable function. It has many arguments to specify the details of how the tree should be drawn. Use <code>help(prp)</code> to see details.</span></p>
<p></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; draw the tree</span></span>
<span id="cb22-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree, <span class="at">nn.cex =</span> <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p>And the decision tree is shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-15">22</a>.</p>
<p><strong>Step 5</strong> is to prune the tree using the R function <code>prune()</code>. Remember that the parameter <code>cp</code> controls the model complexity<label for="tufte-sn-46" class="margin-toggle sidenote-number">46</label><input type="checkbox" id="tufte-sn-46" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">46</span> A larger <code>cp</code> leads to a less complex tree.</span>.</p>
<p>Let us try <code>cp</code> <span class="math inline">\(= 0.03\)</span>. This leads to a decision tree as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-16">23</a>.</p>
<p></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; prune the tree</span></span>
<span id="cb23-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb23-2" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">prune</span>(tree,<span class="at">cp=</span><span class="fl">0.03</span>)</span>
<span id="cb23-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree,<span class="at">nn.cex=</span><span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-16"></span>
<img src="graphics/2_16.png" alt="The pruned decision tree model to predict `DX_bl` of the AD data with `cp = 0.03`" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 23: The pruned decision tree model to predict <code>DX_bl</code> of the AD data with <code>cp = 0.03</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 6</strong> is to evaluate the trained model by predicting the testing data.</p>
<p></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict using your tree model</span></span>
<span id="cb24-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb24-2" aria-hidden="true" tabindex="-1"></a>pred.tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree, data.test, <span class="at">type=</span><span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p></p>
<p>And we can evaluate the prediction performance using error rate.</p>
<p></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The following line calculates the prediction error</span></span>
<span id="cb25-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rate (a number from 0 to 1) for a binary classification problem</span></span>
<span id="cb25-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-3" aria-hidden="true" tabindex="-1"></a>err.tree <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(pred.tree <span class="sc">!=</span></span>
<span id="cb25-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-4" aria-hidden="true" tabindex="-1"></a>                           data.test<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.tree)</span>
<span id="cb25-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) which(pred.tree != data$DX_bl) identifies the locations</span></span>
<span id="cb25-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">#    of the incorrect predictions;</span></span>
<span id="cb25-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) length(any vector) returns the length of that vector;</span></span>
<span id="cb25-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) thus, the ratio of incorrect prediction over the total</span></span>
<span id="cb25-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">#    prediction is the prediction error</span></span>
<span id="cb25-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(err.tree)</span></code></pre></div>
<p></p>
</div>
</div>
<div id="remarks" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="statistical-model-vs.-causal-model" class="section level3 unnumbered">
<h3>Statistical model vs. causal model</h3>
<p>People unconsciously interprets a regression model as a causal model. When an unconscious interpretation is stated, it seems absurd and untrue, but it is fair to say that the line between a statistical model and a causal model is often blurred. We cannot blame ourselves for falling for this temptation before we have had a chance to see it through a critical lens, since both models share the same representation: an asymmetric form where predictors are on one side of the equation and the outcome is on the other side. Plus, the concept of <em>significance</em> is no less confusing: a common misinterpretation is to treat the <em>statistical significance</em> of a predictor as evidence of <em>causal significance</em> in the application context. The fact is that statistical significance doesn’t imply that the relationship between the predictor and the outcome variable is causal.</p>
<p>To see this, in what follows we will show an example that the statistical significance of a variable would disappear when some other variables are added into the model. Still using the AD dataset, we fit a regression model using the variable <code>AGE</code> only.</p>
<p></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb26-1" aria-hidden="true" tabindex="-1"></a>lm.AD.age <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> AGE, <span class="at">data =</span> AD)</span>
<span id="cb26-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.age)</span></code></pre></div>
<p></p>
<p>And the result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb27-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~  AGE, data = AD)</span></span>
<span id="cb27-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb27-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max</span></span>
<span id="cb27-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.7020 -0.9653  0.6948  1.6182  2.5447</span></span>
<span id="cb27-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb27-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb27-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 30.44147    0.94564  32.191   &lt;2e-16 ***</span></span>
<span id="cb27-12"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         -0.03333    0.01296  -2.572   0.0104 *</span></span>
<span id="cb27-13"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb27-14"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb27-15"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-16"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.11 on 515 degrees of freedom</span></span>
<span id="cb27-17"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.01268,    Adjusted R-squared:  0.01076</span></span>
<span id="cb27-18"><a href="chapter-2-abstraction-regression-tree-models.html#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 6.614 on 1 and 515 DF,  p-value: 0.0104</span></span></code></pre></div>
<p></p>
<p>The predictor, <code>AGE</code>, is significant since its <em>p-value</em> is <span class="math inline">\(0.0104\)</span>.</p>
<p>Now let’s include more demographics variables into the model.</p>
<p></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the multiple linear regression model </span></span>
<span id="cb28-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with more than one predictor</span></span>
<span id="cb28-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb28-3" aria-hidden="true" tabindex="-1"></a>lm.AD.demo <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span>  AGE <span class="sc">+</span> PTGENDER <span class="sc">+</span> PTEDUCAT,</span>
<span id="cb28-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb28-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> AD)</span>
<span id="cb28-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.demo)</span></code></pre></div>
<p></p>
<p>And the result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb29-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~ AGE + </span></span>
<span id="cb29-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="do">##    PTGENDER + PTEDUCAT, data = AD)</span></span>
<span id="cb29-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb29-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max</span></span>
<span id="cb29-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.4290 -0.9766  0.5796  1.4252  3.4539</span></span>
<span id="cb29-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb29-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb29-12"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 27.70377    1.11131  24.929  &lt; 2e-16 ***</span></span>
<span id="cb29-13"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         -0.02453    0.01282  -1.913   0.0563 .</span></span>
<span id="cb29-14"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER    -0.43356    0.18740  -2.314   0.0211 *</span></span>
<span id="cb29-15"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     0.17120    0.03432   4.988 8.35e-07 ***</span></span>
<span id="cb29-16"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb29-17"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb29-18"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-19"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.062 on 513 degrees of freedom</span></span>
<span id="cb29-20"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.0612, Adjusted R-squared:  0.05571</span></span>
<span id="cb29-21"><a href="chapter-2-abstraction-regression-tree-models.html#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 11.15 on 3 and 513 DF,  p-value: 4.245e-07</span></span></code></pre></div>
<p></p>
<p>Now we can see that the predictor <code>AGE</code> is on the boardline of significance with a <em>p-value</em> <span class="math inline">\(0.0563\)</span>. The other predictors, <code>PTGENDER</code> and <code>PTEDUCAT</code>, are significant. The reason that the predictor <code>AGE</code> is now no longer significant is an interesting phenomenon, but it is not unusual in practice that a significant predictor becomes insignificant when other variables are included or excluded<label for="tufte-sn-47" class="margin-toggle sidenote-number">47</label><input type="checkbox" id="tufte-sn-47" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">47</span> This is because of the statistical dependence of the estimation of the predictors. Remember that <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are two different entities. In the ground truth the two regression coefficients, <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span>, may be independent with each other, but <span class="math inline">\(\hat{\beta}_i\)</span> and <span class="math inline">\(\hat{\beta}_j\)</span> could still be correlated. As we have known that <span class="math display">\[\begin{equation*}  \small      \operatorname{cov}(\widehat{\boldsymbol{\beta}})=\sigma_{\epsilon}^{2}\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1},     \end{equation*}\]</span> as long as <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> is not an identity matrix, the estimators of the regression parameters are dependent in a complicated and data-dependant way. Due to this reason, we need to be cautious about how to interpret the estimated regression parameters, as they are interrelated constructs.</span>.</p>
<p>One strategy to mitigate this problem is to explore your data from every possible angle, and try out different model formulations. The goal of your data analysis is not to get a final conclusive model that dictates the rest of the analysis process. The data analysis is an exploratory and dynamic process, i.e., as you see, the dynamic interplay of the variables, how they impact each others’ significance in predicting the outcome, is something you could only obtain by analyzing the data in an exploratory and dynamic way. The fact that a model fits the data well and passes the significance test only means that there is nothing significant in the data that is found to be against the model. The goodness-of-fit of the data doesn’t mean that the data says this model is the only causal model and other models are impossible.</p>
</div>
<div id="design-of-experiments" class="section level3 unnumbered">
<h3>Design of experiments</h3>
<p>Related to this issue of “statistical model vs. causal model,” the design of experiments (DOE) is a discipline which provides systematic data collection procedures to render the regression model as a causal model. How this could be done demands a lengthy discussion and illustration<label for="tufte-sn-48" class="margin-toggle sidenote-number">48</label><input type="checkbox" id="tufte-sn-48" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">48</span> Interested readers may start with this book: Goos, P. and Jones, B., <em>Optimal Design of Experiments: A Case Study Approach</em>, Wiley, 2011.</span>. Here, we briefly review its foundation to see why it has the connection with a linear regression model.</p>
<p>We have seen in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:2-betaDist-matrix">(18)</a> that the uncertainty of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> comes from two sources, the noise in the data that is encoded in <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span>, and the structure of <span class="math inline">\(\boldsymbol{X}\)</span>. <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span> reflects essential uncertainty inherent in the system, but <span class="math inline">\(\boldsymbol{X}\)</span> is about how we collect the data. Thus, experimental design methods seek to optimize the structure of <span class="math inline">\(\boldsymbol{X}\)</span> such that the uncertainty of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> could be minimized.</p>
<p>For example, suppose that there are three predictors. Let’s consider the following structure of <span class="math inline">\(\boldsymbol{X}\)</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{X}=\left[ \begin{array}{lll}{1} &amp; {0} &amp; {0} \\ {0} &amp; {1} &amp; {0} \\ {0} &amp; {0} &amp; {1} \end{array}\right].
 
\end{equation*}\]</span></p>
<p>It can be seen that, with this structure, the variance of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is<label for="tufte-sn-49" class="margin-toggle sidenote-number">49</label><input type="checkbox" id="tufte-sn-49" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">49</span> <span class="math inline">\(\boldsymbol{I}\)</span> is the identity matrix. Here, <span class="math inline">\(\boldsymbol{I}_3 = \left[ \begin{array}{lll}{1} &amp; {0} &amp; {0} \\ {0} &amp; {1} &amp; {0} \\ {0} &amp; {0} &amp; {1} \end{array}\right].\)</span></span></p>
<p><span class="math display">\[\begin{equation*}
\small
  cov(\hat{\boldsymbol{\beta}})=\sigma_{\epsilon}^2\boldsymbol{I}_3. 
\end{equation*}\]</span></p>
<p>In other words, we can draw two main observations. First, the estimations of the regression parameters are now independent, given that their correlations are zero. Second, the variances of the estimated regression parameters are the same. Because of these two traits, this data matrix <span class="math inline">\(\boldsymbol X\)</span> is ideal and adopted in DOE to create <em>factorial designs</em>. For a linear regression model built on a dataset with such a data matrix, adding or deleting variables from the regression model will not result in changes of the estimations of other parameters.</p>
</div>
<div id="the-pessimistic-error-estimation-in-post-pruning" class="section level3 unnumbered">
<h3>The pessimistic error estimation in post-pruning</h3>
<p>Let’s look at the tree in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-13">24</a>. It has one root node, one inner node, and three leaf nodes. The target for tree pruning, for this example, is the inner node. In other words, should we prune the inner node and its subsequent child nodes?</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-13"></span>
<img src="graphics/2_13.png" alt="An example of tree pruning using pessimistic error" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 24: An example of tree pruning using pessimistic error<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We have mentioned that if the improvement on error is not significant, we should prune the node. Let’s denote the <strong>empirical error rate</strong><label for="tufte-sn-50" class="margin-toggle sidenote-number">50</label><input type="checkbox" id="tufte-sn-50" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">50</span> Empirical error is derived based on the training data.</span> as <span class="math inline">\(\hat e\)</span>. The reason we give the notation a <em>hat</em> is because it is only an estimate of an underlying parameter, the true error <span class="math inline">\(e\)</span>. <span class="math inline">\(\hat e\)</span> is usually smaller than <span class="math inline">\(e\)</span>, and thus, it is considered to be optimistic. To create a fairer estimate of <span class="math inline">\(e\)</span>, the <strong>pessimistic error estimation</strong> approach is used for tree pruning.</p>
<p>The pessimistic error estimation, like a regression model, builds on a hypothesized data-generating mechanism. Here, the <em>data</em> is the <em>errors</em> we observed from the training data. A data point can be either correctly or wrongly classified, and we can view the probability of being wrongly classified as a Bernoulli trial, while the parameter of this Bernoulli trial, commonly denoted as <span class="math inline">\(p\)</span>, is <span class="math inline">\(e\)</span>. If we denote the total number of errors we have observed on the <span class="math inline">\(n\)</span> data points as <span class="math inline">\(d\)</span>, we can derive that <span class="math inline">\(d\)</span> is distributed as a binomial distribution. We can write this data-generating mechanism as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
d \sim Bino\left(n, e\right).
 
\end{equation*}\]</span></p>
<p>Since <span class="math inline">\(n\)</span> is usually large, we can use the normal approximation for the binomial distribution</p>
<p><span class="math display">\[\begin{equation*}
\small
  
d \sim N\left(ne, ne(1-e)\right).
 
\end{equation*}\]</span></p>
<p>As <span class="math inline">\(\hat e = d/n\)</span>, we have</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\hat e \sim N\left(e, \frac{e(1-e)}{n}\right).
 
\end{equation*}\]</span></p>
<p>Skipping further derivations (more assumptions are imposed, indeed, to derive the following conclusion), we can derive the confidence interval of <span class="math inline">\(e\)</span> as</p>
<p><span class="math display">\[\begin{equation*}
\small
  \hat e - z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} \leq e \leq \hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}}. 
\end{equation*}\]</span></p>
<p>The upper bound of the interval, <span class="math inline">\(\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}}\)</span>, is named as the <em>pessimistic error</em>. The tree pruning methods that use the <em>pessimistic error</em> are motivated by a conservative perspective.</p>
<p>The pessimistic error depends on three values: <span class="math inline">\(\alpha\)</span>, which is often set to be <span class="math inline">\(0.25\)</span> so that <span class="math inline">\(z_{\alpha/2}=1.15\)</span>; <span class="math inline">\(\hat e\)</span>, which is the training error rate; and <span class="math inline">\(n\)</span>, which is the number of data points at the node<label for="tufte-sn-51" class="margin-toggle sidenote-number">51</label><input type="checkbox" id="tufte-sn-51" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">51</span> The pessimistic error is larger with a smaller <span class="math inline">\(n\)</span>, an estimation method that accounts for the sample size.</span>.</p>
<p>Now let’s revisit Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-13">24</a>.</p>
<p>First, let’s derive the pessimistic errors for the two child nodes of the inner node. The empirical error rate for the left child node is <span class="math inline">\(\hat e = \frac{9}{19}=0.4737\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\begin{equation*}
\small
  \hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.4737 + 1.15\sqrt{\frac{0.4737(1-0.4737)}{19}}=0.605. 
\end{equation*}\]</span></p>
<p>With this error rate, for a node with <span class="math inline">\(19\)</span> data points, the total misclassified data points can be <span class="math inline">\(mp=0.605\times 19=11.5\)</span>.</p>
<p>For the right child node, the empirical error rate is <span class="math inline">\(\hat e = \frac{9}{20}=0.45\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\begin{equation*}
\small
  \hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.45 + 1.15\sqrt{\frac{0.45(1-0.45)}{20}}=0.578. 
\end{equation*}\]</span></p>
<p>With this error rate, for a node with <span class="math inline">\(20\)</span> data points, the total misclassified data points can be <span class="math inline">\(mp=0.578\times 20=11.56\)</span>.</p>
<p>Thus, if we keep this branch, the total misclassified data points would be <span class="math inline">\(mp=11.5+11.56=23.06\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-14"></span>
<img src="graphics/2_14.png" alt="The pruned tree of Figure \@ref(fig:f2-13)" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 25: The pruned tree of Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-13">24</a><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now let’s evaluate the alternative: to cut the branch. This means the inner node will become a decision node, as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-14">25</a>. We will label the new decision node as C1, since <span class="math inline">\(20\)</span> of the included data points are labeled as C1, while <span class="math inline">\(19\)</span> are labeled as C2. The empirical error rate <span class="math inline">\(e\)</span> is <span class="math inline">\(\hat e = \frac{19}{39}=0.4871\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\begin{equation*}
\small
  \hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.4871 + 1.15\sqrt{\frac{0.4871(1-0.4871)}{39}}=0.579. 
\end{equation*}\]</span></p>
<p>With this error rate, for a dataset with 39 data points, the total misclassified data points can be <span class="math inline">\(mp=0.579\times 39=22.59\)</span>. This is what would happen if we prune the tree. As <span class="math inline">\(22.59 &lt; 23.06\)</span>, pruning is a better decision.</p>
<p>The pruned tree is shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-14">25</a>. A complete post-pruning method will continue to consider further pruning: now consider pruning the child nodes of the root node. Following the process outlined above, the would-be misclassified data points based on the pessimistic error rate at the root node is <span class="math inline">\(22.92\)</span>, and the total misclassified instances based on the pessimistic error rate from its child nodes is <span class="math inline">\(22.59+0=22.59\)</span>. Pruning the child nodes would lead to increased error. Thus, no further pruning is needed: the child nodes are kept and the final tree consists of three nodes.</p>
</div>
</div>
<div id="exercises" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-HW-lr">Table 4: </span>Dataset for building a linear regression model</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-0.15\)</span></td>
<td align="left"><span class="math inline">\(-0.48\)</span></td>
<td align="left"><span class="math inline">\(0.46\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-0.72\)</span></td>
<td align="left"><span class="math inline">\(-0.54\)</span></td>
<td align="left"><span class="math inline">\(-0.37\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.91\)</span></td>
<td align="left"><span class="math inline">\(-0.27\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0.61\)</span></td>
<td align="left"><span class="math inline">\(1.59\)</span></td>
<td align="left"><span class="math inline">\(1.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(-1.11\)</span></td>
<td align="left"><span class="math inline">\(0.34\)</span></td>
<td align="left"><span class="math inline">\(-0.11\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><!-- begin{enumerate} --></p>
<p>1. Here let’s consider the dataset in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-HW-lr">4</a>. Let’s build a linear regression model, i.e.,
<span class="math display">\[\begin{equation*}
\small
  
    y = \beta_{0}+\beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
     
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
\small
  
    \epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right).
     
\end{equation*}\]</span>
and calculate the regression parameters <span class="math inline">\(\beta_{0},\beta_{1},\beta_{2}\)</span> manually.</p>
<p>2. Follow up the data on Q1. Use the R pipeline to build the linear regression model. Compare the result from R and the result by your manual calculation.</p>
<p>3. Read the following output in R.</p>
<p><!-- end{enumerate} --></p>
<p></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb30-2"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data)</span></span>
<span id="cb30-3"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-4"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb30-5"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="do">##       Min        1Q    Median        3Q       Max </span></span>
<span id="cb30-6"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="do">## -0.239169 -0.065621  0.005689  0.064270  0.310456 </span></span>
<span id="cb30-7"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-8"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb30-9"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb30-10"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  0.009124   0.010473   0.871    0.386    </span></span>
<span id="cb30-11"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="do">## x1           1.008084   0.008696 115.926   &lt;2e-16 ***</span></span>
<span id="cb30-12"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="do">## x2           0.494473   0.009130  54.159   &lt;2e-16 ***</span></span>
<span id="cb30-13"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="do">## x3           0.012988   0.010055   1.292    0.200    </span></span>
<span id="cb30-14"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="do">## x4          -0.002329   0.009422  -0.247    0.805    </span></span>
<span id="cb30-15"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb30-16"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb30-17"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-18"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 0.1011 on 95 degrees of freedom</span></span>
<span id="cb30-19"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.9942, Adjusted R-squared:  0.994 </span></span>
<span id="cb30-20"><a href="chapter-2-abstraction-regression-tree-models.html#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  4079 on 4 and 95 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<p>4. (a) Write the fitted regression model. (b) Identify the significant variables. (c) What is the R-squared of this model? Does the model fit the data well? (d) What would you recommend as the next step in data analysis?</p>
<p>5. Consider the dataset in Table <a href="chapter-2-abstraction-regression-tree-models.html#tab:t2-hw-dt">5</a>. Build a decision tree model by manual calculation. To simplify the process, let’s only try three alternatives for the splits: <span class="math inline">\(x_1\geq0.59\)</span>, <span class="math inline">\(x_1\geq0.37\)</span>, and <span class="math inline">\(x_2\geq0.35\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-hw-dt">Table 5: </span>Dataset for building a decision tree</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.22\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0.58\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0.57\)</span></td>
<td align="left"><span class="math inline">\(0.28\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0.41\)</span></td>
<td align="left"><span class="math inline">\(0.43\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(0.6\)</span></td>
<td align="left"><span class="math inline">\(0.29\)</span></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(0.12\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(0.25\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(8\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p></p>
<p>6. Follow up on the dataset in Q5. Use the R pipeline for building a decision tree model. Compare the result from R and the result by your manual calculation.</p>
<p>7. Use the <code>mtcars</code> dataset in R, select the variable <code>mpg</code> as the outcome variable and other variables as predictors, run the R pipeline for linear regression, and summarize your findings.</p>
<p>8. Use the <code>mtcars</code> dataset in R, select the variable <code>mpg</code> as the outcome variable and other variables as predictors, run the R pipeline for decision tree, and summarize your findings. Another dataset is to use the <code>iris</code> dataset, select the variable <code>Species</code> as the outcome variable (i.e., to build a classification tree).</p>
<p>9. Design a simulated experiment to evaluate the effectiveness of the <code>lm()</code> in R. For instance, you can simulate <span class="math inline">\(100\)</span> samples from a linear regression model with <span class="math inline">\(2\)</span> variables,
<span class="math display">\[\begin{equation*}
\small
  
    y = \beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
     
\end{equation*}\]</span>
where <span class="math inline">\(\beta_{1} = 1\)</span>, <span class="math inline">\(\beta_{2} = 1\)</span>, and
<span class="math display">\[\begin{equation*}
\small
  
    \epsilon \sim N\left(0, 1\right).
     
\end{equation*}\]</span>
You can simulate <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> using the standard normal distribution <span class="math inline">\(N\left(0, 1\right)\)</span>. Run <code>lm()</code> on the simulated data, and see how close the fitted model is with the true model.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-hw-dt"></span>
<img src="graphics/2_hw_dt.png" alt="The true model for simulation experiment in Q12" width="60%"  />
<!--
<p class="caption marginnote">-->Figure 26: The true model for simulation experiment in Q12<!--</p>-->
<!--</div>--></span>
</p>
<p>
10. Follow up on the experiment in Q9. Let’s add two more variables <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_4\)</span> into the dataset but still generate <span class="math inline">\(100\)</span> samples from a linear regression model from the same underlying model
<span class="math display">\[\begin{equation*}
\small
  
    y = \beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
     
\end{equation*}\]</span>
where <span class="math inline">\(\beta_{1} = 1\)</span>, <span class="math inline">\(\beta_{2} = 1\)</span>, and
<span class="math display">\[\begin{equation*}
\small
  
    \epsilon \sim N\left(0, 1\right).
     
\end{equation*}\]</span>
In other words, <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_4\)</span> are insignificant variables. You can simulate <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_4\)</span> using the standard normal distribution <span class="math inline">\(N\left(0, 1\right)\)</span>. Run <code>lm()</code> on the simulated data, and see how close the fitted model is with the true model.</p>
<p><!-- end{enumerate} --></p>
<p><!-- begin{enumerate}[resume] --></p>
<p>11. Follow up on the experiment in Q10. Run <code>rpart()</code> on the simulated data, and see how close the fitted model is with the true model.</p>
<p>12. Design a simulated experiment to evaluate the effectiveness of the <code>rpart()</code> in R package <code>rpart</code>. For instance, you can simulate <span class="math inline">\(100\)</span> samples from a tree model as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-hw-dt">26</a>, run <code>rpart()</code> on the simulated data, and see how close the fitted model is with the true model.</p>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<p style="text-align: center;">
<a href="chapter-1-introduction.html"><button class="btn btn-default">Previous</button></a>
<a href="chapter-3-recognition-logistic-regression-ranking.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
