<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Chapter 10. Synthesis: Architecture &amp; Pipeline | DataAnalytics.utf8" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Analytics Book" />




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Analytics Book">

<title>Chapter 10. Synthesis: Architecture &amp; Pipeline | DataAnalytics.utf8</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#cover">Cover</a></li>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="chapter-10.-synthesis-architecture-pipeline" class="section level1 unnumbered">
<h1>Chapter 10. Synthesis: Architecture &amp; Pipeline</h1>
<div id="overview-8" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 10 is about <em>synthesis</em>. Synthesis is not a rigorous term but refers to a type of common pratice that integrates, consolidates, or streamlines many otherwise stand-alone models into a mega-model or pipeline. Two styles of synthesis will be introduced here, one represented by deep learning, and another takes the form as pipelines.</p>
</div>
<div id="deep-learning" class="section level2 unnumbered">
<h2>Deep learning</h2>
<p>To know more about “deep learning,” we need to start with its name. The word “deep” is ambiguous but expressive, undetermined but significant. This inviting gesture may have a dazzling effect, but it is based on a specific reason: a deep neural network model is truly deep in terms of its architecture—from input variables to output variables there are many layers in between. Other than that, it is not different from other models in this book. The basic framework of learning as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-1">3</a> and Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:ch2-genericmodel">(1)</a> in <strong>Chapter 2</strong> still holds true for deep learning.</p>
<p>The word “deep” doesn’t imply that other models we have learned so far are not deep. Many models have been studied in great depth, such as the linear models<label for="tufte-sn-248" class="margin-toggle sidenote-number">248</label><input type="checkbox" id="tufte-sn-248" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">248</span> Anderson, T. W., <em>An Introduction to Multivariate Statistical Analysis</em>, Wiley, 3rd edition, 2003.</span> and the learning theory developed for the support vector machine<label for="tufte-sn-249" class="margin-toggle sidenote-number">249</label><input type="checkbox" id="tufte-sn-249" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">249</span> Vapnik, V., <em>The Nature of Statistical Learning Theory</em>, Springer, 2000.</span>. In this chapter, we will refer to deep learning, specifically to those neural network (NN) models that have many hidden layers, because for NN models we could take the word “deep” at face value—if a model looks deep, it is a deep model. This superficiality, however, builds on a solid foundation<label for="tufte-sn-250" class="margin-toggle sidenote-number">250</label><input type="checkbox" id="tufte-sn-250" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">250</span> E.g., the Universal approximation theorem; please refer to Hornik, K., Approximation Capabilities of Multilayer Feedforward Networks, <em>Neural Networks</em>, Volume 4, Issue 2, Pages 251-257, 1991.</span>: a neural network with a more complex architecture means a more complex form for <span class="math inline">\(f(x)\)</span> in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:ch2-genericmodel">(1)</a>. In other words, this is an attractive proposal, since it suggests we can easily build up depth and capacity of the model by merely increasing its visual complexity. And there have been tools that allow users to drag ready-made modules and piece them together to create the architecture of the NN model they’d like to build, and automatically translate the architecture into its mathematical form and carry out the computational tasks for model training and prediction<label for="tufte-sn-251" class="margin-toggle sidenote-number">251</label><input type="checkbox" id="tufte-sn-251" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">251</span> E.g., TensorFlow <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>.</span>.</p>
<div id="rationale-and-formulation-16" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><em>An architecture means a function.</em> We have mentioned in <strong>Chapter 2</strong> that the data modeling methods seek explicit forms of <span class="math inline">\(f(x)\)</span> in Eq. <a href="chapter-2-abstraction-regression-tree-models.html#eq:ch2-genericmodel">(1)</a>, while algorithmic modeling methods seek implicit forms. Deep models bend the two. It is like an algorithmic modeling method that you don’t need to write up the specific form of <span class="math inline">\(f(x)\)</span>, while on the other hand, in theory you could write up <span class="math inline">\(f(x)\)</span> after you have had the architecture<label for="tufte-sn-252" class="margin-toggle sidenote-number">252</label><input type="checkbox" id="tufte-sn-252" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">252</span> In this sense, it is also like the kernel trick used in the SVM model. Remember that in <strong>Chapter 7</strong> we have seen that by using the kernel function in SVM, an implicit transformation of the variables is achieved, and we usually do not know what is the explicit form of <span class="math inline">\(\phi(x)\)</span> the SVM model encodes, but in theory there is such a form of <span class="math inline">\(\phi(x)\)</span>.</span>.</p>
<p>The architecture of a NN model could be quite expressive, i.e., Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-nn-architecture">178</a> shows an architecture of a neural network model with one layer that is flexible enough to include existing models such as the linear regression model, logistic regression model, and SVM, as shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-NNexample">53</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-nn-architecture"></span>
<p class="caption marginnote shownote">
Figure 178: Architecture of a simple neural network model. The figure is drawn using Alex LeNail’s online tool: <a href="http://alexlenail.me/NN-SVG/index.html">http://alexlenail.me/NN-SVG/index.html</a>.
</p>
<img src="graphics/10_nn_architecture.png" alt="Architecture of a simple neural network model. The figure is drawn using Alex LeNail's online tool: [http://alexlenail.me/NN-SVG/index.html](http://alexlenail.me/NN-SVG/index.html)." width="80%"  />
</div>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-NNexample">Table 53: </span>Expression of some models using the architecture of a one-layer neural network in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-nn-architecture">178</a></span><!--</caption>--></p>
<table>
<colgroup>
<col width="17%" />
<col width="28%" />
<col width="54%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Model</strong></th>
<th align="left"><strong>Activation Function <span class="math inline">\(\Phi\)</span></strong></th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear regression</td>
<td align="left">Linear: <span class="math inline">\(\Phi(z)=z\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\left(y-\sum_{i=1}^{p} w_{i} x_{i}\right)^2\)</span></td>
</tr>
<tr class="even">
<td align="left">Logistic regression</td>
<td align="left">Sigmoid: <span class="math inline">\(\Phi(z)=\frac{1}{1-e^{-z}}\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\log(1+\exp[-y\sum_{i=1}^{p} w_{i} x_{i}])\)</span></td>
</tr>
<tr class="odd">
<td align="left">Support vector machine</td>
<td align="left">Null: <span class="math inline">\(\Phi(z)=z\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\max(0,1-y\sum_{i=1}^{p} w_{i} x_{i})\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The NN structure shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-nn-architecture">178</a> is a basic form of NN architecture that is called the <strong>perceptron</strong>. As a basic form, it is a module that could be repeatedly used in different kinds of composition, e.g., in parallel, concatenation, or in a sequence. The basic forms are also called architectural primitives or foundational building blocks. Most deep architectures are built by combining these architectural primitives. Figures <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-nn-composition">179</a> and <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-nn-composition2">180</a> show two examples. There have been many of those basic forms developed. Softwares such as TensorFlow build on this concept by allowing users to use graphic user interface (GUI) to compose the architecture of their deep networks using these building blocks<label for="tufte-sn-253" class="margin-toggle sidenote-number">253</label><input type="checkbox" id="tufte-sn-253" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">253</span> For introduction of TensorFlow, readers may check out this book: Ramsundar, B. and Zadeh, R. <em>TensorFlow for Deep Learning: from Linear Regression to Reinforcement Learning</em>, O’Reilly Media, 2017.</span>.</p>
<p>As we have mentioned, for NN models there are theories showing that if a model looks deep, it is a deep model. The universal approximation theorem has shown that a NN model with one hidden layer could characterize all smooth functions. While there is no guarantee that in practice adding more layers will always be better, the theoretical results did imply that is the right direction.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-nn-composition"></span>
<img src="graphics/10_nn_composition.png" alt="Build more complicated NN models with a basic form" width="80%"  />
<p class="caption marginnote shownote">
Figure 179: Build more complicated NN models with a basic form
</p>
</div>
<p></p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-nn-composition2"></span>
<img src="graphics/10_nn_composition2.png" alt="Build deeper NN models with basic forms and activation functions" width="80%"  />
<p class="caption marginnote shownote">
Figure 180: Build deeper NN models with basic forms and activation functions
</p>
</div>
<p></p>
<p>Recall the XOR problem introduced in <strong>Chapter 7</strong> as shown in Figure <a href="chapter-7-learning-ii-svm-ensemble-learning.html#fig:f7-8">124</a>. With a slight modification of the problem to facilitate the presentation here, the dataset has <span class="math inline">\(4\)</span> data points</p>
<p><span class="math display">\[\begin{equation*}
\small
   
\begin{array}{l}{\boldsymbol{x}_{1}=(0,0), y_{1}=0}; \\ {\boldsymbol{x}_{2}=(0,1), y_{2}=1}; \\ {\boldsymbol{x}_{3}=(1,0), y_{3}=1} ;\\ {\boldsymbol{x}_{4}=(1,1), y_{4}=0.}\end{array}
 
\end{equation*}\]</span></p>
<p>This is a typical nonlinear problem. A NN model with one hidden layer as shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-xor-nn">181</a> could solve this problem.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-xor-nn"></span>
<img src="graphics/10_xor_nn.png" alt="Architecture of a neural network with a hidden layer" width="80%"  />
<p class="caption marginnote shownote">
Figure 181: Architecture of a neural network with a hidden layer
</p>
</div>
<p></p>
<p>For instance, for <span class="math inline">\(\boldsymbol{x}_{1}=(0,0)\)</span>, from the input layer to the first node (i.e., the upper one) in the hidden layer, we have</p>
<p><span class="math display">\[\begin{equation*}
\small
  
0 \times 1  + 0 \times 1 + 1 \times 0 = 0.
 
\end{equation*}\]</span></p>
<p>The value <span class="math inline">\(0\)</span> provides the input for the activation function at the hidden node, and we have <span class="math inline">\(\Phi(0) = \max (0,0) = 0\)</span>.</p>
<p>From the input layer to the second node (i.e., the lower one) in the hidden layer, we have</p>
<p><span class="math display">\[\begin{equation*}
\small
  
0 \times 1  + 0 \times 1 + 1 \times -1 = -1.
 
\end{equation*}\]</span></p>
<p>The value <span class="math inline">\(-1\)</span> provides the input for the activation function at the hidden node, and we have <span class="math inline">\(\Phi(-1) = \max (0,-1) = 0\)</span>.</p>
<p>Then, from the hidden layer to the output layer, we have</p>
<p><span class="math display">\[\begin{equation*}
\small
  
1 \times 0 - 2 \times 0 = 0.
 
\end{equation*}\]</span></p>
<p>Using the activation function at the output layer, <span class="math inline">\(\Phi(z) = z\)</span>, the final prediction correctly predicts</p>
<p><span class="math display">\[\begin{equation*}
\small
  
y = 0.
 
\end{equation*}\]</span></p>
<p>We can follow the same process and see that the two-layer NN as shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-xor-nn">181</a> could solve the XOR problem.</p>
<p><em>How to read a deep net.</em> Roughly speaking, there are three major efforts in developing deep learning models: to create basic forms, to design architectural principles or composition rules, and to design learning algorithms that can robustly and efficiently learn the parameters of the deep model using data<label for="tufte-sn-254" class="margin-toggle sidenote-number">254</label><input type="checkbox" id="tufte-sn-254" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">254</span> A deep NN model has massive parameters, so learning these parameters from data had been a challenge in the past. Some contributed the recent revitalization of deep learning—as the neural network model had its “rise and fall” in the past decades—to a range of optimization tricks such as pretraining and dropout, the growth of computing power, and the availability of Big Data, all enabled the data-driven learning of a giant collection of parameters of a deep NN model.</span>. Practical application of deep models is to make the network deeper by stacking these basic forms following some composition rules. From this perspective, it is not a surprise to see why it was quoted, “For reason in this sense is nothing but reckoning, that is adding and subtracting …”<label for="tufte-sn-255" class="margin-toggle sidenote-number">255</label><input type="checkbox" id="tufte-sn-255" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">255</span> Hobbes, T., Leviathan. 1651.</span>, to explain the logic of designing neural networks in Raul Rojas’s book<label for="tufte-sn-256" class="margin-toggle sidenote-number">256</label><input type="checkbox" id="tufte-sn-256" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">256</span> Rojas. R., <em>Neural Networks: a Systematic Introduction</em>. Springer, 1996.</span>.</p>
<p>We can take a look at the convolutional neural networks (CNN) as an example. The CNN is one popular deep NN model and is often used for learning from image data. Its architecture consists of a few basic forms and composition rules that are particularly developed for images.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-cnn-image"></span>
<img src="graphics/10_cnn_structure.png" alt="Architecture of a CNN model" width="80%"  />
<p class="caption marginnote shownote">
Figure 182: Architecture of a CNN model
</p>
</div>
<p></p>
<p>The CNN architecture shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-cnn-image">182</a> has two parts. The first part (i.e., everything before the last <span class="math inline">\(3\)</span> layers) is to translate the image data into vectorized form and provides the input for the second part (i.e., the last <span class="math inline">\(3\)</span> layers) that is a NN as we have discussed earlier. One basic form of CNN is the convolutional layer. The basic purpose of a convolutional layer is to transform the image into a feature map, as shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-conv-op">183</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-conv-op"></span>
<p class="caption marginnote shownote">
Figure 183: A convolutional layer aggregates spatially correlated information as a feature extraction process
</p>
<img src="graphics/10_conv_op.png" alt="A convolutional layer aggregates spatially correlated information as a feature extraction process" width="80%"  />
</div>
<p></p>
<p>Suppose that <span class="math inline">\(w_1=1\)</span>, <span class="math inline">\(w_2=2\)</span>, <span class="math inline">\(w_3=2\)</span>, <span class="math inline">\(w_4=1\)</span> in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-conv-op">183</a>; Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-conv-layer">184</a> further shows the computational details of how the convolutional layer works.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-conv-layer"></span>
<p class="caption marginnote shownote">
Figure 184: How the convolutional layer works.
</p>
<img src="graphics/10_conv_layer.png" alt="How the convolutional layer works." width="80%"  />
</div>
<p></p>
<p>The convolutional layer is good at exploiting the spatial structure<label for="tufte-sn-257" class="margin-toggle sidenote-number">257</label><input type="checkbox" id="tufte-sn-257" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">257</span> I.e., if the entities that are close to each other are semantically related, it is a spatial structure.</span> in its input data. Because of this, CNN is particularly useful for learning from image data, since for images the pixels close to one another are usually semantically related.</p>
<p>The max pooling layer is another basic form of CNN. Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-max-pool">185</a> shows how it works. The max pooling looks too simple an idea, but it works remarkably well. The real mystery when we look at a “simple” idea like this is why it was the max pooling that stood out among many other “simple” ideas. But there has been no conclusive theory to explain it<label for="tufte-sn-258" class="margin-toggle sidenote-number">258</label><input type="checkbox" id="tufte-sn-258" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">258</span> To quote Andrew Ng in his online course for convolutional neural networks (<a href="https://www.coursera.org/learn/convolutional-neural-networks">https://www.coursera.org/learn/convolutional-neural-networks</a>): <em>“… the main reason people use max pooling is because it’s been found in a lot of experiments to work well … I don’t know of anyone who fully knows if that is the real underlying reason.”</em></span>.</p>
<p>But one can compare the max pooling with the convolutional layer. One difference is that the parameters of a convolutional layer is learned from data, making it an adaptive and flexible form to a particular problem. The max pooling, however, is a fixed nonlinear transformation without parameters to learn. In other words, it has no computational cost. No wonder it is believed that one main function of the max pooling is to reduce the number of parameters of the deep NN model and to alleviate the computational cost. This would relieve some computational burden since a deep NN model has a massive number of parameters to be learned from data. Another aspect we should think of is that max pooling is good for image data. It may help increase the robustness of the model against translation invariance, i.e., to recognize an object, say, a cat, in an image, we need the algorithm to be resilient to the potential variation on angle or distance or any other factors that cause scale issues. Max pooling only keeps the “max” and discards the rest.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-max-pool"></span>
<p class="caption marginnote shownote">
Figure 185: How the max pooling layer works
</p>
<img src="graphics/10_max_pool.png" alt="How the max pooling layer works" width="80%"  />
</div>
<p></p>
<p>One can add as many convolutional layers or max pooling layers as needed when designing a CNN model, and the convolutional layers and the max pooling layer could be alternatively arranged as a pipeline to extract features from the image data, e.g., in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-cnn-image">182</a>, there are <span class="math inline">\(2\)</span> convolutional layers and <span class="math inline">\(1\)</span> max pooling layer. It has been found in many cases that for the CNN to be successful, it needs to be made quite deep. For this reason, some consider the deep NN models a different species from NN models.</p>
</div>
<div id="r-lab-15" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline for NN.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get the dataset into R and organize it in required format.</p>
<p></p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb206-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb206-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb206-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb206-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/KR.csv&quot;</span>)</span>
<span id="cb206-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb206-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(data)</span></span>
<span id="cb206-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb206-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb206-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb206-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data<span class="sc">$</span>x</span>
<span id="cb206-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>y</span>
<span id="cb206-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb206-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data </span></span>
<span id="cb206-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-15" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data) <span class="sc">*</span> <span class="dv">4</span><span class="sc">/</span><span class="dv">5</span>) )</span>
<span id="cb206-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-16" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb206-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data </span></span>
<span id="cb206-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb206-18" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates a list of models. For a NN model, important decisions are made on the design of the architecture, e.g., how many hidden layers and how many nodes in each hidden layer. For example, here, we create three NN models, all have one hidden layer but a different number of hidden nodes.</p>
<p></p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb207-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb207-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb207-3" aria-hidden="true" tabindex="-1"></a><span class="co"># NN model with one hidden layer and different # of nodes</span></span>
<span id="cb207-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb207-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb207-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model1: neuralnet(y~x, data=data, hidden=c(3)) </span></span>
<span id="cb207-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb207-6" aria-hidden="true" tabindex="-1"></a><span class="co"># model2: neuralnet(y~x, data=data, hidden=c(5)) </span></span>
<span id="cb207-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb207-7" aria-hidden="true" tabindex="-1"></a><span class="co"># model3: neuralnet(y~x, data=data, hidden=c(8)) </span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses cross-validation to evaluate the candidate models to identify the best model.</p>
<p></p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; cross-validation for model evaluation </span></span>
<span id="cb208-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-3" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">=</span> <span class="dv">10</span> <span class="co"># number of folds</span></span>
<span id="cb208-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the sample size, N, of the dataset</span></span>
<span id="cb208-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(data.train)[<span class="dv">1</span>] </span>
<span id="cb208-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-8" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb208-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(neuralnet)</span>
<span id="cb208-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-11" aria-hidden="true" tabindex="-1"></a><span class="co"># cv_mse records the prediction error for each fold</span></span>
<span id="cb208-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-12" aria-hidden="true" tabindex="-1"></a>cv_mse <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb208-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb208-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># In each iteration of the n_folds iterations</span></span>
<span id="cb208-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-15" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb208-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is the testing data, from the ith fold</span></span>
<span id="cb208-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-17" aria-hidden="true" tabindex="-1"></a>  data.test.cv <span class="ot">&lt;-</span> data.train[test_i, ]  </span>
<span id="cb208-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Then, the remaining data form the training data</span></span>
<span id="cb208-19"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-19" aria-hidden="true" tabindex="-1"></a>  data.train.cv <span class="ot">&lt;-</span> data.train[<span class="sc">-</span>test_i, ] </span>
<span id="cb208-20"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the neural network model with one hidden layer of 3</span></span>
<span id="cb208-21"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-21" aria-hidden="true" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(y<span class="sc">~</span>x, <span class="at">data=</span>data, <span class="at">hidden=</span><span class="fu">c</span>(<span class="dv">3</span>)) </span>
<span id="cb208-22"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb208-23"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-23" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">compute</span> (model1, data.test.cv)  </span>
<span id="cb208-24"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-24" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb208-25"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-25" aria-hidden="true" tabindex="-1"></a>  model1<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb208-26"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get the true y values for the testing data</span></span>
<span id="cb208-27"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-27" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test.cv<span class="sc">$</span>y </span>
<span id="cb208-28"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mean((true_y - y_hat)^2): mean squared error (MSE). </span></span>
<span id="cb208-29"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The smaller this error, the better your model is</span></span>
<span id="cb208-30"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-30" aria-hidden="true" tabindex="-1"></a>  cv_mse[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb208-31"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb208-32"><a href="chapter-10-synthesis-architecture-pipeline.html#cb208-32" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_mse)</span></code></pre></div>
<p></p>
<p>The result is shown below</p>
<p></p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.09439574 # Model1</span></span>
<span id="cb209-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb209-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.04433521 # Model2</span></span>
<span id="cb209-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb209-3" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.1142009  # Model3</span></span></code></pre></div>
<p></p>
<p>Obviously, <code>model2</code> achieves the lowest prediction error.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-visual-3nn"></span>
<p class="caption marginnote shownote">
Figure 186: Visualization of the three fitted models and the data
</p>
<img src="graphics/10_visual_3nn.png" alt="Visualization of the three fitted models and the data" width="80%"  />
</div>
<p></p>
<p>We can also visually examine the fitness of the three models in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-visual-3nn">186</a> to see how well the three models fit the data.</p>
<p></p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use visual inspection to assist the model selection. </span></span>
<span id="cb210-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb210-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb210-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-4" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model1, data.train)  </span>
<span id="cb210-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-5" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb210-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-6" aria-hidden="true" tabindex="-1"></a>model1<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb210-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb210-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-8" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model2, data.train)  </span>
<span id="cb210-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-9" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb210-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-10" aria-hidden="true" tabindex="-1"></a>model2<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb210-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb210-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-12" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model3, data.train) </span>
<span id="cb210-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-13" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb210-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-14" aria-hidden="true" tabindex="-1"></a>model3<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb210-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb210-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb210-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data.train, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb210-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model1<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb210-19"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model2<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb210-20"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model3<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb210-21"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-21" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;NN (3 hidden nodes)&quot;</span>, </span>
<span id="cb210-22"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;NN (5 hidden nodes)&quot;</span>, <span class="st">&quot;NN (8 hidden nodes)&quot;</span>), </span>
<span id="cb210-23"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;black&quot;</span>), </span>
<span id="cb210-24"><a href="chapter-10-synthesis-architecture-pipeline.html#cb210-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="dv">32</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-visual-finalnn"></span>
<p class="caption marginnote shownote">
Figure 187: Visualization of the architecture of the final model
</p>
<img src="graphics/10_visual_finalnn.png" alt="Visualization of the architecture of the final model" width="80%"  />
</div>
<p></p>
<p><strong>Step 5</strong> builds the final model. Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-visual-finalnn">187</a> shows the architecture of the final model.</p>
<p></p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, build your final model</span></span>
<span id="cb211-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb211-2" aria-hidden="true" tabindex="-1"></a>nn.final <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(y<span class="sc">~</span>x, <span class="at">data=</span>data.train, <span class="at">hidden=</span><span class="fu">c</span>(<span class="dv">5</span>)) <span class="co"># </span></span>
<span id="cb211-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb211-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nn.final) <span class="co"># Draw the architecture of the NN model</span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the final model for prediction.</p>
<p></p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Evaluate the prediction performance of your model</span></span>
<span id="cb212-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb212-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-3" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(nn.final, data.test)  </span>
<span id="cb212-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-4" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result </span>
<span id="cb212-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-5" aria-hidden="true" tabindex="-1"></a><span class="co"># get the true y values for the testing data</span></span>
<span id="cb212-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-6" aria-hidden="true" tabindex="-1"></a>true_y <span class="ot">&lt;-</span> data.test<span class="sc">$</span>y  </span>
<span id="cb212-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-7" aria-hidden="true" tabindex="-1"></a><span class="co"># mean((true_y - y_hat)^2): mean squared error (MSE). </span></span>
<span id="cb212-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The smaller this error, the better your model is</span></span>
<span id="cb212-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-9" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb212-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb212-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(mse)</span></code></pre></div>
<p></p>
<p><em>The 6-Step R Pipeline for CNN.</em> Before starting the pipeline, let’s first install the Keras package.</p>
<p></p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;devtools&quot;</span>) <span class="co"># install devtools</span></span>
<span id="cb213-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb213-2" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;rstudio/keras&quot;</span>) <span class="co"># install Keras</span></span></code></pre></div>
<p></p>
<p><strong>Step 1</strong> and <strong>Step 2</strong> get the MNIST handwritten digit dataset into R and process the data in required format. The goal is to classify a handwritten number into one of the <span class="math inline">\(10\)</span> classes (from <span class="math inline">\(0\)</span> to <span class="math inline">\(9\)</span>).</p>
<p></p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read digits classification data </span></span>
<span id="cb214-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb214-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-3" aria-hidden="true" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb214-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb214-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-6" aria-hidden="true" tabindex="-1"></a><span class="co"># code adapted from </span></span>
<span id="cb214-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-7" aria-hidden="true" tabindex="-1"></a><span class="co"># keras.rstudio.com/articles/examples/mnist_cnn.html</span></span>
<span id="cb214-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Input image dimensions</span></span>
<span id="cb214-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-9" aria-hidden="true" tabindex="-1"></a>img_rows <span class="ot">&lt;-</span> <span class="dv">28</span></span>
<span id="cb214-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-10" aria-hidden="true" tabindex="-1"></a>img_cols <span class="ot">&lt;-</span> <span class="dv">28</span></span>
<span id="cb214-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-11" aria-hidden="true" tabindex="-1"></a>num_classes <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb214-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-13" aria-hidden="true" tabindex="-1"></a><span class="co"># The data, shuffled and split between training and testing sets</span></span>
<span id="cb214-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-14" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb214-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-15" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb214-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-16" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb214-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-17" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y</span>
<span id="cb214-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-19"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Redefine  dimension of train/test inputs</span></span>
<span id="cb214-20"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-20" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_train, </span>
<span id="cb214-21"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-21" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(<span class="fu">nrow</span>(x_train), img_rows, img_cols, <span class="dv">1</span>))</span>
<span id="cb214-22"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-22" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_test, </span>
<span id="cb214-23"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-23" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(<span class="fu">nrow</span>(x_test), img_rows, img_cols, <span class="dv">1</span>))</span>
<span id="cb214-24"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-24" aria-hidden="true" tabindex="-1"></a>input_shape <span class="ot">&lt;-</span> <span class="fu">c</span>(img_rows, img_cols, <span class="dv">1</span>)</span>
<span id="cb214-25"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-26"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform RGB values into [0,1] range</span></span>
<span id="cb214-27"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-27" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> x_train <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb214-28"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-28" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> x_test <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb214-29"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-30"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-30" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&#39;x_train_shape:&#39;</span>, <span class="fu">dim</span>(x_train), <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb214-31"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-31" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">nrow</span>(x_train), <span class="st">&#39;train samples</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb214-32"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-32" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">nrow</span>(x_test), <span class="st">&#39;test samples</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb214-33"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-34"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert class vectors to binary class matrices</span></span>
<span id="cb214-35"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-35" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_train, num_classes)</span>
<span id="cb214-36"><a href="chapter-10-synthesis-architecture-pipeline.html#cb214-36" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_test, num_classes)</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates different models. In deep learning, parameters that are determined before training a model are called <strong>hyperparameters</strong>. Hyperparameters for a CNN include number of layers, number of nodes for a layer, kernel size of a convolution layer<label for="tufte-sn-259" class="margin-toggle sidenote-number">259</label><input type="checkbox" id="tufte-sn-259" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">259</span> E.g., in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-conv-layer">184</a> the kernel size is <span class="math inline">\(2\)</span>.</span>, etc. Here we create three models with different kernel sizes for the convolution layers.</p>
<p></p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb215-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-2" aria-hidden="true" tabindex="-1"></a>define_model <span class="ot">&lt;-</span> <span class="cf">function</span>(kernel_size){</span>
<span id="cb215-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-3" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb215-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convolution layer 1</span></span>
<span id="cb215-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">8</span>, </span>
<span id="cb215-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">kernel_size =</span> <span class="fu">c</span>(kernel_size,kernel_size), </span>
<span id="cb215-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb215-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">input_shape =</span> input_shape) <span class="sc">%&gt;%</span> </span>
<span id="cb215-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pooling layer 1</span></span>
<span id="cb215-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb215-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convolution layer 2</span></span>
<span id="cb215-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">16</span>, </span>
<span id="cb215-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">kernel_size =</span> <span class="fu">c</span>(kernel_size,kernel_size), </span>
<span id="cb215-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb215-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pooling layer 2</span></span>
<span id="cb215-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb215-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dense layers</span></span>
<span id="cb215-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb215-19"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb215-20"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> num_classes, <span class="at">activation =</span> <span class="st">&#39;softmax&#39;</span>)</span>
<span id="cb215-21"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb215-22"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compile model</span></span>
<span id="cb215-23"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-23" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb215-24"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> loss_categorical_crossentropy,</span>
<span id="cb215-25"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="fu">optimizer_adadelta</span>(),</span>
<span id="cb215-26"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb215-27"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb215-28"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(model)</span>
<span id="cb215-29"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb215-30"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-30" aria-hidden="true" tabindex="-1"></a><span class="co"># define three models</span></span>
<span id="cb215-31"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-31" aria-hidden="true" tabindex="-1"></a>model_kernel_1 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">2</span>)</span>
<span id="cb215-32"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-32" aria-hidden="true" tabindex="-1"></a>model_kernel_2 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">3</span>)</span>
<span id="cb215-33"><a href="chapter-10-synthesis-architecture-pipeline.html#cb215-33" aria-hidden="true" tabindex="-1"></a>model_kernel_3 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">5</span>)</span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses cross-validation to evaluate the candidate models to identify the best model.</p>
<p></p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Use cross-validation for model evaluation</span></span>
<span id="cb216-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-3" aria-hidden="true" tabindex="-1"></a><span class="co"># set upfunction for evaluating accuracy</span></span>
<span id="cb216-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-4" aria-hidden="true" tabindex="-1"></a>cv_accuracy <span class="ot">&lt;-</span> <span class="cf">function</span>(n_folds, kernel_size,x_train,y_train){</span>
<span id="cb216-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-5" aria-hidden="true" tabindex="-1"></a>  N <span class="ot">&lt;-</span> <span class="fu">dim</span>(x_train)[<span class="dv">1</span>] <span class="co"># the sample size, N, of the dataset</span></span>
<span id="cb216-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-6" aria-hidden="true" tabindex="-1"></a>  folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb216-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb216-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-8" aria-hidden="true" tabindex="-1"></a>  accuracy_v <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb216-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb216-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set up training and testing data</span></span>
<span id="cb216-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-11" aria-hidden="true" tabindex="-1"></a>    test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k)</span>
<span id="cb216-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-12" aria-hidden="true" tabindex="-1"></a>    x.train.cv <span class="ot">&lt;-</span> x_train[<span class="sc">-</span>test_i,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>] </span>
<span id="cb216-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-13" aria-hidden="true" tabindex="-1"></a>    x.test.cv <span class="ot">&lt;-</span> x_train[test_i,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>]   </span>
<span id="cb216-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-14" aria-hidden="true" tabindex="-1"></a>    y.train.cv <span class="ot">&lt;-</span> y_train[<span class="sc">-</span>test_i,,drop<span class="ot">=</span><span class="cn">FALSE</span> ] </span>
<span id="cb216-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-15" aria-hidden="true" tabindex="-1"></a>    y.test.cv <span class="ot">&lt;-</span> y_train[test_i,,drop<span class="ot">=</span><span class="cn">FALSE</span> ]</span>
<span id="cb216-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-16" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb216-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-17" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">define_model</span>(kernel_size)</span>
<span id="cb216-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-18" aria-hidden="true" tabindex="-1"></a>    model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb216-19"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-19" aria-hidden="true" tabindex="-1"></a>      x_train, y_train, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb216-20"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-20" aria-hidden="true" tabindex="-1"></a>      <span class="at">epochs =</span> <span class="dv">2</span>,<span class="at">validation_split =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> <span class="dv">0</span></span>
<span id="cb216-21"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb216-22"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-22" aria-hidden="true" tabindex="-1"></a>    scores <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(</span>
<span id="cb216-23"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-23" aria-hidden="true" tabindex="-1"></a>    x.test.cv, y.test.cv, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb216-24"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb216-25"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-25" aria-hidden="true" tabindex="-1"></a>    accuracy_v <span class="ot">&lt;-</span> <span class="fu">c</span>(accuracy_v, scores[<span class="dv">2</span>])</span>
<span id="cb216-26"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-26" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb216-27"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(accuracy_v)</span>
<span id="cb216-28"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb216-29"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-29" aria-hidden="true" tabindex="-1"></a><span class="co"># get average accuracy for each model</span></span>
<span id="cb216-30"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-30" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_1 <span class="ot">&lt;-</span> </span>
<span id="cb216-31"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">2</span>,x_train,y_train)</span>
<span id="cb216-32"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-32" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_1))</span>
<span id="cb216-33"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-34"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-34" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_2 <span class="ot">&lt;-</span> </span>
<span id="cb216-35"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">3</span>,x_train,y_train)</span>
<span id="cb216-36"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-36" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_2))</span>
<span id="cb216-37"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-38"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-38" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_3 <span class="ot">&lt;-</span> </span>
<span id="cb216-39"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">5</span>,x_train,y_train)</span>
<span id="cb216-40"><a href="chapter-10-synthesis-architecture-pipeline.html#cb216-40" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_3))</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9680667 # Model1</span></span>
<span id="cb217-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9742167 # Model2</span></span>
<span id="cb217-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb217-3" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9760833  # Model3</span></span></code></pre></div>
<p></p>
<p><strong>Step 5</strong> builds the final model based on all the training data.</p>
<p></p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, build your final model</span></span>
<span id="cb218-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb218-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb218-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb218-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">define_model</span>(<span class="dv">5</span>)</span>
<span id="cb218-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb218-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb218-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb218-5" aria-hidden="true" tabindex="-1"></a>      x_train, y_train, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb218-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb218-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">epochs =</span> <span class="dv">2</span>,<span class="at">validation_split =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> <span class="dv">0</span></span>
<span id="cb218-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb218-7" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the final model for prediction.</p>
<p></p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Evaluate the prediction performance of your model</span></span>
<span id="cb219-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb219-2" aria-hidden="true" tabindex="-1"></a>scores <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(</span>
<span id="cb219-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb219-3" aria-hidden="true" tabindex="-1"></a>    x_test, y_test, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb219-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb219-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(scores[<span class="dv">2</span>])</span></code></pre></div>
<p></p>
<p>To visualize the process of how this CNN model works, the following R code is used to visualize the output from each layer, shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-cnn-activations">188</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-cnn-activations"></span>
<p class="caption marginnote shownote">
Figure 188: Visualize the outputs from all layers of the CNN model
</p>
<img src="graphics/10_visual_7_activations.png" alt="Visualize the outputs from all layers of the CNN model" width="80%"  />
</div>
<p></p>
<p></p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize output for a layer</span></span>
<span id="cb220-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-3" aria-hidden="true" tabindex="-1"></a><span class="co"># use the first image from testing data</span></span>
<span id="cb220-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-4" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> x_test[<span class="dv">1</span>,,,]</span>
<span id="cb220-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.raster</span>(img))</span>
<span id="cb220-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-6" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> x_test[<span class="dv">1</span>,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb220-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-8" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to plot an image</span></span>
<span id="cb220-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-9" aria-hidden="true" tabindex="-1"></a>plot_image <span class="ot">&lt;-</span> <span class="cf">function</span>(channel) {</span>
<span id="cb220-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-10" aria-hidden="true" tabindex="-1"></a>    rotate <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">t</span>(<span class="fu">apply</span>(x, <span class="dv">2</span>, rev))</span>
<span id="cb220-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">image</span>(<span class="fu">rotate</span>(channel), <span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">asp =</span> <span class="dv">1</span>, </span>
<span id="cb220-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">col =</span> <span class="fu">gray.colors</span>(<span class="dv">12</span>))</span>
<span id="cb220-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb220-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-14" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the testing image</span></span>
<span id="cb220-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_image</span>(  <span class="dv">1</span> <span class="sc">-</span> img[<span class="dv">1</span>,,,]   )</span>
<span id="cb220-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-17" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the output from the second layer </span></span>
<span id="cb220-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-18" aria-hidden="true" tabindex="-1"></a>layer_number <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb220-19"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-20"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-20" aria-hidden="true" tabindex="-1"></a><span class="co"># print layer name</span></span>
<span id="cb220-21"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-21" aria-hidden="true" tabindex="-1"></a>layer_name <span class="ot">&lt;-</span> model<span class="sc">$</span>layers[[layer_number]]<span class="sc">$</span>name</span>
<span id="cb220-22"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-22" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(layer_name)</span>
<span id="cb220-23"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-24"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-24" aria-hidden="true" tabindex="-1"></a>layer_outputs <span class="ot">&lt;-</span> <span class="fu">lapply</span>(model<span class="sc">$</span>layers[layer_number], </span>
<span id="cb220-25"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-25" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">function</span>(layer) layer<span class="sc">$</span>output)</span>
<span id="cb220-26"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-26" aria-hidden="true" tabindex="-1"></a>activation_model <span class="ot">&lt;-</span> <span class="fu">keras_model</span>(<span class="at">inputs =</span> model<span class="sc">$</span>input, </span>
<span id="cb220-27"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-27" aria-hidden="true" tabindex="-1"></a>                                <span class="at">outputs =</span> layer_outputs)</span>
<span id="cb220-28"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-28" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the outputs from the layer for the image</span></span>
<span id="cb220-29"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-29" aria-hidden="true" tabindex="-1"></a>layer_activation <span class="ot">&lt;-</span> activation_model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(img)</span>
<span id="cb220-30"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-31"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-31" aria-hidden="true" tabindex="-1"></a><span class="co"># check dimension</span></span>
<span id="cb220-32"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-32" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">dim</span>(layer_activation))</span>
<span id="cb220-33"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-34"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-34" aria-hidden="true" tabindex="-1"></a><span class="co"># number of features</span></span>
<span id="cb220-35"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-35" aria-hidden="true" tabindex="-1"></a>n_features <span class="ot">&lt;-</span> <span class="fu">dim</span>(layer_activation)[[<span class="dv">4</span>]] </span>
<span id="cb220-36"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-36" aria-hidden="true" tabindex="-1"></a><span class="co"># image width</span></span>
<span id="cb220-37"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-37" aria-hidden="true" tabindex="-1"></a>image_size <span class="ot">&lt;-</span> <span class="fu">dim</span>(layer_activation)[[<span class="dv">2</span>]] </span>
<span id="cb220-38"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-38" aria-hidden="true" tabindex="-1"></a><span class="co"># number of columns and images per column </span></span>
<span id="cb220-39"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-39" aria-hidden="true" tabindex="-1"></a><span class="co"># (each column plots an image)</span></span>
<span id="cb220-40"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-40" aria-hidden="true" tabindex="-1"></a>n_cols <span class="ot">&lt;-</span> n_features </span>
<span id="cb220-41"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-41" aria-hidden="true" tabindex="-1"></a>images_per_col <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co">#</span></span>
<span id="cb220-42"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-43"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-43" aria-hidden="true" tabindex="-1"></a><span class="co"># plot n_cols of images</span></span>
<span id="cb220-44"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-44" aria-hidden="true" tabindex="-1"></a>op <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(n_cols, images_per_col), </span>
<span id="cb220-45"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-45" aria-hidden="true" tabindex="-1"></a>            <span class="at">mai =</span> <span class="fu">rep_len</span>(<span class="dv">0</span>, <span class="dv">4</span>)) </span>
<span id="cb220-46"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-47"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-47" aria-hidden="true" tabindex="-1"></a><span class="co"># plot each image</span></span>
<span id="cb220-48"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (col <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span>(n_cols<span class="dv">-1</span>)) {</span>
<span id="cb220-49"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-49" aria-hidden="true" tabindex="-1"></a>        col_ix <span class="ot">&lt;-</span> col <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb220-50"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-50" aria-hidden="true" tabindex="-1"></a>        channel_image <span class="ot">&lt;-</span> layer_activation[<span class="dv">1</span>,,,col_ix]</span>
<span id="cb220-51"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-51" aria-hidden="true" tabindex="-1"></a>      <span class="fu">plot_image</span>(<span class="dv">1</span><span class="sc">-</span>channel_image)</span>
<span id="cb220-52"><a href="chapter-10-synthesis-architecture-pipeline.html#cb220-52" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
</div>
</div>
<div id="intrees" class="section level2 unnumbered">
<h2>inTrees</h2>
<div id="rationale-and-formulation-17" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><em>What is a variable?</em> Given an Excel file, we call the column that is titled as the name <em>Age</em> as a variable. And in fact, as a convention, in an Excel file or a data table in some common formats, we usually do not doubt that each column implies a variable. These “variables,” or entities, may have definitions in the domain of common sense (i.e., where we take things for granted), but they may not be the best candidates to characterize the system under study. Recall that in <strong>Chapter 2</strong> we mentioned that the goal of modeling begins with abstraction—“identification of a few main entities from the problem,” and continues to “characterize their relationships.” If we comfortably play the data using the variables that have been defined without examination, we lose sight of a large territory of data analytics—identification of a few main entities from the problem that can sufficiently characterize the problem.</p>
<p>Now let’s switch to the domain of linear regression. A variable is an abstract entity in the equation of the linear regression model<label for="tufte-sn-260" class="margin-toggle sidenote-number">260</label><input type="checkbox" id="tufte-sn-260" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">260</span> I.e., denoted as <span class="math inline">\(x_i\)</span>.</span>, multiplied by a regression coefficient<label for="tufte-sn-261" class="margin-toggle sidenote-number">261</label><input type="checkbox" id="tufte-sn-261" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">261</span> I.e., denoted as <span class="math inline">\(\beta_i\)</span>.</span>. It stands in the equation in parallel with other variables, which jointly determines an outcome variable. This form implies that the difference between the variables are only numeric, characterized by the differences in signs and magnitudes (i.e., encoded in the regression coefficients), but not in semantics. Now comes a reflection: in order for a linear regression model to work out in an application, shouldn’t we ensure that the variables <em>could be</em> lined up in this manner of apposition<label for="tufte-sn-262" class="margin-toggle sidenote-number">262</label><input type="checkbox" id="tufte-sn-262" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">262</span> <em>Apposition</em>—with a little abuse of the term—the grammar used in the linear regression to line up the variables in a parallel and additive manner.</span>?</p>
<p>In other words, if the variables in the domain of common sense are not semantic equals, the application of linear regression on them is questionable. For example, it is probably common sense to line up a few genetic factors in Eq. <a href="chapter-10-synthesis-architecture-pipeline.html#eq:10-apposition">(102)</a>, but could we also put <em>Age</em> as another variable that stands among the genetic factors in a line? In many contexts, we need to work out a better definition of the variable, i.e., it is not uncommon to define two new variables such as <em><span class="math inline">\(\text{Age} \leq 65\)</span></em> and <em><span class="math inline">\(\text{Age} &gt; 65\)</span></em> instead of using the variable <em>Age</em> directly. Sometimes we use the variable <em>Age</em> in a model because it is named as <em>Age</em>. But what is <em>Age</em>? When we put a variable in a model, it is destined to be <em>re</em>defined, either before the analysis, or after, or along the way.
<span class="math display" id="eq:10-apposition">\[\begin{equation}
\small
        y = \ldots + \underbrace{\beta_1 x_1}_{\text{A}} + \underbrace{\beta_2 x_2}_{\text{p}} + \underbrace{\beta_3 x_3}_{\text{p}} + \underbrace{\beta_4 x_4}_{\text{o}} + \underbrace{\beta_5 x_5}_{\text{s}} +
        \underbrace{\beta_6 x_6}_{\text{i}} +
        \underbrace{\beta_7 x_7}_{\text{t}} + \underbrace{\beta_8 x_8}_{\text{i}} + \underbrace{\beta_9 x_9}_{\text{o}} + \underbrace{\beta_{10} x_{10}}_{\text{n}} + \ldots
\tag{102}
\end{equation}\]</span></p>
<p>This effort to redefine variables could be automated by tree models. Recall that the tree models use rule-based semantics. Rules like <em><span class="math inline">\(\text{Age} \leq 65\)</span></em> and <em><span class="math inline">\(\text{Age} &gt; 65\)</span></em> sometimes yield statistically significant <em>and</em> semantically meaningful entities, perfect candidates for variable redefinition purposes. And a tree is essentially a collection of multiple rules. If we could run tree models on a dataset first, we could extract those rules, and each rule is a new variable.</p>
<p>This is the starting point of <code>inTrees</code>. It uses the random forest to collect potentially useful rules<label for="tufte-sn-263" class="margin-toggle sidenote-number">263</label><input type="checkbox" id="tufte-sn-263" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">263</span> This step automates the variable redefinition process.</span>, then puts the rules as the variables into a model<label for="tufte-sn-264" class="margin-toggle sidenote-number">264</label><input type="checkbox" id="tufte-sn-264" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">264</span> I.e., a classification/regression model. This step automates the integration of the variable redefinition with modeling.</span> and employs a computational process to select the final variables. In this way, we have the best parts of both methods: the rules capture the variable-level patterns in the data, and the model (i.e., a regression model) captures the synthetic effects of these patterns in predicting an outcome variable. Note that <code>inTrees</code> is not the first of its kind. It follows a few pioneers such as the <code>rulefit</code><label for="tufte-sn-265" class="margin-toggle sidenote-number">265</label><input type="checkbox" id="tufte-sn-265" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">265</span> Friedman, J.H. and Popescu, B.E., <em>Predictive learning via rule ensembles</em>. Annals of Applied Statistics, Volume 2, Number 3, Pages 916-954, 2008.</span> and modifies existing efforts according to some in-field experiences.</p>
</div>
<div id="theory-and-method-11" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>The <code>inTrees</code> uses a framework that is shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-inTrees">189</a>. In the following text, we introduce each functionality of the <code>inTrees</code> framework.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-inTrees"></span>
<p class="caption marginnote shownote">
Figure 189: The pipeline of <code>inTrees</code>
</p>
<img src="graphics/10_intrees.png" alt="The pipeline of `inTrees`" width="80%"  />
</div>
<p></p>
<p>Consider the dataset that has <span class="math inline">\(2\)</span> predictors and <span class="math inline">\(7\)</span> instances as shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-1">Table 54: </span>Example of a dataset with <span class="math inline">\(7\)</span> instances</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<!-- % It can be seen that the tree was built based on the resampled dataset that includes the instances $\{1,1,2,2,7,7,7\}$. In other words, in this resampled dataset, the instance (ID: $1$) was resampled twice, the instance (ID: $2$) was resampled twice, and the instance (ID: $7$) was resampled three times. In the tree, the root and inner nodes are labeled with the data point IDs and leaf nodes are labeled with the data point IDs and the decisions (i.e., which class to predict).   -->
<p><em>Extract rules.</em> The <code>inTrees</code> uses a tree emsemble learning method to grow many trees. A decision tree can be dissembled into a set of rules. For example, suppose that a random forest model has been built on the dataset shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a>. One tree of this random forest model is shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-3">190</a>. Three rules (each rule corresponds to a leaf node) are extracted and shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-3"></span>
<img src="graphics/10_3.png" alt="Example of a decision tree; leaf nodes (a.k.a., decision nodes) are shadowed in gray." width="100%"  />
<!--
<p class="caption marginnote">-->Figure 190: Example of a decision tree; leaf nodes (a.k.a., decision nodes) are shadowed in gray.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<!-- % \begin{equation*}
\small
   -->
<!-- % \begin{array}{l}{\text{Rule 1: } \{ X_1=0\to Class=C_1\}}; \\ { \text{Rule 2: } \{ X_1 \neq 0,X_2=0\to Class=C_1\}}; \\ { \text{Rule 3: } \{X_1 \neq 0,X_2 \neq 0 \to Class=C_0\}.}\end{array} -->
<!-- %  
\end{equation*} -->
<p>Each rule is evaluated by three criteria: the <strong>length</strong> of a rule that is defined as the number of variables in the rule; the <strong>frequency</strong> of a rule that is the proportion of data points in the dataset that meet the condition of the rule, and the <strong>error rate</strong> of a rule. For classification problems, the error rate of a rule is the number of data points incorrectly identified by the rule divided by the number of data points that meet the condition of the rule.</p>
<p>For regression problems, the error rate of a rule is the <em>mean squared error (MSE)</em>, that is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  MSE = \frac{1}{N}\sum_{i=1}^N \left(y_i - \bar{y}\right)^2, 
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the number of data points in the leaf node that corresponds to the rule, <span class="math inline">\(y_i\)</span> is the value of the outcome variable of the <span class="math inline">\(i^{th}\)</span> data point, and <span class="math inline">\(\bar y\)</span> is the average of the outcome variable (i.e., as the prediction at the leaf node).</p>
<p>Based on these three criteria, the evaluation of the three rules is shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a>.<label for="tufte-sn-266" class="margin-toggle sidenote-number">266</label><input type="checkbox" id="tufte-sn-266" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">266</span> Apply each rule on the data points in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a>.</span></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-2">Table 55: </span>Evaluation of the three rules extracted from the tree in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-3">190</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Rule</th>
<th align="left">Length</th>
<th align="left">Frequency</th>
<th align="left">Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 = 0 \to Class = C_0\}\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(5/7\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1/7\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1/7\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>Prune rules.</em> A lengthy rule, i.e., a rule with many variables, is hard to interpret. For example, consider a rule</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\text{Rule: } \{ \text{Age} \leq 65, \text{Gene A } = \text{Type 1}, \text{Gene B } = \text{Type 3} \to Class= \text{No risk}\}.
 
\end{equation*}\]</span></p>
<p>It is unknown if the three variables are equally important in making the prediction. And, because the way the random forests grow the trees, it is possible that some variables in a rule are not significant at all<label for="tufte-sn-267" class="margin-toggle sidenote-number">267</label><input type="checkbox" id="tufte-sn-267" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">267</span> I.e., these variables are selected because the random forest model purposely <em>randomizes</em> the learning process.</span>. Therefore, it is beneficial to prune the rules and remove irrelevant variables from the rules.</p>
<p>Take Rule <span class="math inline">\(2\)</span> <span class="math inline">\(\{x_1 \neq 0, x_2=0\to Class=C_1\}\)</span> for example. The error rate for this rule is <span class="math inline">\(0\)</span>.<label for="tufte-sn-268" class="margin-toggle sidenote-number">268</label><input type="checkbox" id="tufte-sn-268" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">268</span> See Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a>.</span> Now remove <span class="math inline">\(x_1 \neq 0\)</span> from the rule, and the new rule becomes <span class="math display">\[\begin{equation*}
\small
  \{x_2=0\to Class=C_1\}, 
\end{equation*}\]</span> which has an error of <span class="math inline">\(3/5\)</span>.<label for="tufte-sn-269" class="margin-toggle sidenote-number">269</label><input type="checkbox" id="tufte-sn-269" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">269</span> Use Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a>.</span> Therefore, the error rate increases by <span class="math inline">\(3/5\)</span>. This increase of error rate is named <em>decay</em> in the terminology of <code>inTrees</code>. A threshold is set by the user, i.e., here, if the threshold is set to be <span class="math inline">\(0.05\)</span>, we should not remove <span class="math inline">\(x_1\)</span> from Rule <span class="math inline">\(2\)</span> since <span class="math inline">\(3/5&gt;0.05\)</span>.</p>
<p>Now let’s remove <span class="math inline">\(x_2=0\)</span>. The resulting rule is <span class="math display">\[\begin{equation*}
\small
   \{x_1 \neq 0\to Class=C_1\},  
\end{equation*}\]</span> which has an error of <span class="math inline">\(1/2\)</span>. Therefore, <span class="math inline">\(x_2\)</span> should not be pruned either.</p>
<p><em>Rules are variables.</em> Each rule leads to a redefined variable. For example, consider the dataset in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a>. We name the three rules shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a> as variables <span class="math inline">\(z_1\)</span>, <span class="math inline">\(z_2\)</span>, and <span class="math inline">\(z_3\)</span>, respectively. We only use the <strong>condition of a rule</strong> to define the variable. For instance, the condition of a rule is illustrated below</p>
<p><span class="math display">\[\begin{equation*}
\small
   \{\underbrace{x_1 \neq 0}_{condition}\to \underbrace{Class=C_1}_{outcome}\}.  
\end{equation*}\]</span></p>
<p>Consider <span class="math inline">\(z_1\)</span> first<label for="tufte-sn-270" class="margin-toggle sidenote-number">270</label><input type="checkbox" id="tufte-sn-270" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">270</span> I.e., <span class="math inline">\(\{x_1=0\}\)</span>.</span>. The data points in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a> that meet the condition <span class="math inline">\(\{x_1=0\}\)</span> include <span class="math inline">\(\text{ID} = \{3,4,5,6,7\}\)</span>. Thus, the values of <span class="math inline">\(z_1\)</span> are <span class="math inline">\(\{0,0,1,1,1,1,1\}\)</span>. For <span class="math inline">\(z_2\)</span>,<label for="tufte-sn-271" class="margin-toggle sidenote-number">271</label><input type="checkbox" id="tufte-sn-271" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">271</span> I.e., <span class="math inline">\(\{x_1 \neq 0,x_2=0\}\)</span>.</span> only data point <span class="math inline">\(\text{ID} = \{2\}\)</span> meets the condition, and therefore, the values of <span class="math inline">\(z_2\)</span> are <span class="math inline">\(\{0,1,0,0,0,0,0\}\)</span>. Similarly, the values of <span class="math inline">\(z_3\)</span> are <span class="math inline">\(\{1,0,0,0,0,0,0\}\)</span>.<label for="tufte-sn-272" class="margin-toggle sidenote-number">272</label><input type="checkbox" id="tufte-sn-272" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">272</span> I.e., <span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0\}\)</span>.</span></p>
<p>The new dataset is shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-5">56</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-5">Table 56: </span>The <em>binarized</em> dataset of Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a> by the rules in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(z_1\)</span></th>
<th align="left"><span class="math inline">\(z_2\)</span></th>
<th align="left"><span class="math inline">\(z_3\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C_0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>Select rules.</em> A feature selection method could be applied on the new dataset to select the significant variables. Methods such as the <span class="math inline">\(L_1\)</span> regularized logistics regression (i.e., the equivalent of LASSO for logistic regression model) and regularized random forests are used in the <code>inTrees</code>.</p>
<p>Note that most existing methods don’t concern the length of the rules. But, given two rules with the same predictive power, the rule with a shorter length should be preferred<label for="tufte-sn-273" class="margin-toggle sidenote-number">273</label><input type="checkbox" id="tufte-sn-273" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">273</span> A shorter rule means a simpler model, better interpretability, etc.</span>. In <code>inTrees</code>, the <strong>Guided Regularized Random Forest</strong> (<strong>GRRF</strong>) is also an option for feature selection: the GRRF can assign a weight to each variable, so that when two variables have similar predictive power, the variable with higher weight is more likely to be selected. In our case, we could set higher weight<label for="tufte-sn-274" class="margin-toggle sidenote-number">274</label><input type="checkbox" id="tufte-sn-274" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">274</span> What is the optimal weight? We could use cross-validation to decide.</span> for shorter rules in GRRF.</p>
<p><em>Rule ensemble.</em> As the rules are taken as new variables, a new dataset such as the one shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-5">56</a> is created. So theoretically, any model could be applied on the new dataset to build a prediction model. There are preferences in different packages. For example, in <code>RuleFit</code>, a linear regression model is used that takes the rules as predictors. In <code>inTrees</code>, a simple rule ensemble method summarizes the rules into an <em>ordered rule set</em> for prediction.</p>
<p>It takes a few iterations to develop the ordered rule set. First, we create a default rule, denoted as <span class="math inline">\(r_0\)</span>, that has a null <em>condition</em> and classifies all the data points to be the most frequent class (if it is a regression model, then <span class="math inline">\(r_0\)</span> predicts all the data points to be the population average).</p>
<p>Denote the ordered rule set as <em>R</em>, which is set to be empty at the beginning. Then, the algorithm searches through the available rules and identifies the best rule and adds it into <em>R</em>. The best rule is defined as the rule with the minimum error evaluated by the training data. If there are ties, the rule with higher frequency and smaller length is selected. Then, the data points that meet the condition of the best rule are removed, and the default rule <span class="math inline">\(r_0\)</span> is re-calculated with the data points left. The algorithm iterates to search for the next best rule and update <span class="math inline">\(r_0\)</span> after each iteration. This iterative process continues until no data point is left in the training dataset, or the default rule <span class="math inline">\(r_0\)</span> beats all other available rules that have not been added into the rule ensemble set <span class="math inline">\(R\)</span>. Note that, the selected rules in <em>R</em> are ordered according to the sequential order of their inclusion.</p>
<p>Consider the dataset shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a> and the rules shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a>. The error rate and frequency of each rule is shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-9">57</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-9">Table 57: </span>Error rates and frequencies of the rules in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a> using the dataset in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Rule</th>
<th align="left">Error</th>
<th align="left">Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(\{Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(3/7\)</span></td>
<td align="left"><span class="math inline">\(7/7\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 = 0 \to Class = C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
<td align="left"><span class="math inline">\(5/7\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
<td align="left"><span class="math inline">\(1/7\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
<td align="left"><span class="math inline">\(1/7\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>At the beginning, the default rule is <span class="math inline">\(\{Class=C_0\}\)</span>, and its error rate is <span class="math inline">\(3/7\)</span>. The algorithm then searches for the best rule in the available rules (i.e., shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a>). Rule 2 and Rule 3 have the least errors, and their frequency and length are also the same. Thus, we can add either of them into <span class="math inline">\(R\)</span>. Assume that Rule 2 is selected: <span class="math inline">\(R=\{x_1 \neq 0,x_2=0\to Class=C_1\}\)</span>. Then, the data point (ID:<span class="math inline">\(2\)</span>) classified by this rule is removed from Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a>. The default rule <span class="math inline">\(r_0\)</span> is still <span class="math inline">\(\{Class=C_0\}\)</span>, and the error and frequency of each rule on the updated dataset<label for="tufte-sn-275" class="margin-toggle sidenote-number">275</label><input type="checkbox" id="tufte-sn-275" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">275</span> The data point (ID:<span class="math inline">\(2\)</span>) is removed.</span> is updated, as shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-10">58</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-10">Table 58: </span>Updated error rates and frequencies of the rules in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a> using the reduced dataset, i.e., data point (ID:<span class="math inline">\(2\)</span>) in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a> is removed</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Rule</th>
<th align="left">Error</th>
<th align="left">Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(\{Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/6\)</span></td>
<td align="left"><span class="math inline">\(6/6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 = 0 \to Class = C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
<td align="left"><span class="math inline">\(5/6\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
<td align="left">NA</td>
<td align="left"><span class="math inline">\(0/6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
<td align="left"><span class="math inline">\(1/6\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>A new iteration begins and <span class="math inline">\(\{x_1 \neq 0,x_2\neq 0\to Class=C_0\}\)</span> is added to <em>R</em>, and the data point (ID:<span class="math inline">\(1\)</span>) is removed. The default rule remains unchanged and the error and frequency of each rule on the updated dataset<label for="tufte-sn-276" class="margin-toggle sidenote-number">276</label><input type="checkbox" id="tufte-sn-276" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">276</span> The data points (ID:<span class="math inline">\(1\)</span> and ID:<span class="math inline">\(2\)</span>) in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a> are removed.</span> is updated in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-11">59</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-11">Table 59: </span>Updated error rates and frequencies of the rules in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-2">55</a> using the reduced dataset, i.e., data points (ID:<span class="math inline">\(1\)</span> and ID:<span class="math inline">\(2\)</span>) in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-1">54</a> are removed</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Rule</th>
<th align="left">Error</th>
<th align="left">Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(\{Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
<td align="left"><span class="math inline">\(5/5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 = 0 \to Class = C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
<td align="left"><span class="math inline">\(5/5\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
<td align="left">NA</td>
<td align="left"><span class="math inline">\(0/5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_0\}\)</span></td>
<td align="left">NA</td>
<td align="left"><span class="math inline">\(0/5\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Now the default rule <span class="math inline">\(Class=C_0\)</span> has the minimum error <span class="math inline">\(2/5\)</span>, the same as <span class="math inline">\(\{x_1=0\to Class=C_0\}\)</span>. Therefore, the default rule is added to <em>R</em> and the process stops. The final ordered rule set <em>R</em> is summarized in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-12">60</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-12">Table 60: </span>Final results of <em>R</em></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Order</th>
<th align="left">Rule</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(\{Class=C_0\}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_1\}\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>When predicting on an instance, the first rule in <em>R</em> that <em>hits</em> the data point is used for prediction. For example, for a data point <span class="math inline">\(\{x_1 \neq 0,x_2=1\}\)</span>, it meets the condition of Rule 2 in <em>R</em>. The prediction on this data point is <span class="math inline">\(C_1\)</span>. For data point <span class="math inline">\(\{x_1=0,x_2=1\}\)</span>, it does not meet the condition of either Rule <span class="math inline">\(1\)</span> or Rule <span class="math inline">\(2\)</span> in <em>R</em>. Therefore, the default rule is used, and the prediction is <span class="math inline">\(C_0\)</span>.</p>
</div>
<div id="r-lab-16" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We use <code>inTrees</code> on the AD datasedt. Based on the random forest model, <span class="math inline">\(4555\)</span> rules are extracted.</p>
<p></p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>(<span class="at">all =</span> <span class="cn">TRUE</span>))</span>
<span id="cb221-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;arules&quot;</span>)</span>
<span id="cb221-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;randomForest&quot;</span>)</span>
<span id="cb221-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;RRF&quot;</span>)</span>
<span id="cb221-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;inTrees&quot;</span>)</span>
<span id="cb221-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;reshape&quot;</span>)</span>
<span id="cb221-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb221-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb221-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-9" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb221-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb221-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb221-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb221-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-13" aria-hidden="true" tabindex="-1"></a>target_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">==</span> <span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb221-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-14" aria-hidden="true" tabindex="-1"></a>target <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;class_&quot;</span>, <span class="fu">as.character</span>(data[, target_indx]))</span>
<span id="cb221-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-15" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> </span>
<span id="cb221-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-16" aria-hidden="true" tabindex="-1"></a>            <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>, <span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>, <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb221-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-17" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data</span>
<span id="cb221-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-18" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> X[, <span class="sc">-</span>rm_indx]</span>
<span id="cb221-19"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(X)) X[, i] <span class="ot">&lt;-</span> </span>
<span id="cb221-20"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-20" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as.factor</span>(<span class="fu">dicretizeVector</span>(X[, i], <span class="at">K =</span> <span class="dv">3</span>))</span>
<span id="cb221-21"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb221-22"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Use random forests to grow the trees</span></span>
<span id="cb221-23"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-23" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(target))</span>
<span id="cb221-24"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb221-25"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-25" aria-hidden="true" tabindex="-1"></a><span class="co"># transform rf object to an inTrees&#39; format</span></span>
<span id="cb221-26"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-26" aria-hidden="true" tabindex="-1"></a>treeList <span class="ot">&lt;-</span> <span class="fu">RF2List</span>(rf)  </span>
<span id="cb221-27"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-27" aria-hidden="true" tabindex="-1"></a>exec <span class="ot">&lt;-</span> <span class="fu">extractRules</span>(treeList, X)  <span class="co"># Extract the rules</span></span>
<span id="cb221-28"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb221-29"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-29" aria-hidden="true" tabindex="-1"></a><span class="do">## The rules are measured by length, error and frequency.</span></span>
<span id="cb221-30"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-30" aria-hidden="true" tabindex="-1"></a>class <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;class_&quot;</span>, <span class="fu">as.character</span>(target))</span>
<span id="cb221-31"><a href="chapter-10-synthesis-architecture-pipeline.html#cb221-31" aria-hidden="true" tabindex="-1"></a>rules <span class="ot">&lt;-</span> <span class="fu">getRuleMetric</span>(exec, X, target)</span></code></pre></div>
<p></p>
<p>The statistics of the rules could be extracted, e.g., <span class="math inline">\(5\)</span> rules are shown below.</p>
<p></p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rules[<span class="fu">order</span>(<span class="fu">as.numeric</span>(rules[, <span class="st">&quot;len&quot;</span>])), ][<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ])</span>
<span id="cb222-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-3" aria-hidden="true" tabindex="-1"></a><span class="co">#      len freq    err     </span></span>
<span id="cb222-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-4" aria-hidden="true" tabindex="-1"></a><span class="co"># [1,] &quot;2&quot; &quot;0.118&quot; &quot;0.098&quot; </span></span>
<span id="cb222-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-5" aria-hidden="true" tabindex="-1"></a><span class="co"># [2,] &quot;2&quot; &quot;0.182&quot; &quot;0&quot;     </span></span>
<span id="cb222-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-6" aria-hidden="true" tabindex="-1"></a><span class="co"># [3,] &quot;2&quot; &quot;0.182&quot; &quot;0&quot;     </span></span>
<span id="cb222-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-7" aria-hidden="true" tabindex="-1"></a><span class="co"># [4,] &quot;2&quot; &quot;0.081&quot; &quot;0.024&quot; </span></span>
<span id="cb222-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-8" aria-hidden="true" tabindex="-1"></a><span class="co"># [5,] &quot;2&quot; &quot;0.043&quot; &quot;0.136&quot; </span></span>
<span id="cb222-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-9" aria-hidden="true" tabindex="-1"></a><span class="co">#      condition                                    pred</span></span>
<span id="cb222-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-10" aria-hidden="true" tabindex="-1"></a><span class="co"># [1,] &quot;X[,6] %in% c(&#39;L1&#39;) &amp; X[,11] %in% c(&#39;L1&#39;)&quot;  &quot;class_1&quot;</span></span>
<span id="cb222-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-11" aria-hidden="true" tabindex="-1"></a><span class="co"># [2,] &quot;X[,4] %in% c(&#39;L1&#39;) &amp; X[,6] %in% c(&#39;L1&#39;)&quot;   &quot;class_1&quot;</span></span>
<span id="cb222-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-12" aria-hidden="true" tabindex="-1"></a><span class="co"># [3,] &quot;X[,4] %in% c(&#39;L1&#39;) &amp; X[,6] %in% c(&#39;L1&#39;)&quot;   &quot;class_1&quot;</span></span>
<span id="cb222-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-13" aria-hidden="true" tabindex="-1"></a><span class="co"># [4,] &quot;X[,3] %in% c(&#39;L3&#39;) &amp; X[,4] %in% c(&#39;L3&#39;)&quot;    &quot;class_0&quot;</span></span>
<span id="cb222-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb222-14" aria-hidden="true" tabindex="-1"></a><span class="co"># [5,] &quot;X[,6] %in% c(&#39;L3&#39;) &amp; X[,7] %in% c(&#39;L3&#39;)&quot;    &quot;class_0&quot;</span></span></code></pre></div>
<p></p>
<p>We then prune the rules. Recall that we need to specify the threshold of <em>decay</em>. This could be done in R by setting the value of the parameter <code>maxDecay</code>. The statistics of the rules before and after pruning are shown in Figures <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-4">191</a>—<a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-6">193</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-4"></span>
<img src="graphics/10_4.png" alt="Histogram of *lengths of the rules* before and after the pruning " width="100%"  />
<!--
<p class="caption marginnote">-->Figure 191: Histogram of <em>lengths of the rules</em> before and after the pruning <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The R code below generates Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-4">191</a>.</p>
<p></p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-1" aria-hidden="true" tabindex="-1"></a>rules.pruned <span class="ot">&lt;-</span> <span class="fu">pruneRule</span>(rules, X, target, <span class="at">maxDecay =</span> <span class="fl">0.005</span>,</span>
<span id="cb223-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">typeDecay =</span> <span class="dv">2</span>)</span>
<span id="cb223-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb223-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-4" aria-hidden="true" tabindex="-1"></a>length <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">original =</span> <span class="fu">as.numeric</span>(rules[, <span class="st">&quot;len&quot;</span>]),</span>
<span id="cb223-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">pruned =</span> <span class="fu">as.numeric</span>(rules.pruned[,<span class="st">&quot;len&quot;</span>]))</span>
<span id="cb223-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb223-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Visualize the result</span></span>
<span id="cb223-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">melt</span>(length), <span class="fu">aes</span>(value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb223-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>,<span class="at">binwidth =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb223-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Histogram of Lengths&quot;</span>) <span class="sc">+</span></span>
<span id="cb223-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb223-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-5"></span>
<img src="graphics/10_5.png" alt="Histogram of *frequencies of the rules* before and after the pruning" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 192: Histogram of <em>frequencies of the rules</em> before and after the pruning<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The R code below generates Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-5">192</a>.</p>
<p></p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb224-1" aria-hidden="true" tabindex="-1"></a>frequency <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb224-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb224-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">original =</span> <span class="fu">as.numeric</span>(rules[, <span class="st">&quot;freq&quot;</span>]),</span>
<span id="cb224-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb224-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">pruned =</span> <span class="fu">as.numeric</span>(rules.pruned[,<span class="st">&quot;freq&quot;</span>]))</span>
<span id="cb224-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb224-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">melt</span>(frequency), <span class="fu">aes</span>(value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb224-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb224-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>,<span class="at">binwidth =</span> <span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb224-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb224-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Histogram of Frequencies&quot;</span>) <span class="sc">+</span></span>
<span id="cb224-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb224-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p></p>
<p>The R code below generates Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-6">193</a>.</p>
<p></p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb225-1" aria-hidden="true" tabindex="-1"></a>error <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">original =</span> <span class="fu">as.numeric</span>(rules[, <span class="st">&quot;err&quot;</span>]),</span>
<span id="cb225-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb225-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">pruned =</span> <span class="fu">as.numeric</span>(rules.pruned[,<span class="st">&quot;err&quot;</span>]))</span>
<span id="cb225-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb225-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb225-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb225-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Visualize the result</span></span>
<span id="cb225-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb225-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">melt</span>(error), <span class="fu">aes</span>(value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb225-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb225-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>,<span class="at">binwidth =</span> <span class="fl">0.01</span>) <span class="sc">+</span></span>
<span id="cb225-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb225-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Histogram of Errors&quot;</span>) <span class="sc">+</span></span>
<span id="cb225-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb225-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-6"></span>
<img src="graphics/10_6.png" alt="Histogram of *errors of the rules* before and after the pruning" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 193: Histogram of <em>errors of the rules</em> before and after the pruning<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-4">191</a> shows that the lengths of the rules are substantially reduced. For example, a majority of the original rules have a length of <span class="math inline">\(6\)</span>, while after pruning, only a slight percentage of the rules have a length of <span class="math inline">\(6\)</span>. Also, since rules are shortened, reduction of frequencies of the rules is also significant, as shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-5">192</a>. The errors are also reduced; e.g., Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-6">193</a> shows the distribution of errors shifted to the left after pruning. Overall, the quality of the rules is improved with a proper choice of the pruning parameters.</p>
<p>The following R code prunes the rule set.</p>
<p></p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb226-1" aria-hidden="true" tabindex="-1"></a>rules.selected <span class="ot">&lt;-</span> <span class="fu">selectRuleRRF</span>(rules.pruned, X, target)</span>
<span id="cb226-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb226-2" aria-hidden="true" tabindex="-1"></a>rules.present <span class="ot">&lt;-</span> <span class="fu">presentRules</span>(rules.selected, <span class="fu">colnames</span>(X))</span>
<span id="cb226-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb226-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb226-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb226-4" aria-hidden="true" tabindex="-1"></a><span class="do">## See the specific contents of the selected rules</span></span>
<span id="cb226-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb226-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cbind</span>(<span class="at">ID =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(rules.present), </span>
<span id="cb226-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb226-6" aria-hidden="true" tabindex="-1"></a>            rules.present[, <span class="fu">c</span>(<span class="st">&quot;condition&quot;</span>, <span class="st">&quot;pred&quot;</span>)]))</span></code></pre></div>
<p></p>
<p>Finally, <span class="math inline">\(16\)</span> rules are selected. Their performances are shown below<label for="tufte-sn-277" class="margin-toggle sidenote-number">277</label><input type="checkbox" id="tufte-sn-277" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">277</span> Details of the rules could also be printed out in R.</span>.</p>
<p></p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cbind</span>(<span class="at">ID =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(rules.present),</span>
<span id="cb227-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-2" aria-hidden="true" tabindex="-1"></a>            rules.present[, <span class="fu">c</span>(<span class="st">&quot;len&quot;</span>, <span class="st">&quot;freq&quot;</span>, <span class="st">&quot;err&quot;</span>)]))</span>
<span id="cb227-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-3" aria-hidden="true" tabindex="-1"></a><span class="do">##       ID   len freq    err</span></span>
<span id="cb227-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1,] &quot;1&quot;  &quot;2&quot; &quot;0.279&quot; &quot;0.083&quot;</span></span>
<span id="cb227-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  [2,] &quot;2&quot;  &quot;2&quot; &quot;0.279&quot; &quot;0.09&quot;</span></span>
<span id="cb227-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  [3,] &quot;3&quot;  &quot;5&quot; &quot;0.029&quot; &quot;0.133&quot;</span></span>
<span id="cb227-7"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [4,] &quot;4&quot;  &quot;3&quot; &quot;0.122&quot; &quot;0.016&quot;</span></span>
<span id="cb227-8"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-8" aria-hidden="true" tabindex="-1"></a><span class="do">##  [5,] &quot;5&quot;  &quot;4&quot; &quot;0.031&quot; &quot;0.312&quot;</span></span>
<span id="cb227-9"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  [6,] &quot;6&quot;  &quot;2&quot; &quot;0.207&quot; &quot;0.121&quot;</span></span>
<span id="cb227-10"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-10" aria-hidden="true" tabindex="-1"></a><span class="do">##  [7,] &quot;7&quot;  &quot;3&quot; &quot;0.172&quot; &quot;0.124&quot;</span></span>
<span id="cb227-11"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-11" aria-hidden="true" tabindex="-1"></a><span class="do">##  [8,] &quot;8&quot;  &quot;4&quot; &quot;0.06&quot;  &quot;0.194&quot;</span></span>
<span id="cb227-12"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  [9,] &quot;9&quot;  &quot;5&quot; &quot;0.006&quot; &quot;0&quot;</span></span>
<span id="cb227-13"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [10,] &quot;10&quot; &quot;4&quot; &quot;0.044&quot; &quot;0.13&quot;</span></span>
<span id="cb227-14"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [11,] &quot;11&quot; &quot;5&quot; &quot;0.019&quot; &quot;0.2&quot;</span></span>
<span id="cb227-15"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [12,] &quot;12&quot; &quot;3&quot; &quot;0.043&quot; &quot;0.182&quot;</span></span>
<span id="cb227-16"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-16" aria-hidden="true" tabindex="-1"></a><span class="do">## [13,] &quot;13&quot; &quot;4&quot; &quot;0.037&quot; &quot;0.158&quot;</span></span>
<span id="cb227-17"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-17" aria-hidden="true" tabindex="-1"></a><span class="do">## [14,] &quot;14&quot; &quot;3&quot; &quot;0.114&quot; &quot;0.203&quot;</span></span>
<span id="cb227-18"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-18" aria-hidden="true" tabindex="-1"></a><span class="do">## [15,] &quot;15&quot; &quot;2&quot; &quot;0.234&quot; &quot;0.215&quot;</span></span>
<span id="cb227-19"><a href="chapter-10-synthesis-architecture-pipeline.html#cb227-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [16,] &quot;16&quot; &quot;3&quot; &quot;0.282&quot; &quot;0.144&quot;</span></span></code></pre></div>
<p></p>
</div>
</div>
<div id="remarks-8" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="images-text-and-audio" class="section level3 unnumbered">
<h3>Images, text, and audio</h3>
<p>To learn more about deep learning we recommend readers to start with this book<label for="tufte-sn-278" class="margin-toggle sidenote-number">278</label><input type="checkbox" id="tufte-sn-278" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">278</span> Goodfellow, I., Bengio, Y., and Courville, A., <em>Deep Learning</em>. The MIT Press, 2016.</span>. There are many online lecture notes and tutorials that are informative. Here, it is worth mentioning that there has not been a unified theory about deep learning, and even the definition of what is a deep model is up to debate. This is good. If we look back at the developmental processes of many statistics and machine learning models, we may observe that some models were developed based on inspiration from theory and we often call these models too theoretical. These models usually wobble and stumble in their early years, gradually become mature and a proven approach, and eventually establish themselves as effective models in practice. Some other models, however, were developed ahead of theory, and theory only comes later to explain the model’s success. For deep learning, it is hard to say if it was theory that inspired the models, or it was the models that inspired theory. Many efforts are committed to give an overarching theory to explain the success of deep learning, at least in some special cases. Yet there has not been such an overarching theory about deep learning, only competing narratives<label for="tufte-sn-279" class="margin-toggle sidenote-number">279</label><input type="checkbox" id="tufte-sn-279" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">279</span> Interested readers may read this article by Colah, C., <em>Neural Networks, Types, and Functional Programming</em>, <a href="https://colah.github.io/posts/2015-09-NN-Types-FP/">https://colah.github.io/posts/2015-09-NN-Types-FP/</a>.</span>.</p>
<p>It is natural to wonder why we use a deep model. Can’t we just use a nondeep model? Readers may have been using nondeep models and found those nondeep models sufficient to solve problems in practice. Certainly we can just use nondeep models. There have been plenty of examples in practice that nondeep models were the best<label for="tufte-sn-280" class="margin-toggle sidenote-number">280</label><input type="checkbox" id="tufte-sn-280" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">280</span> A recent example: The Math of March Madness, <em>New York Times</em>, <a href="https://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy.html">https://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy.html</a>.</span>, only if we have the best variables (e.g., the <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_p\)</span>) that are sufficient to explain the “movement” of our target <span class="math inline">\(y\)</span>. The availability of high-quality and ready-to-use <span class="math inline">\(\boldsymbol{x}\)</span> is a precondition for the success of nondeep models. This precondition, however, is not always held in practice.</p>
<p>This is one reason why deep models can make a difference. Most nondeep models deal with a data structure that is Excel-sheet-like, i.e., they are stored or could be stored in an Excel spreadsheet. In many applications, particularly in recent years, the raw data is in free-form (sometimes it is also called unstructured data) such as images, text, and audio data. That means, to use the nondeep models for these applications, there should be a preprocessing/translational step that could extract the variables <span class="math inline">\(\boldsymbol{x}\)</span> from the raw data. It is notable that the translational process itself takes up a larger portion of effort of a data scientist in practice, and since the process involves multiple steps and layers, it naturally adopts a deep form, and further includes the nondeep model as its last layer to be part of its architecture.</p>
<p>It is no surpise then that mature practices of deep learning have been mainly found on unstructured data such as images, text, and audio data. From the raw data such as an X-ray image to the final outcome such as diagnosis of a disease, there are plenty of steps to transform the raw data into interpretable information. These steps put together creates a deep model. In other words, deep learning automates and optimizes this translational process.</p>
</div>
<div id="a-key-is-made-to-unlock-but-what-is-the-lock" class="section level3 unnumbered">
<h3>A key is made to unlock, but what is the lock?</h3>
<p>There is another magic dimension to deep learning. Researchers in different disciplines have created many basic forms of functions that could be used as building blocks to build larger functions. On the other hand, as model validation has been made automatic and data-driven<label for="tufte-sn-281" class="margin-toggle sidenote-number">281</label><input type="checkbox" id="tufte-sn-281" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">281</span> I.e., given a few candidate models, we no longer need to validate the models by their scientific implication but only check how well they fit the data. See Figure <a href="chapter-5-learning-i-cross-validation-oob.html#fig:f5-flowchart">95</a> in <strong>Chapter 5</strong>.</span>, a deep model doesn’t demand interpretability or validity to be useful. In practice, your task is empirical: put together the architecture of a deep model, and if it obtains superior performance on data, it is a superior model. Now, if a complex problem in the real world is a sophisticated lock, deep learning’s real appeal is that we only need to spend effort in guessing at the key (i.e., the architecture of the deep neural network), but not necessarily in understanding the lock. And what makes it more convenient is that we can try every key we made (i.e., by fitting it with data) until the lock is opened. Is this a rational practice? There have been discussions around this topic and readers may be interested to look into this<label for="tufte-sn-282" class="margin-toggle sidenote-number">282</label><input type="checkbox" id="tufte-sn-282" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">282</span> See, Hutson, M., Has artifical intelligence become alchemy?, <em>Science</em>, 2018.</span>.</p>
</div>
<div id="decay-and-relative-decay" class="section level3 unnumbered">
<h3>Decay and relative decay</h3>
<p>To prune a rule, <code>inTrees</code> uses <em>leave-one-out</em> pruning, i.e., at each round, it removes one variable and checks how much error this removal will induce. We have introduced the concept <em>decay</em>. For the <span class="math inline">\(i^{th}\)</span> variable in the condition of a rule, its decay is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  decay_i = Err_{-i}-Err, 
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(Err\)</span> is the error of the original rule, <span class="math inline">\(Err_{-i}\)</span> is the error of the rule with the <span class="math inline">\(i^{th}\)</span> variable removed.</p>
<p>There is another definition of decay in <code>inTrees</code>, called <em>relative decay</em>, which is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  decay_i = \frac{Err_{-i}-Err}{\max(Err,s)}, 
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(s\)</span> is a small positive constant (e.g., <span class="math inline">\(0.001\)</span>) that bounds the value of decay when <span class="math inline">\(Err\)</span> is zero or close to zero. An advantage of using relative decay is that one does not need to know the level of error of a dataset<label for="tufte-sn-283" class="margin-toggle sidenote-number">283</label><input type="checkbox" id="tufte-sn-283" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">283</span> For instance, for one dataset the error rate <span class="math inline">\(0.01\)</span> is probably insignificant, but for another <span class="math inline">\(0.01\)</span> is a big difference.</span>.</p>
</div>
</div>
<div id="exercises-8" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-convolution-hw"></span>
<img src="graphics/10_convolution_hw.png" alt="A NN model with its parameters" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 194: A NN model with its parameters<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- begin{enumerate} --></p>
<p>1. Complete the convolution operation as shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-convolution-hw">194</a>.</p>
<p>2. Use the <code>convolution()</code> function in R package <code>OpenImageR</code> to run the data in Q1.</p>
<p>3. Let’s try applying the convolution operation on a real image. For example, use the following R code to get the image shown in Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-parrot">195</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-parrot"></span>
<img src="graphics/10_parrot.png" alt="Data for Q3" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 195: Data for Q3<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- end{enumerate} --></p>
<p></p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(EBImage)</span>
<span id="cb228-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb228-2" aria-hidden="true" tabindex="-1"></a><span class="fu">readImage</span>(<span class="fu">system.file</span>(<span class="st">&quot;images&quot;</span>, <span class="st">&quot;sample-color.png&quot;</span>, </span>
<span id="cb228-3"><a href="chapter-10-synthesis-architecture-pipeline.html#cb228-3" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> <span class="fu">readImage</span>(<span class="fu">system.file</span>(<span class="st">&quot;images&quot;</span>, <span class="st">&quot;sample-color.png&quot;</span>, </span>
<span id="cb228-4"><a href="chapter-10-synthesis-architecture-pipeline.html#cb228-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">package=</span><span class="st">&quot;EBImage&quot;</span>))</span>
<span id="cb228-5"><a href="chapter-10-synthesis-architecture-pipeline.html#cb228-5" aria-hidden="true" tabindex="-1"></a>grayimage<span class="ot">&lt;-</span><span class="fu">channel</span>(img,<span class="st">&quot;gray&quot;</span>)</span>
<span id="cb228-6"><a href="chapter-10-synthesis-architecture-pipeline.html#cb228-6" aria-hidden="true" tabindex="-1"></a><span class="fu">display</span>(grayimage)</span></code></pre></div>
<p>
Use the <code>convolution()</code> function in R package <code>OpenImageR</code> to filter this image. You can use the high-pass Laplacian filter, that would be defined in R as</p>
<p></p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="chapter-10-synthesis-architecture-pipeline.html#cb229-1" aria-hidden="true" tabindex="-1"></a>kernel <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">1</span>, <span class="at">nc=</span><span class="dv">3</span>, <span class="at">nr=</span><span class="dv">3</span>)</span>
<span id="cb229-2"><a href="chapter-10-synthesis-architecture-pipeline.html#cb229-2" aria-hidden="true" tabindex="-1"></a>kernel[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">=</span> <span class="sc">-</span><span class="dv">8</span></span></code></pre></div>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<p>4. Figure <a href="chapter-10-synthesis-architecture-pipeline.html#fig:f10-xor-hw">196</a> shows a NN model with its parameters. Use this NN model to predict on the data points shown in Table <a href="chapter-10-synthesis-architecture-pipeline.html#tab:t10-hw-nn">61</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-xor-hw"></span>
<p class="caption marginnote shownote">
Figure 196: A NN model for Q4
</p>
<img src="graphics/10_xor_hw.png" alt="A NN model for Q4" width="80%"  />
</div>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-hw-nn">Table 61: </span>Test dataset for the NN model in Q4</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p>5. Use the <code>BostonHousing</code> dataset from the R package <code>mlbench</code> and select the variable <code>medv</code> as the outcome and all other numeric variables as predictors. Run the R pipeline for NN on it. Use <span class="math inline">\(10\)</span>-fold cross-validation to evaluate a NN model with <span class="math inline">\(2\)</span> hidden layers, while each layer has a number of nodes of your choice. Comment on the result.</p>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<p style="text-align: center;">
<a href="chapter-9-pragmatism-experience-experimental.html"><button class="btn btn-default">Previous</button></a>
<a href="conclusion.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
