<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Appendix: A Brief Review of Background Knowledge | book_migrate.utf8" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Book for analalytics" />




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Book for analalytics">

<title>Appendix: A Brief Review of Background Knowledge | book_migrate.utf8</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#cover">Cover</a></li>
<li><a href="#epigraph">Epigraph</a></li>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="epigraph.html#epigraph">Epigraph</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="appendix-a-brief-review-of-background-knowledge" class="section level1 unnumbered">
<h1>Appendix: A Brief Review of Background Knowledge</h1>
<p>Recall that in this book, we use lower case letters, e.g., <span class="math inline">\(x\)</span>, to represent scalars; bold face, lower case letters, e.g., <span class="math inline">\(\boldsymbol{x}\)</span>, to represent vectors; and bold face, upper case letters, e.g., <span class="math inline">\(\boldsymbol{X}\)</span>, to represent matrices.</p>
<div id="the-normal-distribution" class="section level2 unnumbered">
<h2>The Normal Distribution</h2>
<p>A distribution model characterizes the random behavior of a random variable. A random variable takes value from a predefined set, range, or a continuum, but not all values are taken with equal probabilities. How these probabilities are distributed is characterized by the distribution model. Before the computer age, for a distribution model to acquire a status of natural law it usually has an elegant geometric shape that comes with a delicate mathematical form, as many examples shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-errorlaws">5</a>. As we have computers now doing a lot of computation, a distribution could be just an empirical histogram that has not yet found its explicit mathematical form. Whether or not this empirical form of distribution would repeat itself as a natural law remains to be seen. In practice, a competitive edge could be gained before you find scientific explanation, as long as it works.</p>
<p>In this book we will not have extensive coverage of distribution models. We will focus on normal distribution; but other than that, everything we learn about the normal distribution is also to help us extend beyond it and establish the concept of distribution as an abstract one.</p>
<p>A random variable <span class="math inline">\(x\)</span> distributed as a normal distribution is denoted as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
x \sim N\left(\mu, \sigma^{2}\right).
 
\end{equation*}\]</span></p>
<p>The normal distribution has the mathematical form</p>
<p><span class="math display">\[\begin{equation*}
\small
  
N\left(\mu, \sigma^{2}\right) = \frac{1}{\sqrt{2\pi} \sigma}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2 }.
 
\end{equation*}\]</span></p>
<p>If we multiply <span class="math inline">\(x\)</span> with a constant <span class="math inline">\(a\)</span>, then</p>
<p><span class="math display" id="eq:apx-normal">\[\begin{equation*}
\small
  
ax \sim N\left(a\mu, a^2\sigma^{2}\right).
\tag{103}
 
\end{equation*}\]</span></p>
<p>Extending the concept of distribution to <span class="math inline">\(p\)</span>-dimensional space, we have the multivariate normal distribution (MVN) of vector <span class="math inline">\(\boldsymbol{x}\)</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{x} \sim MVN\left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right),
 
\end{equation*}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\mu}=\left[ \begin{array}{c}{\mu_{1}} \\ {\mu_{2}} \\ {\vdots} \\ {\mu_{p}}\end{array}\right], \text {         }  \boldsymbol{\Sigma}=\left[ \begin{array}{cccc} {\sigma^2_{1}} &amp; {\sigma_{12}} &amp; {\cdots} &amp; {\sigma_{1p}} \\ {\sigma_{21}} &amp; {\sigma^2_{2}} &amp; {\cdots} &amp; {\sigma_{2p}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {\sigma_{p1}} &amp; {\sigma_{p2}} &amp; {\cdots} &amp; {\sigma^2_{p }}\end{array}\right],
 
\end{equation*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation*}
\small
  
MVN\left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right) = \frac{1}{\sqrt{(2\pi)^p\det{\boldsymbol{\Sigma}}}}\exp\left({-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}}(\boldsymbol{x}-\boldsymbol{\mu})\right).
 
\end{equation*}\]</span></p>
<p>To interpret the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, let’s look at an example where <span class="math inline">\(p=2\)</span>. Its covariance matrix is</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\Sigma_1} = \left[ \begin{array}{cc} {\sigma^2_1} &amp; {\sigma_{12}} \\ {\sigma_{21}} &amp; {\sigma_2^2}\end{array}\right].  
 
\end{equation*}\]</span></p>
<p>The element <span class="math inline">\(\sigma^2_1\)</span> is the marginal variance of variable <span class="math inline">\(x_1\)</span>, <span class="math inline">\(\sigma^2_2\)</span> is the marginal variance of variable <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(\sigma_{12}\)</span> that equals to <span class="math inline">\(\sigma_{21}\)</span> is the covariance between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.<label for="tufte-sn-286" class="margin-toggle sidenote-number">286</label><input type="checkbox" id="tufte-sn-286" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">286</span> Covariance is closely related to the concept of correlation. For instance, denote the correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> as <span class="math inline">\(r\)</span>, which is defined as <span class="math inline">\(r = \frac{\sigma_{12}}{\sigma_1\sigma_2}\)</span>. It could be shown that <span class="math inline">\(r\)</span> takes value from <span class="math inline">\(-1\)</span> (i.e., perfect negative correlation) to <span class="math inline">\(1\)</span> (i.e., perfect positive correlation). Note that this correlation concept is built on the normal distribution, and the correlation <span class="math inline">\(0\)</span> doesn’t imply the two variables have no relationship in any possible form. Rather, it only implies that there is no <em>linear</em> relationship between the two.</span></p>
<p>Three examples of the covariance matrix are shown below</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\Sigma_1} = \left[ \begin{array}{cc} {1} &amp; {0} \\ {0} &amp; {1}\end{array}\right], \text {         }  \boldsymbol{\Sigma_2} = \left[ \begin{array}{cc} {1} &amp; {0.8} \\ {0.8} &amp; {1}\end{array}\right], \text {         }  \boldsymbol{\Sigma_3} = \left[ \begin{array}{cc} {1} &amp; {1} \\ {1} &amp; {1}\end{array}\right].  
 
\end{equation*}\]</span></p>
<p>The corresponding contour plots of the three bivariate normal distributions are shown in Figure <a href="appendix-a-brief-review-of-background-knowledge.html#fig:fapx-binormal">199</a>.</p>
<p>If we add <span class="math inline">\(\boldsymbol{x}\)</span> (i.e., <span class="math inline">\(\boldsymbol{x} \in R^{p \times 1}\)</span>) with a constant vector <span class="math inline">\(\boldsymbol{a}\)</span> (i.e., <span class="math inline">\(\boldsymbol{a} \in R^{p \times 1}\)</span>), then</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:fapx-binormal"></span>
<p class="caption marginnote shownote">
Figure 199: The contour plots of the three bivariate normal distributions
</p>
<img src="graphics/apx_binormal.png" alt="The contour plots of the three bivariate normal distributions" width="80%"  />
</div>
<p></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{a + x} \sim MVN\left(\boldsymbol{a + \mu}, \boldsymbol{\Sigma}\right).
 
\end{equation*}\]</span></p>
<p>If we multiply <span class="math inline">\(\boldsymbol{x}\)</span> (i.e., <span class="math inline">\(\boldsymbol{x} \in R^{p \times 1}\)</span>) with a constant <span class="math inline">\(\boldsymbol{a}\)</span> (i.e., <span class="math inline">\(\boldsymbol{a} \in R^{p \times 1}\)</span>), then</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{a^Tx} \sim MVN\left(\boldsymbol{a^T\mu}, \boldsymbol{a^T\Sigma a}\right).
 
\end{equation*}\]</span></p>
</div>
<div id="matrix-operations" class="section level2 unnumbered">
<h2>Matrix Operations</h2>
<p>A matrix is a basic structure in data analytics that organizes data in a rectangular array, e.g., a matrix <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span> with <span class="math inline">\(p\)</span> rows and <span class="math inline">\(q\)</span> columns is</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{X}=\left[ \begin{array}{cccc} {x_{11}} &amp; {x_{12}} &amp; {\cdots} &amp; {x_{1q}} \\ {x_{21}} &amp; {x_{22}} &amp; {\cdots} &amp; {x_{2q}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {x_{p1}} &amp; {x_{p2}} &amp; {\cdots} &amp; {x_{pq}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p><em>Matrix transposition.</em> A matrix <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span> could be transposed into a matrix <span class="math inline">\(\boldsymbol{X}^T \in R^{q \times p}\)</span>, i.e.,</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{X}^T=\left[ \begin{array}{cccc} {x_{11}} &amp; {x_{21}} &amp; {\cdots} &amp; {x_{q1}} \\ {x_{12}} &amp; {x_{22}} &amp; {\cdots} &amp; {x_{q2}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {x_{1p}} &amp; {x_{2p}} &amp; {\cdots} &amp; {x_{qp}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p><em>Matrix addition.</em> Two matrices of the same dimensions could be added together entrywise, i.e., <span class="math inline">\(\boldsymbol{X} + \boldsymbol{Y}\)</span> is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{X + Y}=\left[ \begin{array}{cccc} {x_{11}+y_{11}} &amp; {x_{12}+y_{12}} &amp; {\cdots} &amp; {x_{1q}+y_{1q}} \\ {x_{21}+y_{21}} &amp; {x_{22}+y_{22}} &amp; {\cdots} &amp; {x_{2q}+y_{2q}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {x_{p1}+y_{p1}} &amp; {x_{p2}+y_{p2}} &amp; {\cdots} &amp; {x_{pq}+y_{pq}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p><em>Scalar multiplication:</em> The product of a constant <span class="math inline">\(c\)</span> and a matrix <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span> is computed by multiplying every entry of <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span> by <span class="math inline">\(c\)</span>, i.e.,</p>
<p><span class="math display">\[\begin{equation*}
\small
  
c\boldsymbol{X}=\left[ \begin{array}{cccc} {cx_{11}} &amp; {cx_{12}} &amp; {\cdots} &amp; {cx_{1q}} \\ {cx_{21}} &amp; {cx_{22}} &amp; {\cdots} &amp; {cx_{2q}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {cx_{p1}} &amp; {cx_{p2}} &amp; {\cdots} &amp; {cx_{pq}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p><em>Matrix multiplication.</em> Two matrices could be multiplied if the number of columns of the left matrix is the same as the number of rows of the right matrix, i.e., for <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span>, it could be multiplied with any matrix that has <span class="math inline">\(q\)</span> rows. Let’s say we have two matrices, <span class="math inline">\(\boldsymbol{X} \in R^{2 \times 3}\)</span> and <span class="math inline">\(\boldsymbol{Y} \in R^{3 \times 2}\)</span>, the multiplication <span class="math inline">\(\boldsymbol{XY}\)</span> is a matrix <span class="math inline">\(\in R^{2 \times 2}\)</span>, where</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{XY}=\left[ \begin{array}{cccc} {x_{11}y_{11}+ x_{12}y_{21}+ x_{13}y_{31}}  &amp; {x_{11}y_{12}+ x_{12}y_{22}+ x_{13}y_{32}} \\ {x_{21}y_{11}+ x_{22}y_{21}+ x_{23}y_{31}} &amp; {x_{21}y_{12}+ x_{22}y_{22}+ x_{23}y_{32}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p><em>Matrix derivative.</em> Matrix derivative is a rich category that includes many situations. Readers may find a comprehensive coverage in a few books<label for="tufte-sn-287" class="margin-toggle sidenote-number">287</label><input type="checkbox" id="tufte-sn-287" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">287</span> Harville, D.A., <em>Matrix Algebra From a Statistician’s Perspective</em>, Springer, 2000.</span> or find a quick reference in online resources<label for="tufte-sn-288" class="margin-toggle sidenote-number">288</label><input type="checkbox" id="tufte-sn-288" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">288</span> Petersen, K.B. and Pedersen, M.S., <em>The Matrix Cookbook</em>, online document (<a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf</a>).</span>. Here, we mention a few examples that are related topics in this book.</p>
<p>Denote that <span class="math inline">\(y=f(\boldsymbol{X})\)</span> is a scalar function of the matrix <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span>. Then derivative of <span class="math inline">\(y\)</span> with respect to the matrix <span class="math inline">\(\boldsymbol{X}\)</span> is given by</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\frac{\partial y }{\partial \boldsymbol{X}} = \left[ \begin{array}{cccc} \frac{\partial y }{\partial x_{11}} &amp; \frac{\partial y }{\partial x_{12}} &amp; {\cdots} &amp; \frac{\partial y }{\partial x_{1q}} \\ \frac{\partial y }{\partial x_{21}} &amp; \frac{\partial y }{\partial x_{22}} &amp; {\cdots} &amp; \frac{\partial y }{\partial x_{2q}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ \frac{\partial y }{\partial x_{p1}} &amp; \frac{\partial y }{\partial x_{p2}} &amp; {\cdots} &amp; \frac{\partial y }{\partial x_{pq}}\end{array}\right].
 
\end{equation*}\]</span></p>
<p>Based on this definition, we can derive that</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\frac{\partial \boldsymbol{a}^T\boldsymbol{x} }{\partial \boldsymbol{x}} = \boldsymbol{a};
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\frac{\partial \boldsymbol{x}^T\boldsymbol{B}\boldsymbol{x} }{\partial \boldsymbol{x}} = (\boldsymbol{B} + \boldsymbol{B}^T)\boldsymbol{x};
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\frac{\partial \boldsymbol{(x-a)}^T\boldsymbol{B}\boldsymbol{(x-a)} }{\partial \boldsymbol{x}} = 2\boldsymbol{B}\boldsymbol{(x-a)};
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\frac{\partial \boldsymbol{(Ax+b)}^T\boldsymbol{W}\boldsymbol{(Cx+d)} }{\partial \boldsymbol{x}} = \boldsymbol{A}^T\boldsymbol{W}\boldsymbol{(Cx+d)} + \boldsymbol{C}^T\boldsymbol{W}\boldsymbol{(Ax+b)}.
 
\end{equation*}\]</span></p>
<p><em>Matrix norm.</em> The <span class="math inline">\(L_1\)</span> norm of a vector <span class="math inline">\(\boldsymbol{x}\)</span> is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\lVert \boldsymbol{x} \rVert_1 = \sum_{i=1}^p \lvert x_i \rvert.
 
\end{equation*}\]</span></p>
<p>The <span class="math inline">\(L_2\)</span> norm of a vector <span class="math inline">\(\boldsymbol{x}\)</span> is defined as</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\lVert \boldsymbol{x} \rVert^2_2 = \sum_{i=1}^p x_i^2.
 
\end{equation*}\]</span></p>
</div>
<div id="optimization" class="section level2 unnumbered">
<h2>Optimization</h2>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:fapx-fdt-normal"></span>
<img src="graphics/apx_fdt_normal.png" alt="Illustration of the application of the **First Derivative Test** on the density function of a normal distribution to identify the location where the probability density is maximal." width="100%"  />
<!--
<p class="caption marginnote">-->Figure 200: Illustration of the application of the <strong>First Derivative Test</strong> on the density function of a normal distribution to identify the location where the probability density is maximal.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The <strong>First Derivative Test</strong>, illustrated in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-1stderivativetest">10</a> in <strong>Chapter 2</strong>, is widely used in statistics and machine learning to find optimal solutions of a model formulation. Given a function <span class="math inline">\(f(x)\)</span>, the First Derivative Test finds the location <span class="math inline">\(x^*\)</span> that leads to <span class="math inline">\(\frac{\partial f(x) }{\partial x} = 0\)</span>, i.e., denoted as <span class="math inline">\(f&#39;(x^*)=0\)</span>. For instance, we can apply the First Derivative Test on the density function of a normal distribution to identify the location where the probability density is maximal<label for="tufte-sn-289" class="margin-toggle sidenote-number">289</label><input type="checkbox" id="tufte-sn-289" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">289</span> That is, the mean <span class="math inline">\(\mu\)</span>.</span>. An illustration of this application is shown in Figure <a href="appendix-a-brief-review-of-background-knowledge.html#fig:fapx-fdt-normal">200</a>.</p>
<p>The locations that are identified by the First Derivative Test may not be the global optimal points, as shown in Figure <a href="chapter-2-abstraction-regression-tree-models.html#fig:f2-localoptimal">21</a> in <strong>Chapter 2</strong>. On the other hand, it is relatively easy to use and is found to be quite effective in practice. Gradient-based optimization algorithms have been built on this concept to iteratively search for the locations where the first derivative could be set to zero. One such example is shown in Figure <a href="chapter-3-recognition-logistic-regression-ranking.html#fig:f3-RWalgor">31</a> in <strong>Chapter 3</strong>.</p>

</div>
</div>
<p style="text-align: center;">
<a href="conclusion.html"><button class="btn btn-default">Previous</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
