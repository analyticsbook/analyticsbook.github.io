<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Remarks | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Remarks | Data Analytics">

<title>Remarks | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="remarks" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="statistical-model-vs.-causal-model" class="section level3 unnumbered">
<h3>Statistical model vs. causal model</h3>
<p>People unconsciously interprets a regression model as a causal model. When an unconscious interpretation is stated, it seems absurd and untrue, but it is fair to say that the line between a statistical model and a causal model is often blurred. We cannot blame ourselves for falling for this temptation before we have had a chance to see it through a critical lens, since both models share the same representation: an asymmetric form where predictors are on one side of the equation and the outcome is on the other side. Plus, the concept of <em>significance</em> is no less confusing: a common misinterpretation is to treat the <em>statistical significance</em> of a predictor as evidence of <em>causal significance</em> in the application context. The fact is that statistical significance doesn’t imply that the relationship between the predictor and the outcome variable is causal.</p>
<p>To see this, in what follows we will show an example that the statistical significance of a variable would disappear when some other variables are added into the model. Still using the AD dataset, we fit a regression model using the variable <code>AGE</code> only.</p>
<p></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="remarks.html#cb26-1" aria-hidden="true" tabindex="-1"></a>lm.AD.age <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> AGE, <span class="at">data =</span> AD)</span>
<span id="cb26-2"><a href="remarks.html#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.age)</span></code></pre></div>
<p></p>
<p>And the result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="remarks.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-2"><a href="remarks.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb27-3"><a href="remarks.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~  AGE, data = AD)</span></span>
<span id="cb27-4"><a href="remarks.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-5"><a href="remarks.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb27-6"><a href="remarks.html#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max</span></span>
<span id="cb27-7"><a href="remarks.html#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.7020 -0.9653  0.6948  1.6182  2.5447</span></span>
<span id="cb27-8"><a href="remarks.html#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-9"><a href="remarks.html#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb27-10"><a href="remarks.html#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb27-11"><a href="remarks.html#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 30.44147    0.94564  32.191   &lt;2e-16 ***</span></span>
<span id="cb27-12"><a href="remarks.html#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         -0.03333    0.01296  -2.572   0.0104 *</span></span>
<span id="cb27-13"><a href="remarks.html#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb27-14"><a href="remarks.html#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb27-15"><a href="remarks.html#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-16"><a href="remarks.html#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.11 on 515 degrees of freedom</span></span>
<span id="cb27-17"><a href="remarks.html#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.01268,    Adjusted R-squared:  0.01076</span></span>
<span id="cb27-18"><a href="remarks.html#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 6.614 on 1 and 515 DF,  p-value: 0.0104</span></span></code></pre></div>
<p></p>
<p>The predictor, <code>AGE</code>, is significant since its <em>p-value</em> is <span class="math inline">\(0.0104\)</span>.</p>
<p>Now let’s include more demographics variables into the model.</p>
<p></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="remarks.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the multiple linear regression model </span></span>
<span id="cb28-2"><a href="remarks.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with more than one predictor</span></span>
<span id="cb28-3"><a href="remarks.html#cb28-3" aria-hidden="true" tabindex="-1"></a>lm.AD.demo <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span>  AGE <span class="sc">+</span> PTGENDER <span class="sc">+</span> PTEDUCAT,</span>
<span id="cb28-4"><a href="remarks.html#cb28-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> AD)</span>
<span id="cb28-5"><a href="remarks.html#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.demo)</span></code></pre></div>
<p></p>
<p>And the result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="remarks.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-2"><a href="remarks.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb29-3"><a href="remarks.html#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~ AGE + </span></span>
<span id="cb29-4"><a href="remarks.html#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="do">##    PTGENDER + PTEDUCAT, data = AD)</span></span>
<span id="cb29-5"><a href="remarks.html#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-6"><a href="remarks.html#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb29-7"><a href="remarks.html#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max</span></span>
<span id="cb29-8"><a href="remarks.html#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.4290 -0.9766  0.5796  1.4252  3.4539</span></span>
<span id="cb29-9"><a href="remarks.html#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-10"><a href="remarks.html#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb29-11"><a href="remarks.html#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb29-12"><a href="remarks.html#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 27.70377    1.11131  24.929  &lt; 2e-16 ***</span></span>
<span id="cb29-13"><a href="remarks.html#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         -0.02453    0.01282  -1.913   0.0563 .</span></span>
<span id="cb29-14"><a href="remarks.html#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER    -0.43356    0.18740  -2.314   0.0211 *</span></span>
<span id="cb29-15"><a href="remarks.html#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     0.17120    0.03432   4.988 8.35e-07 ***</span></span>
<span id="cb29-16"><a href="remarks.html#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb29-17"><a href="remarks.html#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb29-18"><a href="remarks.html#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-19"><a href="remarks.html#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.062 on 513 degrees of freedom</span></span>
<span id="cb29-20"><a href="remarks.html#cb29-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.0612, Adjusted R-squared:  0.05571</span></span>
<span id="cb29-21"><a href="remarks.html#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 11.15 on 3 and 513 DF,  p-value: 4.245e-07</span></span></code></pre></div>
<p></p>
<p>Now we can see that the predictor <code>AGE</code> is on the boardline of significance with a <em>p-value</em> <span class="math inline">\(0.0563\)</span>. The other predictors, <code>PTGENDER</code> and <code>PTEDUCAT</code>, are significant. The reason that the predictor <code>AGE</code> is now no longer significant is an interesting phenomenon, but it is not unusual in practice that a significant predictor becomes insignificant when other variables are included or excluded^[This is because of the statistical dependence of the estimation of the predictors. Remember that <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are two different entities. In the ground truth the two regression coefficients, <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span>, may be independent with each other, but <span class="math inline">\(\hat{\beta}_i\)</span> and <span class="math inline">\(\hat{\beta}_j\)</span> could still be correlated.</p>
<p>As we have known that</p>
<p><span class="math display">\[
\operatorname{cov}(\widehat{\boldsymbol{\beta}})=\sigma_{\epsilon}^{2}\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1},
\]</span></p>
<p>as long as <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> is not an identity matrix, the estimators of the regression parameters are dependent in a complicated and data-dependant way. Due to this reason, we need to be cautious about how to interpret the estimated regression parameters, as they are interrelated constructs.].</p>
<p>One strategy to mitigate this problem is to explore your data from every possible angle, and try out different model formulations. The goal of your data analysis is not to get a final conclusive model that dictates the rest of the analysis process. The data analysis is an exploratory and dynamic process, i.e., as you see, the dynamic interplay of the variables, how they impact each others’ significance in predicting the outcome, is something you could only obtain by analyzing the data in an exploratory and dynamic way. The fact that a model fits the data well and passes the significance test only means that there is nothing significant in the data that is found to be against the model. The goodness-of-fit of the data doesn’t mean that the data says this model is the only causal model and other models are impossible.</p>
</div>
<div id="design-of-experiments" class="section level3 unnumbered">
<h3>Design of experiments</h3>
<p>Related to this issue of “statistical model vs. causal model,” the design of experiments (DOE) is a discipline which provides systematic data collection procedures to render the regression model as a causal model. How this could be done demands a lengthy discussion and illustration<label for="tufte-sn-47" class="margin-toggle sidenote-number">47</label><input type="checkbox" id="tufte-sn-47" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">47</span> Interested readers may start with this book: Goos, P. and Jones, B., <em>Optimal Design of Experiments: A Case Study Approach</em>, Wiley, 2011.</span>. Here, we briefly review its foundation to see why it has the connection with a linear regression model.</p>
<p>We have seen in Eq. <a href="regression-models.html#eq:2-betaDist-matrix">(18)</a> that the uncertainty of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> comes from two sources, the noise in the data that is encoded in <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span>, and the structure of <span class="math inline">\(\boldsymbol{X}\)</span>. <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span> reflects essential uncertainty inherent in the system, but <span class="math inline">\(\boldsymbol{X}\)</span> is about how we collect the data. Thus, experimental design methods seek to optimize the structure of <span class="math inline">\(\boldsymbol{X}\)</span> such that the uncertainty of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> could be minimized.</p>
<p>For example, suppose that there are three predictors. Let’s consider the following structure of <span class="math inline">\(\boldsymbol{X}\)</span></p>
<p><span class="math display">\[
\boldsymbol{X}=\left[ \begin{array}{lll}{1} &amp; {0} &amp; {0} \\ {0} &amp; {1} &amp; {0} \\ {0} &amp; {0} &amp; {1} \end{array}\right].
\]</span></p>
<p>It can be seen that, with this structure, the variance of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is<label for="tufte-sn-48" class="margin-toggle sidenote-number">48</label><input type="checkbox" id="tufte-sn-48" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">48</span> <span class="math inline">\(\boldsymbol{I}\)</span> is the identity matrix. Here, <span class="math inline">\(\boldsymbol{I}_3 = \left[ \begin{array}{lll}{1} &amp; {0} &amp; {0} \\ {0} &amp; {1} &amp; {0} \\ {0} &amp; {0} &amp; {1} \end{array}\right].\)</span></span></p>
<p><span class="math display">\[cov(\hat{\boldsymbol{\beta}})=\sigma_{\epsilon}^2\boldsymbol{I}_3.\]</span></p>
<p>In other words, we can draw two main observations. First, the estimations of the regression parameters are now independent, given that their correlations are zero. Second, the variances of the estimated regression parameters are the same. Because of these two traits, this data matrix <span class="math inline">\(\boldsymbol X\)</span> is ideal and adopted in DOE to create <em>factorial designs</em>. For a linear regression model built on a dataset with such a data matrix, adding or deleting variables from the regression model will not result in changes of the estimations of other parameters.</p>
</div>
<div id="the-pessimistic-error-estimation-in-post-pruning" class="section level3 unnumbered">
<h3>The pessimistic error estimation in post-pruning</h3>
<p>Let’s look at the tree in Figure <a href="remarks.html#fig:f2-13">22</a>. It has one root node, one inner node, and three leaf nodes. The target for tree pruning, for this example, is the inner node. In other words, should we prune the inner node and its subsequent child nodes?</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-13"></span>
<img src="graphics/2_13.png" alt="An example of tree pruning using pessimistic error" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 22: An example of tree pruning using pessimistic error<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We have mentioned that if the improvement on error is not significant, we should prune the node. Let’s denote the <strong>empirical error rate</strong><label for="tufte-sn-49" class="margin-toggle sidenote-number">49</label><input type="checkbox" id="tufte-sn-49" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">49</span> Empirical error is derived based on the training data.</span> as <span class="math inline">\(\hat e\)</span>. The reason we give the notation a <em>hat</em> is because it is only an estimate of an underlying parameter, the true error <span class="math inline">\(e\)</span>. <span class="math inline">\(\hat e\)</span> is usually smaller than <span class="math inline">\(e\)</span>, and thus, it is considered to be optimistic. To create a fairer estimate of <span class="math inline">\(e\)</span>, the <strong>pessimistic error estimation</strong> approach is used for tree pruning.</p>
<p>The pessimistic error estimation, like a regression model, builds on a hypothesized data-generating mechanism . Here, the <em>data</em> is the <em>errors</em> we observed from the training data. A data point can be either correctly or wrongly classified, and we can view the probability of being wrongly classified as a Bernoulli trial, while the parameter of this Bernoulli trial, commonly denoted as <span class="math inline">\(p\)</span>, is <span class="math inline">\(e\)</span>. If we denote the total number of errors we have observed on the <span class="math inline">\(n\)</span> data points as <span class="math inline">\(d\)</span>, we can derive that <span class="math inline">\(d\)</span> is distributed as a binomial distribution. We can write this data-generating mechanism as</p>
<p><span class="math display">\[
d \sim Bino\left(n, e\right).
\]</span></p>
<p>Since <span class="math inline">\(n\)</span> is usually large, we can use the normal approximation for the binomial distribution</p>
<p><span class="math display">\[
d \sim N\left(ne, ne(1-e)\right).
\]</span></p>
<p>As <span class="math inline">\(\hat e = d/n\)</span>, we have</p>
<p><span class="math display">\[
\hat e \sim N\left(e, \frac{e(1-e)}{n}\right).
\]</span></p>
<p>Skipping further derivations (more assumptions are imposed, indeed, to derive the following conclusion), we can derive the confidence interval of <span class="math inline">\(e\)</span> as</p>
<p><span class="math display">\[\hat e - z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} \leq e \leq \hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}}.\]</span></p>
<p>The upper bound of the interval, <span class="math inline">\(\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}}\)</span>, is named as the <em>pessimistic error</em>. The tree pruning methods that use the <em>pessimistic error</em> are motivated by a conservative perspective.</p>
<p>The pessimistic error depends on three values: <span class="math inline">\(\alpha\)</span>, which is often set to be <span class="math inline">\(0.25\)</span> so that <span class="math inline">\(z_{\alpha/2}=1.15\)</span>; <span class="math inline">\(\hat e\)</span>, which is the training error rate; and <span class="math inline">\(n\)</span>, which is the number of data points at the node<label for="tufte-sn-50" class="margin-toggle sidenote-number">50</label><input type="checkbox" id="tufte-sn-50" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">50</span> The pessimistic error is larger with a smaller <span class="math inline">\(n\)</span>, an estimation method that accounts for the sample size.</span>.</p>
<p>Now let’s revisit Figure <a href="remarks.html#fig:f2-13">22</a>.</p>
<p>First, let’s derive the pessimistic errors for the two child nodes of the inner node. The empirical error rate for the left child node is <span class="math inline">\(\hat e = \frac{9}{19}=0.4737\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.4737 + 1.15\sqrt{\frac{0.4737(1-0.4737)}{19}}=0.605.\]</span></p>
<p>With this error rate, for a node with <span class="math inline">\(19\)</span> data points, the total misclassified data points can be <span class="math inline">\(mp=0.605\times 19=11.5\)</span>.</p>
<p>For the right child node, the empirical error rate is <span class="math inline">\(\hat e = \frac{9}{20}=0.45\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.45 + 1.15\sqrt{\frac{0.45(1-0.45)}{20}}=0.578.\]</span></p>
<p>With this error rate, for a node with <span class="math inline">\(20\)</span> data points, the total misclassified data points can be <span class="math inline">\(mp=0.578\times 20=11.56\)</span>.</p>
<p>Thus, if we keep this branch, the total misclassified data points would be <span class="math inline">\(mp=11.5+11.56=23.06\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-14"></span>
<img src="graphics/2_14.png" alt="The pruned tree of Figure \@ref(fig:f2-13)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 23: The pruned tree of Figure <a href="remarks.html#fig:f2-13">22</a><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now let’s evaluate the alternative: to cut the branch. This means the inner node will become a decision node, as shown in Figure <a href="remarks.html#fig:f2-14">23</a>. We will label the new decision node as C1, since <span class="math inline">\(20\)</span> of the included data points are labeled as C1, while <span class="math inline">\(19\)</span> are labeled as C2. The empirical error rate <span class="math inline">\(e\)</span> is <span class="math inline">\(\hat e = \frac{19}{39}=0.4871\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.4871 + 1.15\sqrt{\frac{0.4871(1-0.4871)}{39}}=0.579.\]</span></p>
<p>With this error rate, for a dataset with 39 data points, the total misclassified data points can be <span class="math inline">\(mp=0.579\times 39=22.59\)</span>. This is what would happen if we prune the tree. As <span class="math inline">\(22.59 &lt; 23.06\)</span>, pruning is a better decision.</p>
<p>The pruned tree is shown in Figure <a href="remarks.html#fig:f2-14">23</a>. A complete post-pruning method will continue to consider further pruning: now consider pruning the child nodes of the root node. Following the process outlined above, the would-be misclassified data points based on the pessimistic error rate at the root node is <span class="math inline">\(22.92\)</span>, and the total misclassified instances based on the pessimistic error rate from its child nodes is <span class="math inline">\(22.59+0=22.59\)</span>. Pruning the child nodes would lead to increased error. Thus, no further pruning is needed: the child nodes are kept and the final tree consists of three nodes.</p>
</div>
</div>
<p style="text-align: center;">
<a href="tree-models.html"><button class="btn btn-default">Previous</button></a>
<a href="exercises.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
