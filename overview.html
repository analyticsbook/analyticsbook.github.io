<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Overview | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Overview | Data Analytics">

<title>Overview | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>â˜° Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="overview" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 2 is about <em>Abstraction</em>. It concerns how we model and formulate a problem using <em>specific mathematical models</em>. Abstraction is powerful. It begins with identification of a few main entities from the problem, and continues to characterize their relationships. Then we focus on the study of these interconnected entities as a pure mathematical system. Consequences can be analytically established within this abstracted framework, while a phenomenon in a concerned context could be identified as special instances, or manifestations, of the abstracted model. In other words, by making abstraction of a real-world problem, we free ourselves from the application context that is usually unbounded and not well defined.</p>
<p>People often adopt a blackbox view of a real-world problem, as shown in Figure <a href="overview.html#fig:f2-1">2</a>. There is one (or more) key performance metrics of the system, called the output variable<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> Denoted as <span class="math inline">\(y\)</span>, e.g., the yield of a chemical process, the mortality rate of an ICU, the GDP of a nation, etc.</span>, and there is a set of input variables<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> Denoted as <span class="math inline">\(x_{1}, x_{2}, \ldots, x_{p}\)</span>; also called predictors, covariates, features, and, sometimes, factors.</span> that may help us predict the output variable. These variables are the <em>few main entities</em> identified from the problem, and how the input variables impact the output variable is <em>one</em> main type of relationship we develop models to characterize.</p>
<!--

 \small

<div class="figure">
<p class="caption">(\#fig:unnamed-chunk-1)\label{fig:2-1} The blackbox nature of many data science problems</p><img src="graphics/2_1.png" alt="\label{fig:2-1} The blackbox nature of many data science problems"  /></div>

 \normalsize
-->
<p></p>
<div class="figure"><span id="fig:f2-1"></span>
<p class="caption marginnote shownote">
Figure 2: The blackbox nature of many data science problems
</p>
<img src="graphics/2_1.png" alt="The blackbox nature of many data science problems" width="100%"  />
</div>
<p></p>
<p>These relationships are usually unknown, due to our lack of understanding of the system. It is not always plausible or economically feasible to develop a Newtonian style characterization of the system<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> I.e., using differential equations.</span>. Thus, statistical models are needed. They collect data from this blackbox system and build models to characterize the relationship between the input variables and the output variable. Generally, there are two cultures for statistical modeling<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> Breiman, L., * Statistical Modeling: The Two Cultures,* Statistical Science, Volume 16, Issue 3, 199-231, 2001.</span>: One is the <strong>data modeling</strong> culture , while another is the <strong>algorithmic modeling</strong> culture . Linear regression models are examples of the <em>data modeling</em> culture; decision tree models are examples of the <em>algorithmic modeling</em> culture.</p>
<p>Two goals are shared by the two cultures: (1) to understand the relationships between the predictors and the output, and (2) to predict the output based on the predictors. The two also share some common criteria to evaluate the success of their models, such as the prediction performance. Another commonality they share is a generic form of their models</p>
<p><span class="math display" id="eq:ch2-genericmodel">\[\begin{equation}
    y=f(\boldsymbol{x})+\epsilon,
\tag{1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f(\boldsymbol{x})\)</span> reflects the <em>signal</em> part of <span class="math inline">\(y\)</span> that can be ascertained by knowing <span class="math inline">\(\boldsymbol{x}\)</span>, and <span class="math inline">\(\epsilon\)</span> reflects the <em>noise</em> part of <span class="math inline">\(y\)</span> that remains uncertain even when we know <span class="math inline">\(x\)</span>. To better illustrate this, we could annotate the model form in Eq. <a href="overview.html#eq:ch2-genericmodel">(1)</a> as<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> An interesting book about the antagonism between signal and noise: Silver, N., <em>The Signal and the Noise: Why So Many Predictions Failâ€“but Some Donâ€™t</em>, Penguin Books, 2015. The authorâ€™s prediction model, however, failed to predict Donald Trumpâ€™s victory of the 2016 US Election.</span></p>
<p><span class="math display" id="eq:2-genericmodel">\[\begin{equation}
    \underbrace{y}_{data} = \underbrace{f(\boldsymbol{x})}_{signal} + \underbrace{\epsilon}_{noise},
\tag{2}
\end{equation}\]</span></p>
<p>The two cultures differ in their ideas about how to model these two parts. A brief illustration is shown in Table <a href="overview.html#tab:t2-1">1</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-1">Table 1: </span>Comparison between the two cultures of models</span><!--</caption>--></p>
<table>
<colgroup>
<col width="12%" />
<col width="19%" />
<col width="33%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left"><span class="math inline">\(f(\boldsymbol{x})\)</span></th>
<th align="left"><span class="math inline">\(\epsilon\)</span></th>
<th align="left">Ideology</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Data Modeling</strong></td>
<td align="left">Explicit form (e.g., linear regression).</td>
<td align="left">Statistical distribution (e.g., Gaussian).</td>
<td align="left">Imply <em>Cause</em> and <em>effect</em>; uncertainty has a structure.</td>
</tr>
<tr class="even">
<td align="left"><strong>Algorithmic Modeling</strong></td>
<td align="left">Implicit form (e.g., tree model).</td>
<td align="left">Rarely modeled as structured uncertainty; taken as meaningless noise.</td>
<td align="left">More focus on prediction; to <em>fit</em> data rather than to <em>explain</em> data.</td>
</tr>
</tbody>
</table>
<p></p>
<p>An illustration of the <em>data modeling</em>, using linear regression model , is shown in Figure <a href="overview.html#fig:f2-datamodel">3</a>. To develop such a model, we need efforts in two endeavors: the modeling of the signal, and the modeling of the noise (also called errors). It was probably the modeling of the errors, rather than the modeling of the signal, that eventually established a science: Statistics<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> Errors, as the name suggests, are embarrassment to a theory that claims to be rational. Errors are irrational, like a crack on the smooth surface of rationality. But rationally, if we could find <em>a law of errors</em>, we then find the law of irrationality. With that, once again rationality trumps irrationality, and the crack is sealed.</span>.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f2-datamodel"></span>
<img src="graphics/2_datamodel.png" alt="Illustration of the *ideology* of data modeling, i.e., data is used to calibrate, or, estimate, the parameters of a pre-specified mathematical structure" width="100%"  />
<p class="caption marginnote shownote">
Figure 3: Illustration of the <em>ideology</em> of data modeling, i.e., data is used to calibrate, or, estimate, the parameters of a pre-specified mathematical structure
</p>
</div>
<p></p>
<p>One only needs to take a look at the beautiful form of the normal distribution (and notice the name as well) to have an impression of its grand status as the law of errors. Comparing with other candidate forms that historically were its competitors, this concentrated, symmetrical, round and smooth form seems a more rational form that a law should take, i.e., see Figure <a href="overview.html#fig:f2-errorlaws">4</a>.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f2-errorlaws"></span>
<img src="graphics/2_errorlaws.png" alt="Hypothesized laws of errors, including the normal distribution (also called the Gaussian distribution, developed by Gauss in 1809) and some of its old rivalries" width="100%"  />
<p class="caption marginnote shownote">
Figure 4: Hypothesized laws of errors, including the normal distribution (also called the Gaussian distribution, developed by Gauss in 1809) and some of its old rivalries
</p>
</div>
<p></p>
<p>The <span class="math inline">\(\epsilon\)</span> in Eq. <a href="overview.html#eq:ch2-genericmodel">(1)</a> is often called the <strong>error term</strong> , noise term, or residual term . <span class="math inline">\(\epsilon\)</span> is usually modeled as a Gaussian distribution with mean as <span class="math inline">\(0\)</span>. The mean has to be <span class="math inline">\(0\)</span>; otherwise, it contradicts with the name <em>error</em>. <span class="math inline">\(f(\boldsymbol{x})\)</span> is also called the model of the mean structure<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> To see that, notice that <span class="math inline">\(\mathrm{E}{(y)} = \mathrm{E}{[f(\boldsymbol{x}) + \epsilon]} = \mathrm{E}{[f(\boldsymbol{x})]} + \mathrm{E}{[\epsilon]}\)</span>. Since <span class="math inline">\(\mathrm{E}{(\epsilon)} = 0\)</span> and <span class="math inline">\(f(\boldsymbol{x})\)</span> is not a random variable, we have <span class="math inline">\(\mathrm{E}{(y)} = f(\boldsymbol{x})\)</span>. Thus, <span class="math inline">\(f(\boldsymbol{x})\)</span> essentially predicts the mean of the output variable.</span>.</p>
</div>
<p style="text-align: center;">
<a href="chapter-2-abstraction-regression-tree-models.html"><button class="btn btn-default">Previous</button></a>
<a href="regression-models.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
