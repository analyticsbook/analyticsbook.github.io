<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Preface | book_migrate.utf8" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Book for analalytics" />




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Book for analalytics">

<title>Preface | book_migrate.utf8</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>







</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#cover">Cover</a></li>
<li><a href="#epigraph">Epigraph</a></li>
<li><a href="#preface">Preface</a></li>
<li><a href="#preface-1">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="epigraph.html#epigraph">Epigraph</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="preface-1.html#preface-1">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="preface-1" class="section level1 unnumbered">
<h1>Preface</h1>
<p>This book is suitable for an introductory course of data analytics to help students understand some main statistical learning models, such as linear regression, logistic regression, tree models and random forests, ensemble learning, sparse learning, principal component analysis, kernel methods including the support vector machine and kernel regression, etc. Data science practice is a process that should be told as a story, rather than a one-time implementation of one single model. This process is a main focus of this book, with many course materials about exploratory data analysis, residual analysis, and flowcharts to develop and validate models and data pipelines.</p>
<p>There are <span class="math inline">\(10\)</span> chapters. Except for Chapter 1, which gives an overview of the book, each chapter will introduce two or three techniques. For each technique, we will highlight the intuition and rationale behind it. We then articulate the intuition, use math to formulate the learning problem, and present the full version of the analytic formulation. We use R to implement the technique on both simulated or real-world datasets, present the analysis process (together with R code), show the dynamics in the analysis process, and comment on the results. Some Remarks are also made at the end of each chapter to enhance understanding of the techniques, reveal their different natures by other perspectives, reveal their limitations, and mention existing remedies to overcome these limitations.</p>
<p>There are three unique aspects to this book.</p>
<p>First, instructors will find many small datasets (i.e., consisting of 5—10 data points of 2—4 variables) in this book for models to be manually implemented by students using step-by-step process. The idea is to let students work out pencil solutions and then compare them with results obtained from established R packages. For example, a dataset with <span class="math inline">\(3\)</span> data points and <span class="math inline">\(2\)</span> predictors is used to illustrate how the shooting algorithm of LASSO could be implemented both on paper and in the R package <code>glmnet</code>. Another example is that, to understand the concept of the support vector machine (SVM), we use a dataset with <span class="math inline">\(4\)</span> data points and <span class="math inline">\(2\)</span> predictors to illustrate how the dual formulation of SVM could be solved manually. Furthermore, by this small dataset we help students see the connection between the computational algorithm with the geometric pattern of the data, i.e., the correspondence between the numeric solution with the so-called support vectors clearly visible in the scatterplot of the data.</p>
<p>Second, instructors will find graphical illustrations to explain some methods to students. These angles exploit connections between the methods; for example, the SVM is illustrated as a neural network; the kernel regression is introduced as a departure from the mindset of global models; and the logistic regression model is introduced as a few creative twists of the modeling process to apply the linear method for a binary classification problem, etc. On a larger scale, the connection between classic statistical models with machine learning algorithms is illustrated by focusing on the understanding of the iterative nature of the computational algorithms enabled by computers. We help students develop an eye for a method’s connection with other models that only appear to be different. This understanding will help us know a method’s strength and limitations, the importance of the context, and the assumptions we have carried in our data analysis.</p>
<p>Third, it is important for students to understand the storytelling component of data science. Data scientists tell stories every day. A story conveys a message, and a skillful data scientist must have the experience that the message changes its shape and meaning, depending on which model is used, how the model is tuned, or what part of the data is used. And some models have assumed a particular storytelling mode or structure. For example, we found hypothesis testing is a difficult concept for students to understand its essence, because it is a “negative” reading of data. It is not to translate what the data says, but to seek evidence from data against the null hypothesis we will need to come up with first. Examples as such will be found in the book to help students have a larger and deeper view of what they will learn.</p>
<!-- Students who have not systematically learned statistics or machine learning, but have had some exposure on basic statistical knowledge such as normal distribution, hypothesis testing, and are interested in finding data scientist jobs in a variety of areas, will find this book useful. And practitioners, who are practicing, or will be practicing, data science in interdisciplinary areas may also find this book an useful addition. For example, I know a friend who learned statistics in college, more or less in a way as applied mathematics that less emphasized data, computation, and storytelling, had found a remarkable resemblance between many data science methods with some concepts that she learned 20 years ago. She said if she could have a book that helps connect all the dots, and go through the materials with an easy-to-follow programming tool, it would help her transit to a new field of work, as she is a physicist and now she is working on genetics data.-->
<!-- We feel the same way. We have been working with medical doctors to diagnose surgical site infections using mobile phone images, with healthcare professionals to use hospital data to optimize the care process, with biologists and epidemiologies to understand the natural history of diseases, with manufacturing companies to build internet-of-things (do you think that microsoft is also a manufacturing company?) ... The challenge of interdisciplinary collaboration is to cross boundaries and build new platforms. To embark on this adventure, a fluid understanding of our methods is important, also the skill of storytelling. The importance of storytelling for a statistician --- now generally, data scientists --- could not be more emphasized by Mark Twain: "There are three kinds of lies: lies, damned lies, and statistics."^[Interested readers may see this online article of digging into the origin of this quotation: [https://www.york.ac.uk/depts/maths/histstat/lies.htm](https://www.york.ac.uk/depts/maths/histstat/lies.htm).]  -->
<!-- Data scientists tell stories everyday. A story conveys a message, and a skillful data scientist must have the experience that the message changes its shape and meaning, depending on which model you use, how you tune the model, or what part of the data is used. As the medium that emits the message is fluid, we need to take careful examinations of a method's analytic details and its fluidity. This understanding will help us to know a method's strength and limitations, the importance of the context, and the preassumptions we have carried in our data analysis. All these efforts will not weaken but rather enhance your storytelling power.-->
<!-- For example, we found hypothesis testing is a particular difficult concept for students to understand its essence, because it is a "negative" reading of data. It is not to translate what the data says, but to seek evidence from data to against the null hypothesis we will need to come up with first. In other words, as a technique it is created for answering certain questions or meeting certain needs --- we are saying the obvious --- less obvious is that techniques give a structure for the questions that we are able to formulate in practice. Giving a structure is at the same time giving a refined scope, and thus, only certain types of structured questions can be formulated. The structure is its strength, also its limitation.-->
<!-- To help readers develop a fluid understanding of some main techniques in data science and foster their storytelling power, the style of the book highlights a combination of two aspects: technical concreteness and holistic thinking^[The Chapters are named with different qualities of holistic thinking in decision-makings, including "Abstraction", "Recognition", "Resonance", "Learning", "Diagnosis", "Scalability", "Pragmatism", and "Synthesis".]. Holistic thinking is the foundation of how we formulate problems and why we could trust our formulations, knowing that our formulations inevitably are only a partial representation of a real-world problem. Holistic thinking is also the foundation of communication between team members of different backgrounds. With a diverse team, things that make sense intuitively are important to build team-wide trust in decision-making. And technical concreteness is the springboard for us to jump into a higher awareness and understanding of the problem to make holistic decisions.-->
<!-- This book is a collaborative work between two authors who do have their favorite models: Shuai focused on the regression-based models, and Houtao focused on the tree-based models. The two types of models represent two cultures in statistical modeling, but we have found many common considerations and principles underlying both cultures. We hope that a juxtaposition of both methods, a bold innovation we took, could help readers develop a unified picture of many existing data analytics tools.  -->
<!-- Last words to conclude the preface. This book highlights the use of small data to facilitate the learning of data analytics for a reason. Not to play devil's advocate. It does not mean that the methods introduced in this book could only be applied to small datasets. It is the approach of this book to introduce analytics methods through exemplary datasets as small as possible, small enough that we could grasp with intuition. We illustrate what questions we could ask and what types of models we can build based on these small datasets. In this way, we hope to connect perceivable intuition with abstract formulations. We hope this endeavor is achieved by this book.-->
<!-- Students come to a classroom for knowledge. This is true, but we feel something is missing. Knowledge is written, circulated, and read by students. Still, there is a gap between us and *knowledge* --- there is always something in life that reminds us about what we don't know about what we know. What connects us with knowledge is confidence. Thus, students come to a classroom not just for knowledge, but also for confidence about the knowledge. To help students achieve this goal, it is helpful to provide comments on the process of how knowledge was created. And the process revealed by one teacher is not necessary the same as the process revealed by another^[The knowledge creation process has been a myth. Monographs were written to rationalize/romanticize this activity of mankind. Two examples: Thomas S. Kuhn's *The Structure of Scientific Revolutions* (rationalized); Arthur Koestler's *The Act of Creation* (romanticized).]. -->
<!-- It is fun to retrospectively speculate what had happened when new knowledge was discovered. Doing educated speculation is helpful, since it will enhance our faculty of critical thinking, capacity of creating theory, and commitment on practice. -->
<!-- Bearing this ambitious goal in mind, while battling with our busy routines in our works, we hope to write up a book that is not just a book *of* techniques. We hope it is a book *about* techniques, about the workshops of researchers who work in the frontier of our academic area. Techniques are created for answering certain questions or meeting certain needs --- we are saying the obvious --- less obvious is that, techniques give a structure for the questions that we are able to formulate in practice. Giving a structure is at the same time giving a refined scope, and thus, only certain types of structured questions can be formulated. For example, thinking of hypothesis testing, which first proposes a null hypothesis, then seeks evidence from the data to reject the null hypothesis. In this classic setting of hypothesis testing, "accepting" the hypothesis is not a valid option. It is a structured way to ask questions in a certain way. The structure is its strength, also its limitation. -->
<!-- This is probably why it is often we see newcomers in a field find it is hard to ask the right questions. It is because that professionals in these areas have been educated with the mindset of asking the type of questions their techniques enable them to ask. This also means that knowing how the techniques work is important. There are many books about data analytics techniques, so in this book, we discuss what principles we can use to invent these techniques, what assumptions are made, how mathematics is used to articulate these assumptions, and how these formulations generalize a range of real-world applications into generic and abstract forms. This makes us scientists. Meanwhile, as analytics is a practical area, we also need to develop engineer's craftsmanship. This means computational competency, programming skills, experiences, and insights that we can gain by practice, the type of practice informed by theory. Practice becomes a site where theory encounters the real world. Lessons learned from practices will reflect on theory as well. -->
<!-- This is probably why it is often we see newcomers in a field find it is hard to ask the right questions. It is because that professionals in these areas have been educated with the mindset of asking the type of questions their techniques enable them to ask. This also means that knowing how the techniques work is important. There are many books about data analytics techniques, so in this book, we discuss what principles we can use to invent these techniques, what assumptions are made, how mathematics is used to articulate these assumptions, and how these formulations generalize a range of real-world applications into generic and abstract forms. This makes us scientists. Meanwhile, as analytics is a practical area, we also need to develop engineer's craftsmanship. This means computational competency, programming skills, experiences, and insights that we can gain by practice, the type of practice informed by theory. Practice becomes a site where theory encounters the real world. Lessons learned from practices will reflect on theory as well. -->

</div>
<p style="text-align: center;">
<a href="preface.html"><button class="btn btn-default">Previous</button></a>
<a href="acknowledgments.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
