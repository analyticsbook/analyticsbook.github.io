<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Exercises | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Exercises | Data Analytics">

<title>Exercises | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="exercises" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-HW-lr">Table 4: </span>Dataset for building a linear regression model</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-0.15\)</span></td>
<td align="left"><span class="math inline">\(-0.48\)</span></td>
<td align="left"><span class="math inline">\(0.46\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-0.72\)</span></td>
<td align="left"><span class="math inline">\(-0.54\)</span></td>
<td align="left"><span class="math inline">\(-0.37\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.91\)</span></td>
<td align="left"><span class="math inline">\(-0.27\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0.61\)</span></td>
<td align="left"><span class="math inline">\(1.59\)</span></td>
<td align="left"><span class="math inline">\(1.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(-1.11\)</span></td>
<td align="left"><span class="math inline">\(0.34\)</span></td>
<td align="left"><span class="math inline">\(-0.11\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Here let’s consider the dataset in Table <a href="exercises.html#tab:t2-HW-lr">4</a>. Let’s build a linear regression model, i.e.,
<span class="math display">\[
  y = \beta_{0}+\beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
  \]</span>
and
<span class="math display">\[
  \epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right).
  \]</span>
and calculate the regression parameters <span class="math inline">\(\beta_{0},\beta_{1},\beta_{2}\)</span> manually.</p></li>
<li><p> Follow up the data on Q1. Use the R pipeline to build the linear regression model. Compare the result from R and the result by your manual calculation.</p></li>
<li><p> Read the following output in R.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="exercises.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb30-2"><a href="exercises.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data)</span></span>
<span id="cb30-3"><a href="exercises.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-4"><a href="exercises.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb30-5"><a href="exercises.html#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="do">##       Min        1Q    Median        3Q       Max </span></span>
<span id="cb30-6"><a href="exercises.html#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="do">## -0.239169 -0.065621  0.005689  0.064270  0.310456 </span></span>
<span id="cb30-7"><a href="exercises.html#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-8"><a href="exercises.html#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb30-9"><a href="exercises.html#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb30-10"><a href="exercises.html#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  0.009124   0.010473   0.871    0.386    </span></span>
<span id="cb30-11"><a href="exercises.html#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="do">## x1           1.008084   0.008696 115.926   &lt;2e-16 ***</span></span>
<span id="cb30-12"><a href="exercises.html#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="do">## x2           0.494473   0.009130  54.159   &lt;2e-16 ***</span></span>
<span id="cb30-13"><a href="exercises.html#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="do">## x3           0.012988   0.010055   1.292    0.200    </span></span>
<span id="cb30-14"><a href="exercises.html#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="do">## x4          -0.002329   0.009422  -0.247    0.805    </span></span>
<span id="cb30-15"><a href="exercises.html#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb30-16"><a href="exercises.html#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb30-17"><a href="exercises.html#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-18"><a href="exercises.html#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 0.1011 on 95 degrees of freedom</span></span>
<span id="cb30-19"><a href="exercises.html#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.9942, Adjusted R-squared:  0.994 </span></span>
<span id="cb30-20"><a href="exercises.html#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  4079 on 4 and 95 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<ul>
<li><p> (a) Write the fitted regression model. (b) Identify the significant variables. (c) What is the R-squared of this model? Does the model fit the data well? (d) What would you recommend as the next step in data analysis?</p></li>
<li><p> Consider the dataset in Table <a href="exercises.html#tab:t2-hw-dt">5</a>. Build a decision tree model by manual calculation. To simplify the process, let’s only try three alternatives for the splits: <span class="math inline">\(x_1\geq0.59\)</span>, <span class="math inline">\(x_1\geq0.37\)</span>, and <span class="math inline">\(x_2\geq0.35\)</span>.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-hw-dt">Table 5: </span>Dataset for building a decision tree</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.22\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0.58\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0.57\)</span></td>
<td align="left"><span class="math inline">\(0.28\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0.41\)</span></td>
<td align="left"><span class="math inline">\(0.43\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(0.6\)</span></td>
<td align="left"><span class="math inline">\(0.29\)</span></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(0.12\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(0.25\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(8\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Follow up on the dataset in Q5. Use the R pipeline for building a decision tree model. Compare the result from R and the result by your manual calculation.</p></li>
<li><p> Use the <code>mtcars</code> dataset in R, select the variable <code>mpg</code> as the outcome variable and other variables as predictors, run the R pipeline for linear regression, and summarize your findings.</p></li>
<li><p> Use the <code>mtcars</code> dataset in R, select the variable <code>mpg</code> as the outcome variable and other variables as predictors, run the R pipeline for decision tree, and summarize your findings. Another dataset is to use the <code>iris</code> dataset, select the variable <code>Species</code> as the outcome variable (i.e., to build a classification tree).</p></li>
<li><p> Design a simulated experiment to evaluate the effectiveness of the <code>lm()</code> in R. For instance, you can simulate <span class="math inline">\(100\)</span> samples from a linear regression model with <span class="math inline">\(2\)</span> variables,
<span class="math display">\[
  y = \beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
  \]</span>
where <span class="math inline">\(\beta_{1} = 1\)</span>, <span class="math inline">\(\beta_{2} = 1\)</span>, and
<span class="math display">\[
  \epsilon \sim N\left(0, 1\right).
  \]</span>
You can simulate <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> using the standard normal distribution <span class="math inline">\(N\left(0, 1\right)\)</span>. Run <code>lm()</code> on the simulated data, and see how close the fitted model is with the true model.</p></li>
<li><p> Follow up on the experiment in Q9. Let’s add two more variables <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_4\)</span> into the dataset but still generate <span class="math inline">\(100\)</span> samples from a linear regression model from the same underlying model
<span class="math display">\[
  y = \beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
  \]</span>
where <span class="math inline">\(\beta_{1} = 1\)</span>, <span class="math inline">\(\beta_{2} = 1\)</span>, and
<span class="math display">\[
  \epsilon \sim N\left(0, 1\right).
  \]</span>
In other words, <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_4\)</span> are insignificant variables. You can simulate <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_4\)</span> using the standard normal distribution <span class="math inline">\(N\left(0, 1\right)\)</span>. Run <code>lm()</code> on the simulated data, and see how close the fitted model is with the true model.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-hw-dt"></span>
<img src="graphics/2_hw_dt.png" alt="The true model for simulation experiment in Q12" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 24: The true model for simulation experiment in Q12<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<ul>
<li><p> Follow up on the experiment in Q10. Run <code>rpart()</code> on the simulated data, and see how close the fitted model is with the true model.</p></li>
<li><p> Design a simulated experiment to evaluate the effectiveness of the <code>rpart()</code> in R package <code>rpart</code>. For instance, you can simulate <span class="math inline">\(100\)</span> samples from a tree model as shown in Figure <a href="exercises.html#fig:f2-hw-dt">24</a>, run <code>rpart()</code> on the simulated data, and see how close the fitted model is with the true model.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="remarks.html"><button class="btn btn-default">Previous</button></a>
<a href="chapter-3-recognition-logistic-regression-ranking.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
