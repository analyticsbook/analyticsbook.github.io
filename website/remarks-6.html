<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Remarks | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Remarks | Data Analytics">

<title>Remarks | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="remarks-6" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="why-lasso-uses-the-l" class="section level3" number="0.0.1">
<h3><span class="header-section-number">0.0.1</span> Why LASSO uses the L</h3>
<p>LASSO is often compared with another model, the <strong>Ridge regression</strong> that was developed about <span class="math inline">\(30\)</span> years before LASSO<label for="tufte-sn-217" class="margin-toggle sidenote-number">217</label><input type="checkbox" id="tufte-sn-217" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">217</span> Hoerl, A.E. and Kennard, R.W. <em>Ridge regression: biased estimation for nonorthogonal problems</em>, Technometrics, Volume 12, Issue 1, Pages 55-67, 1970.</span>.</p>
<p>The formulation of Ridge regression is</p>
<p><span class="math display" id="eq:8-RIDGE">\[\begin{equation}
        \boldsymbol{\hat \beta} = \arg\min_{\boldsymbol \beta} \left \{   \underbrace{(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})}_{\text{Least squares}} + \underbrace{\lambda \lVert \boldsymbol{\beta}\rVert^2_2}_{L_2 \text{ norm penalty}} \right \}
\tag{97}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\lVert \boldsymbol \beta \rVert^2_2=\sum_{i=1}^p \lvert\beta_i\rvert^2\)</span>.</p>
<p>Ridge regression seems to bear the same spirit of LASSO—they both penalize the magnitudes of the regression parameters. However, it has been noticed that in the Ridge regression model the estimated regression parameters are less likely to be <span class="math inline">\(0\)</span>. Even with a very large <span class="math inline">\(\lambda\)</span>, many elements in <span class="math inline">\(\boldsymbol{\beta}\)</span> may be close to zero (i.e., with a tiny numerical magnitude), but not zero<label for="tufte-sn-218" class="margin-toggle sidenote-number">218</label><input type="checkbox" id="tufte-sn-218" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">218</span> If they are not zero, these variables can still generate impact on the estimation of other regression parameters.</span>. This may not be entirely a surprise, as the Ridge regression is often used as a <em>stabilization</em> strategy to handle the <em>multicollinearity</em> issue or other issues that result in numerical instability of parameter estimation in linear regression, while LASSO is mainly used as a <em>variable selection</em> strategy.</p>
<p></p>
<div class="figure"><span id="fig:f8-10"></span>
<p class="caption marginnote shownote">
Figure 155: Why LASSO (left) generates sparse estimates, while Ridge regression (right) does not
</p>
<img src="graphics/8_10.png" alt="Why LASSO (left) generates sparse estimates, while Ridge regression (right) does not" width="100%"  />
</div>
<p></p>
<p>To reveal why the <span class="math inline">\(L_1\)</span> norm in LASSO regression differs from the <span class="math inline">\(L_2\)</span> norm used in the Ridge regression, we adopt an explanation<label for="tufte-sn-219" class="margin-toggle sidenote-number">219</label><input type="checkbox" id="tufte-sn-219" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">219</span> Hastie, T., Tibshirani, R. and Friedman, J. <em>The Elements of Statistical Learning</em>, <span class="math inline">\(2^{nd}\)</span> edition. Springer, 2009.</span> as shown in Figure <a href="remarks-6.html#fig:f8-10">155</a>. There are <span class="math inline">\(2\)</span> predictors, thus, two regression coefficients <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>. The contour plot corresponds to the least squares loss function which is shared by both the LASSO and the Ridge regression models. And the least squares estimator, <span class="math inline">\(\boldsymbol{\hat\beta}\)</span>, is in the center of the contour plots. The shadowed rhombus in Figure <a href="remarks-6.html#fig:f8-10">155</a> (left) corresponds to the <span class="math inline">\(L_1\)</span> norm, and the shadowed circle in Figure <a href="remarks-6.html#fig:f8-10">155</a> (right) corresponds to the <span class="math inline">\(L_2\)</span> norm. For either model, the optimal solution happens at the <em>contact point</em> of the two shapes.</p>
<p>Figure <a href="remarks-6.html#fig:f8-10">155</a> shows that the contact point of the elliptic contour plot with the shadowed rhombus is likely to be one of the <em>sharp</em> corner points. A feature of these corner points is that some variables are zero, e.g., in Figure <a href="remarks-6.html#fig:f8-10">155</a> (left), the point of contact implies that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
<p>As a comparison, in Ridge regression, the shadowed circle has no such sharp corner points. Given the infinite number of potential contact points of the elliptic contour plot with the shadowed circle, it is expected that the Ridge regression will not result in sparse solutions with exact zeros in the estimated regression coefficients.</p>
<p>Following this idea<label for="tufte-sn-220" class="margin-toggle sidenote-number">220</label><input type="checkbox" id="tufte-sn-220" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">220</span> I.e., to create sharp contact points between the elliptical contour with the shape representing the norm.</span>, the <span class="math inline">\(L_1\)</span> norm is extended to the <span class="math inline">\(L_q\)</span> norm, where <span class="math inline">\(q \leq 1\)</span>. For any <span class="math inline">\(q\leq 1\)</span>, we could generate sharp corner points to enable sparse solutions. The advantage of using <span class="math inline">\(q&lt;1\)</span> is to reduce bias in the model<label for="tufte-sn-221" class="margin-toggle sidenote-number">221</label><input type="checkbox" id="tufte-sn-221" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">221</span> I.e., the <span class="math inline">\(L_1\)</span> norm not only penalizes the regression coefficients of the irrelevant variables to be zero, it also penalizes the regression coefficients of the relevant variables. This is a <em>bias</em> in the model.</span>. The cost of using <span class="math inline">\(q&lt;1\)</span> is that it will result in <em>non-convex</em> penalty terms, creating a more challenging optimization problem than LASSO. Considerable amounts of efforts have been devoted to two main directions: development of new norms, and development of new algorithms (i.e., which are usually iterative procedures with closed-form solution in each iteration, like the Shooting algorithm). Interested readers can read more of these works<label for="tufte-sn-222" class="margin-toggle sidenote-number">222</label><input type="checkbox" id="tufte-sn-222" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">222</span> A good place to start with: <a href="https://github.com/jiayuzhou/SLEP">https://github.com/jiayuzhou/SLEP</a> and its manual (in PDF).</span>.</p>
<!-- % The shooting algorithm ^[Fu, WJ. Penalized regressions: the bridge versus the lasso. *Journal of Computational and Graphical Statistics* , 1998.]  has been widely used in many extension models of LASSO in the statistics community. The shooting algorithm is easy to use and has nice interpretation of each iteration. But it could be slow in very high-dimensional situations. Also, with more complex penalty terms such as those $L_21$-norm regularization or group regularization terms, the shooting algorithm may not work anymore. In machine learning community where the computational efficiency is of particular interest, many scalable algorithms such as the projection operator based methods have been developed. Interested readers can read more of these works^[[https://github.com/jiayuzhou/SLEP](https://github.com/jiayuzhou/SLEP)] in this direction that provided closed form iterative updating rules by projection operator on a variety of regularization terms. -->
</div>
<div id="the-myth-of-pca" class="section level3 unnumbered">
<h3>The myth of PCA</h3>
<p>While PCA has been widely used, it is often criticized as a <em>black box</em> model that lacks <em>interpretability</em>. It depends on the circumstances where the PCA is used. Sometimes, it is not easy to connect the identified principal components with physical entities. The applications of PCA in many areas have formed a convention, or a myth—some statisticians may say—such that formulistic rubrics have been invented to convert their data into patterns, then further convert these patterns into formulated sentences such as, “the variables that have larger magnitudes in the first <span class="math inline">\(3\)</span> PCs correspond to the brain regions in the hippocampus area, indicating that these brain regions manifest significant functional connectivity to deliver the verbal function,” or “we have identified <span class="math inline">\(5\)</span> significant PCs, and the genes that show dominant magnitudes in the loading of the <span class="math inline">\(1^{st}\)</span> PC are all related to T-cell production and immune functions … each of the PC indicates a biological pathway that consists of these constitutional genes working together to produce specific types of proteins.” Then hear what had been said by financial analysts: “using PCA on <span class="math inline">\(100\)</span> stocks<label for="tufte-sn-223" class="margin-toggle sidenote-number">223</label><input type="checkbox" id="tufte-sn-223" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">223</span> Each stock is a variable.</span>, we found that the <span class="math inline">\(1^{st}\)</span> PC consists of <span class="math inline">\(10\)</span> stocks as their weights in the loading are significantly larger than the other stocks. This may indicate that there is strong correlation between these <span class="math inline">\(10\)</span> stocks … consider this fact when you come up with your investment strategy…”</p>
<p>Having said that, sometimes there is magic in PCA.</p>
<p>Consider another small data example that is shown in Table <a href="remarks-6.html#tab:t8-PCAnet">38</a>. It has <span class="math inline">\(3\)</span> variables and <span class="math inline">\(8\)</span> data points.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-PCAnet">Table 38: </span>A dataset example for PCA</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-3\)</span></td>
<td align="left"><span class="math inline">\(-2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>First, we normalize the variables, i.e., for <span class="math inline">\(x_1\)</span>, we compute its mean and standard derivation first, which are <span class="math inline">\(2.375\)</span> and <span class="math inline">\(3.159\)</span>, respectively. Then, we distract each measurement of <span class="math inline">\(x_1\)</span> from its mean and further divide it by its standard derivation. For example, for the first measurement of <span class="math inline">\(x_1\)</span>, <span class="math inline">\(-1\)</span>, it is converted as</p>
<p><span class="math display">\[
\frac{-1-2.375}{3.159}=-1.07.
\]</span></p>
<p>The second measurement, <span class="math inline">\(3\)</span>, is converted as</p>
<p><span class="math display">\[
\frac{3-2.375}{3.159}=0.20.
\]</span></p>
<p>And so on.</p>
<p>Similarly, for <span class="math inline">\(x_2\)</span>, we compute its mean and standard derivation, which are <span class="math inline">\(3\)</span> and <span class="math inline">\(2.88\)</span>, respectively. For <span class="math inline">\(x_3\)</span>, we compute its mean and standard derivation, which are <span class="math inline">\(0.88\)</span> and <span class="math inline">\(0.35\)</span>, respectively …. Then the standardized dataset is shown in Table <a href="remarks-6.html#tab:t8-standardx2">39</a>.<label for="tufte-sn-224" class="margin-toggle sidenote-number">224</label><input type="checkbox" id="tufte-sn-224" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">224</span> In this example, numbers are rounded to <span class="math inline">\(2\)</span> decimal places.</span></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-standardx2">Table 39: </span>Standardized dataset of Table <a href="remarks-6.html#tab:t8-PCAnet">38</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-1.07\)</span></td>
<td align="left"><span class="math inline">\(-1.04\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.2\)</span></td>
<td align="left"><span class="math inline">\(0.00\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(0.2\)</span></td>
<td align="left"><span class="math inline">\(0.69\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-1.70\)</span></td>
<td align="left"><span class="math inline">\(-1.73\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(0.20\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.83\)</span></td>
<td align="left"><span class="math inline">\(1.04\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1.46\)</span></td>
<td align="left"><span class="math inline">\(1.04\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-0.11\)</span></td>
<td align="left"><span class="math inline">\(-0.35\)</span></td>
<td align="left"><span class="math inline">\(-2.48\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>We calculate the sample covariance matrix<label for="tufte-sn-225" class="margin-toggle sidenote-number">225</label><input type="checkbox" id="tufte-sn-225" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">225</span> From <span class="math inline">\(\boldsymbol{S}\)</span> we see that the correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> is quite large, while the correlation between them with <span class="math inline">\(x_3\)</span> is very small. Figure <a href="remarks-6.html#fig:f8-PCAnet">156</a> visualizes this relationship of the three variables.</span> <span class="math inline">\(\boldsymbol{S}\)</span> as</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-PCAnet"></span>
<img src="graphics/8_PCAnet.png" alt="Visualization of the relationship between the three variables" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 156: Visualization of the relationship between the three variables<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display">\[ \boldsymbol{S}=\frac{\boldsymbol{X}^T \boldsymbol{X}}{N-1} = \begin{bmatrix}
1 &amp; 0.96 &amp; 0.05 \\
0.96 &amp; 1 &amp; 0.14 \\
0.05 &amp; 0.14 &amp; 1\\
\end{bmatrix}.
\]</span></p>
<p>By solving the eigenvalue decomposition problem of the matrix <span class="math inline">\(\boldsymbol{S}\)</span>, we obtain the PCs and their loadings.</p>
<p>For the <span class="math inline">\(1^{st}\)</span> PC, we get</p>
<p><span class="math display">\[
\lambda_1=1.98 \, \text{ and } \, \boldsymbol{w}_{(1)}=\left[ -0.69, \, -0.70, \, -0.14\right].
\]</span></p>
<p>For the <span class="math inline">\(2^{nd}\)</span> PC, we get</p>
<p><span class="math display">\[
\lambda_2=0.98 \, \text{ and } \, \boldsymbol{w}_{(2)}=\left[  0.14, \,0.05, \, -0.99\right].
\]</span></p>
<p>For the <span class="math inline">\(3^{rd}\)</span> PC, we get</p>
<p><span class="math display">\[
\lambda_3=0.04 \, \text{ and } \, \boldsymbol{w}_{(3)}=\left[  0.70, \, -0.71, \, 0.07\right].
\]</span></p>
<p>We can calculate the cumulative contributions of the three PCs</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-PCA3net-scree"></span>
<img src="graphics/8_PCA3net_scree.png" alt="Scree plot of the PCA analysis on data in Table \@ref(tab:t8-standardx2)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 157: Scree plot of the PCA analysis on data in Table <a href="remarks-6.html#tab:t8-standardx2">39</a><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display">\[
\text{For the } 1^{st} \text{ PC: } 1.98/(1.98+0.98+0.04) = 0.66.
\]</span></p>
<p><span class="math display">\[
\text{For the } 2^{nd} \text{ PC: } 0.98/(1.98+0.98+0.04) = 0.33.
\]</span></p>
<p><span class="math display">\[
\text{For the } 3^{rd} \text{ PC: } 0.04/(1.98+0.98+0.04) = 0.01.
\]</span></p>
<p>The <span class="math inline">\(3^{rd}\)</span> PC is statistically insignificant. The scree plot is shown in Figure <a href="remarks-6.html#fig:f8-PCA3net-scree">157</a>.</p>
<p>We look into the details of the learned PCA model, e.g., the <em>loadings</em> of the PCs. It leads to Figure <a href="remarks-6.html#fig:f8-PCAnet-pc">158</a>.</p>
<p></p>
<div class="figure"><span id="fig:f8-PCAnet-pc"></span>
<p class="caption marginnote shownote">
Figure 158: Loadings of the <span class="math inline">\(1^{st}\)</span> PC (left), <span class="math inline">\(2^{nd}\)</span> PC (middle), and <span class="math inline">\(3^{rd}\)</span> PC (right)
</p>
<img src="graphics/8_PCAnet_pc1.png" alt="Loadings of the $1^{st}$ PC (left), $2^{nd}$ PC (middle), and $3^{rd}$ PC (right)" width="30%"  /><img src="graphics/8_PCAnet_pc2.png" alt="Loadings of the $1^{st}$ PC (left), $2^{nd}$ PC (middle), and $3^{rd}$ PC (right)" width="30%"  /><img src="graphics/8_PCAnet_pc3.png" alt="Loadings of the $1^{st}$ PC (left), $2^{nd}$ PC (middle), and $3^{rd}$ PC (right)" width="30%"  />
</div>
<p></p>
<p>Figure <a href="remarks-6.html#fig:f8-PCAnet-pc">158</a> shows that the <span class="math inline">\(1^{st}\)</span> PC is mainly defined by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, the <span class="math inline">\(2^{nd}\)</span> PC is mainly defined by <span class="math inline">\(x_3\)</span>, and the <span class="math inline">\(3^{rd}\)</span> PC, despite its small proportion of importance, mainly consists of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> as well<label for="tufte-sn-226" class="margin-toggle sidenote-number">226</label><input type="checkbox" id="tufte-sn-226" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">226</span> Readers may compare Figure <a href="remarks-6.html#fig:f8-PCAnet-pc">158</a> with Figure <a href="remarks-6.html#fig:f8-PCAnet">156</a>—Is this a coincidence?</span>.</p>
<p>The R code for generating Figure <a href="remarks-6.html#fig:f8-PCAnet-pc">158</a> is shown below.</p>
<p></p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="remarks-6.html#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA example</span></span>
<span id="cb180-2"><a href="remarks-6.html#cb180-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">2</span>)</span>
<span id="cb180-3"><a href="remarks-6.html#cb180-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="sc">-</span><span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">2</span>)</span>
<span id="cb180-4"><a href="remarks-6.html#cb180-4" aria-hidden="true" tabindex="-1"></a>x3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb180-5"><a href="remarks-6.html#cb180-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x1,x2,x3)</span>
<span id="cb180-6"><a href="remarks-6.html#cb180-6" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(FactoMineR)</span>
<span id="cb180-7"><a href="remarks-6.html#cb180-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(factoextra)</span>
<span id="cb180-8"><a href="remarks-6.html#cb180-8" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">PCA</span>(X)</span>
<span id="cb180-9"><a href="remarks-6.html#cb180-9" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>eig</span>
<span id="cb180-10"><a href="remarks-6.html#cb180-10" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>var<span class="sc">$</span>coord</span>
<span id="cb180-11"><a href="remarks-6.html#cb180-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-12"><a href="remarks-6.html#cb180-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the screeplot</span></span>
<span id="cb180-13"><a href="remarks-6.html#cb180-13" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_screeplot</span>(t, <span class="at">addlabels =</span> <span class="cn">TRUE</span>)</span>
<span id="cb180-14"><a href="remarks-6.html#cb180-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-15"><a href="remarks-6.html#cb180-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the variable loadings plot</span></span>
<span id="cb180-16"><a href="remarks-6.html#cb180-16" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_contrib</span>(t, <span class="at">choice =</span> <span class="st">&quot;var&quot;</span>, <span class="at">axes =</span> <span class="dv">1</span>, <span class="at">top =</span> <span class="dv">20</span>,</span>
<span id="cb180-17"><a href="remarks-6.html#cb180-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">sort.val =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb180-18"><a href="remarks-6.html#cb180-18" aria-hidden="true" tabindex="-1"></a>            <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">20</span>))</span>
<span id="cb180-19"><a href="remarks-6.html#cb180-19" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_contrib</span>(t, <span class="at">choice =</span> <span class="st">&quot;var&quot;</span>, <span class="at">axes =</span> <span class="dv">2</span>, <span class="at">top =</span> <span class="dv">20</span>,</span>
<span id="cb180-20"><a href="remarks-6.html#cb180-20" aria-hidden="true" tabindex="-1"></a>             <span class="at">sort.val =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb180-21"><a href="remarks-6.html#cb180-21" aria-hidden="true" tabindex="-1"></a>            <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">20</span>))</span>
<span id="cb180-22"><a href="remarks-6.html#cb180-22" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_contrib</span>(t, <span class="at">choice =</span> <span class="st">&quot;var&quot;</span>, <span class="at">axes =</span> <span class="dv">3</span>, <span class="at">top =</span> <span class="dv">20</span>,</span>
<span id="cb180-23"><a href="remarks-6.html#cb180-23" aria-hidden="true" tabindex="-1"></a>             <span class="at">sort.val =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb180-24"><a href="remarks-6.html#cb180-24" aria-hidden="true" tabindex="-1"></a>            <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">20</span>))</span></code></pre></div>
<p></p>
<p>Now, suppose that there is an outcome variable <span class="math inline">\(y\)</span>. Data in Table <a href="remarks-6.html#tab:t8-PCAnet">38</a> is augmented with a new column, as shown in Table <a href="remarks-6.html#tab:t8-PCAnet2">40</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-PCAnet2">Table 40: </span>Table <a href="remarks-6.html#tab:t8-PCAnet">38</a> is augmented with an outcome variable</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1.33\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.70\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.99\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-3\)</span></td>
<td align="left"><span class="math inline">\(-2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-1.78\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.07\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(4.62\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3.87\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.58\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The goal is to build a linear regression model to predict <span class="math inline">\(y\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="remarks-6.html#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a linear regression model</span></span>
<span id="cb181-2"><a href="remarks-6.html#cb181-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">2</span>)</span>
<span id="cb181-3"><a href="remarks-6.html#cb181-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="sc">-</span><span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">2</span>)</span>
<span id="cb181-4"><a href="remarks-6.html#cb181-4" aria-hidden="true" tabindex="-1"></a>x3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb181-5"><a href="remarks-6.html#cb181-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x1,x2,x3)</span>
<span id="cb181-6"><a href="remarks-6.html#cb181-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.33</span>,<span class="fl">0.7</span>,<span class="fl">2.99</span>,<span class="sc">-</span><span class="fl">1.78</span>,<span class="fl">0.07</span>,<span class="fl">4.62</span>,<span class="fl">3.87</span>,<span class="fl">0.58</span>)</span>
<span id="cb181-7"><a href="remarks-6.html#cb181-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(y,X))</span>
<span id="cb181-8"><a href="remarks-6.html#cb181-8" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>., <span class="at">data =</span> data)</span>
<span id="cb181-9"><a href="remarks-6.html#cb181-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="remarks-6.html#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb182-2"><a href="remarks-6.html#cb182-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data)</span></span>
<span id="cb182-3"><a href="remarks-6.html#cb182-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb182-4"><a href="remarks-6.html#cb182-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb182-5"><a href="remarks-6.html#cb182-5" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) -0.64698    1.60218  -0.404    0.707</span></span>
<span id="cb182-6"><a href="remarks-6.html#cb182-6" aria-hidden="true" tabindex="-1"></a><span class="do">## x1          -0.03686    0.67900  -0.054    0.959</span></span>
<span id="cb182-7"><a href="remarks-6.html#cb182-7" aria-hidden="true" tabindex="-1"></a><span class="do">## x2           0.65035    0.75186   0.865    0.436</span></span>
<span id="cb182-8"><a href="remarks-6.html#cb182-8" aria-hidden="true" tabindex="-1"></a><span class="do">## x3           0.37826    1.75338   0.216    0.840</span></span>
<span id="cb182-9"><a href="remarks-6.html#cb182-9" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb182-10"><a href="remarks-6.html#cb182-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.546 on 4 degrees of freedom</span></span>
<span id="cb182-11"><a href="remarks-6.html#cb182-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6999, Adjusted R-squared:  0.4749</span></span>
<span id="cb182-12"><a href="remarks-6.html#cb182-12" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  3.11 on 3 and 4 DF,  p-value: 0.1508</span></span></code></pre></div>
<p>
The <em>R-squared</em> is <span class="math inline">\(0.6999\)</span>, but the three variables are not significant. This is unusual. Recall that <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are highly correlated—there is an issue of <em>multicollinearity</em> in this dataset.</p>
<p>Try a linear regression model with <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_3\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="remarks-6.html#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a linear regression model</span></span>
<span id="cb183-2"><a href="remarks-6.html#cb183-2" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data)</span>
<span id="cb183-3"><a href="remarks-6.html#cb183-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="remarks-6.html#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb184-2"><a href="remarks-6.html#cb184-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data)</span></span>
<span id="cb184-3"><a href="remarks-6.html#cb184-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb184-4"><a href="remarks-6.html#cb184-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb184-5"><a href="remarks-6.html#cb184-5" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  -0.4764     1.5494  -0.307   0.7709</span></span>
<span id="cb184-6"><a href="remarks-6.html#cb184-6" aria-hidden="true" tabindex="-1"></a><span class="do">## x1            0.5282     0.1805   2.927   0.0328 *</span></span>
<span id="cb184-7"><a href="remarks-6.html#cb184-7" aria-hidden="true" tabindex="-1"></a><span class="do">## x3            0.8793     1.6127   0.545   0.6090</span></span>
<span id="cb184-8"><a href="remarks-6.html#cb184-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb184-9"><a href="remarks-6.html#cb184-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.507 on 5 degrees of freedom</span></span>
<span id="cb184-10"><a href="remarks-6.html#cb184-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6438, Adjusted R-squared:  0.5013</span></span>
<span id="cb184-11"><a href="remarks-6.html#cb184-11" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 4.519 on 2 and 5 DF,  p-value: 0.07572</span></span></code></pre></div>
<p>
Now <span class="math inline">\(x_1\)</span> is significant. Without fitting another model, we know that <span class="math inline">\(x_2\)</span> has to be significant as well, i.e., as shown in Figure <a href="remarks-6.html#fig:f8-PCAnet">156</a>, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are two highly correlated variables that are just like one variable. But because of the multicollinearity, when both are included in the model, neither turns out to be significant.</p>
<p>To overcome the multicollinearity, we have mentioned that the Principal Component Regression (PCR) method is a good approach. First, we calculate the transformed data (i.e., the projections of the data points on the PCs, as shown in Figure <a href="principal-component-analysis.html#fig:f8-PCA-line">148</a>) using Eq. <a href="principal-component-analysis.html#eq:8-PCA-z">(92)</a>. Results are shown in Table <a href="remarks-6.html#tab:t8-PCAnet-PC">41</a>. An important characteristic of the new variables, <span class="math inline">\(\text{PC}_1\)</span>, <span class="math inline">\(\text{PC}_2\)</span>, and <span class="math inline">\(\text{PC}_3\)</span>, is that they are orthogonal to each other, and thus, their correlations are <span class="math inline">\(0\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-PCAnet-PC">Table 41: </span>The coordinates of the <em>white dots</em>, i.e., aka, the projections of the data points on the PCs</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(\text{PC}_1\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_2\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1.43\)</span></td>
<td align="left"><span class="math inline">\(-0.55\)</span></td>
<td align="left"><span class="math inline">\(0.01\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-0.18\)</span></td>
<td align="left"><span class="math inline">\(-0.32\)</span></td>
<td align="left"><span class="math inline">\(0.16\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-0.67\)</span></td>
<td align="left"><span class="math inline">\(-0.29\)</span></td>
<td align="left"><span class="math inline">\(-0.33\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2.36\)</span></td>
<td align="left"><span class="math inline">\(-0.68\)</span></td>
<td align="left"><span class="math inline">\(0.06\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-0.43\)</span></td>
<td align="left"><span class="math inline">\(-0.30\)</span></td>
<td align="left"><span class="math inline">\(-0.08\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.18\)</span></td>
<td align="left"><span class="math inline">\(-0.13\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-1.80\)</span></td>
<td align="left"><span class="math inline">\(-0.09\)</span></td>
<td align="left"><span class="math inline">\(0.31\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.66\)</span></td>
<td align="left"><span class="math inline">\(2.41\)</span></td>
<td align="left"><span class="math inline">\(-0.01\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Then we can build a linear regression model of <span class="math inline">\(y\)</span> using the three new predictors, <span class="math inline">\(\text{PC}_1\)</span>, <span class="math inline">\(\text{PC}_2\)</span>, and <span class="math inline">\(\text{PC}_3\)</span>. The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="remarks-6.html#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb185-2"><a href="remarks-6.html#cb185-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ PC1 + PC2 + PC3, data = data)</span></span>
<span id="cb185-3"><a href="remarks-6.html#cb185-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb185-4"><a href="remarks-6.html#cb185-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb185-5"><a href="remarks-6.html#cb185-5" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  1.54750    0.54668   2.831   0.0473 *</span></span>
<span id="cb185-6"><a href="remarks-6.html#cb185-6" aria-hidden="true" tabindex="-1"></a><span class="do">## PC1         -1.25447    0.41571  -3.018   0.0393 *</span></span>
<span id="cb185-7"><a href="remarks-6.html#cb185-7" aria-hidden="true" tabindex="-1"></a><span class="do">## PC2         -0.06022    0.58848  -0.102   0.9234</span></span>
<span id="cb185-8"><a href="remarks-6.html#cb185-8" aria-hidden="true" tabindex="-1"></a><span class="do">## PC3         -1.39950    3.02508  -0.463   0.6677</span></span>
<span id="cb185-9"><a href="remarks-6.html#cb185-9" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb185-10"><a href="remarks-6.html#cb185-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.546 on 4 degrees of freedom</span></span>
<span id="cb185-11"><a href="remarks-6.html#cb185-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6999, Adjusted R-squared:  0.4749</span></span>
<span id="cb185-12"><a href="remarks-6.html#cb185-12" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  3.11 on 3 and 4 DF,  p-value: 0.1508</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-PCAnetY"></span>
<img src="graphics/8_PCAnetY.png" alt="Visualization of the relationship between all the variables" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 159: Visualization of the relationship between all the variables<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math inline">\(\text{PC}_1\)</span> is significant. Since <span class="math inline">\(\text{PC}_1\)</span> is mainly defined by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, this is consistent with all the analysis done so far, and a structure of the relationships between the variables is revealed in Figure <a href="remarks-6.html#fig:f8-PCAnetY">159</a>.</p>
</div>
</div>
<p style="text-align: center;">
<a href="principal-component-analysis.html"><button class="btn btn-default">Previous</button></a>
<a href="exercises-6.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
