<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Support vector machine | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Support vector machine | Data Analytics">

<title>Support vector machine | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="support-vector-machine" class="section level2 unnumbered">
<h2>Support vector machine</h2>
<div id="rationale-and-formulation-10" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>A learning algorithm has an <em>objective function</em> and sometimes a set of <em>constraints</em>. The objective function corresponds to a quality of the learned model that could help it succeed on the unseen testing data. Eqs. <a href="regression-models.html#eq:2-multiLR-LS-matrix">(16)</a>, <a href="logistic-regression-model.html#eq:3-likelihood">(28)</a>, and <a href="remarks-4.html#eq:6-complete-loglike2">(40)</a>, are examples of objective functions. They are developed based on the <em>likelihood principle</em>. Besides the likelihood principle, researchers have been studying what else quality a model should have and what objective function we should optimize to enhance this quality of the model. The constraints, on the other hand, guard the bottom line: the learned model needs to at least perform well on the training data so it is possible to perform well on future unseen data<label for="tufte-sn-167" class="margin-toggle sidenote-number">167</label><input type="checkbox" id="tufte-sn-167" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">167</span> The testing data, while unseen, is assumed to be statistically the same as the training data. This is a basic assumption in machine learning.</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-models"></span>
<img src="graphics/7_models.png" alt="Which model (i.e., here, which line) should we use as our classification model to separate the two classes of data points?" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 114: Which model (i.e., here, which line) should we use as our classification model to separate the two classes of data points?<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="support-vector-machine.html#fig:f7-models">114</a> shows an example of a binary classification problem. The constraints here are obvious: the models should correctly classify the data points. And the <span class="math inline">\(3\)</span> models all perform well, while we hesitate to say that the <span class="math inline">\(3\)</span> models are equally good. Common sense tells us that Model <span class="math inline">\(3\)</span> is the least favorable. Unlike the other two, Model <span class="math inline">\(3\)</span> is close to a few data points. This makes Model <span class="math inline">\(3\)</span> bear a risk of misclassification on future unseen data: the locations of the existing data points provide a suggestion about where future unseen data may locate; but this is a suggestion, not a hard boundary.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-margins"></span>
<img src="graphics/7_margins.png" alt="The model that has a larger margin is better---the basic idea of SVM" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 115: The model that has a larger margin is better—the basic idea of SVM<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>In other words, the line of Model <span class="math inline">\(3\)</span> is too close to the data points and therefore lacks a safe <strong>margin</strong>. The concept of margin is shown in Figure <a href="support-vector-machine.html#fig:f7-margins">115</a>. To reduce risk, we should have the margin as large as possible. The other two models have larger margins, and Model <span class="math inline">\(2\)</span> is the best because it has the largest margin.</p>
<p>In summary, while all the models shown in Figure <a href="support-vector-machine.html#fig:f7-models">114</a> meet the <em>constraints</em> (i.e., perform well on the training data points), this is just the bottom line for a model to be good, and they are ranked differently based on an <em>objective function</em> that maximizes the margin of the model. This is the <strong>maximum margin</strong> principle invented in SVM.</p>
</div>
<div id="theory-and-method-7" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p><em>Derivation of the SVM formulation.</em></p>
<p>Consider a binary classification problem as shown in Figure <a href="support-vector-machine.html#fig:f7-margins">115</a>. At this moment, we consider situations that all data points could be correctly classified by a line, which is clearly the case in Figure <a href="support-vector-machine.html#fig:f7-models">114</a>. This is called <strong>the linearly separable case</strong> . Denote the data points as <span class="math inline">\(\left\{\left(x_{n}, y_{n}\right), n=1,2, \dots, N\right\}\)</span>. Here, the outcome variable <span class="math inline">\(y\)</span> is denoted as <span class="math inline">\(y_n \in \{1,-1\}\)</span>, i.e., <span class="math inline">\(y=1\)</span> denotes the circle points; <span class="math inline">\(y=-1\)</span> denotes the square points.</p>
<p>The mathematical model to represent a line is <span class="math inline">\(\boldsymbol{w}^{T} \boldsymbol{x}+b = 0\)</span>. Based on this form, we can segment the space into <span class="math inline">\(5\)</span> regions, as shown in Figure <a href="support-vector-machine.html#fig:f7-5regions">116</a>. And by looking at the value of <span class="math inline">\(\boldsymbol{w}^{T} \boldsymbol{x}+b\)</span>, we know which region the data point <span class="math inline">\(\boldsymbol{x}\)</span> falls into. In other words, Figure <a href="support-vector-machine.html#fig:f7-5regions">116</a> tells us a <em>classification rule</em></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-5regions"></span>
<img src="graphics/7_model_values.png" alt="The $5$ regions" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 116: The <span class="math inline">\(5\)</span> regions<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display" id="eq:7-DM-SVM">\[\begin{equation}
  \begin{aligned}
  \text { If } \boldsymbol{w}^{T} \boldsymbol{x}+b&gt;0, \text { then } y=1;  \\
  \text { Otherwise, } y=-1.
  \end{aligned}
\tag{57}
\end{equation}\]</span></p>
<p>Note that</p>
<p><span class="math display" id="eq:7-5regions">\[\begin{equation}
\begin{gathered}
    \text{For data points on the margin: } \left|\boldsymbol{w}^{T} \boldsymbol{x}+b\right|=1; \\
    \text {For data points beyond the margin: } \left|\boldsymbol{w}^{T} \boldsymbol{x}+b\right|&gt;1.
\end{gathered}
\tag{58}
\end{equation}\]</span></p>
<!-- % \begin{equation} -->
<!-- % \begin{aligned} -->
<!-- %  \text {Data points on the margin:} & \left|\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right|=1; \\ -->
<!-- %  \text {Data points beyond the margin:} & \left|\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right|>1. -->
<!-- %  (\#eq:7-SVMcons) -->
<!-- % \end{aligned} -->
<!-- % \end{equation} -->
<p>These two equations in Eq. <a href="support-vector-machine.html#eq:7-5regions">(58)</a> provide the <em>constraints</em> for the SVM formulation, i.e., the bottom line for a model to be a good model. The two equations can be succinctly rewritten as one</p>
<p><span class="math display">\[
y\left(\boldsymbol{w}^{T} \boldsymbol{x}+b\right) \geq 1.
\]</span></p>
<p>Thus, a draft version of the SVM formulation is</p>
<p><span class="math display" id="eq:7-SVM-draft">\[\begin{equation}
\begin{gathered}
    \text{*Objective function*: Maximize Margin}, \\
    \text { *Subject to*: } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right) \geq 1 \text { for } n=1,2, \ldots, N.
\end{gathered}
\tag{59}
\end{equation}\]</span></p>
<p>The <em>objective function</em> is to maximize the <em>margin</em> of the model. Note that a model is characterized by its parameters <span class="math inline">\(\boldsymbol{w}\)</span> and <span class="math inline">\(b\)</span>. And the goal of Eq. <a href="support-vector-machine.html#eq:7-SVM-draft">(59)</a> is to find the model—and therefore, the parameters—that maximizes the margin. In order to carry out this idea, we need the margin to be a concrete mathematical entity that <em>could be</em> characterized by the parameters <span class="math inline">\(\boldsymbol{w}\)</span> and <span class="math inline">\(b\)</span><label for="tufte-sn-168" class="margin-toggle sidenote-number">168</label><input type="checkbox" id="tufte-sn-168" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">168</span> Not all good ideas could be readily materialized in concrete mathematical forms. There is no guaranteed mathematical reality and if there is one it is always hard-earned.</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-margin-w"></span>
<img src="graphics/7_margin_w.png" alt="Illustration of the margin as a function of $\boldsymbol{w}$" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 117: Illustration of the margin as a function of <span class="math inline">\(\boldsymbol{w}\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We refer readers to the <strong>Remarks</strong> section to see details of how the margin is derived as a function of <span class="math inline">\(\boldsymbol{w}\)</span>. Figure <a href="support-vector-machine.html#fig:f7-margin-w">117</a> shows the result: the margin of the model is <span class="math inline">\(\frac{2}{\|\boldsymbol{w}\|}\)</span>. Here, <span class="math inline">\(\|\boldsymbol{w}\|^{2} = \boldsymbol{w}^{T} \boldsymbol{w}\)</span>. And note that to <em>maximize the margin of a model</em> is equivalent to <em>minimize <span class="math inline">\(\|\boldsymbol{w}\|\)</span></em>. This gives us the objective function of the SVM model<label for="tufte-sn-169" class="margin-toggle sidenote-number">169</label><input type="checkbox" id="tufte-sn-169" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">169</span> Note that here we use <span class="math inline">\(\|\boldsymbol{w}\|^{2}\)</span> instead of <span class="math inline">\(\|\boldsymbol{w}\|\)</span>. This formulation is easier to solve.</span></p>
<p><span class="math display" id="eq:7-SVMobj">\[\begin{equation}
  \text {Maximize Margin} = \min _{\boldsymbol{w}} \frac{1}{2}\|\boldsymbol{w}\|^{2}.
\tag{60}
\end{equation}\]</span></p>
<p>Thus, the final SVM formulation is</p>
<p><span class="math display" id="eq:7-SVM">\[\begin{equation}
\begin{gathered}
    \min _{\boldsymbol{w}} \frac{1}{2}\|\boldsymbol{w}\|^{2}, \\
    \text { Subject to: } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right) \geq 1 \text { for } n=1,2, \ldots, N.
\end{gathered}
\tag{61}
\end{equation}\]</span></p>
<p><em>Optimization solution.</em></p>
<p>Eq. <a href="support-vector-machine.html#eq:7-SVM">(61)</a> is called the <strong>primal formulation</strong> of SVM. To solve it, it is often converted into its dual form, the <strong>dual formulation</strong> of SVM. This could be done by the method of <strong>Lagrange multiplier</strong> that introduces a dummy variable, <span class="math inline">\(\alpha_{n}\)</span>, for each constraint, i.e., <span class="math inline">\(y_{n}\left(\boldsymbol{w}^{T}\boldsymbol{x}_{n}+b\right)\geq 1\)</span>, such that we could move the constraints into the objective function. By definition, <span class="math inline">\(\alpha_{n} \geq 0\)</span>.</p>
<p><span class="math display">\[ 
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}-\sum_{n=1}^{N} \alpha_{n}\left[y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1\right].
\]</span></p>
<p>This could be rewritten as</p>
<p><span class="math display" id="eq:7-SVM-lag">\[\begin{equation}
    L(\boldsymbol{w}, b, \boldsymbol{\alpha}) = \underbrace{\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}}_{(1)} - \underbrace{\sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{w}^{T} \boldsymbol{x}_{n}}_{(2)}-\underbrace{b \sum_{n=1}^{N} \alpha_{n} y_{n}}_{(3)}+\underbrace{\sum_{n=1}^{N} \alpha_{n}}_{(4)}.
\tag{62}
\end{equation}\]</span></p>
<p>Then we use the First Derivative Test again: differentiating <span class="math inline">\(L(\boldsymbol{w}, b, \boldsymbol{\alpha})\)</span> with respect to <span class="math inline">\(\boldsymbol{w} \text { and } b\)</span>, and setting them to <span class="math inline">\(0\)</span> yields the following solutions</p>
<p><span class="math display" id="eq:7-SVM-w">\[\begin{equation}
    \boldsymbol{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{x}_{n};
\tag{63}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:7-SVM-alpha">\[\begin{equation}
    \sum_{n=1}^{N} \alpha_{n} y_{n}=0.
\tag{64}
\end{equation}\]</span></p>
<p>Using the conclusion in Eq. <a href="support-vector-machine.html#eq:7-SVM-w">(63)</a>, part (1) of Eq. <a href="support-vector-machine.html#eq:7-SVM-lag">(62)</a> could be rewritten as</p>
<p><span class="math display">\[ 
\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}=\frac{1}{2} \boldsymbol{w}^{T} \sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{x}_{n}=\frac{1}{2} \sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{w}^{T} \boldsymbol{x}_{n}.
\]</span></p>
<p>It has the same form as part (2) of Eq. <a href="support-vector-machine.html#eq:7-SVM-lag">(62)</a>. The two could be merged together into <span class="math inline">\(-\frac{1}{2} \sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{w}^{T} \boldsymbol{x}_{n}\)</span>. Note that<label for="tufte-sn-170" class="margin-toggle sidenote-number">170</label><input type="checkbox" id="tufte-sn-170" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">170</span> I.e., use the conclusion in Eq. <a href="support-vector-machine.html#eq:7-SVM-w">(63)</a> again.</span></p>
<p><span class="math display">\[ 
\frac{1}{2} \sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{w}^{T} \boldsymbol{x}_{n}=\frac{1}{2} \sum_{n=1}^{N} \alpha_{n} y_{n}\left(\sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{x}_{n}\right)^{T} \boldsymbol{x}_{n}=\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} \boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}.
\]</span></p>
<p>Part (3) of Eq. <a href="support-vector-machine.html#eq:7-SVM-lag">(62)</a>, according to the conclusion in Eq. <a href="support-vector-machine.html#eq:7-SVM-alpha">(64)</a>, is <span class="math inline">\(0\)</span>.</p>
<p>Based on these results, we can rewrite <span class="math inline">\(L(\boldsymbol{w}, b, \boldsymbol{\alpha})\)</span> as</p>
<p><span class="math display">\[ 
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} \boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}.
\]</span></p>
<p>This is the objective function of the dual formulation of Eq. <a href="support-vector-machine.html#eq:7-SVM">(61)</a>. The decision variables are the Lagrange multipliers, the <span class="math inline">\(\boldsymbol{\alpha}\)</span>. By definition the Lagrange multipliers should be non-negative, and we have the constraint of the Lagrange multipliers described in Eq. <a href="support-vector-machine.html#eq:7-SVM-alpha">(64)</a>. All together, the <strong>dual formulation</strong> of the SVM model is</p>
<p><span class="math display" id="eq:7-SVM-dual">\[\begin{equation}
\begin{gathered}
    \max _{\boldsymbol{\alpha}} \sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} \boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}, \\
    \text { Subject to: } \alpha_{n} \geq 0 \text { for } n=1,2, \dots, N \text {, and } \sum_{n=1}^{N} \alpha_{n} y_{n}=0.
\end{gathered}
\tag{65}
\end{equation}\]</span></p>
<p>This is a <em>quadratic programming</em> problem that can be solved using many existing well established algorithms.</p>
<p><em>Support vectors.</em></p>
<p>The data points that lay on the margins, as shown in Figure <a href="support-vector-machine.html#fig:f7-sv">118</a>, are called <strong>support vectors</strong> . These geometrically unique data points are also found to be numerically interesting: in the solution of the dual formulation of SVM as shown in Eq. <a href="support-vector-machine.html#eq:7-SVM-dual">(65)</a>, the <span class="math inline">\(\alpha_{n}\)</span>s that correspond to the support vectors are those that are nonzero. In other words, the data points that are not support vectors will have their <span class="math inline">\(\alpha_{n}\)</span>s to be zero in the solution of Eq. <a href="support-vector-machine.html#eq:7-SVM-dual">(65)</a>.<label for="tufte-sn-171" class="margin-toggle sidenote-number">171</label><input type="checkbox" id="tufte-sn-171" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">171</span> Note that each data point contributes a constraint in the primal formulation of SVM, and therefore, corresponds to a <span class="math inline">\(\alpha_{n}\)</span> in the dual formulation.</span></p>
<p>If we revisit Eq. <a href="support-vector-machine.html#eq:7-SVM-w">(63)</a>, we can see that only the nonzero <span class="math inline">\(\alpha_n\)</span> contribute to the estimation of <span class="math inline">\(\boldsymbol{w}\)</span>. Indeed, Figure <a href="support-vector-machine.html#fig:f7-sv">118</a> shows that support vectors are sufficient to geometrically define the margins. And if we know the margins, the decision boundary is determined, i.e., as the central line in the middle of the two margins.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-sv"></span>
<img src="graphics/7_sv.png" alt="Support vectors are the data points that lay on the margins. In other words, the support vectors define the margins." width="250px"  />
<!--
<p class="caption marginnote">-->Figure 118: Support vectors are the data points that lay on the margins. In other words, the support vectors define the margins.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The support vectors hold crucial implications for the learned model. Theoretical evidences showed that the number of support vectors is a metric that can indicate the “healthiness” of the model, i.e., the smaller the total number of support vectors, the better the model. It also reveals that the main statistical information of a given dataset the SVM model uses is the support vectors. The number of support vectors is usually much smaller than the number of data points <span class="math inline">\(N\)</span>. Some works have been inspired to accelerate the SVM model training by discarding the data points that are probably not support vectors<label for="tufte-sn-172" class="margin-toggle sidenote-number">172</label><input type="checkbox" id="tufte-sn-172" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">172</span> If we can screen the data points before we solve Eq. <a href="support-vector-machine.html#eq:7-SVM-dual">(65)</a> by discarding some data points that are not support vectors, the size of the optimization problem in Eq. <a href="support-vector-machine.html#eq:7-SVM-dual">(65)</a> could be reduced.</span>. To understand why the nonzero <span class="math inline">\(\alpha_n\)</span> correspond to the support vectors, interested readers can find the derivation in the <strong>Remarks</strong> section.</p>
<p><em>Summary.</em> After solving Eq. <a href="support-vector-machine.html#eq:7-SVM-dual">(65)</a>, we obtain the solutions of <span class="math inline">\(\boldsymbol{\alpha}\)</span>. With that, we estimate the parameter <span class="math inline">\(\boldsymbol{w}\)</span> based on Eq. <a href="support-vector-machine.html#eq:7-SVM-w">(63)</a>. To estimate the parameter <span class="math inline">\(b\)</span>, we use any <em>support vector</em>, i.e., say, <span class="math inline">\((\boldsymbol{x}_{n}, y_n)\)</span>, and estimate <span class="math inline">\(b\)</span> by</p>
<p><span class="math display">\[\begin{equation*}
    \text{If } y_n = 1, b=1-\boldsymbol{w}^{T} \boldsymbol{x}_{n};
\end{equation*}\]</span></p>
<p><span class="math display" id="eq:7-SVM-b">\[\begin{equation}
    \text{If } y_n = -1, b=-1-\boldsymbol{w}^{T} \boldsymbol{x}_{n}.\tag{66}
\end{equation}\]</span></p>
<p><em>Extension to nonseparable cases.</em></p>
<p>We have assumed that the two classes are separable. Since this is impossible in some applications, we revise the SVM formulation—specifically, to revise the constraints of the SVM formulation—by allowing some data points to be within the margins or even on the wrong side of the decision boundary.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-slackvar"></span>
<img src="graphics/7_slackvar.png" alt="Behaviors of the slack variables" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 119: Behaviors of the slack variables<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Note that the original constraint structure in Eq. <a href="support-vector-machine.html#eq:7-SVM">(61)</a> is derived based on the linearly separable case shown in Figure <a href="support-vector-machine.html#fig:f7-models">114</a>. For the nonseparable case, Figure <a href="support-vector-machine.html#fig:f7-slackvar">119</a> shows three scenarios: the <em>Type A</em> data points fall within the margins but still on the right side of their class, the <em>Type B</em> data points fall on the wrong side of their class, and the <em>Type C</em> data points fall on the right side of their class and also beyond or on the margin.</p>
<p>The <em>Type A</em> data points and the <em>Type B</em> data points are both <em>compromised</em>, and we introduce a <strong>slack variable</strong> to describe the <em>degree</em> of compromise for both types of data points.</p>
<p>For instance, consider the circle points that belong to the class (<span class="math inline">\(y_n=1\)</span>), we have<label for="tufte-sn-173" class="margin-toggle sidenote-number">173</label><input type="checkbox" id="tufte-sn-173" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">173</span> Readers may revisit Figure <a href="support-vector-machine.html#fig:f7-5regions">116</a> to understand Eq. <a href="support-vector-machine.html#eq:7-SVMcons-a">(67)</a>.</span></p>
<p><span class="math display" id="eq:7-SVMcons-a">\[\begin{equation}
  \begin{gathered}
  \text {Data points (Type A): } \boldsymbol{w}^{T} \boldsymbol{x}_{n}+b \in (0,1); \\
  \text {Data points (Type B): }
  \boldsymbol{w}^{T} \boldsymbol{x}_{n}+b &lt; 0.
  \end{gathered}
\tag{67}
\end{equation}\]</span></p>
<p>Then we define a slack variable <span class="math inline">\(\xi_{n}\)</span> for any data point <span class="math inline">\(n\)</span> of Types A or B</p>
<p><span class="math display">\[
    \text {The slack variable $\xi_{n}$}: \xi_{n} = 1 - \left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right).
\]</span></p>
<p>And we define <span class="math inline">\(\xi_{n}\)</span> for any data point of Type C to be <span class="math inline">\(0\)</span> since there is no compromise.</p>
<p>All together, as shown in Figure <a href="support-vector-machine.html#fig:f7-slackvar">119</a>, we have</p>
<p><span class="math display" id="eq:7-SVMcons">\[\begin{equation}
  \begin{gathered}
  \text {Data points (Type A): } \xi_{n} \in (0,1]; \\
  \text {Data points (Type B): } \xi_{n} &gt; 1; \\
  \text {Data points (Type C): } \xi_{n}=0.
  \end{gathered}
\tag{68}
\end{equation}\]</span></p>
<p>Similarly, for the square points that belong to the class (<span class="math inline">\(y= -1\)</span>), we define a slack variable <span class="math inline">\(\xi_{n}\)</span> for each data point <span class="math inline">\(n\)</span></p>
<p><span class="math display">\[
    \text {The slack variable $\xi_{n}$}: \xi_{n} = 1 + \left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right).
\]</span></p>
<p>The same result in Eq. <a href="support-vector-machine.html#eq:7-SVMcons">(68)</a> could be derived.</p>
<p>As the slack variable <span class="math inline">\(\xi_{n}\)</span> describes the <em>degree</em> of compromise for the data point <span class="math inline">\(\boldsymbol{x}_{n}\)</span>, an optimal SVM model should also minimize the total amount of compromise. Based on this additional learning principle, we revise the objective function in Eq. <a href="support-vector-machine.html#eq:7-SVM">(61)</a> and get</p>
<p><span class="math display" id="eq:7-SVMobj2">\[\begin{equation}
  \underbrace{\min _{\boldsymbol{w}} \frac{1}{2}\|\boldsymbol{w}\|^{2}}_{\text{*Maximize Margin*}} + \underbrace{C \sum_{n=1}^{N} \xi_{n}.}_{\text{*Minimize Slacks*}}
\tag{69}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(C\)</span> is a user-specified parameter to control the balance between the two objectives: <em>maximum margin</em> and <em>minimum sum of slacks</em>.</p>
<p>Then we revise the constraints<label for="tufte-sn-174" class="margin-toggle sidenote-number">174</label><input type="checkbox" id="tufte-sn-174" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">174</span> I.e., use the results in Figure <a href="support-vector-machine.html#fig:f7-5regions">116</a> and Figure <a href="support-vector-machine.html#fig:f7-slackvar">119</a>.</span> to be</p>
<p><span class="math display" id="eq:7-SVMcons2">\[ 
y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right) \geq 1-\xi_{n} \text {, for } n=1,2, \dots, N.
\tag{70}
\]</span></p>
<p>Putting the revised objective function and constraints together, the formulation of the SVM model for nonseparable case becomes</p>
<p><span class="math display" id="eq:7-SVM2">\[\begin{equation}
    \begin{gathered}
    \min _{\boldsymbol{w}} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{n=1}^{N} \xi_{n}, \\
    \text { Subject to: } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right) \geq 1-\xi_{n}, \\
    \xi_{n} \geq 0, \text { for } n=1,2, \ldots, N.
    \end{gathered}
\tag{71}  
\end{equation}\]</span></p>
<p>A dual form that is similar to Eq. <a href="support-vector-machine.html#eq:7-SVM-dual">(65)</a> could be derived, which is skipped here<label for="tufte-sn-175" class="margin-toggle sidenote-number">175</label><input type="checkbox" id="tufte-sn-175" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">175</span> Interested readers could read this book for a comprehensive and deep understanding of SVM: Scholkopf, B. and Smola, A.J., <em>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</em> MIT Press, 2001.</span>.</p>
<p><em>Extension to nonlinear SVM.</em></p>
<p>Sometimes, the decision boundary could not be characterized as linear models, i.e., see Figure <a href="support-vector-machine.html#fig:f7-7">120</a> (a).</p>
<p></p>
<div class="figure"><span id="fig:f7-7"></span>
<p class="caption marginnote shownote">
Figure 120: (a) A nonseparable dataset; (b) with the right transformation, (a) becomes linearly separable
</p>
<img src="graphics/7_7.png" alt="(a) A nonseparable dataset; (b) with the right transformation, (a) becomes linearly separable" width="100%"  />
</div>
<p></p>
<p>A common strategy to create a nonlinear model is to conduct <em>transformation</em> of the original variables. For Figure <a href="support-vector-machine.html#fig:f7-7">120</a> (a), we conduct a transformation from the original two-dimensional coordinate system <span class="math inline">\(\boldsymbol{x}\)</span> to a new coordinate system <span class="math inline">\(\boldsymbol{z}\)</span> that is three-dimensional</p>
<p><span class="math display" id="eq:7-SVM-xtoz">\[\begin{equation}
z_{1}=x_{1}^{2},
z_{2}=\sqrt{2} x_{1} x_{2},
z_{3}=x_{2}^{2}.
\tag{72}
\end{equation}\]</span></p>
<p>In the new coordinate system, as shown in Figure <a href="support-vector-machine.html#fig:f7-7">120</a> (b), the data points of the two classes become linearly separable.</p>
<!-- % ^[This is an approach we often use in linear regression models as well to capture nonlinearity in the data. It is needed to create *explicit* transformation that asks us to write up how the *transformed features* $\boldsymbol{z}$ could be represented by the original features $\boldsymbol{x}$.] -->
<p>The transformation employed in Eq. <a href="support-vector-machine.html#eq:7-SVM-xtoz">(72)</a> is <em>explicit</em>, which may not be suitable for applications where we don’t know what is a good transformation<label for="tufte-sn-176" class="margin-toggle sidenote-number">176</label><input type="checkbox" id="tufte-sn-176" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">176</span> Try a ten-dimensional <span class="math inline">\(\boldsymbol{x}\)</span> and see how troublesome it is to define an explicit transformation to enable linear separability of the classes.</span>. Thus, transformation that could be automatically identified by the learning algorithm is needed, even if the transformation is <em>implicit</em>. A remarkable thing about SVM is that its formulation allows automatic transformation.</p>
<p>Let’s revisit the dual formulation of SVM for the linearly separable case, as shown in Eq. <a href="support-vector-machine.html#eq:7-SVM-dual">(65)</a>. Assume that the transformation has been performed and now we build the SVM model based on the transformed features, <span class="math inline">\(\boldsymbol{z}\)</span>. The dual formulation of SVM on the transformed variables is</p>
<p><span class="math display" id="eq:7-SVM2-dual">\[\begin{equation}
\begin{gathered}
    \max _{\boldsymbol{\alpha}} \sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} \boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}, \\
    \text { Subject to: } \alpha_{n} \geq 0 \text { for } n=1,2, \dots, N, \\
    \sum_{n=1}^{N} \alpha_{n} y_{n}=0.
\end{gathered}
\tag{73}
\end{equation}\]</span></p>
<p>It can be seen that, the dual formulation of SVM doesn’t directly concern <span class="math inline">\(\boldsymbol{z}_{n}\)</span>. Rather, only the inner product of <span class="math inline">\(\boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}\)</span> is needed. As <span class="math inline">\(\boldsymbol{z}\)</span> is essentially a function of <span class="math inline">\(\boldsymbol{x}\)</span>, i.e., denote it as <span class="math inline">\(\boldsymbol{z}=\phi(\boldsymbol{x})\)</span>, <span class="math inline">\(\boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}\)</span> is essentially a function of <span class="math inline">\(\boldsymbol{x}_{n} \text { and } \boldsymbol{x}_{m}\)</span>. We can write it up as <span class="math inline">\(\boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}=K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)\)</span>. This is called the <strong>kernel function</strong> .</p>
<p>A kernel function is a function that entails a transformation <span class="math inline">\(\boldsymbol{z}=\phi(\boldsymbol{x})\)</span> such that <span class="math inline">\(K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)\)</span> is an inner product: <span class="math inline">\(K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=\phi(\boldsymbol{x}_{n})^{T} \phi(\boldsymbol{x}_{m})\)</span>. In other words, we now do not seek explicit form of <span class="math inline">\(\phi(\boldsymbol{x}_{n})\)</span>; rather, we seek kernel functions that entail such transformations<label for="tufte-sn-177" class="margin-toggle sidenote-number">177</label><input type="checkbox" id="tufte-sn-177" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">177</span> If a kernel function is proven to entail a transformation function <span class="math inline">\(\phi(\boldsymbol{x})\)</span>—even it is only proven <em>in theory</em> and never really made explicit in practice—it is as good as explicit transformation, because only the inner product of <span class="math inline">\(\boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}\)</span> is needed in Eq. <a href="support-vector-machine.html#eq:7-SVM2-dual">(73)</a>.</span>.</p>
<p>Many kernel functions have been developed. For example, the <strong>Gaussian radial basis kernel function</strong> is a popular choice</p>
<p><span class="math display">\[ 
K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=e^{-\gamma\left\|\boldsymbol{x}_{n}-\boldsymbol{x}_{m}\right\|^{2}},
\]</span></p>
<p>where the transformation <span class="math inline">\(\boldsymbol{z}=\phi(\boldsymbol{x})\)</span> is implicit and is proved to be infinitely long<label for="tufte-sn-178" class="margin-toggle sidenote-number">178</label><input type="checkbox" id="tufte-sn-178" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">178</span> Which means it is very flexible and can represent any smooth function.</span>.</p>
<p>The polynomial kernel function is defined as</p>
<p><span class="math display">\[
K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=\left(\boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}+1\right)^{q}.
\]</span></p>
<p>The linear kernel function<label for="tufte-sn-179" class="margin-toggle sidenote-number">179</label><input type="checkbox" id="tufte-sn-179" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">179</span> For linear kernel function, the transformation is trivial, i.e., <span class="math inline">\(\phi(\boldsymbol{x}) = \boldsymbol{x}\)</span>.</span> is defined as</p>
<p><span class="math display">\[ 
K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=\boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}.
\]</span></p>
<p>With a given kernel function, the dual formulation of SVM is</p>
<p><span class="math display" id="eq:7-SVM-dual2">\[\begin{equation}
\begin{gathered}
    \max _{\boldsymbol{\alpha}} \sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right), \\
    \text { Subject to: } \alpha_{n} \geq 0 \text { for } n=1,2, \dots, N, \\
    \sum_{n=1}^{N} \alpha_{n} y_{n}=0.
\end{gathered}
\tag{74}
\end{equation}\]</span></p>
<p>After solving Eq. <a href="support-vector-machine.html#eq:7-SVM-dual2">(74)</a>, <em>in theory</em> we could obtain the estimation of the parameter <span class="math inline">\(\boldsymbol{w}\)</span> based on Eq. <a href="support-vector-machine.html#eq:7-SVM-w2">(75)</a></p>
<p><span class="math display" id="eq:7-SVM-w2">\[\begin{equation}
    \boldsymbol{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \phi(\boldsymbol{x_{n}}).
\tag{75}
\end{equation}\]</span></p>
<p>However, for kernel functions that we don’t know the explicit transformation function <span class="math inline">\(\phi(\boldsymbol{x})\)</span>, it is no longer possible to write the parameter <span class="math inline">\(\boldsymbol{w}\)</span> in the same way as in linear SVM models. This won’t prevent us from using the learned SVM model for prediction. For a data point, denoted as <span class="math inline">\(\boldsymbol{x}_{*}\)</span>, we can use the learned SVM model to predict on it<label for="tufte-sn-180" class="margin-toggle sidenote-number">180</label><input type="checkbox" id="tufte-sn-180" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">180</span> I.e., combine Eq. <a href="support-vector-machine.html#eq:7-SVM-w2">(75)</a> and Eq. <a href="support-vector-machine.html#eq:7-DM-SVM">(57)</a> we could derive Eq. <a href="support-vector-machine.html#eq:7-DM-SVM-kernel">(76)</a>.</span></p>
<p><span class="math display" id="eq:7-DM-SVM-kernel">\[\begin{equation}
\begin{gathered}
    \text { If } \sum_{n=1}^{N} \alpha_{n} y_{n} K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{*}\right)+b&gt;0, \text { then } y_{*}=1; \\
\text { Otherwise, } y_{*}=-1.
\end{gathered}
\tag{76}
\end{equation}\]</span></p>
<p>Again, the specific form of <span class="math inline">\(\phi(\boldsymbol{x})\)</span> is not needed since only the kernel function is used.</p>
<p><em>A small-data example.</em></p>
<p>Consider a dataset with <span class="math inline">\(4\)</span> data points</p>
<p><span class="math display">\[ 
\begin{array}{l}{\boldsymbol{x}_{1}=(-1,-1)^{T}, y_{1}=-1}; \\ {\boldsymbol{x}_{2}=(-1,+1)^{T}, y_{2}=+1}; \\ {\boldsymbol{x}_{3}=(+1,-1)^{T}, y_{3}=+1} ;\\ {\boldsymbol{x}_{4}=(+1,+1)^{T}, y_{4}=-1.}\end{array}
\]</span></p>
<p>The dataset is visualized in Figure <a href="support-vector-machine.html#fig:f7-8">121</a>. The R code to draw Figure <a href="support-vector-machine.html#fig:f7-8">121</a> is shown below.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-8"></span>
<img src="graphics/7_8.png" alt="A linearly inseparable dataset" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 121: A linearly inseparable dataset<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="support-vector-machine.html#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For the toy problem</span></span>
<span id="cb144-2"><a href="support-vector-machine.html#cb144-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">4</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb144-3"><a href="support-vector-machine.html#cb144-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb144-4"><a href="support-vector-machine.html#cb144-4" aria-hidden="true" tabindex="-1"></a>linear.train <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x,y)</span>
<span id="cb144-5"><a href="support-vector-machine.html#cb144-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-6"><a href="support-vector-machine.html#cb144-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of data points of two classes</span></span>
<span id="cb144-7"><a href="support-vector-machine.html#cb144-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>( <span class="st">&#39;ggplot2&#39;</span> )</span>
<span id="cb144-8"><a href="support-vector-machine.html#cb144-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">qplot</span>( <span class="at">data=</span>linear.train, X1, X2, </span>
<span id="cb144-9"><a href="support-vector-machine.html#cb144-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">colour=</span><span class="fu">factor</span>(y),<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>), </span>
<span id="cb144-10"><a href="support-vector-machine.html#cb144-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>))</span>
<span id="cb144-11"><a href="support-vector-machine.html#cb144-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Scatterplot of data points of two classes&quot;</span>)</span>
<span id="cb144-12"><a href="support-vector-machine.html#cb144-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p>It is a <em>nonlinear</em> case. We use a nonlinear kernel function to build the SVM model.</p>
<p>Consider the polynomial kernel function with <code>df=2</code></p>
<p><span class="math display" id="eq:7-polykernel2">\[\begin{equation}
K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=\left(\boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}+1\right)^{2}, 
\tag{77}
\end{equation}\]</span></p>
<p>which corresponds to the transformation</p>
<p><span class="math display" id="eq:7-polykernel2-tran">\[\begin{equation}
\phi\left(\boldsymbol{x}_{n}\right)=\left[1, \sqrt{2} x_{n, 1}, \sqrt{2} x_{n, 2}, \sqrt{2} x_{n, 1} x_{n, 2}, x_{n, 1}^{2}, x_{n, 2}^{2}\right]^{T}.
\tag{78}
\end{equation}\]</span></p>
<p>Based on Eq. <a href="support-vector-machine.html#eq:7-SVM-dual">(65)</a>, a specific formulation of the SVM model of this dataset is</p>
<p><span class="math display" id="eq:7-SVM-4points">\[\begin{equation}
    \begin{gathered}
    \max _{\boldsymbol{\alpha}} \sum_{n=1}^{4} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{4} \sum_{m=1}^{4} \alpha_{n} \alpha_{m} y_{n} y_{m} K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right), \\
    \text { Subject to: } \alpha_{n} \geq 0 \text { for } n=1,2, \dots, 4, \\
    \text { and } \sum_{n=1}^{4} \alpha_{n} y_{n}=0.
    \end{gathered}
\tag{79}
\end{equation}\]</span></p>
<p>We calculate the kernel matrix as<label for="tufte-sn-181" class="margin-toggle sidenote-number">181</label><input type="checkbox" id="tufte-sn-181" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">181</span> E.g., using Eq. <a href="support-vector-machine.html#eq:7-polykernel2">(77)</a>, <span class="math inline">\(K\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right) = \left(\boldsymbol{x}_{1}^{T} \boldsymbol{x}_{2}+1\right)^{2} = 3^2 = 9\)</span>. Readers can try other instances.</span></p>
<p><span class="math display">\[
\boldsymbol{K}=\left[\begin{array}{cccc}{9} &amp; {1} &amp; {1} &amp; {1} \\ {1} &amp; {9} &amp; {1} &amp; {1} \\ {1} &amp; {1} &amp; {9} &amp; {1} \\ {1} &amp; {1} &amp; {1} &amp; {9}\end{array}\right].
\]</span></p>
<p>We solve the quadratic programming problem<label for="tufte-sn-182" class="margin-toggle sidenote-number">182</label><input type="checkbox" id="tufte-sn-182" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">182</span> I.e., use the R package <code>quadprog</code>.</span> in Eq. <a href="support-vector-machine.html#eq:7-SVM-4points">(79)</a> and get</p>
<p><span class="math display" id="eq:7-alpha">\[\begin{equation}
\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0.125.
\tag{80}
\end{equation}\]</span></p>
<p>In this particular case, since we can write up the transformation explicitly<label for="tufte-sn-183" class="margin-toggle sidenote-number">183</label><input type="checkbox" id="tufte-sn-183" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">183</span> I.e., as shown in Eq. <a href="support-vector-machine.html#eq:7-polykernel2-tran">(78)</a></span>, we can write up <span class="math inline">\(\boldsymbol{w}\)</span> explicitly as well<label for="tufte-sn-184" class="margin-toggle sidenote-number">184</label><input type="checkbox" id="tufte-sn-184" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">184</span> It should be written as <span class="math inline">\(\widehat{\boldsymbol{w}}\)</span>, since it is an estimator of <span class="math inline">\(\boldsymbol{w}\)</span>. Here for simplicity we skip this.</span></p>
<p><span class="math display">\[
\boldsymbol{w}=\sum_{n=1}^{4} \alpha_{n} y_{n} \phi\left(\boldsymbol{x}_{n}\right)=[0,0,0,1 / \sqrt{2}, 0,0]^{T}.
\]</span></p>
<p>For any given data point <span class="math inline">\(\boldsymbol{x}_{*}\)</span>, the explicit decision function is</p>
<p><span class="math display">\[
f\left(\boldsymbol{x}_{*}\right)=\boldsymbol{w}^{T} \phi\left(\boldsymbol{x}_{*}\right)=x_{*, 1} x_{*, 2}.
\]</span></p>
<p>This is the decision boundary for a typical <strong>XOR</strong> problem<label for="tufte-sn-185" class="margin-toggle sidenote-number">185</label><input type="checkbox" id="tufte-sn-185" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">185</span> Also known as <em>exclusive or</em> or <em>exclusive disjunction</em>, the XOR problem is a logical operation that outputs <em>true</em> only when inputs differ (e.g., one is <em>true</em>, the other is <em>false</em>).</span>.</p>
<p>We then use R to build an SVM model on this dataset<label for="tufte-sn-186" class="margin-toggle sidenote-number">186</label><input type="checkbox" id="tufte-sn-186" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">186</span> We use the R package <code>kernlab</code>—more details are shown in the section <strong>R Lab</strong>.</span>. The R code is shown in below.</p>
<p></p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="support-vector-machine.html#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a nonlinear SVM model </span></span>
<span id="cb145-2"><a href="support-vector-machine.html#cb145-2" aria-hidden="true" tabindex="-1"></a><span class="co"># polynomial kernel function with `df=2`</span></span>
<span id="cb145-3"><a href="support-vector-machine.html#cb145-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">poly</span>(x, <span class="at">degree =</span> <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>))</span>
<span id="cb145-4"><a href="support-vector-machine.html#cb145-4" aria-hidden="true" tabindex="-1"></a>coefs <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="fu">sqrt</span>(<span class="dv">2</span>),<span class="dv">1</span>,<span class="fu">sqrt</span>(<span class="dv">2</span>),<span class="fu">sqrt</span>(<span class="dv">2</span>),<span class="dv">1</span>)</span>
<span id="cb145-5"><a href="support-vector-machine.html#cb145-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> x <span class="sc">*</span> <span class="fu">t</span>(<span class="fu">matrix</span>(<span class="fu">rep</span>(coefs,<span class="dv">4</span>),<span class="at">nrow=</span><span class="dv">6</span>,<span class="at">ncol=</span><span class="dv">4</span>))</span>
<span id="cb145-6"><a href="support-vector-machine.html#cb145-6" aria-hidden="true" tabindex="-1"></a>linear.train <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x,y)</span>
<span id="cb145-7"><a href="support-vector-machine.html#cb145-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>( <span class="st">&#39;kernlab&#39;</span> )</span>
<span id="cb145-8"><a href="support-vector-machine.html#cb145-8" aria-hidden="true" tabindex="-1"></a>linear.svm <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(y <span class="sc">~</span> ., <span class="at">data=</span>linear.train, </span>
<span id="cb145-9"><a href="support-vector-machine.html#cb145-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">type=</span><span class="st">&#39;C-svc&#39;</span>, <span class="at">kernel=</span><span class="st">&#39;vanilladot&#39;</span>, <span class="at">C=</span><span class="dv">10</span>, <span class="at">scale=</span><span class="fu">c</span>())</span></code></pre></div>
<p></p>
<p>The function <code>alpha()</code> returns the values of <span class="math inline">\(\alpha_{n} \text { for } n=1,2, \dots, 4\)</span>. Our results as shown in Eq. <a href="support-vector-machine.html#eq:7-alpha">(80)</a> are consistent with the results obtained by using R.<label for="tufte-sn-187" class="margin-toggle sidenote-number">187</label><input type="checkbox" id="tufte-sn-187" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">187</span> If your answer is different, check if the <code>alpha()</code> function in the <code>kernlab</code>() package scales the vector <span class="math inline">\(\alpha\)</span>, i.e., to make the sum as <span class="math inline">\(1\)</span>.</span></p>
<p></p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="support-vector-machine.html#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">alpha</span>(linear.svm) <span class="co">#scaled alpha vector</span></span>
<span id="cb146-2"><a href="support-vector-machine.html#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [[1]]</span></span>
<span id="cb146-3"><a href="support-vector-machine.html#cb146-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.125 0.125 0.125 0.125</span></span></code></pre></div>
<p></p>
<div style="page-break-after: always;"></div>
</div>
<div id="r-lab-9" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 7-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get data into R and make appropriate preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="support-vector-machine.html#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb147-2"><a href="support-vector-machine.html#cb147-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-3"><a href="support-vector-machine.html#cb147-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb147-4"><a href="support-vector-machine.html#cb147-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb147-5"><a href="support-vector-machine.html#cb147-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb147-6"><a href="support-vector-machine.html#cb147-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb147-7"><a href="support-vector-machine.html#cb147-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-8"><a href="support-vector-machine.html#cb147-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb147-9"><a href="support-vector-machine.html#cb147-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb147-10"><a href="support-vector-machine.html#cb147-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb147-11"><a href="support-vector-machine.html#cb147-11" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl</span>
<span id="cb147-12"><a href="support-vector-machine.html#cb147-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-13"><a href="support-vector-machine.html#cb147-13" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, Y) </span>
<span id="cb147-14"><a href="support-vector-machine.html#cb147-14" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Y) </span>
<span id="cb147-15"><a href="support-vector-machine.html#cb147-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-16"><a href="support-vector-machine.html#cb147-16" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb147-17"><a href="support-vector-machine.html#cb147-17" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb147-18"><a href="support-vector-machine.html#cb147-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-19"><a href="support-vector-machine.html#cb147-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb147-20"><a href="support-vector-machine.html#cb147-20" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb147-21"><a href="support-vector-machine.html#cb147-21" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb147-22"><a href="support-vector-machine.html#cb147-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb147-23"><a href="support-vector-machine.html#cb147-23" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> puts together a list of candidate models.</p>
<p></p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="support-vector-machine.html#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb148-2"><a href="support-vector-machine.html#cb148-2" aria-hidden="true" tabindex="-1"></a><span class="co"># SVM: often to compare models with different kernels, </span></span>
<span id="cb148-3"><a href="support-vector-machine.html#cb148-3" aria-hidden="true" tabindex="-1"></a><span class="co"># different values of C, different set of variables</span></span>
<span id="cb148-4"><a href="support-vector-machine.html#cb148-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-5"><a href="support-vector-machine.html#cb148-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use different set of variables</span></span>
<span id="cb148-6"><a href="support-vector-machine.html#cb148-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-7"><a href="support-vector-machine.html#cb148-7" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(DX_bl <span class="sc">~</span> .)</span>
<span id="cb148-8"><a href="support-vector-machine.html#cb148-8" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(DX_bl <span class="sc">~</span> AGE <span class="sc">+</span> PTEDUCAT <span class="sc">+</span> FDG </span>
<span id="cb148-9"><a href="support-vector-machine.html#cb148-9" aria-hidden="true" tabindex="-1"></a>                     <span class="sc">+</span> AV45 <span class="sc">+</span> HippoNV <span class="sc">+</span> rs3865444)</span>
<span id="cb148-10"><a href="support-vector-machine.html#cb148-10" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(DX_bl <span class="sc">~</span> AGE <span class="sc">+</span> PTEDUCAT)</span>
<span id="cb148-11"><a href="support-vector-machine.html#cb148-11" aria-hidden="true" tabindex="-1"></a>model4 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(DX_bl <span class="sc">~</span> FDG <span class="sc">+</span> AV45 <span class="sc">+</span> HippoNV)</span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses <span class="math inline">\(10\)</span>-fold cross-validation to evaluate the performance of the candidate models. Below we show how it works for one model. For other models, the same script could be used with a slight modification.</p>
<p></p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="support-vector-machine.html#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Use 10-fold cross-validation to evaluate the models</span></span>
<span id="cb149-2"><a href="support-vector-machine.html#cb149-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-3"><a href="support-vector-machine.html#cb149-3" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">=</span> <span class="dv">10</span> </span>
<span id="cb149-4"><a href="support-vector-machine.html#cb149-4" aria-hidden="true" tabindex="-1"></a><span class="co"># number of fold </span></span>
<span id="cb149-5"><a href="support-vector-machine.html#cb149-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(data.train)[<span class="dv">1</span>] </span>
<span id="cb149-6"><a href="support-vector-machine.html#cb149-6" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N))  </span>
<span id="cb149-7"><a href="support-vector-machine.html#cb149-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-8"><a href="support-vector-machine.html#cb149-8" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the first model</span></span>
<span id="cb149-9"><a href="support-vector-machine.html#cb149-9" aria-hidden="true" tabindex="-1"></a>cv_err <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb149-10"><a href="support-vector-machine.html#cb149-10" aria-hidden="true" tabindex="-1"></a><span class="co"># cv_err makes records of the prediction error for each fold</span></span>
<span id="cb149-11"><a href="support-vector-machine.html#cb149-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb149-12"><a href="support-vector-machine.html#cb149-12" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb149-13"><a href="support-vector-machine.html#cb149-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># In each iteration, use one fold of data as the testing data</span></span>
<span id="cb149-14"><a href="support-vector-machine.html#cb149-14" aria-hidden="true" tabindex="-1"></a>  data.test.cv <span class="ot">&lt;-</span> data.train[test_i, ] </span>
<span id="cb149-15"><a href="support-vector-machine.html#cb149-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The remaining 9 folds&#39; data form our training data</span></span>
<span id="cb149-16"><a href="support-vector-machine.html#cb149-16" aria-hidden="true" tabindex="-1"></a>  data.train.cv <span class="ot">&lt;-</span> data.train[<span class="sc">-</span>test_i, ]   </span>
<span id="cb149-17"><a href="support-vector-machine.html#cb149-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">require</span>( <span class="st">&#39;kernlab&#39;</span> )</span>
<span id="cb149-18"><a href="support-vector-machine.html#cb149-18" aria-hidden="true" tabindex="-1"></a>  linear.svm <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(model1, <span class="at">data=</span>data.train.cv, </span>
<span id="cb149-19"><a href="support-vector-machine.html#cb149-19" aria-hidden="true" tabindex="-1"></a>                     <span class="at">type=</span><span class="st">&#39;C-svc&#39;</span>, <span class="at">kernel=</span><span class="st">&#39;vanilladot&#39;</span>, <span class="at">C=</span><span class="dv">10</span>) </span>
<span id="cb149-20"><a href="support-vector-machine.html#cb149-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the linear SVM model with the training data</span></span>
<span id="cb149-21"><a href="support-vector-machine.html#cb149-21" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear.svm, data.test.cv)  </span>
<span id="cb149-22"><a href="support-vector-machine.html#cb149-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb149-23"><a href="support-vector-machine.html#cb149-23" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test.cv<span class="sc">$</span>DX_bl  </span>
<span id="cb149-24"><a href="support-vector-machine.html#cb149-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get the the error rate</span></span>
<span id="cb149-25"><a href="support-vector-machine.html#cb149-25" aria-hidden="true" tabindex="-1"></a>  cv_err[k] <span class="ot">&lt;-</span><span class="fu">length</span>(<span class="fu">which</span>(y_hat <span class="sc">!=</span> true_y))<span class="sc">/</span><span class="fu">length</span>(y_hat) </span>
<span id="cb149-26"><a href="support-vector-machine.html#cb149-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb149-27"><a href="support-vector-machine.html#cb149-27" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_err)</span>
<span id="cb149-28"><a href="support-vector-machine.html#cb149-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-29"><a href="support-vector-machine.html#cb149-29" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the second model ...</span></span>
<span id="cb149-30"><a href="support-vector-machine.html#cb149-30" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the third model ...</span></span>
<span id="cb149-31"><a href="support-vector-machine.html#cb149-31" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span></code></pre></div>
<p></p>
<p>Results are shown below.</p>
<p></p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="support-vector-machine.html#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1781538</span></span>
<span id="cb150-2"><a href="support-vector-machine.html#cb150-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1278462</span></span>
<span id="cb150-3"><a href="support-vector-machine.html#cb150-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.4069231</span></span>
<span id="cb150-4"><a href="support-vector-machine.html#cb150-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1316923</span></span></code></pre></div>
<p></p>
<p>The second model is the best.</p>
<p><strong>Step 5</strong> uses the training data to fit a final model, through the <code>ksvm()</code> function in the package <code>kernlab</code>.</p>
<p></p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="support-vector-machine.html#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, </span></span>
<span id="cb151-2"><a href="support-vector-machine.html#cb151-2" aria-hidden="true" tabindex="-1"></a><span class="co"># use ksvm() function to build your final model</span></span>
<span id="cb151-3"><a href="support-vector-machine.html#cb151-3" aria-hidden="true" tabindex="-1"></a>linear.svm <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(model2, <span class="at">data=</span>data.train,</span>
<span id="cb151-4"><a href="support-vector-machine.html#cb151-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">type=</span><span class="st">&#39;C-svc&#39;</span>, <span class="at">kernel=</span><span class="st">&#39;vanilladot&#39;</span>, <span class="at">C=</span><span class="dv">10</span>) </span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the fitted final model for prediction on the testing data.</p>
<p></p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="support-vector-machine.html#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict using your SVM model</span></span>
<span id="cb152-2"><a href="support-vector-machine.html#cb152-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear.svm, data.test) </span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-BS-ROC"></span>
<img src="graphics/7_BS_ROC.png" alt="The ROC curve of the final SVM model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 122: The ROC curve of the final SVM model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 7</strong> evaluates the performance of the model.</p>
<p></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="support-vector-machine.html#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7 -&gt; Evaluate the prediction performance of the SVM model</span></span>
<span id="cb153-2"><a href="support-vector-machine.html#cb153-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-3"><a href="support-vector-machine.html#cb153-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (1) The confusion matrix</span></span>
<span id="cb153-4"><a href="support-vector-machine.html#cb153-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-5"><a href="support-vector-machine.html#cb153-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret) </span>
<span id="cb153-6"><a href="support-vector-machine.html#cb153-6" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(y_hat, data.test<span class="sc">$</span>DX_bl)</span>
<span id="cb153-7"><a href="support-vector-machine.html#cb153-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-8"><a href="support-vector-machine.html#cb153-8" aria-hidden="true" tabindex="-1"></a><span class="co"># (2) ROC curve </span></span>
<span id="cb153-9"><a href="support-vector-machine.html#cb153-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC) </span>
<span id="cb153-10"><a href="support-vector-machine.html#cb153-10" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear.svm, data.test, <span class="at">type =</span> <span class="st">&#39;decision&#39;</span>) </span>
<span id="cb153-11"><a href="support-vector-machine.html#cb153-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">roc</span>(data.test<span class="sc">$</span>DX_bl, y_hat),</span>
<span id="cb153-12"><a href="support-vector-machine.html#cb153-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">main=</span><span class="st">&quot;ROC Curve&quot;</span>)</span></code></pre></div>
<p></p>
<p>Results are shown below. And the ROC curve is shown in Figure <a href="support-vector-machine.html#fig:f7-BS-ROC">122</a>.</p>
<p></p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="support-vector-machine.html#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb154-2"><a href="support-vector-machine.html#cb154-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb154-3"><a href="support-vector-machine.html#cb154-3" aria-hidden="true" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb154-4"><a href="support-vector-machine.html#cb154-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction  c0  c1</span></span>
<span id="cb154-5"><a href="support-vector-machine.html#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="do">##         c0 131  27</span></span>
<span id="cb154-6"><a href="support-vector-machine.html#cb154-6" aria-hidden="true" tabindex="-1"></a><span class="do">##         c1  11  90</span></span>
<span id="cb154-7"><a href="support-vector-machine.html#cb154-7" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-8"><a href="support-vector-machine.html#cb154-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                Accuracy : 0.8533         </span></span>
<span id="cb154-9"><a href="support-vector-machine.html#cb154-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                  95% CI : (0.8042, 0.894)</span></span>
<span id="cb154-10"><a href="support-vector-machine.html#cb154-10" aria-hidden="true" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5483         </span></span>
<span id="cb154-11"><a href="support-vector-machine.html#cb154-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : &lt; 2e-16        </span></span>
<span id="cb154-12"><a href="support-vector-machine.html#cb154-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-13"><a href="support-vector-machine.html#cb154-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Kappa : 0.7002         </span></span>
<span id="cb154-14"><a href="support-vector-machine.html#cb154-14" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-15"><a href="support-vector-machine.html#cb154-15" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.01496        </span></span>
<span id="cb154-16"><a href="support-vector-machine.html#cb154-16" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-17"><a href="support-vector-machine.html#cb154-17" aria-hidden="true" tabindex="-1"></a><span class="do">##             Sensitivity : 0.9225         </span></span>
<span id="cb154-18"><a href="support-vector-machine.html#cb154-18" aria-hidden="true" tabindex="-1"></a><span class="do">##             Specificity : 0.7692         </span></span>
<span id="cb154-19"><a href="support-vector-machine.html#cb154-19" aria-hidden="true" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8291         </span></span>
<span id="cb154-20"><a href="support-vector-machine.html#cb154-20" aria-hidden="true" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.8911         </span></span>
<span id="cb154-21"><a href="support-vector-machine.html#cb154-21" aria-hidden="true" tabindex="-1"></a><span class="do">##             Prevalence : 0.5483         </span></span>
<span id="cb154-22"><a href="support-vector-machine.html#cb154-22" aria-hidden="true" tabindex="-1"></a><span class="do">##          Detection Rate : 0.5058         </span></span>
<span id="cb154-23"><a href="support-vector-machine.html#cb154-23" aria-hidden="true" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.6100         </span></span>
<span id="cb154-24"><a href="support-vector-machine.html#cb154-24" aria-hidden="true" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8459         </span></span>
<span id="cb154-25"><a href="support-vector-machine.html#cb154-25" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-26"><a href="support-vector-machine.html#cb154-26" aria-hidden="true" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : c0</span></span></code></pre></div>
<p></p>
<p><em>Beyond the 7-Step R Pipeline.</em></p>
<p>In the 7-step pipeline, we create a list of candidate models by different selections of predictors. There are other parameters, such as the kernel function, the value of <span class="math inline">\(C\)</span>, that should be concerned in model selection. The R package <code>caret</code> can automate the process of cross-validation and facilitate the optimization of multiple parameters simultaneously. Below is an example</p>
<p></p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="support-vector-machine.html#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb155-2"><a href="support-vector-machine.html#cb155-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb155-3"><a href="support-vector-machine.html#cb155-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb155-4"><a href="support-vector-machine.html#cb155-4" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb155-5"><a href="support-vector-machine.html#cb155-5" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span>
<span id="cb155-6"><a href="support-vector-machine.html#cb155-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Train and Tune the SVM</span></span>
<span id="cb155-7"><a href="support-vector-machine.html#cb155-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(AD)[<span class="dv">1</span>]</span>
<span id="cb155-8"><a href="support-vector-machine.html#cb155-8" aria-hidden="true" tabindex="-1"></a>n.train <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fl">0.8</span> <span class="sc">*</span> n)</span>
<span id="cb155-9"><a href="support-vector-machine.html#cb155-9" aria-hidden="true" tabindex="-1"></a>idx.train <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n.train)</span>
<span id="cb155-10"><a href="support-vector-machine.html#cb155-10" aria-hidden="true" tabindex="-1"></a>AD[<span class="fu">which</span>(AD[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">0</span>),<span class="dv">1</span>] <span class="ot">=</span> <span class="fu">rep</span>(<span class="st">&quot;Normal&quot;</span>,<span class="fu">length</span>(<span class="fu">which</span>(AD[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">0</span>)))</span>
<span id="cb155-11"><a href="support-vector-machine.html#cb155-11" aria-hidden="true" tabindex="-1"></a>AD[<span class="fu">which</span>(AD[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">1</span>),<span class="dv">1</span>] <span class="ot">=</span> <span class="fu">rep</span>(<span class="st">&quot;Diseased&quot;</span>,<span class="fu">length</span>(<span class="fu">which</span>(AD[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">1</span>)))</span>
<span id="cb155-12"><a href="support-vector-machine.html#cb155-12" aria-hidden="true" tabindex="-1"></a>AD.train <span class="ot">&lt;-</span> AD[idx.train,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>)]</span>
<span id="cb155-13"><a href="support-vector-machine.html#cb155-13" aria-hidden="true" tabindex="-1"></a>AD.test <span class="ot">&lt;-</span> AD[<span class="sc">-</span>idx.train,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>)]</span>
<span id="cb155-14"><a href="support-vector-machine.html#cb155-14" aria-hidden="true" tabindex="-1"></a>trainX <span class="ot">&lt;-</span> AD.train[,<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>)]</span>
<span id="cb155-15"><a href="support-vector-machine.html#cb155-15" aria-hidden="true" tabindex="-1"></a>trainy<span class="ot">=</span> AD.train[,<span class="dv">1</span>]</span>
<span id="cb155-16"><a href="support-vector-machine.html#cb155-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-17"><a href="support-vector-machine.html#cb155-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup for cross-validation:</span></span>
<span id="cb155-18"><a href="support-vector-machine.html#cb155-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 10-fold cross validation</span></span>
<span id="cb155-19"><a href="support-vector-machine.html#cb155-19" aria-hidden="true" tabindex="-1"></a><span class="co"># do 5 repetitions of cv</span></span>
<span id="cb155-20"><a href="support-vector-machine.html#cb155-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Use AUC to pick the best model</span></span>
<span id="cb155-21"><a href="support-vector-machine.html#cb155-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-22"><a href="support-vector-machine.html#cb155-22" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb155-23"><a href="support-vector-machine.html#cb155-23" aria-hidden="true" tabindex="-1"></a>                     <span class="at">repeats=</span><span class="dv">1</span>,</span>
<span id="cb155-24"><a href="support-vector-machine.html#cb155-24" aria-hidden="true" tabindex="-1"></a>                     <span class="at">summaryFunction=</span>twoClassSummary,</span>
<span id="cb155-25"><a href="support-vector-machine.html#cb155-25" aria-hidden="true" tabindex="-1"></a>                     <span class="at">classProbs=</span><span class="cn">TRUE</span>)</span>
<span id="cb155-26"><a href="support-vector-machine.html#cb155-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-27"><a href="support-vector-machine.html#cb155-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the expand.grid to specify the search space   </span></span>
<span id="cb155-28"><a href="support-vector-machine.html#cb155-28" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">sigma =</span> <span class="fu">c</span>(<span class="fl">0.002</span>, <span class="fl">0.005</span>, <span class="fl">0.01</span>, <span class="fl">0.012</span>, <span class="fl">0.015</span>),</span>
<span id="cb155-29"><a href="support-vector-machine.html#cb155-29" aria-hidden="true" tabindex="-1"></a><span class="at">C =</span> <span class="fu">c</span>(<span class="fl">0.3</span>,<span class="fl">0.4</span>,<span class="fl">0.5</span>,<span class="fl">0.6</span>)</span>
<span id="cb155-30"><a href="support-vector-machine.html#cb155-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb155-31"><a href="support-vector-machine.html#cb155-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-32"><a href="support-vector-machine.html#cb155-32" aria-hidden="true" tabindex="-1"></a><span class="co"># method: Radial kernel </span></span>
<span id="cb155-33"><a href="support-vector-machine.html#cb155-33" aria-hidden="true" tabindex="-1"></a><span class="co"># tuneLength: 9 values of the cost function</span></span>
<span id="cb155-34"><a href="support-vector-machine.html#cb155-34" aria-hidden="true" tabindex="-1"></a><span class="co"># preProc: Center and scale data</span></span>
<span id="cb155-35"><a href="support-vector-machine.html#cb155-35" aria-hidden="true" tabindex="-1"></a>svm.tune <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">x =</span> trainX, <span class="at">y =</span> trainy, </span>
<span id="cb155-36"><a href="support-vector-machine.html#cb155-36" aria-hidden="true" tabindex="-1"></a>                  <span class="at">method =</span> <span class="st">&quot;svmRadial&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">9</span>,</span>
<span id="cb155-37"><a href="support-vector-machine.html#cb155-37" aria-hidden="true" tabindex="-1"></a>                  <span class="at">preProc =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>), <span class="at">metric=</span><span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb155-38"><a href="support-vector-machine.html#cb155-38" aria-hidden="true" tabindex="-1"></a>                  <span class="at">tuneGrid =</span> grid,</span>
<span id="cb155-39"><a href="support-vector-machine.html#cb155-39" aria-hidden="true" tabindex="-1"></a>                  <span class="at">trControl=</span>ctrl)</span>
<span id="cb155-40"><a href="support-vector-machine.html#cb155-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-41"><a href="support-vector-machine.html#cb155-41" aria-hidden="true" tabindex="-1"></a>svm.tune</span></code></pre></div>
<p></p>
<p>Then we can obtain the following results</p>
<p></p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="support-vector-machine.html#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Support Vector Machines with Radial Basis Function Kernel </span></span>
<span id="cb156-2"><a href="support-vector-machine.html#cb156-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb156-3"><a href="support-vector-machine.html#cb156-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 413 samples</span></span>
<span id="cb156-4"><a href="support-vector-machine.html#cb156-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  15 predictor</span></span>
<span id="cb156-5"><a href="support-vector-machine.html#cb156-5" aria-hidden="true" tabindex="-1"></a><span class="do">##   2 classes: &#39;Diseased&#39;, &#39;Normal&#39; </span></span>
<span id="cb156-6"><a href="support-vector-machine.html#cb156-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb156-7"><a href="support-vector-machine.html#cb156-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Pre-processing: centered (15), scaled (15) </span></span>
<span id="cb156-8"><a href="support-vector-machine.html#cb156-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold, repeated 1 times) </span></span>
<span id="cb156-9"><a href="support-vector-machine.html#cb156-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Summary of sample sizes: 371, 372, 372, 371, 372, 372, ... </span></span>
<span id="cb156-10"><a href="support-vector-machine.html#cb156-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb156-11"><a href="support-vector-machine.html#cb156-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb156-12"><a href="support-vector-machine.html#cb156-12" aria-hidden="true" tabindex="-1"></a><span class="do">##   sigma  C    ROC        Sens       Spec     </span></span>
<span id="cb156-13"><a href="support-vector-machine.html#cb156-13" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.002  0.3  0.8929523  0.9121053  0.5932900</span></span>
<span id="cb156-14"><a href="support-vector-machine.html#cb156-14" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.002  0.4  0.8927130  0.8757895  0.6619048</span></span>
<span id="cb156-15"><a href="support-vector-machine.html#cb156-15" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.002  0.5  0.8956402  0.8452632  0.7627706</span></span>
<span id="cb156-16"><a href="support-vector-machine.html#cb156-16" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.002  0.6  0.8953759  0.8192105  0.7991342</span></span>
<span id="cb156-17"><a href="support-vector-machine.html#cb156-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.005  0.3  0.8965129  0.8036842  0.8036797</span></span>
<span id="cb156-18"><a href="support-vector-machine.html#cb156-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.005  0.4  0.8996565  0.7989474  0.8357143</span></span>
<span id="cb156-19"><a href="support-vector-machine.html#cb156-19" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.005  0.5  0.9020830  0.7936842  0.8448052</span></span>
<span id="cb156-20"><a href="support-vector-machine.html#cb156-20" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.005  0.6  0.9032422  0.7836842  0.8450216</span></span>
<span id="cb156-21"><a href="support-vector-machine.html#cb156-21" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.010  0.3  0.9030514  0.7889474  0.8541126</span></span>
<span id="cb156-22"><a href="support-vector-machine.html#cb156-22" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.010  0.4  0.9058248  0.7886842  0.8495671</span></span>
<span id="cb156-23"><a href="support-vector-machine.html#cb156-23" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.010  0.5  0.9060999  0.8044737  0.8541126</span></span>
<span id="cb156-24"><a href="support-vector-machine.html#cb156-24" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.010  0.6  0.9077848  0.8094737  0.8450216</span></span>
<span id="cb156-25"><a href="support-vector-machine.html#cb156-25" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.012  0.3  0.9032308  0.7781579  0.8538961</span></span>
<span id="cb156-26"><a href="support-vector-machine.html#cb156-26" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.012  0.4  0.9049043  0.7989474  0.8538961</span></span>
<span id="cb156-27"><a href="support-vector-machine.html#cb156-27" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.012  0.5  0.9063505  0.8094737  0.8495671</span></span>
<span id="cb156-28"><a href="support-vector-machine.html#cb156-28" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.012  0.6  0.9104511  0.8042105  0.8586580</span></span>
<span id="cb156-29"><a href="support-vector-machine.html#cb156-29" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.015  0.3  0.9060412  0.7886842  0.8493506</span></span>
<span id="cb156-30"><a href="support-vector-machine.html#cb156-30" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.015  0.4  0.9068165  0.8094737  0.8495671</span></span>
<span id="cb156-31"><a href="support-vector-machine.html#cb156-31" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.015  0.5  0.9109051  0.8042105  0.8541126</span></span>
<span id="cb156-32"><a href="support-vector-machine.html#cb156-32" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.015  0.6  0.9118615  0.8042105  0.8632035</span></span>
<span id="cb156-33"><a href="support-vector-machine.html#cb156-33" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb156-34"><a href="support-vector-machine.html#cb156-34" aria-hidden="true" tabindex="-1"></a><span class="do">## ROC was used to select the optimal model using  the largest </span></span>
<span id="cb156-35"><a href="support-vector-machine.html#cb156-35" aria-hidden="true" tabindex="-1"></a><span class="do">## value. The final values used for the model were </span></span>
<span id="cb156-36"><a href="support-vector-machine.html#cb156-36" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma = 0.015 and C = 0.6.</span></span></code></pre></div>
<p></p>
</div>
</div>
<p style="text-align: center;">
<a href="overview-5.html"><button class="btn btn-default">Previous</button></a>
<a href="ensemble-learning.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
