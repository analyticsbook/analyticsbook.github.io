<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Kernel regression model | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Kernel regression model | Data Analytics">

<title>Kernel regression model | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="kernel-regression-model" class="section level2 unnumbered">
<h2>Kernel regression model</h2>
<div id="rationale-and-formulation-14" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>Simple models, similar to the linear regression model, are like <em>parents who tell white lies</em>. A model is simple, not only in the sense that it looks simple, but also because it builds on assumptions that simplify reality. Among all the simple models, the linear regression model is particularly good at disguising its simplicity—it seems so natural that we often forget that its simplicity is its assumption. Simple in its cosmology, not necessary in its terminology—that is what the phrase <em>simple model</em> means<label for="tufte-sn-229" class="margin-toggle sidenote-number">229</label><input type="checkbox" id="tufte-sn-229" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">229</span> In this sense, a model, regardless of how sophisticated its mathematical representation is, is a simple model if its assumptions simplify reality to such an extent that demands our leap of faith.</span>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-1"></span>
<p class="caption marginnote shownote">
Figure 161: (Left) A single outlier (a local pattern) could impact the regression model as a whole; (right) a <em>localized</em> regression model (i.e., the curvature adapts to the locality instead of following a prescribed global form such as a straight line)
</p>
<img src="graphics/9_1_left.png" alt="(Left) A single outlier (a local pattern) could impact the regression model as a whole; (right) a *localized* regression model (i.e., the curvature adapts to the locality instead of following a prescribed global form such as a straight line) " width="49%" height="49%"  /><img src="graphics/9_1_right.png" alt="(Left) A single outlier (a local pattern) could impact the regression model as a whole; (right) a *localized* regression model (i.e., the curvature adapts to the locality instead of following a prescribed global form such as a straight line) " width="49%" height="49%"  />
</div>
<p></p>
<p>“<em>Simple, but not simpler</em>” said Albert Einstein.</p>
<p>One such assumption the linear regression model has made, obviously, is <em>linearity</em>. It would be OK in practice, as Figure <a href="#fig:f2-2"><strong>??</strong></a> in <strong>Chapter 2</strong> assures us: it is not perfect but it is a good approximation.</p>
<p>Now let’s look at Figure <a href="kernel-regression-model.html#fig:f9-1">161</a> (left). The <em>true model</em>, represented by the black line, is truly a line. But the fitted model, the orange line, deviates from the black line. In other words, even if the linearity assumption is correct, the consequence is not what we hope for.</p>
<p>The troublemaker appears to be the outlier located on the upper right corner of the figure. As discernible data scientists, we should be aware that the <em>model</em> is general, while the <em>dataset</em> at hand is particular. It is OK to say the outlier is the troublemaker, but note that this outlier is accidental. The real troublemaker is what <em>enables</em> the possibility of outlier to be a troublemaker. The real troublemaker lies deeper.</p>
<p>A common theme of the methods in this book is to establish certainty in a world of uncertainty. The linearity assumption is an assumption, since real data rarely give you a perfect straight line. The way it deals with uncertainty is to use the least squares principle for model estimation. It aims to look for a line that could pierce through <em>all</em> the data points. This makes each data point have a <em>global</em> impact: mentally move any data point in Figure <a href="kernel-regression-model.html#fig:f9-1">161</a> (left) up and down and imagine how the fitted orange line would move up and down accordingly. In other words, as a data point in any location could change the line dramatically, the linear regression model, together with its least squares estimation method, has imposed an even stronger assumption than merely linearity: it assumes that knowledge learned from one location would be universally useful to all other locations. This implicit assumption<label for="tufte-sn-230" class="margin-toggle sidenote-number">230</label><input type="checkbox" id="tufte-sn-230" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">230</span> I.e., models that have made this assumption are often termed as <em>global models</em>.</span> could be irrational in some applications, where the data points collected in one location may only tell information about that local area, not easily generalizable to the whole space. Thus, when the global models fail, we need <em>local models</em><label for="tufte-sn-231" class="margin-toggle sidenote-number">231</label><input type="checkbox" id="tufte-sn-231" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">231</span> A <em>local model</em> more relies on the data points in a neighborhood to build up the part of the curve that comes through that particular neighborhood.</span> to fit the data, as shown in Figure <a href="kernel-regression-model.html#fig:f9-1">161</a> (right).</p>
</div>
<div id="theory-and-method-9" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>Suppose there are <span class="math inline">\(N\)</span> data points, denoted as, <span class="math inline">\(\left(x_{n}, y_{n}\right)\)</span> for <span class="math inline">\(n=1,2, \dots, N\)</span>. To predict on a point <span class="math inline">\(x^*\)</span>, a <em>local model</em> assumes the following structure</p>
<p><span class="math display" id="eq:9-kr">\[\begin{equation}
    y^* = \sum_{n=1}^{N} y_n w(x_n, x^*).
\tag{98}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(w(x_n,x^*)\)</span> is the <strong>weight function</strong> that characterizes the <em>similarity</em> between <span class="math inline">\(x^*\)</span> and the training data points, <span class="math inline">\(x_n\)</span>, for <span class="math inline">\(n=1,2,\dots,N\)</span>. The idea is to predict on a data point based on the data points that are nearby. Methods differ from each other in terms of how they define <span class="math inline">\(w(x_n,x^*)\)</span>.</p>
<p>Roughly speaking, there are two main methods. One is the <strong>K-nearest neighbor</strong> (<strong>KNN</strong>) smoother , and another is the <strong>kernel</strong> smoother .</p>
<p><em>The KNN smoother.</em> The KNN smoother defines <span class="math inline">\(w(x_n,x^*)\)</span> as</p>
<p><span class="math display">\[w\left(x_{n}, x^{*}\right)=\left\{\begin{array}{l}{\frac{1}{k}, \text { if } x_{n} \text { is one of the } k \text { nearest neighbors of } x^{*}}; \\ {0, \text { if } x_{n} \text { is NOT among the } k \text{ nearest neighbors of } x^{*}}.\end{array}\right.\]</span></p>
<p>Here, to define the <em>nearest neighbors</em> of a data point, a <em>distance function</em> is needed. Examples include the <em>Euclidean</em><label for="tufte-sn-232" class="margin-toggle sidenote-number">232</label><input type="checkbox" id="tufte-sn-232" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">232</span> E.g., <span class="math inline">\(d\left(\boldsymbol{x}_n, \boldsymbol{x}_m\right) = \sqrt {\sum _{i=1}^{p} \left( x_{ni}-x_{mi}\right)^2 }\)</span>.</span>, <em>Mahalanobis</em>, and <em>Cosine</em> distance functions. What distance function to use depends on the characteristics of the data. Model selection methods such as the cross-validation can be used to select the best distance function for a dataset.</p>
<p></p>
<div class="figure"><span id="fig:f9-knn"></span>
<p class="caption marginnote shownote">
Figure 162: Three KNN smoother models (<span class="math inline">\(k=1\)</span>, <span class="math inline">\(k=2\)</span>, and <span class="math inline">\(k=6\)</span>)
</p>
<img src="graphics/9_knn_illu.png" alt="Three KNN smoother models ($k=1$, $k=2$, and $k=6$)" width="100%"  />
</div>
<p></p>
<p>Consider a data example as shown in Table <a href="kernel-regression-model.html#tab:t9-knn">46</a>. A visualization of the data points is shown in Figure <a href="kernel-regression-model.html#fig:f9-knn">162</a>, i.e., the gray data points.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-knn">Table 46: </span>Example of a dataset with <span class="math inline">\(6\)</span> data points</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(9\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(10\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Let’s build <span class="math inline">\(3\)</span> KNN smoother models (e.g., <span class="math inline">\(k=1\)</span>, <span class="math inline">\(k=2\)</span>, and <span class="math inline">\(k=6\)</span>) and use the Euclidean distance function to identify the <em>nearest neighbors</em> of a data point. Results are presented in Tables <a href="kernel-regression-model.html#tab:t9-knnK1">47</a>, <a href="kernel-regression-model.html#tab:t9-knnK2">48</a>, and <a href="kernel-regression-model.html#tab:t9-knnK6">49</a>, respectively. Note that, in this dataset, as there are in total <span class="math inline">\(6\)</span> data points, the KNN model with <span class="math inline">\(k=6\)</span> is the same as the trivial model that uses the average of <span class="math inline">\(y\)</span> as predictions for all data points.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-knnK1">Table 47: </span>Predictions by a KNN smoother model with <span class="math inline">\(k=1\)</span> on some locations of <span class="math inline">\(x^*\)</span></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x^*\)</span></th>
<th align="left">KNN</th>
<th align="left"><span class="math inline">\(y^*\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0.4\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span></td>
<td align="left"><span class="math inline">\(y_1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1.6\)</span></td>
<td align="left"><span class="math inline">\(x_3\)</span></td>
<td align="left"><span class="math inline">\(y_3\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3.2\)</span></td>
<td align="left"><span class="math inline">\(x_4\)</span></td>
<td align="left"><span class="math inline">\(y_4\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4.8\)</span></td>
<td align="left"><span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(y_6\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-knnK2">Table 48: </span>Predictions by a KNN smoother model with <span class="math inline">\(k=2\)</span> on some locations of <span class="math inline">\(x^*\)</span></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x^*\)</span></th>
<th align="left">KNN</th>
<th align="left"><span class="math inline">\(y^*\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0.4\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span></td>
<td align="left"><span class="math inline">\((y_1 + y_2)/2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1.6\)</span></td>
<td align="left"><span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span></td>
<td align="left"><span class="math inline">\((y_2 + y_3)/2\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3.2\)</span></td>
<td align="left"><span class="math inline">\(x_4\)</span>, <span class="math inline">\(x_5\)</span></td>
<td align="left"><span class="math inline">\((y_4 + y_5)/2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4.8\)</span></td>
<td align="left"><span class="math inline">\(x_5\)</span>, <span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\((y_5 + y_6)/2\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-knnK6">Table 49: </span>Predictions by a KNN smoother model with <span class="math inline">\(k=6\)</span> on some locations of <span class="math inline">\(x^*\)</span></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x^*\)</span></th>
<th align="left">KNN</th>
<th align="left"><span class="math inline">\(y^*\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0.4\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(\sum_{n=1}^{6} y_n/6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1.6\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(\sum_{n=1}^{6} y_n/6\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3.2\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(\sum_{n=1}^{6} y_n/6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4.8\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(\sum_{n=1}^{6} y_n/6\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The <span class="math inline">\(3\)</span> KNN smoother models are also shown in Figure <a href="kernel-regression-model.html#fig:f9-knn">162</a>.</p>
<p>A distinct feature of the <strong>KNN smoother</strong> is the <em>discrete</em> manner to define the similarity between data points, which is, for any data point <span class="math inline">\(x^*\)</span>, the data point <span class="math inline">\(x_n\)</span> is either a neighbor or not. The KNN smoother only uses the <span class="math inline">\(k\)</span> nearest neighbors of <span class="math inline">\(x^*\)</span> to predict <span class="math inline">\(y^*\)</span>. This discrete manner of the KNN smoother results in the serrated curves shown in Figure <a href="kernel-regression-model.html#fig:f9-knn">162</a>. This is obviously artificial, pointing out a systematic <em>bias</em> imposed by the KNN smoother model.</p>
<p><em>The kernel smoother.</em> To remove this bias, the <strong>kernel smoother</strong> creates <em>continuity</em> in the similarity between data points. A kernel smoother defines <span class="math inline">\(w(x_n,x^* )\)</span> in the following manner</p>
<p><span class="math display">\[w\left(x_{n}, x^{*}\right)=\frac{K\left(x_{n}, x^{*}\right)}{\sum_{n=1}^{N} K\left(x_{n}, x^{*}\right)}.\]</span></p>
<p>Here, <span class="math inline">\(K\left(x_{n}, x^{*}\right)\)</span> is a <em>kernel function</em> as we have discussed in <strong>Chapter 7</strong>. There have been many kernel functions developed, for example, as shown in Table <a href="kernel-regression-model.html#tab:t9-1">50</a> .</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-1">Table 50: </span>Some kernel functions used in machine learning</span><!--</caption>--></p>
<table>
<colgroup>
<col width="9%" />
<col width="83%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Kernel Function</strong></th>
<th align="left"><strong>Mathematical Form</strong></th>
<th align="left"><strong>Parameters</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Line</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \boldsymbol{x}_i^T\boldsymbol{x}_j\)</span></td>
<td align="left"><em>null</em></td>
</tr>
<tr class="even">
<td align="left">Polynomial</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j)= \left(\boldsymbol{x}_i^T\boldsymbol{x}_j + 1\right)^q\)</span></td>
<td align="left"><span class="math inline">\(q\)</span></td>
</tr>
<tr class="odd">
<td align="left">Gaussian radial basis</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = e^{-\gamma\Vert \boldsymbol{x}_i - \boldsymbol{x}_j\Vert^2}\)</span></td>
<td align="left"><span class="math inline">\(\gamma \geq 0\)</span></td>
</tr>
<tr class="even">
<td align="left">Laplace radial basis</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = e^{-\gamma\Vert \boldsymbol{x}_i - \boldsymbol{x}_j\Vert}\)</span></td>
<td align="left"><span class="math inline">\(\gamma \geq 0\)</span></td>
</tr>
<tr class="odd">
<td align="left">Hyperbolic tangent</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = tanh(\boldsymbol{x}_i^T\boldsymbol{x}_j+b)\)</span></td>
<td align="left">b</td>
</tr>
<tr class="even">
<td align="left">Sigmoid</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = tanh(a\boldsymbol{x}_i^T\boldsymbol{x}_j+b)\)</span></td>
<td align="left">a,b</td>
</tr>
<tr class="odd">
<td align="left">Bessel function</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \frac{bessel_{v+1}^n(\sigma\Vert \boldsymbol{x}_i - \boldsymbol{x}_j \Vert)}{\left(\Vert \boldsymbol{x}_i -\boldsymbol{x}_j \Vert\right)^{-n(v+1)}}\)</span></td>
<td align="left"><span class="math inline">\(\sigma, n,v\)</span></td>
</tr>
<tr class="even">
<td align="left">ANOVA radial basis</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \left( \sum_{k=1}^n e^{-\sigma\left(x_i^k - x_j^k\right)}\right)^d\)</span></td>
<td align="left"><span class="math inline">\(\sigma, d\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Many kernel functions are smooth functions. To understand a kernel function, using R to draw it is a good approach. For example, the following R code draws a few instances of the <em>Gaussian radial basis</em> kernel function and shows them in Figure <a href="kernel-regression-model.html#fig:f9-gauss">163</a>. The curve illustrates how the similarity <em>smoothly</em> decreases when the distance between the two data points increases. And the <em>bandwidth</em> parameter <span class="math inline">\(\gamma\)</span> controls the rate of decrease, i.e., the smaller the <span class="math inline">\(\gamma\)</span>, the less sensitive the kernel function to the <em>Euclidean</em> distance of the data points (measured by <span class="math inline">\(\Vert \boldsymbol{x}_i - \boldsymbol{x}_j\Vert^2\)</span>).</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-gauss"></span>
<img src="graphics/9_gauss.png" alt="Three instances of the *Gaussian radial basis* kernel function ($\gamma=0.2$, $\gamma=0.5$, and $\gamma=1$)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 163: Three instances of the <em>Gaussian radial basis</em> kernel function (<span class="math inline">\(\gamma=0.2\)</span>, <span class="math inline">\(\gamma=0.5\)</span>, and <span class="math inline">\(\gamma=1\)</span>)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="kernel-regression-model.html#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use R to visualize a kernel function</span></span>
<span id="cb186-2"><a href="kernel-regression-model.html#cb186-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(latex2exp) <span class="co"># enable the use of latex in R graphics</span></span>
<span id="cb186-3"><a href="kernel-regression-model.html#cb186-3" aria-hidden="true" tabindex="-1"></a><span class="co"># write a function for the kernel function</span></span>
<span id="cb186-4"><a href="kernel-regression-model.html#cb186-4" aria-hidden="true" tabindex="-1"></a>gauss <span class="ot">&lt;-</span> <span class="cf">function</span>(x,gamma) <span class="fu">exp</span>(<span class="sc">-</span> gamma <span class="sc">*</span> x<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb186-5"><a href="kernel-regression-model.html#cb186-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">3</span>, <span class="at">to =</span> <span class="dv">3</span>, <span class="at">by =</span> <span class="fl">0.001</span>) </span>
<span id="cb186-6"><a href="kernel-regression-model.html#cb186-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">gauss</span>(x,<span class="fl">0.2</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">xlab =</span> <span class="fu">TeX</span>(<span class="st">&#39;$x_i  - x_j$&#39;</span>),</span>
<span id="cb186-7"><a href="kernel-regression-model.html#cb186-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Gaussian radial basis kernel&quot;</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb186-8"><a href="kernel-regression-model.html#cb186-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">gauss</span>(x,<span class="fl">0.5</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb186-9"><a href="kernel-regression-model.html#cb186-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">gauss</span>(x,<span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb186-10"><a href="kernel-regression-model.html#cb186-10" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>, </span>
<span id="cb186-11"><a href="kernel-regression-model.html#cb186-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="fu">TeX</span>(<span class="st">&#39;$</span><span class="sc">\\</span><span class="st">gamma = 0.2$&#39;</span>), <span class="fu">TeX</span>(<span class="st">&#39;$</span><span class="sc">\\</span><span class="st">gamma = 0.5$&#39;</span>),</span>
<span id="cb186-12"><a href="kernel-regression-model.html#cb186-12" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">TeX</span>(<span class="st">&#39;$</span><span class="sc">\\</span><span class="st">gamma = 1$&#39;</span>)), </span>
<span id="cb186-13"><a href="kernel-regression-model.html#cb186-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,</span>
<span id="cb186-14"><a href="kernel-regression-model.html#cb186-14" aria-hidden="true" tabindex="-1"></a>                                <span class="st">&quot;darkorange&quot;</span>,<span class="st">&quot;forestgreen&quot;</span>))</span></code></pre></div>
<p></p>
</div>
<div id="r-lab-13" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get the dataset into R and organize it in required format.</p>
<p></p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="kernel-regression-model.html#cb187-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb187-2"><a href="kernel-regression-model.html#cb187-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-3"><a href="kernel-regression-model.html#cb187-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb187-4"><a href="kernel-regression-model.html#cb187-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb187-5"><a href="kernel-regression-model.html#cb187-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/KR.csv&quot;</span>)</span>
<span id="cb187-6"><a href="kernel-regression-model.html#cb187-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb187-7"><a href="kernel-regression-model.html#cb187-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(data)</span></span>
<span id="cb187-8"><a href="kernel-regression-model.html#cb187-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-9"><a href="kernel-regression-model.html#cb187-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb187-10"><a href="kernel-regression-model.html#cb187-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb187-11"><a href="kernel-regression-model.html#cb187-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data<span class="sc">$</span>x</span>
<span id="cb187-12"><a href="kernel-regression-model.html#cb187-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>y</span>
<span id="cb187-13"><a href="kernel-regression-model.html#cb187-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-14"><a href="kernel-regression-model.html#cb187-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data </span></span>
<span id="cb187-15"><a href="kernel-regression-model.html#cb187-15" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data) <span class="sc">*</span> <span class="dv">4</span><span class="sc">/</span><span class="dv">5</span>) )</span>
<span id="cb187-16"><a href="kernel-regression-model.html#cb187-16" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb187-17"><a href="kernel-regression-model.html#cb187-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data </span></span>
<span id="cb187-18"><a href="kernel-regression-model.html#cb187-18" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates a list of models. For a kernel regression model, important decisions are made on the kernel function and its parameter(s). For example, here, we create two models with two kernel functions and their parameters:</p>
<p></p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="kernel-regression-model.html#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb188-2"><a href="kernel-regression-model.html#cb188-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-3"><a href="kernel-regression-model.html#cb188-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model1: ksmooth(x,y, kernel = &quot;normal&quot;, bandwidth=10)</span></span>
<span id="cb188-4"><a href="kernel-regression-model.html#cb188-4" aria-hidden="true" tabindex="-1"></a><span class="co"># model2: ksmooth(x,y, kernel = &quot;box&quot;, bandwidth=5)</span></span>
<span id="cb188-5"><a href="kernel-regression-model.html#cb188-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model3: ...</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses cross-validation to evaluate the candidate models to identify the best model.</p>
<p></p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="kernel-regression-model.html#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Use 5-fold cross-validation to evaluate the models</span></span>
<span id="cb189-2"><a href="kernel-regression-model.html#cb189-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-3"><a href="kernel-regression-model.html#cb189-3" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">=</span> <span class="dv">10</span> <span class="co"># number of fold </span></span>
<span id="cb189-4"><a href="kernel-regression-model.html#cb189-4" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(data.train)[<span class="dv">1</span>] </span>
<span id="cb189-5"><a href="kernel-regression-model.html#cb189-5" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb189-6"><a href="kernel-regression-model.html#cb189-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-7"><a href="kernel-regression-model.html#cb189-7" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate model1</span></span>
<span id="cb189-8"><a href="kernel-regression-model.html#cb189-8" aria-hidden="true" tabindex="-1"></a>cv_mse <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb189-9"><a href="kernel-regression-model.html#cb189-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb189-10"><a href="kernel-regression-model.html#cb189-10" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb189-11"><a href="kernel-regression-model.html#cb189-11" aria-hidden="true" tabindex="-1"></a>  data.train.cv <span class="ot">&lt;-</span> data.train[<span class="sc">-</span>test_i, ] </span>
<span id="cb189-12"><a href="kernel-regression-model.html#cb189-12" aria-hidden="true" tabindex="-1"></a>  data.test.cv <span class="ot">&lt;-</span> data.train[test_i, ]  </span>
<span id="cb189-13"><a href="kernel-regression-model.html#cb189-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">require</span>( <span class="st">&#39;kernlab&#39;</span> )</span>
<span id="cb189-14"><a href="kernel-regression-model.html#cb189-14" aria-hidden="true" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">ksmooth</span>(data.train.cv<span class="sc">$</span>x, data.train.cv<span class="sc">$</span>y, </span>
<span id="cb189-15"><a href="kernel-regression-model.html#cb189-15" aria-hidden="true" tabindex="-1"></a>                    <span class="at">kernel =</span> <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth =</span> <span class="dv">10</span>,</span>
<span id="cb189-16"><a href="kernel-regression-model.html#cb189-16" aria-hidden="true" tabindex="-1"></a>                    <span class="at">x.points=</span>data.test.cv[,<span class="dv">1</span>]) </span>
<span id="cb189-17"><a href="kernel-regression-model.html#cb189-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (1) Fit the kernel regression model with Gaussian kernel</span></span>
<span id="cb189-18"><a href="kernel-regression-model.html#cb189-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (argument: kernel = &quot;normal&quot;) and bandwidth = 0.5; (2) There is</span></span>
<span id="cb189-19"><a href="kernel-regression-model.html#cb189-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># no predict() for ksmooth. Use the argument</span></span>
<span id="cb189-20"><a href="kernel-regression-model.html#cb189-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># &quot;x.points=data.test.cv&quot; instead. </span></span>
<span id="cb189-21"><a href="kernel-regression-model.html#cb189-21" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> model1<span class="sc">$</span>y  </span>
<span id="cb189-22"><a href="kernel-regression-model.html#cb189-22" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test.cv<span class="sc">$</span>y  </span>
<span id="cb189-23"><a href="kernel-regression-model.html#cb189-23" aria-hidden="true" tabindex="-1"></a>  cv_mse[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb189-24"><a href="kernel-regression-model.html#cb189-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb189-25"><a href="kernel-regression-model.html#cb189-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-26"><a href="kernel-regression-model.html#cb189-26" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_mse)</span>
<span id="cb189-27"><a href="kernel-regression-model.html#cb189-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-28"><a href="kernel-regression-model.html#cb189-28" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate model2 using the same script above</span></span>
<span id="cb189-29"><a href="kernel-regression-model.html#cb189-29" aria-hidden="true" tabindex="-1"></a><span class="co"># ... </span></span></code></pre></div>
<p></p>
<p>The result is shown below</p>
<p></p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="kernel-regression-model.html#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.2605955  # Model1</span></span>
<span id="cb190-2"><a href="kernel-regression-model.html#cb190-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.2662046  # Model2</span></span></code></pre></div>
<p>
<strong>Step 5</strong> builds the final model.</p>
<p></p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="kernel-regression-model.html#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, use ksmooth() function to </span></span>
<span id="cb191-2"><a href="kernel-regression-model.html#cb191-2" aria-hidden="true" tabindex="-1"></a><span class="co"># build your final model</span></span>
<span id="cb191-3"><a href="kernel-regression-model.html#cb191-3" aria-hidden="true" tabindex="-1"></a>kr.final <span class="ot">&lt;-</span> <span class="fu">ksmooth</span>(data.train<span class="sc">$</span>x, data.train<span class="sc">$</span>y, <span class="at">kernel =</span> <span class="st">&quot;normal&quot;</span>,</span>
<span id="cb191-4"><a href="kernel-regression-model.html#cb191-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">bandwidth =</span> <span class="dv">10</span>, <span class="at">x.points=</span>data.test[,<span class="dv">1</span>]) <span class="co"># </span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the final model for prediction.</p>
<p></p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="kernel-regression-model.html#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Evaluate the prediction performance of your model</span></span>
<span id="cb192-2"><a href="kernel-regression-model.html#cb192-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> kr.final<span class="sc">$</span>y  </span>
<span id="cb192-3"><a href="kernel-regression-model.html#cb192-3" aria-hidden="true" tabindex="-1"></a>true_y <span class="ot">&lt;-</span> data.test<span class="sc">$</span>y   </span>
<span id="cb192-4"><a href="kernel-regression-model.html#cb192-4" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb192-5"><a href="kernel-regression-model.html#cb192-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(mse)</span></code></pre></div>
<p></p>
<p>This pipeline could be easily extended to KNN smoother model, i.e., using the <code>knn.reg</code> in the <code>FNN</code> package.</p>
<p><em>Simulation Experiment.</em> We have created a R script in <strong>Chapter 5</strong> to simulate data from nonlinear regression models. Here, we use the same R script as shown below.</p>
<p></p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="kernel-regression-model.html#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate one batch of data</span></span>
<span id="cb193-2"><a href="kernel-regression-model.html#cb193-2" aria-hidden="true" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb193-3"><a href="kernel-regression-model.html#cb193-3" aria-hidden="true" tabindex="-1"></a><span class="co"># coefficients of the true model</span></span>
<span id="cb193-4"><a href="kernel-regression-model.html#cb193-4" aria-hidden="true" tabindex="-1"></a>coef <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.68</span>,<span class="fl">0.82</span>,<span class="sc">-</span><span class="fl">0.417</span>,<span class="fl">0.32</span>,<span class="sc">-</span><span class="fl">0.68</span>) </span>
<span id="cb193-5"><a href="kernel-regression-model.html#cb193-5" aria-hidden="true" tabindex="-1"></a>v_noise <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb193-6"><a href="kernel-regression-model.html#cb193-6" aria-hidden="true" tabindex="-1"></a>n_df <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb193-7"><a href="kernel-regression-model.html#cb193-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n_df</span>
<span id="cb193-8"><a href="kernel-regression-model.html#cb193-8" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_train, coef, v_noise)</span></code></pre></div>
<p></p>
<p>The simulated data are shown in Figure <a href="kernel-regression-model.html#fig:f9-3">164</a> (i.e., the gray data points).</p>
<p></p>
<div class="figure"><span id="fig:f9-3"></span>
<p class="caption marginnote shownote">
Figure 164: Kernel regression models with different choices on the <em>bandwidth</em> parameter (<span class="math inline">\(\gamma\)</span>) of the Gaussian radial basis kernel function
</p>
<img src="graphics/9_3.png" alt="Kernel regression models with different choices on the *bandwidth* parameter ($\gamma$) of the Gaussian radial basis kernel function" width="100%"  />
</div>
<p></p>
<p>The following R code overlays the <em>true</em> model, i.e., as the black curve, in Figure <a href="kernel-regression-model.html#fig:f9-3">164</a>.</p>
<p></p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="kernel-regression-model.html#cb194-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the true model</span></span>
<span id="cb194-2"><a href="kernel-regression-model.html#cb194-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb194-3"><a href="kernel-regression-model.html#cb194-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, X <span class="sc">%*%</span> coef, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p></p>
<p>Let’s use the kernel regression model to fit the data. We use the <em>Gaussian radial basis</em> kernel function, with three different choices of the <em>bandwidth</em> parameter (<span class="math inline">\(\gamma\)</span>), i.e., (<span class="math inline">\(\gamma = 2\)</span>, <span class="math inline">\(\gamma = 5\)</span>, <span class="math inline">\(\gamma = 15\)</span>). Then we overlay the three fitted kernel regression models in Figure <a href="kernel-regression-model.html#fig:f9-3">164</a> using the following R code.</p>
<p></p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="kernel-regression-model.html#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">ksmooth</span>(x,y, <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth=</span><span class="dv">2</span>),<span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb195-2"><a href="kernel-regression-model.html#cb195-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb195-3"><a href="kernel-regression-model.html#cb195-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">ksmooth</span>(x,y, <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth=</span><span class="dv">5</span>),<span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb195-4"><a href="kernel-regression-model.html#cb195-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;dodgerblue4&quot;</span>)</span>
<span id="cb195-5"><a href="kernel-regression-model.html#cb195-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">ksmooth</span>(x,y, <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth=</span><span class="dv">15</span>),<span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb195-6"><a href="kernel-regression-model.html#cb195-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb195-7"><a href="kernel-regression-model.html#cb195-7" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>,</span>
<span id="cb195-8"><a href="kernel-regression-model.html#cb195-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>, <span class="st">&quot;Kernel Reg (bw = 2)&quot;</span>,</span>
<span id="cb195-9"><a href="kernel-regression-model.html#cb195-9" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Kernel Reg (bw = 5)&quot;</span>, <span class="st">&quot;Kernel Reg (bw = 15)&quot;</span>), </span>
<span id="cb195-10"><a href="kernel-regression-model.html#cb195-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">4</span>),</span>
<span id="cb195-11"><a href="kernel-regression-model.html#cb195-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;darkorange&quot;</span>,<span class="st">&quot;dodgerblue4&quot;</span>,<span class="st">&quot;forestgreen&quot;</span>), </span>
<span id="cb195-12"><a href="kernel-regression-model.html#cb195-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="dv">32</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p>As shown in Figure <a href="kernel-regression-model.html#fig:f9-3">164</a>, the <em>bandwidth</em> parameter determines how smooth are the fitted curves: the larger the bandwidth, the smoother the regression curve<label for="tufte-sn-233" class="margin-toggle sidenote-number">233</label><input type="checkbox" id="tufte-sn-233" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">233</span> Revisit Figure <a href="kernel-regression-model.html#fig:f9-gauss">163</a> and connect the observations made in both figures, i.e., which one in Figure <a href="kernel-regression-model.html#fig:f9-gauss">163</a> leads to the smoothest curve in Figure <a href="kernel-regression-model.html#fig:f9-3">164</a> and why?</span>.</p>
<p>Similarly, we can use the same simulation experiment to study the KNN smoother model. We build three KNN smoother models with <span class="math inline">\(k=3\)</span>, <span class="math inline">\(k=10\)</span>, and <span class="math inline">\(k=50\)</span>, respectively.</p>
<p></p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="kernel-regression-model.html#cb196-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;FNN&quot;)</span></span>
<span id="cb196-2"><a href="kernel-regression-model.html#cb196-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(FNN)</span>
<span id="cb196-3"><a href="kernel-regression-model.html#cb196-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Loading required package: FNN</span></span>
<span id="cb196-4"><a href="kernel-regression-model.html#cb196-4" aria-hidden="true" tabindex="-1"></a>xy.knn3<span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train =</span> x, <span class="at">y =</span> y, <span class="at">k=</span><span class="dv">3</span>)</span>
<span id="cb196-5"><a href="kernel-regression-model.html#cb196-5" aria-hidden="true" tabindex="-1"></a>xy.knn10<span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train =</span> x, <span class="at">y =</span> y, <span class="at">k=</span><span class="dv">10</span>)</span>
<span id="cb196-6"><a href="kernel-regression-model.html#cb196-6" aria-hidden="true" tabindex="-1"></a>xy.knn50<span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train =</span> x, <span class="at">y =</span> y, <span class="at">k=</span><span class="dv">50</span>)</span></code></pre></div>
<p></p>
<p>Similar to Figure <a href="kernel-regression-model.html#fig:f9-3">164</a>, we use the following R code to draw Figure <a href="kernel-regression-model.html#fig:f9-2">165</a> that contains the true model, the sampled data points, and the three fitted models.</p>
<p></p>
<div class="figure"><span id="fig:f9-2"></span>
<p class="caption marginnote shownote">
Figure 165: KNN regression models with different choices on the number of nearest neighbors
</p>
<img src="graphics/9_2.png" alt="KNN regression models with different choices on the number of nearest neighbors" width="100%"  />
</div>
<p></p>
<p></p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="kernel-regression-model.html#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data</span></span>
<span id="cb197-2"><a href="kernel-regression-model.html#cb197-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb197-3"><a href="kernel-regression-model.html#cb197-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, X <span class="sc">%*%</span> coef, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb197-4"><a href="kernel-regression-model.html#cb197-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, xy.knn3<span class="sc">$</span>pred, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb197-5"><a href="kernel-regression-model.html#cb197-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, xy.knn10<span class="sc">$</span>pred, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue4&quot;</span>)</span>
<span id="cb197-6"><a href="kernel-regression-model.html#cb197-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, xy.knn50<span class="sc">$</span>pred, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb197-7"><a href="kernel-regression-model.html#cb197-7" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb197-8"><a href="kernel-regression-model.html#cb197-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>, <span class="st">&quot;KNN (k = 3)&quot;</span>,</span>
<span id="cb197-9"><a href="kernel-regression-model.html#cb197-9" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;KNN (k = 10)&quot;</span>, <span class="st">&quot;KNN (k = 50)&quot;</span>), </span>
<span id="cb197-10"><a href="kernel-regression-model.html#cb197-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">4</span>),</span>
<span id="cb197-11"><a href="kernel-regression-model.html#cb197-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;dodgerblue4&quot;</span>,</span>
<span id="cb197-12"><a href="kernel-regression-model.html#cb197-12" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;forestgreen&quot;</span>), </span>
<span id="cb197-13"><a href="kernel-regression-model.html#cb197-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">text.width =</span> <span class="dv">32</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p>Comparing Figures <a href="kernel-regression-model.html#fig:f9-3">164</a> and <a href="kernel-regression-model.html#fig:f9-2">165</a>, it seems that the curve of the kernel regression model is generally <em>smoother</em> than the curve of a KNN model. This observation relates to the <em>discrete</em> manner the KNN model employs, while the kernel model uses smooth kernel functions that introduce smoothness and continuity into the definition of the neighbors of a data point (thus no hard thresholding is needed to classify whether or not a data point is a neighbor of another data point).</p>
<p>With a smaller <span class="math inline">\(k\)</span>, the fitted curve by the KNN smoother model is less smooth. This is because a KNN smoother model with a smaller <span class="math inline">\(k\)</span> predicts on a data point by relying on fewer data points in the training dataset, ignoring information provided by the other data points that are considered far away<label for="tufte-sn-234" class="margin-toggle sidenote-number">234</label><input type="checkbox" id="tufte-sn-234" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">234</span> What about a linear regression model? When it predicts on a given data point, does it use all the data points in the training data, or just a few local data points?</span>.</p>
<p>In terms of model complexity, the smaller the parameter <span class="math inline">\(k\)</span> in the KNN model, the larger the complexity of the model. Most beginners think of the opposite when they first encounter this question.</p>
</div>
</div>
<p style="text-align: center;">
<a href="overview-7.html"><button class="btn btn-default">Previous</button></a>
<a href="conditional-variance-regression-model.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
