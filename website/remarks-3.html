<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Remarks | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Remarks | Data Analytics">

<title>Remarks | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="remarks-3" class="section level2 unnumbered">
<h2>Remarks</h2>
<!-- % ***More about cross-validation***: Usually, there is a relationship between the performance of the model on training dataset and its performance on testing dataset, as shown in Figure \@ref(fig:f5-11). Note that this relationship is theoretical, but has very high relevance with real applications. In our experiments, as shown in Figures \@ref(fig:f5-6) and \@ref(fig:f5-7), we have seen this relationship. This relationship predicts that, while the performance on the training data will decrease if we increase the model complexity, at a certain point, the gain on performance by increasing model complexity will stop. Beyond this point, the performance would be worse. Thus, a model that has a good performance on the training data and a reasonable complexity is likely to be among the best models that will perform well on the testing data (unseen).  -->
<div id="the-law-of-learning-errors" class="section level3 unnumbered">
<h3>The “law” of learning errors</h3>
<p>We have seen the <em>R-squared</em> could be manipulated to become larger, i.e., by adding into the model with more variables even if these variables are not predictive. This <em>bug</em> is not a special trait of the linear regression model only. The <em>R-squared</em> by its definition is computed based on the training data, and therefore, is essentially a <em>training error</em> . For any model that offers a flexible degree of complexity (e.g., examples are shown in Table <a href="remarks-3.html#tab:t5-modelComplexity">19</a>), its <em>training error</em> could be decreased if we make the model more complex.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-modelComplexity">Table 19: </span>The complexity parameters of some models</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><strong>Model</strong></th>
<th align="left"><strong>Complexity parameter</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear regression</td>
<td align="left">Number of variables</td>
</tr>
<tr class="even">
<td align="left">Decision tree</td>
<td align="left">Depth of tree</td>
</tr>
<tr class="odd">
<td align="left">Random forest</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<div class="figure"><span id="fig:f5-traintest-tree"></span>
<p class="caption marginnote shownote">
Figure 89: A much more complex decision tree model than the one in Figure <a href="remarks-1.html#fig:f3-tree-boundary">45</a>; (left) the tree model perfectly fits the <em>training data</em>; (right) the tree performs poorly on the <em>testing data</em>
</p>
<img src="graphics/5_traintest_tree.png" alt="A much more complex decision tree model than the one in Figure \@ref(fig:f3-tree-boundary); (left) the tree model perfectly fits the *training data*; (right) the tree performs poorly on the *testing data*" width="100%"  />
</div>
<p></p>
<!-- \caption[][-15mm]{The complexity parameters of some models}
 -->
<p>For example, let’s revisit the decision tree model shown in Figure <a href="remarks-1.html#fig:f3-tree-boundary">45</a> in <strong>Chapter 3</strong>. A deeper tree segments the space into smaller rectangular regions, guided by the distribution of the <em>training data</em>, as shown in Figure <a href="remarks-3.html#fig:f5-traintest-tree">89</a>. The model achieves <span class="math inline">\(100\%\)</span> accuracy—but this is an illusion, since the training data contains noise that could not be predicted. These rectangular regions, particularly those smaller ones, are susceptible to the noise. When we apply this deeper tree model on a <em>testing data</em> that is sampled from the same distribution of the <em>training data</em><label for="tufte-sn-131" class="margin-toggle sidenote-number">131</label><input type="checkbox" id="tufte-sn-131" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">131</span> The overall <em>morphology</em> of the two datasets looks alike; the differences, however, are due to the noise that is unpredictable.</span>, the model performs poorly.</p>
<p>It is generally true that the more complex a model gets, the lower the error on the training dataset becomes, as shown in Figure <a href="remarks-3.html#fig:f5-law-errors">90</a> (left). This is the “law” of the <em>training error</em>, and training a model based on the training error could easily “spoil” the model. If there is a testing dataset, the error curve would look like <em>U-shaped</em>, as shown in Figure <a href="remarks-3.html#fig:f5-law-errors">90</a> (middle), and the curve’s dip point helps us identify the best model complexity. While on the other hand, if there is no testing dataset, we could use cross-validation to obtain error estimates. The error curve obtained by cross-validation on the training data, as shown in
Figure <a href="remarks-3.html#fig:f5-law-errors">90</a> (right), should provide a good approximation of the error curve of the testing data. The three figures in Figure <a href="remarks-3.html#fig:f5-law-errors">90</a>, from left to right, illustrate a big picture of the <em>laws</em> of the errors and why some techniques such as the cross-validation have central importance in data analytics.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f5-law-errors"></span>
<img src="graphics/5_law_errors.png" alt="The law of learning errors" width="100%"  />
<p class="caption marginnote shownote">
Figure 90: The law of learning errors
</p>
</div>
<p></p>
<p>There are other approaches that play similar roles as the cross-validation, i.e., to approximate the error curve on unseen testing data. Examples include the <strong>Akaike information criterion</strong> (<strong>AIC</strong>), the <strong>Bayesian information criterion</strong> (<strong>BIC</strong>) , and many other model selection criteria alike. Different from the cross-validation, they don’t resample the training data. Rather, they are analytic approaches that evaluate a model’s performance by offsetting the model’s training error with a complexity penalty, i.e., the more complex a model gets, the larger the penalty imposed. Skipping their mathematical details, Figure <a href="remarks-3.html#fig:f5-AIC">91</a> illustrates the basic idea of these approaches.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f5-AIC"></span>
<img src="graphics/5_AIC.png" alt="The basic idea of the AIC and BIC criteria" width="100%"  />
<p class="caption marginnote shownote">
Figure 91: The basic idea of the AIC and BIC criteria
</p>
</div>
<p></p>
</div>
<div id="a-larger-view-of-model-selection-and-validation" class="section level3 unnumbered">
<h3>A larger view of <em>model selection and validation</em></h3>
<p>The practice of data analytics has evolved and developed an elaborate process to protect us from overfitting or underfitting a model. The 5-step process is illustrated in Figure <a href="remarks-3.html#fig:f5-flowchart">92</a>.</p>
<p></p>
<div class="figure"><span id="fig:f5-flowchart"></span>
<p class="caption marginnote shownote">
Figure 92: A typical process of how data scientists work with clients to develop robust models
</p>
<img src="graphics/5_flowchart.png" alt="A typical process of how data scientists work with clients to develop robust models" width="100%"  />
</div>
<p></p>
<p>In the <span class="math inline">\(1^{st}\)</span> step, the client collects two datasets, one is the <em>training dataset</em> and another is the <em>testing dataset</em> .</p>
<p>In the <span class="math inline">\(2^{nd}\)</span> step, the client sends the <em>training dataset</em> to the data scientist to train the model. The client keeps the <em>testing dataset</em> for the client’s own use to test the final model submitted by the data scientist.</p>
<p>Now the data scientist should keep in mind that, no matter how the model is obtained<label for="tufte-sn-132" class="margin-toggle sidenote-number">132</label><input type="checkbox" id="tufte-sn-132" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">132</span> In a real application, you may try all you could think of to find your best model. Deep understanding of your models always help. Sometimes it is also luck, insight, and hard-working trial and error. What matters is your model is really good and can outperform your competitor’s. Data scientists survive in a harsh competitive environment.</span>, its goal is to predict well on the <em>unseen</em> <em>testing dataset</em>. How shall we do so, without access to the <em>testing dataset</em>?</p>
<!-- % ^[Three types of upset when we data scientists anxiously wait for results: for those who have the model as shown in the left panel of Figure \@ref(fig:f5-1), we know our model is under-performing, but it is better than random guess; for those who have the model as shown in the middle panel of Figure \@ref(fig:f5-1), we know we had been objective in training the model, the model should be fine, and we hope we had followed the right amount of balance and restrain to get the best model as we could; for those who have the model as shown in the right panel of Figure \@ref(fig:f5-1), if the twists of the curve around that few red squares still haven't raised red flags ... ]. -->
<!-- % After we build the model and deliver it to our client, the model will be evaluated on the testing dataset by the client. And our goal is to make sure that, although we don't have access to the testing dataset, the model we trained on the training dataset would succeed on the testing dataset as well  -->
<p>Just like in Bootstrap, we mimic the process.</p>
<p>In the <span class="math inline">\(3^{rd}\)</span> step, the data scientist mimics the testing procedure as the client would use. The data scientist splits the <em>training dataset</em> into two parts, one for model training and one for model testing<label for="tufte-sn-133" class="margin-toggle sidenote-number">133</label><input type="checkbox" id="tufte-sn-133" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">133</span> Generate a “training dataset” and a “testing dataset” from the <em>training dataset</em>. To avoid confusion, these two are often called <strong>internal</strong> training dataset and <strong>internal</strong> testing dataset , respectively. The training and testing datasets the client creates are often called <strong>external</strong> training dataset and <strong>external</strong> testing dataset , respectively.</span>.</p>
<p>In the <span class="math inline">\(4^{th}\)</span> step, the data scientist creates a model that should fit the <em>internal training dataset</em> well. Cross-validation is often used in this step.</p>
<p>In the <span class="math inline">\(5^{th}\)</span> step, the data scientist tests the model obtained in the <span class="math inline">\(4^{th}\)</span> step using the <em>internal testing data</em>. This is the final pass that will be conducted in house, before the final model is submitted to the client. Note that, the <span class="math inline">\(5^{th}\)</span> step could not be integrated into the model selection process conducted in the <span class="math inline">\(4^{th}\)</span> step—otherwise, the <em>internal testing data</em> is essentially used as an <em>internal training dataset</em><label for="tufte-sn-134" class="margin-toggle sidenote-number">134</label><input type="checkbox" id="tufte-sn-134" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">134</span> After all, the usage of the dataset dictates its name.</span>.</p>
<p>In the <span class="math inline">\(6^{th}\)</span> step, the data scientist submits the final model to the client. The model will be evaluated by the client on the <em>internal testing dataset</em>. The data scientist may or may not learn the evaluation result of the final model from the client.</p>
</div>
<div id="the-confusion-matrix" class="section level3 unnumbered">
<h3>The confusion matrix</h3>
<p>The <em>rare disease</em> example mentioned earlier in this chapter implies that the context matters. It also implies that <em>how we evaluate a model’s performance</em> matters as well.</p>
<p><em>Accuracy</em>, naturally, is a most important evaluation metric. As any <em>overall</em> evaluation metric, it averages things and blurs boundaries between categories, and for the same reason, it could be broken down into more <em>sub</em>categories. For example, a binary classification problem has two classes. We often care about specific accuracy on either class, i.e., if one class represents disease (positive) while another represents normal (negative), as a convention in medicine, we name the correct prediction on a positive case as <strong>true positive</strong> (<strong>TP</strong>) and name the correct prediction on a negative case as <strong>true negative</strong> (<strong>TN</strong>). Correspondingly, we define the <strong>false positive</strong> (<strong>FP</strong>) as incorrect prediction on a true negative case, and <strong>false negative</strong> (<strong>FN</strong>) as incorrect prediction on a true positive case. This is illustrated in Table <a href="remarks-3.html#tab:t5-1">20</a>, the so-called <strong>confusion matrix</strong> .</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-1">Table 20: </span>The confusion matrix</span><!--</caption>--></p>
<table>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left"><strong>Reality</strong>: <em>Positive</em></td>
<td align="left"><strong>Reality</strong>: <em>Negative</em></td>
</tr>
<tr class="even">
<td align="left"><strong>Prediction</strong>: <em>Positive</em></td>
<td align="left">True positive (<strong>TP</strong>)</td>
<td align="left">False positive (<strong>FP</strong>)</td>
</tr>
<tr class="odd">
<td align="left"><strong>Prediction</strong>: <em>Negative</em></td>
<td align="left">False negative (<strong>FN</strong>)</td>
<td align="left">True negative (<strong>TN</strong>)</td>
</tr>
</tbody>
</table>
<p></p>
<p>Based on <em>TP</em>, the concept <strong>true positive rate</strong> (<strong>TPR</strong>) could also be defined, i.e., <em>TPR</em> = TP/(TP+FN). Similarly, we can also define the <strong>false positive rate</strong> (<strong>FPR</strong>) as FPR = FP/(FP+TN).</p>
</div>
<div id="the-roc-curve" class="section level3 unnumbered">
<h3>The ROC curve</h3>
<p>Building on the <em>confusion matrix</em> , the <strong>receiver operating characteristic curve</strong> (<strong>ROC curve</strong>) is an important evaluation metric for classification models.</p>
<p>Recall that, in a logistic regression model, before we make the final prediction, an intermediate result is obtained first</p>
<p><span class="math display">\[
p(\boldsymbol x)=\frac{1}{1+e^{-\left(\beta_{0}+\Sigma_{i=1}^{p} \beta_{i} x_{i}\right)}}.
\]</span></p>
<p>A <strong>cut-off value</strong><label for="tufte-sn-135" class="margin-toggle sidenote-number">135</label><input type="checkbox" id="tufte-sn-135" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">135</span> By default, <span class="math inline">\(0.5\)</span>.</span> is used to make the binary predictions, i.e., it classifies the cases whose <span class="math inline">\(p(\boldsymbol x)\)</span> are larger than the cut-off value as <em>positive</em>; otherwise, if <span class="math inline">\(p(\boldsymbol x)\)</span> is smaller than the cut-off value, <em>negative</em>. This means that, for each cut-off value, we can obtain a confusion matrix with different values of the TP, FP, FN, and TN. As there are many possible cut-off values, the <em>ROC curve</em> is a succinct way to synthesize all the scenarios of all possible cut-off values, i.e., it tries many cut-off values and plots the FPR (x-axis) against the TPR (y-axis). This is illustrated in Figure <a href="remarks-3.html#fig:f5-12">93</a>.</p>
<p></p>
<div class="figure"><span id="fig:f5-12"></span>
<p class="caption marginnote shownote">
Figure 93: The logistic model produces an intermediate result <span class="math inline">\(p(\boldsymbol x)\)</span> for the cases of both classes: (left) shows the distributions of <span class="math inline">\(p(\boldsymbol x)\)</span> of both classes and a particular cut-off value; and (right) shows the ROC curve that synthesizes all the scenarios of all the cut-off values
</p>
<img src="graphics/5_12.png" alt="The logistic model produces an intermediate result $p(\boldsymbol x)$ for the cases of both classes: (left) shows the distributions of $p(\boldsymbol x)$ of both classes and a particular cut-off value; and (right) shows the ROC curve that synthesizes all the scenarios of all the cut-off values" width="100%"  />
</div>
<p></p>
<p>The ROC curve is more useful to evaluate a model’s <em>potential</em>, i.e., Figure <a href="remarks-3.html#fig:f5-12">93</a> presents the performances of the logistic regression model for <em>all</em> cut-off values rather than <em>one</em> cut-off value. The <span class="math inline">\(45^{\circ}\)</span> line represents a model that is equivalent to <em>random guess</em>. In other words, the ROC curve of a model that lacks potential for prediction will be close to the <span class="math inline">\(45^{\circ}\)</span> line. A better model will show a ROC curve that is closer to the upper left corner point. Because of this, the <strong>area under the curve</strong> (<strong>AUC</strong>) is often used to summarize the ROC curve of a model. The higher the AUC, the better the model.</p>
<p><em>A Small Data Example.</em> Let’s study how a ROC curve could be created using an example. Consider a random forest model of <span class="math inline">\(100\)</span> trees and its prediction on <span class="math inline">\(9\)</span> data points. A random forest model uses the <em>majority voting</em> to aggregate the predictions of its trees to reach a final binary prediction. The <em>cut-off value</em> concerned here is the threshold of votes, i.e., here, we try three cut-off values, C=<span class="math inline">\(50\)</span> (default in <code>randomForest</code>), C=<span class="math inline">\(37\)</span>, and C=<span class="math inline">\(33\)</span>, as shown in Table <a href="remarks-3.html#tab:t5-exampleROCrf">21</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-exampleROCrf">Table 21: </span>Prediction on <span class="math inline">\(9\)</span> data points via a random forest model of <span class="math inline">\(100\)</span> trees, with different cut-off values of the vote threshold, C=<span class="math inline">\(50\)</span> (default in <code>ra ndomForest</code>), C=<span class="math inline">\(37\)</span>, and C=<span class="math inline">\(33\)</span></span><!--</caption>--></p>
<table>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left">Vote</td>
<td align="left">True Label</td>
<td align="left">C=<span class="math inline">\(50\)</span></td>
<td align="left">C=<span class="math inline">\(37\)</span></td>
<td align="left">C=<span class="math inline">\(33\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(38\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(49\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(48\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(76\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(32\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(57\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(36\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(8\)</span></td>
<td align="left"><span class="math inline">\(36\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(9\)</span></td>
<td align="left"><span class="math inline">\(35\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Based on the definition of the confusion matrix in Table <a href="remarks-3.html#tab:t5-1">20</a>, we calculate the metrics in Table <a href="remarks-3.html#tab:t5-exampleROCrf2">22</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-exampleROCrf2">Table 22: </span>Metrics for predictions in Table <a href="remarks-3.html#tab:t5-exampleROCrf">21</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">C=<span class="math inline">\(50\)</span></th>
<th align="left">C=<span class="math inline">\(37\)</span></th>
<th align="left">C=<span class="math inline">\(33\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Accuracy</td>
<td align="left"><span class="math inline">\(5/9\)</span></td>
<td align="left"><span class="math inline">\(6/9\)</span></td>
<td align="left"><span class="math inline">\(5/9\)</span></td>
</tr>
<tr class="even">
<td align="left">TP</td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
</tr>
<tr class="odd">
<td align="left">FP</td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
</tr>
<tr class="even">
<td align="left">FN</td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left">TN</td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left">FPR = FP/(FP+TN)</td>
<td align="left"><span class="math inline">\(1/(1+4)\)</span></td>
<td align="left"><span class="math inline">\(2/(2+3)\)</span></td>
<td align="left"><span class="math inline">\(4/(4+1)\)</span></td>
</tr>
<tr class="odd">
<td align="left">TPR = TP/(TP+FN)</td>
<td align="left"><span class="math inline">\(1/(1+3)\)</span></td>
<td align="left"><span class="math inline">\(3/(3+1)\)</span></td>
<td align="left"><span class="math inline">\(4/(4+0)\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>With three cut-off values, we map the three points in Figure <a href="remarks-3.html#fig:f5-ROC">94</a> by plotting the <em>FPR</em> (x-axis) against the <em>TPR</em> (y-axis). There are a few R packages to generate a ROC curve for a classification model. Figure <a href="remarks-3.html#fig:f5-ROC">94</a> illustrates the basic idea implemented in these packages to draw a ROC curve: sample a few cut-off values and map a few points in the figure, then draw a smooth curve that connects the point.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-ROC"></span>
<img src="graphics/5_ROC.png" alt="Illustration of how to draw a ROC curve using the data in Tables~\@ref(tab:t5-exampleROCrf) and~\@ref(tab:t5-exampleROCrf2)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 94: Illustration of how to draw a ROC curve using the data in Tables~<a href="remarks-3.html#tab:t5-exampleROCrf">21</a> and~<a href="remarks-3.html#tab:t5-exampleROCrf2">22</a><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>R Example.</em></p>
<p>We build a logistic regression model using the AD data as we have done in <strong>Chapter 3</strong>.</p>
<p></p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="remarks-3.html#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ROC and more performance metrics of logistic regression model</span></span>
<span id="cb123-2"><a href="remarks-3.html#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the AD dataset</span></span>
<span id="cb123-3"><a href="remarks-3.html#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb123-4"><a href="remarks-3.html#cb123-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb123-5"><a href="remarks-3.html#cb123-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb123-6"><a href="remarks-3.html#cb123-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb123-7"><a href="remarks-3.html#cb123-7" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span>
<span id="cb123-8"><a href="remarks-3.html#cb123-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb123-9"><a href="remarks-3.html#cb123-9" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(AD)[<span class="dv">1</span>]</span>
<span id="cb123-10"><a href="remarks-3.html#cb123-10" aria-hidden="true" tabindex="-1"></a>n.train <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fl">0.8</span> <span class="sc">*</span> n)</span>
<span id="cb123-11"><a href="remarks-3.html#cb123-11" aria-hidden="true" tabindex="-1"></a>idx.train <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n.train)</span>
<span id="cb123-12"><a href="remarks-3.html#cb123-12" aria-hidden="true" tabindex="-1"></a>AD.train <span class="ot">&lt;-</span> AD[idx.train,]</span>
<span id="cb123-13"><a href="remarks-3.html#cb123-13" aria-hidden="true" tabindex="-1"></a>AD.test <span class="ot">&lt;-</span> AD[<span class="sc">-</span>idx.train,]</span>
<span id="cb123-14"><a href="remarks-3.html#cb123-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-15"><a href="remarks-3.html#cb123-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic selection of the model</span></span>
<span id="cb123-16"><a href="remarks-3.html#cb123-16" aria-hidden="true" tabindex="-1"></a>logit.AD.full <span class="ot">&lt;-</span> <span class="fu">glm</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> AD.train[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>)], </span>
<span id="cb123-17"><a href="remarks-3.html#cb123-17" aria-hidden="true" tabindex="-1"></a>                     <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb123-18"><a href="remarks-3.html#cb123-18" aria-hidden="true" tabindex="-1"></a>logit.AD.final <span class="ot">&lt;-</span> <span class="fu">step</span>(logit.AD.full, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>, <span class="at">trace =</span> <span class="dv">0</span>)</span>
<span id="cb123-19"><a href="remarks-3.html#cb123-19" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.AD.final)</span></code></pre></div>
<p></p>
<p>Then we use the function, <code>confusionMatrix()</code> from the R package <code>caret</code> to obtain the confusion matrix</p>
<p></p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="remarks-3.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(e1071)</span>
<span id="cb124-2"><a href="remarks-3.html#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(caret)</span>
<span id="cb124-3"><a href="remarks-3.html#cb124-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prediction scores</span></span>
<span id="cb124-4"><a href="remarks-3.html#cb124-4" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(logit.AD.final, <span class="at">newdata=</span>AD.test,<span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb124-5"><a href="remarks-3.html#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data=</span><span class="fu">factor</span>(pred<span class="sc">&gt;</span><span class="fl">0.5</span>), <span class="fu">factor</span>(AD.test[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">1</span>))</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="remarks-3.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb125-2"><a href="remarks-3.html#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb125-3"><a href="remarks-3.html#cb125-3" aria-hidden="true" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb125-4"><a href="remarks-3.html#cb125-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction FALSE TRUE</span></span>
<span id="cb125-5"><a href="remarks-3.html#cb125-5" aria-hidden="true" tabindex="-1"></a><span class="do">##      FALSE    48    7</span></span>
<span id="cb125-6"><a href="remarks-3.html#cb125-6" aria-hidden="true" tabindex="-1"></a><span class="do">##      TRUE      7   42</span></span>
<span id="cb125-7"><a href="remarks-3.html#cb125-7" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb125-8"><a href="remarks-3.html#cb125-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                Accuracy : 0.8654          </span></span>
<span id="cb125-9"><a href="remarks-3.html#cb125-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                  95% CI : (0.7845, 0.9244)</span></span>
<span id="cb125-10"><a href="remarks-3.html#cb125-10" aria-hidden="true" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5288          </span></span>
<span id="cb125-11"><a href="remarks-3.html#cb125-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : 3.201e-13       </span></span>
<span id="cb125-12"><a href="remarks-3.html#cb125-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb125-13"><a href="remarks-3.html#cb125-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Kappa : 0.7299          </span></span>
<span id="cb125-14"><a href="remarks-3.html#cb125-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 1               </span></span>
<span id="cb125-15"><a href="remarks-3.html#cb125-15" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb125-16"><a href="remarks-3.html#cb125-16" aria-hidden="true" tabindex="-1"></a><span class="do">##             Sensitivity : 0.8727          </span></span>
<span id="cb125-17"><a href="remarks-3.html#cb125-17" aria-hidden="true" tabindex="-1"></a><span class="do">##             Specificity : 0.8571          </span></span>
<span id="cb125-18"><a href="remarks-3.html#cb125-18" aria-hidden="true" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8727          </span></span>
<span id="cb125-19"><a href="remarks-3.html#cb125-19" aria-hidden="true" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.8571          </span></span>
<span id="cb125-20"><a href="remarks-3.html#cb125-20" aria-hidden="true" tabindex="-1"></a><span class="do">##              Prevalence : 0.5288          </span></span>
<span id="cb125-21"><a href="remarks-3.html#cb125-21" aria-hidden="true" tabindex="-1"></a><span class="do">##          Detection Rate : 0.4615          </span></span>
<span id="cb125-22"><a href="remarks-3.html#cb125-22" aria-hidden="true" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5288          </span></span>
<span id="cb125-23"><a href="remarks-3.html#cb125-23" aria-hidden="true" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8649          </span></span>
<span id="cb125-24"><a href="remarks-3.html#cb125-24" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb125-25"><a href="remarks-3.html#cb125-25" aria-hidden="true" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : FALSE           </span></span>
<span id="cb125-26"><a href="remarks-3.html#cb125-26" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
<p></p>
<p>The ROC curve could be drawn using the R Package <code>ROCR</code>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-13"></span>
<img src="graphics/5_13.png" alt="ROC curve of the logistic regression model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 95: ROC curve of the logistic regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="remarks-3.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the ROC curve using the testing data</span></span>
<span id="cb126-2"><a href="remarks-3.html#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute ROC and Precision-Recall curves</span></span>
<span id="cb126-3"><a href="remarks-3.html#cb126-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(<span class="st">&#39;ROCR&#39;</span>)</span>
<span id="cb126-4"><a href="remarks-3.html#cb126-4" aria-hidden="true" tabindex="-1"></a>linear.roc.curve <span class="ot">&lt;-</span> <span class="fu">performance</span>(<span class="fu">prediction</span>(pred, AD.test[,<span class="dv">1</span>]),</span>
<span id="cb126-5"><a href="remarks-3.html#cb126-5" aria-hidden="true" tabindex="-1"></a>                                <span class="at">measure=</span><span class="st">&#39;tpr&#39;</span>, <span class="at">x.measure=</span><span class="st">&#39;fpr&#39;</span> )</span>
<span id="cb126-6"><a href="remarks-3.html#cb126-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(linear.roc.curve,  <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;orange3&quot;</span>, </span>
<span id="cb126-7"><a href="remarks-3.html#cb126-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">&quot;Validation of the logistic model using testing data&quot;</span>)</span></code></pre></div>
<p></p>
<p>The ROC curve is shown in Figure <a href="remarks-3.html#fig:f5-13">95</a>.</p>
</div>
</div>
<p style="text-align: center;">
<a href="out-of-bag-error-in-random-forests.html"><button class="btn btn-default">Previous</button></a>
<a href="exercises-3.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
