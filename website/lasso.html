<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="LASSO | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="LASSO | Data Analytics">

<title>LASSO | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="lasso" class="section level2 unnumbered">
<h2>LASSO</h2>
<p></p>
<div id="rationale-and-formulation-12" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-intro1"></span>
<img src="graphics/8_intro1.png" alt="A line is not a model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 140: A line is not a model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Two points determine a line, as shown in Figure <a href="lasso.html#fig:f8-intro1">140</a>. It shares the same geometric form as a linear regression model, but it is a deterministic geometric pattern and has nothing to do with <em>error</em>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-intro"></span>
<img src="graphics/8_intro.png" alt="Revisit the linear regression model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 141: Revisit the linear regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>With one more data point, magic happens: as shown in Figure <a href="lasso.html#fig:f8-intro">141</a>, now we can estimate the <em>residuals</em> and study the systematic patterns of <em>error</em>. The line in Figure <a href="lasso.html#fig:f8-intro">141</a> becomes a statistical model.</p>
<p>The two lines in Figures <a href="lasso.html#fig:f8-intro1">140</a> and <a href="lasso.html#fig:f8-intro">141</a>, one is a deterministic pattern, while another is a statistical model, are like <em>homonym</em>. The different meanings share the same form of their signifier (e.g., like the word <em>bass</em> that means a certain sound that is low and deep, or a type of fish).</p>
<p>The <em>error</em> is a defining component of a statistical model. It models the <em>noise</em> in the data. In an application context, understanding the noise and knowing how much proportion the noise contributes to the total variation of the dataset is important knowledge. And, to derive the <em>p-values</em> of the regression coefficients, we need the noise so that we can compare the strength of the estimated coefficients with the noise to evaluate if the estimated coefficients are significantly different from random manifestation (i.e., if we cannot model the noise, then we have no basis to define what is random manifestation.).</p>
<p>To model a linear regression model, we need enough data points to estimate the error. For the simple example when there is only one predictor <span class="math inline">\(x\)</span>, as shown in Figures <a href="lasso.html#fig:f8-intro1">140</a> and <a href="lasso.html#fig:f8-intro">141</a>, we would need at least <span class="math inline">\(3\)</span> data points to estimate the error<label for="tufte-sn-199" class="margin-toggle sidenote-number">199</label><input type="checkbox" id="tufte-sn-199" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">199</span> While this is obvious from Figures <a href="lasso.html#fig:f8-intro1">140</a> and <a href="lasso.html#fig:f8-intro">141</a>, we can also obtain the conclusion by derivation. I.e., given two data points, <span class="math inline">\((x_1, y_1)\)</span> and <span class="math inline">\((x_2, y_2)\)</span> we could write two equations, <span class="math inline">\(y_1 = \beta_{0}+\beta_{1} x_1\)</span> and <span class="math inline">\(y_2 = \beta_{0}+\beta_{1} x_2\)</span>.</span>. This is just enough to solve for the <span class="math inline">\(2\)</span> regression coefficients. Consider a problem with <span class="math inline">\(10\)</span> variables, what is the minimum number of data points needed to enable the estimation of error<label for="tufte-sn-200" class="margin-toggle sidenote-number">200</label><input type="checkbox" id="tufte-sn-200" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">200</span> The answer is <span class="math inline">\(12\)</span>. Suppose that we only have <span class="math inline">\(11\)</span> data points. The regression <em>line</em> is defined by <span class="math inline">\(11\)</span> regression coefficients. For each data point, we can write up an equation. Thus, <span class="math inline">\(11\)</span> data points are just enough to estimate the <span class="math inline">\(11\)</span> regression coefficients, leaving no room for estimating errors.</span>?</p>
<p>From the examples aforementioned, we could deduce that the number of data points, i.e., denoted as <span class="math inline">\(N\)</span>, needs to be larger than the number of variables, i.e., denoted as <span class="math inline">\(p\)</span>. This is barely a minimum requirement of linear regression, as we haven’t asked how many data points are needed to ensure high-quality estimation of the parameters. In classic settings in statistics, <span class="math inline">\(N\)</span> is assumed to be much larger than <span class="math inline">\(p\)</span> in order to prove asymptotics—a common approach to prove a statistical model is valid. Practically, linear regression model finds difficulty in applications where the ratio <span class="math inline">\(N/p\)</span> is small. In recent years, there are applications where the number of data points is even smaller than the number of variables, i.e., commonly referred to as <span class="math inline">\(N &lt; p\)</span> problems .</p>
<p>When increasing <span class="math inline">\(N\)</span> is not always a feasible option, reducing <span class="math inline">\(p\)</span> is a necessity. Some variables may be irrelevant or simply noise. Even if all variables are statistically informative, when considered as a whole, some of them may be redundant, and some are weaker than others. In those scenarios, there is room for us to wriggle with the problematic dataset and improve on the ratio <span class="math inline">\(N/p\)</span> by reducing <span class="math inline">\(p\)</span>.</p>
<p>LASSO was invented in 1996 to sparsify the linear regression model and allow the regression model to select significant predictors automatically<label for="tufte-sn-201" class="margin-toggle sidenote-number">201</label><input type="checkbox" id="tufte-sn-201" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">201</span> Tibshirani, R. <em>Regression shrinkage and selection via the Lasso,</em> Journal of the Royal Statistical Society (Series B), Volume 58, Issue 1, Pages 267-288, 1996.</span>.</p>
<p>Remember that, to estimate <span class="math inline">\(\boldsymbol{\beta}\)</span>, the least squares estimation of linear regression is</p>
<p><span class="math display" id="eq:8-LS">\[\begin{equation}
        \boldsymbol{\hat \beta} = \arg\min_{\boldsymbol \beta} {  (\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta}), }   
\tag{83}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{N \times 1}\)</span> is the measurement vector of the outcome variable, <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{N \times p}\)</span> is the data matrix of the <span class="math inline">\(N\)</span> measurement vectors of the <span class="math inline">\(p\)</span> predictors, <span class="math inline">\(\boldsymbol \beta \in \mathbb{R}^{p \times 1}\)</span> is the regression coefficient vector<label for="tufte-sn-202" class="margin-toggle sidenote-number">202</label><input type="checkbox" id="tufte-sn-202" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">202</span> Here, we assume that the data is normalized/standardized and no intercept coefficient <span class="math inline">\(\beta_0\)</span> is needed. Normalization means <span class="math inline">\(\sum_{n=1}^N x_{nj}/N=0\)</span>, <span class="math inline">\(\sum_{n=1}^N x_{ij}^2/N=1\)</span> for <span class="math inline">\(j=1,2,\dots,p\)</span> and <span class="math inline">\(\sum_{n=1}^N y_n/N=0\)</span>. Normalization is a common practice, and some R packages automatically normalize the data as a default preprocessing step before the application of a model.</span>.</p>
<p>The formulation of LASSO is</p>
<p><span class="math display" id="eq:8-LASSO">\[\begin{equation}
        \boldsymbol{\hat \beta} = \arg\min_{\boldsymbol \beta} \left \{   \underbrace{(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})}_{\text{Least squares}} + \underbrace{\lambda \lVert \boldsymbol{\beta}\rVert_{1}}_{L_1 \text{ norm penalty}} \right \}
\tag{84}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\lVert \boldsymbol{\beta} \rVert_1 = \sum_{i=1}^p \lvert \beta_i \rvert\)</span>. The parameter, <span class="math inline">\(\lambda\)</span>, is called the <strong>penalty parameter</strong> that is specified by user of LASSO. The larger the parameter <span class="math inline">\(\lambda\)</span>, the more zeros in <span class="math inline">\(\boldsymbol{\hat \beta}\)</span>.</p>
<p>It could be seen that LASSO embodies two components in its formulation. The <span class="math inline">\(1\)</span>^{st} term is the least squares loss function inherited from linear regression that is used to measure the goodness-of-fit of the model. The <span class="math inline">\(2\)</span> term is the sum of absolute values of elements in <span class="math inline">\(\boldsymbol{\beta}\)</span> that is called the <span class="math inline">\(L_1\)</span> norm penatly . It measures the <em>model complexity</em> , i.e., smaller <span class="math inline">\(\lVert \boldsymbol{\beta} \rVert_1\)</span> tends to create more zeros in <span class="math inline">\(\boldsymbol{\beta}\)</span>, leading to a simpler model. In practice, by tuning the parameter <span class="math inline">\(\lambda\)</span>, we hope to find the best model with an optimal balance between model fit and model complexity.</p>
<!-- $\left\lVert \boldsymbol{\beta} \right\rVert_1 $ -->
<p></p>
<div class="figure"><span id="fig:f8-1"></span>
<p class="caption marginnote shownote">
Figure 142: Path solution trajectory of the coefficients; each curve corresponds to a regression coefficient
</p>
<img src="graphics/8_1.png" alt="Path solution trajectory of the coefficients; each curve corresponds to a regression coefficient" width="100%"  />
</div>
<p></p>
<p>As shown in Figure <a href="lasso.html#fig:f8-1">142</a>, LASSO can generate a <strong>path solution trajectory</strong> that visualizes the solutions of <span class="math inline">\(\boldsymbol{\beta}\)</span> for a continuum of values of <span class="math inline">\(\lambda\)</span>. Model selection criteria such as the Akaike Information Criteria (AIC) or cross-validation can be used to identify the best <span class="math inline">\(\lambda\)</span> that would help us find the final model, i.e., as the vertical line shown in Figure <a href="lasso.html#fig:f8-1">142</a>. When many variables are deleted from the model, the dimensionality of the model is reduced, and <span class="math inline">\(N/p\)</span> is increased.</p>
<!-- %\begin{equation} -->
<!-- %    \begin{gathered} -->
<!-- %    y = \beta_{0}+\beta_{1} x + \epsilon, \\ -->
<!-- %    \epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right). -->
<!-- %    \end{gathered} -->
<!-- %(\#eq:8-simLR) -->
<!-- %\end{equation} -->
</div>
<div id="the-shooting-algorithm" class="section level3 unnumbered">
<h3>The shooting algorithm</h3>
<p>We introduce the <strong>shooting algorithm</strong> to solve for the optimization problem shown in Eq. <a href="lasso.html#eq:8-LASSO">(84)</a>. Let’s consider a simple example when there is only one predictor <span class="math inline">\(x\)</span>. The objective function in Eq. <a href="lasso.html#eq:8-LASSO">(84)</a> could be rewritten as</p>
<p><span class="math display" id="eq:8-simLR-LASSO">\[\begin{equation}
    l\left(\beta\right)=(\boldsymbol{y} - \boldsymbol{X}\beta)^{T}(\boldsymbol{y} - \boldsymbol{X}\beta) + \lambda \lvert \beta \rvert.
\tag{85}
\end{equation}\]</span></p>
<p>To solve Eq. <a href="lasso.html#eq:8-simLR-LASSO">(85)</a>, we take the differential of <span class="math inline">\(l\left(\beta\right)\)</span> and put it equal to zero</p>
<p><span class="math display" id="eq:8-simLASSO-grad">\[\begin{equation}
    \frac {\partial l(\beta)}{\partial \beta}=0.
\tag{86}
\end{equation}\]</span></p>
<p>A complication of this differential operation is that the <span class="math inline">\(L_1\)</span>-norm term, <span class="math inline">\(\lvert \beta \rvert\)</span>, has no gradient when <span class="math inline">\(\beta=0\)</span>. There are three scenarios:</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> If <span class="math inline">\(\beta&gt;0\)</span>, then <span class="math inline">\(\frac {\partial L(\beta)}{\partial \beta}=2\beta-2\boldsymbol{X}^T\boldsymbol{y}+\lambda\)</span>. Based on Eq. <a href="lasso.html#eq:8-simLASSO-grad">(86)</a>, we can estimate <span class="math inline">\(\beta\)</span> as <span class="math inline">\(\hat \beta =\boldsymbol{X}^T\boldsymbol{y}-\lambda/2\)</span>. Note that this estimate of <span class="math inline">\(\beta\)</span> may turn out to be negative. If that happens, it would be a contradiction since we have assumed <span class="math inline">\(\beta&gt;0\)</span> to derive the result. This contradiction points to the only possibility that <span class="math inline">\(\beta=0\)</span>.</p></li>
<li><p> If <span class="math inline">\(\beta&lt;0\)</span>, then <span class="math inline">\(\frac {\partial L(\beta)}{\partial \beta}=2\beta-2\boldsymbol{X}^T\boldsymbol{y}-\lambda\)</span>. Based on Eq. <a href="lasso.html#eq:8-simLASSO-grad">(86)</a>, we have <span class="math inline">\(\beta = \boldsymbol{X}^T\boldsymbol{y}+\lambda/2\)</span>. Note that this estimate of <span class="math inline">\(\beta\)</span> may turn out to be positive. If that happens, it would be a contradiction since we have assumed <span class="math inline">\(\beta&lt;0\)</span> to derive the result. This contradiction points to the only possibility that <span class="math inline">\(\beta=0\)</span>.</p></li>
<li><p> If <span class="math inline">\(\beta=0\)</span>, then we have had the solution and no longer need to derive the gradient.</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>In summary, the solution of <span class="math inline">\(\beta\)</span> is</p>
<p><span class="math display" id="eq:8-simLASSO-sol">\[\begin{equation}
    \hat \beta = \begin{cases}
    \boldsymbol{X}^T\boldsymbol{y}-\lambda/2, &amp;if \, \boldsymbol{X}^T\boldsymbol{y}-\lambda/2&gt;0 \\
    \boldsymbol{X}^T\boldsymbol{y}+\lambda/2, &amp;if \, \boldsymbol{X}^T\boldsymbol{y}+\lambda/2&lt;0 \\
    0, &amp; if \, \lambda/2 \geq \lvert\boldsymbol{X}^T\boldsymbol{y}\rvert.
    \end{cases}\tag{87}
\end{equation}\]</span></p>
<p>Now let’s consider the general case as shown in Eq. <a href="lasso.html#eq:8-LASSO">(84)</a>. Figure <a href="lasso.html#fig:f8-lasso-iter">143</a> illustrates the basic idea: to apply the conclusion (with a slight variation) we have obtained in Eq. <a href="lasso.html#eq:8-simLASSO-sol">(87)</a> to solve Eq. <a href="lasso.html#eq:8-LASSO">(84)</a>. Each iteration solves for one regression coefficient, assuming that all the other coefficients are fixed (i.e., to their latest values).</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-lasso-iter"></span>
<img src="graphics/8_lasso_iter.png" alt="The shooting algorithm iterates through the coefficients" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 143: The shooting algorithm iterates through the coefficients<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>In each iteration, we solve a similar problem with the one-predictor special problem shown in Eq. <a href="lasso.html#eq:8-simLR-LASSO">(85)</a>. For instance, denote <span class="math inline">\(\beta_j^{(t)}\)</span> as the estimate of <span class="math inline">\(\beta_j\)</span> in the <span class="math inline">\(t^{th}\)</span> iteration. If we fix the other regression coefficients as their latest estimates, we can rewrite Eq. <a href="lasso.html#eq:8-LASSO">(84)</a> as a function of <span class="math inline">\(\beta_j^{(t)}\)</span> only</p>
<p><span class="math display" id="eq:8-LR-LASSOsim">\[\begin{equation}
    l(\beta_j^{(t)}) = \left (\boldsymbol{y}^{(t)}_j - \boldsymbol{X}_{(:,j)}\beta_j^{(t)} \right )^{T} \left (\boldsymbol{y}^{(t)}_j - \boldsymbol{X}_{(:,j)}\beta_j^{(t)}\right ) +
    \lambda \lvert \beta_j^{(t)} \rvert,
\tag{88}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{X}_{(:,j)}\)</span> is the <span class="math inline">\(j^{th}\)</span> column of the matrix <span class="math inline">\(\boldsymbol{X}\)</span>, and</p>
<p><span class="math display" id="eq:8-yt">\[\begin{equation}
    \boldsymbol{y}^{(t)}_j = \boldsymbol{y-} \sum\nolimits_{k\neq j}\boldsymbol{X}_{(:,k)}\hat\beta^{(t)}_{k}.
\tag{89}
\end{equation}\]</span></p>
<p>Eq. <a href="lasso.html#eq:8-LR-LASSOsim">(88)</a> has the same structure as Eq. <a href="lasso.html#eq:8-simLR-LASSO">(85)</a>. We can readily apply the conclusion in Eq. <a href="lasso.html#eq:8-simLASSO-sol">(87)</a> here and obtain</p>
<p><span class="math display" id="eq:8-LASSO-sol">\[\begin{equation}
    \hat\beta_j^{(t)}=\begin{cases}
    q^{(t)}_j - \lambda / 2, &amp; if \, q^{(t)}_j - \lambda/2 &gt;0 \\
    q^{(t)}_j + \lambda / 2, &amp; if \, q^{(t)}_j + \lambda/2 &lt;0 \\
    0, &amp; if \, \lambda/2 \geq \lvert q^{(t)}_j \rvert, \end{cases}
\tag{90}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display" id="eq:8-qj">\[\begin{equation}
    q^{(t)}_j=\boldsymbol{X}_{(:, j)}^T \boldsymbol{y}^{(t)}_j.
\tag{91}
\end{equation}\]</span></p>
</div>
<div id="a-small-data-example" class="section level3 unnumbered">
<h3>A small data example</h3>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-1">Table 34: </span>A dataset example for LASSO</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-0.707\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(-0.77\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.707\)</span></td>
<td align="left"><span class="math inline">\(0.15\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(0.707\)</span></td>
<td align="left"><span class="math inline">\(-0.707\)</span></td>
<td align="left"><span class="math inline">\(0.62\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Consider a dataset example as shown in Table <a href="lasso.html#tab:t8-1">34</a>.<label for="tufte-sn-203" class="margin-toggle sidenote-number">203</label><input type="checkbox" id="tufte-sn-203" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">203</span> To generate this dataset, we sampled the values of <span class="math inline">\(y\)</span> using the model
<span class="math display">\[
y =0.8x_1+\varepsilon, \text{ where } \varepsilon \sim N(0,0.5).
\]</span>
Only <span class="math inline">\(x_1\)</span> is important.</span></p>
<p>In matrix form, the dataset is rewritten as</p>
<p><span class="math display">\[
\boldsymbol{y}=\left[ \begin{array}{c}{-0.77} \\ {0.15} \\ {0.62}\end{array}\right], \text {     }  \boldsymbol{X}=\left[ \begin{array}{ccccc} {-0.707} &amp; {0} \\ {0} &amp; {0.707} \\ {0.707} &amp; {-0.707}\end{array}\right].
\]</span></p>
<p>Now let’s implement the <em>Shooting algorithm</em> on this data. The objective function of LASSO on this case is</p>
<p><span class="math display">\[
\sum\nolimits_{n=1}\nolimits^3 [y_n-(\beta_1x_{n,1}+\beta_2x_{n,2})]^2+\lambda(\lvert \beta_1 \rvert+\lvert \beta_2 \rvert).
\]</span></p>
<p>Suppose that <span class="math inline">\(\lambda=1\)</span>, and we initiate the regression coefficients as <span class="math inline">\(\hat \beta_1^{(0)}=0\)</span> and <span class="math inline">\(\hat \beta_2^{(0)}=1\)</span>.</p>
<p>To update <span class="math inline">\(\hat \beta_1^{(1)}\)</span>, based on Eq. <a href="lasso.html#eq:8-LASSO-sol">(90)</a>, we first calculate <span class="math inline">\(\boldsymbol{y}^{(1)}_1\)</span> using Eq. <a href="lasso.html#eq:8-yt">(89)</a></p>
<p><span class="math display">\[
\boldsymbol{y}^{(1)}_1 = \boldsymbol{y-}\boldsymbol{X}_{(:,2)}\hat\beta_2^{(0)} = \begin{bmatrix}
-0.7700 \\
-0.557 \\
1.3270 \\
\end{bmatrix}
.\]</span></p>
<p>Then we calculate <span class="math inline">\(q^{(1)}_1\)</span> using Eq. <a href="lasso.html#eq:8-qj">(91)</a></p>
<p><span class="math display">\[
q^{(1)}_1=\boldsymbol{X}_{(:,1)}^T\boldsymbol{y}^{(1)}_1 = 1.7654.
\]</span>
As
<span class="math display">\[
q^{(1)}_1 - \lambda/2 &gt;0,
\]</span>
based on Eq. <a href="lasso.html#eq:8-LASSO-sol">(90)</a> we know that</p>
<p><span class="math display">\[\hat\beta_1^{(1)} = q^{(1)}_1 - \lambda/2 = 1.2654.
\]</span></p>
<p>Then we update <span class="math inline">\(\hat \beta_2^{(1)}\)</span>. We can obtain that
<span class="math display">\[
\boldsymbol{y}^{(1)}_2 = \boldsymbol{y-}\boldsymbol{X}_{(:,1)}\hat\beta_1^{(1)} = \begin{bmatrix}
0.1876 \\
0.1500 \\
-0.2746\\
\end{bmatrix}
.\]</span>
And we can get
<span class="math display">\[
q^{(1)}_2=\boldsymbol{X}_{(:,2)}^T\boldsymbol{y}^{(1)}_2 = 0.3002.
\]</span>
As
<span class="math display">\[\lambda / 2 \geq \lvert q^{(1)}_2\rvert ,
\]</span>
we know that
<span class="math display">\[\hat \beta_2^{(1)} = 0.\]</span></p>
<p>Thus, with only one iteration, the <em>Shooting algorithm</em> identified the irrelevant variable.</p>
</div>
<div id="r-lab-11" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 7-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get dataset into R and organize the dataset in required format.</p>
<p></p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="lasso.html#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb163-2"><a href="lasso.html#cb163-2" aria-hidden="true" tabindex="-1"></a><span class="do">#### Read data from a CSV file</span></span>
<span id="cb163-3"><a href="lasso.html#cb163-3" aria-hidden="true" tabindex="-1"></a><span class="do">#### Example: Alzheimer&#39;s Disease</span></span>
<span id="cb163-4"><a href="lasso.html#cb163-4" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb163-5"><a href="lasso.html#cb163-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb163-6"><a href="lasso.html#cb163-6" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb163-7"><a href="lasso.html#cb163-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD_hd.csv&quot;</span>)</span>
<span id="cb163-8"><a href="lasso.html#cb163-8" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb163-9"><a href="lasso.html#cb163-9" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="lasso.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb164-2"><a href="lasso.html#cb164-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and Y </span></span>
<span id="cb164-3"><a href="lasso.html#cb164-3" aria-hidden="true" tabindex="-1"></a><span class="co"># vector (outcome variable)</span></span>
<span id="cb164-4"><a href="lasso.html#cb164-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)]</span>
<span id="cb164-5"><a href="lasso.html#cb164-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span>
<span id="cb164-6"><a href="lasso.html#cb164-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-7"><a href="lasso.html#cb164-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb164-8"><a href="lasso.html#cb164-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Y,X)</span>
<span id="cb164-9"><a href="lasso.html#cb164-9" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">1</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span>
<span id="cb164-10"><a href="lasso.html#cb164-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-11"><a href="lasso.html#cb164-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data</span></span>
<span id="cb164-12"><a href="lasso.html#cb164-12" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)) <span class="sc">*</span> <span class="dv">4</span> <span class="sc">/</span> <span class="dv">5</span> )</span>
<span id="cb164-13"><a href="lasso.html#cb164-13" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb164-14"><a href="lasso.html#cb164-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data</span></span>
<span id="cb164-15"><a href="lasso.html#cb164-15" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span>
<span id="cb164-16"><a href="lasso.html#cb164-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-17"><a href="lasso.html#cb164-17" aria-hidden="true" tabindex="-1"></a><span class="co"># as.matrix is used here, because the package </span></span>
<span id="cb164-18"><a href="lasso.html#cb164-18" aria-hidden="true" tabindex="-1"></a><span class="co"># glmnet requires this data format.</span></span>
<span id="cb164-19"><a href="lasso.html#cb164-19" aria-hidden="true" tabindex="-1"></a>trainX <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.train[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb164-20"><a href="lasso.html#cb164-20" aria-hidden="true" tabindex="-1"></a>testX <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb164-21"><a href="lasso.html#cb164-21" aria-hidden="true" tabindex="-1"></a>trainY <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.train[,<span class="dv">1</span>])</span>
<span id="cb164-22"><a href="lasso.html#cb164-22" aria-hidden="true" tabindex="-1"></a>testY <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.test[,<span class="dv">1</span>])</span></code></pre></div>
<p></p>
<p></p>
<div class="figure"><span id="fig:f8-BC-path"></span>
<p class="caption marginnote shownote">
Figure 144: Path trajectory of the fitted regression parameters. The figure should be read from right to left (i.e., <span class="math inline">\(\lambda\)</span> from small to large). Variables that become zero later are stronger (i.e., since a larger <span class="math inline">\(\lambda\)</span> is needed to make them become <span class="math inline">\(0\)</span>). The variables that quickly become zero are weak or insignificant variables.
</p>
<img src="graphics/8_BC_path.png" alt="Path trajectory of the fitted regression parameters. The figure should be read from right to left (i.e., $\lambda$ from small to large). Variables that become zero later are stronger (i.e., since a larger $\lambda$ is needed to make them become $0$). The variables that quickly become zero are weak or insignificant variables. " width="100%"  />
</div>
<p></p>
<p><strong>Step 3</strong> uses the R package <code>glmnet</code><label for="tufte-sn-204" class="margin-toggle sidenote-number">204</label><input type="checkbox" id="tufte-sn-204" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">204</span> Check out the argument <code>glmnet</code> to learn more.</span> to build a LASSO model.</p>
<p></p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="lasso.html#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; Use glmnet to conduct LASSO</span></span>
<span id="cb165-2"><a href="lasso.html#cb165-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;glmnet&quot;)</span></span>
<span id="cb165-3"><a href="lasso.html#cb165-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(glmnet)</span>
<span id="cb165-4"><a href="lasso.html#cb165-4" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">glmnet</span>(trainX,trainY, <span class="at">family=</span><span class="fu">c</span>(<span class="st">&quot;gaussian&quot;</span>))</span>
<span id="cb165-5"><a href="lasso.html#cb165-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-6"><a href="lasso.html#cb165-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(fit<span class="sc">$</span>beta) </span>
<span id="cb165-7"><a href="lasso.html#cb165-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The fitted sparse regression parameters under </span></span>
<span id="cb165-8"><a href="lasso.html#cb165-8" aria-hidden="true" tabindex="-1"></a><span class="co"># different lambda values are stored in fit$beta.</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> draws the path trajectory of the LASSO models (i.e., as the one shown in Figure <a href="lasso.html#fig:f8-1">142</a>). The result is shown in Figure <a href="lasso.html#fig:f8-BC-path">144</a>. It displays the information stored in <code>fit$beta</code>. Each curve shows how the estimated regression coefficient of a variable changes according to the value of <span class="math inline">\(\lambda\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="lasso.html#cb166-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; visualization of the path trajectory of </span></span>
<span id="cb166-2"><a href="lasso.html#cb166-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the fitted sparse regression parameters</span></span>
<span id="cb166-3"><a href="lasso.html#cb166-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit,<span class="at">label =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-BC-cv"></span>
<img src="graphics/8_BC_cv.png" alt="Cross-validation result. It is hoped (because it is not always the case in practice) that it is *U-shaped*, like the one shown here, so that we can spot the optimal value of $\lambda$, i.e., the one that corresponds to the lowest dip point. " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 145: Cross-validation result. It is hoped (because it is not always the case in practice) that it is <em>U-shaped</em>, like the one shown here, so that we can spot the optimal value of <span class="math inline">\(\lambda\)</span>, i.e., the one that corresponds to the lowest dip point. <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 5</strong> uses cross-validation to identify the best <span class="math inline">\(\lambda\)</span> value for the LASSO model. The result is shown in Figure <a href="lasso.html#fig:f8-BC-cv">145</a>.</p>
<p></p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="lasso.html#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; Use cross-validation to decide which lambda to use</span></span>
<span id="cb167-2"><a href="lasso.html#cb167-2" aria-hidden="true" tabindex="-1"></a>cv.fit <span class="ot">=</span> <span class="fu">cv.glmnet</span>(trainX,trainY)</span>
<span id="cb167-3"><a href="lasso.html#cb167-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.fit) </span>
<span id="cb167-4"><a href="lasso.html#cb167-4" aria-hidden="true" tabindex="-1"></a><span class="co"># look for the u-shape, and identify the lowest </span></span>
<span id="cb167-5"><a href="lasso.html#cb167-5" aria-hidden="true" tabindex="-1"></a><span class="co"># point that corresponds to the best model</span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> views the best model and evaluates its predictions.</p>
<p></p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="lasso.html#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; To view the best model and the </span></span>
<span id="cb168-2"><a href="lasso.html#cb168-2" aria-hidden="true" tabindex="-1"></a><span class="co"># corresponding coefficients</span></span>
<span id="cb168-3"><a href="lasso.html#cb168-3" aria-hidden="true" tabindex="-1"></a>cv.fit<span class="sc">$</span>lambda.min </span>
<span id="cb168-4"><a href="lasso.html#cb168-4" aria-hidden="true" tabindex="-1"></a><span class="co"># cv.fit$lambda.min is the best lambda value that results </span></span>
<span id="cb168-5"><a href="lasso.html#cb168-5" aria-hidden="true" tabindex="-1"></a><span class="co"># in the best model with smallest mean squared error (MSE)</span></span>
<span id="cb168-6"><a href="lasso.html#cb168-6" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(cv.fit, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>) </span>
<span id="cb168-7"><a href="lasso.html#cb168-7" aria-hidden="true" tabindex="-1"></a><span class="co"># This extracts the fitted regression parameters of </span></span>
<span id="cb168-8"><a href="lasso.html#cb168-8" aria-hidden="true" tabindex="-1"></a><span class="co"># the linear regression model using the given lambda value.</span></span>
<span id="cb168-9"><a href="lasso.html#cb168-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-10"><a href="lasso.html#cb168-10" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv.fit, <span class="at">newx =</span> testX, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>) </span>
<span id="cb168-11"><a href="lasso.html#cb168-11" aria-hidden="true" tabindex="-1"></a><span class="co"># This is to predict using the best model</span></span>
<span id="cb168-12"><a href="lasso.html#cb168-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(y_hat, data.test<span class="sc">$</span>MMSCORE)</span>
<span id="cb168-13"><a href="lasso.html#cb168-13" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((y_hat <span class="sc">-</span> data.test<span class="sc">$</span>MMSCORE)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb168-14"><a href="lasso.html#cb168-14" aria-hidden="true" tabindex="-1"></a><span class="co"># The mean squared error (mse)</span></span>
<span id="cb168-15"><a href="lasso.html#cb168-15" aria-hidden="true" tabindex="-1"></a>mse</span></code></pre></div>
<p></p>
<p>Results are shown below.</p>
<p></p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="lasso.html#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.2969686 # cor(y_hat, data.test$MMSCORE)</span></span>
<span id="cb169-2"><a href="lasso.html#cb169-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 2.453638  # mse</span></span></code></pre></div>
<p></p>
<p><strong>Step 7</strong> re-fits the regression model using the variables selected by LASSO. As LASSO put <span class="math inline">\(L_1\)</span> norm on the regression parameters, it not only penalizes the regression coefficients of the irrelevant variables to be zero, but also penalizes the regression coefficients of the selected variable. Thus, the estimated regression coefficients of a LASSO model tend to be smaller than they are (i.e., this is called <em>bias</em> in machine learning terminology).</p>
<p></p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="lasso.html#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7 -&gt; Re-fit the regression model with selected variables </span></span>
<span id="cb170-2"><a href="lasso.html#cb170-2" aria-hidden="true" tabindex="-1"></a><span class="co"># by LASSO</span></span>
<span id="cb170-3"><a href="lasso.html#cb170-3" aria-hidden="true" tabindex="-1"></a>var_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">coef</span>(cv.fit, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>) <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb170-4"><a href="lasso.html#cb170-4" aria-hidden="true" tabindex="-1"></a>lm.AD.reduced <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> ., <span class="at">data =</span> </span>
<span id="cb170-5"><a href="lasso.html#cb170-5" aria-hidden="true" tabindex="-1"></a>                      data.train[,var_idx,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb170-6"><a href="lasso.html#cb170-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.reduced)</span></code></pre></div>
<p></p>
</div>
</div>
<p style="text-align: center;">
<a href="overview-6.html"><button class="btn btn-default">Previous</button></a>
<a href="principal-component-analysis.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
