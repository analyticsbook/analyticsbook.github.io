<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Remarks | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Remarks | Data Analytics">

<title>Remarks | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="remarks-5" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="is-svm-a-more-complex-model" class="section level3 unnumbered">
<h3>Is SVM a more complex model?</h3>
<p>In the preface of his seminar book<label for="tufte-sn-194" class="margin-toggle sidenote-number">194</label><input type="checkbox" id="tufte-sn-194" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">194</span> Vapnik, V., <em>The Nature of Statistical Learning Theory</em>, Springer, 2000.</span>, Vladimir Vapnik wrote that “<em>…during the last few years at different computer science conferences, I heard reiteration of the following claim: ‘Complex theories do not work, simple algorithms do’…this is not true…Nothing is more practical than a good theory…</em>.” He created the concept of <em>VC dimension</em> to specifically characterize his concept of the complexity of a model.</p>
<p>A model is often <em>perceived</em> to be complex. The SVM model looks more complex than the linear regression model. It asks us to characterize the margin using model parameters, write the optimization formulation, learn the trick of kernel function, and understand the support vectors and the slack variables for the nonseparable case. But, don’t forget that the reason for a model to look simple is probably only because this model may presuppose stronger conditions, too strong that we forget they are assumptions.</p>
<p>It is fair to say that a model is more complex if it provides more capacity to represent the statistical phenomena in the training data. In other words, a more complex model is more flexible to respond to subtle patterns in the data by adjusting itself. In this sense, SVM with kernel functions is a complex model since it can model nonlinearity in the data. But on the other hand, comparing the SVM model with other linear models as shown in Figure <a href="remarks-5.html#fig:f7-20">134</a>, it is hard to tell that the SVM model is simpler, but it is clear that it is more stubborn; because of its pursuit of maximum margin, it ends up with one model only. If you are looking for an example of an idea that is radical and conservative, flexible and disciplined, this is it.</p>
<p></p>
<div class="figure"><span id="fig:f7-20"></span>
<p class="caption marginnote shownote">
Figure 134: (Left) some other linear models; (b) the SVM model
</p>
<img src="graphics/7_20.png" alt="(Left) some other linear models; (b) the SVM model" width="100%"  />
</div>
<p></p>
</div>
<div id="is-svm-a-neural-network-model" class="section level3 unnumbered">
<h3>Is SVM a neural network model?</h3>
<p>Another interesting fact about SVM is that, when it was developed, it was named “support vector network”<label for="tufte-sn-195" class="margin-toggle sidenote-number">195</label><input type="checkbox" id="tufte-sn-195" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">195</span> Cortes, C. and Vapnik, V., <em>Support-vector networks,</em> Machine Learning, Volume 20, Issue 3, Pages 273–297, 1995.</span>. In other words, it has a connection with the artificial neural network that will be discussed in <strong>Chapter 10</strong>. This is revealed in Figure <a href="remarks-5.html#fig:f7-21">135</a>. Readers who know neural network models are encouraged to write up the mathematical model of the SVM model following the neural network format as shown in Figure <a href="remarks-5.html#fig:f7-21">135</a>.</p>
<p></p>
<div class="figure"><span id="fig:f7-21"></span>
<p class="caption marginnote shownote">
Figure 135: SVM as a neural network model
</p>
<img src="graphics/7_21.png" alt="SVM as a neural network model " width="100%"  />
</div>
<p></p>
</div>
<div id="derivation-of-the-margin" class="section level3 unnumbered">
<h3>Derivation of the margin</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-margin-proj"></span>
<img src="graphics/7_margin_proj.png" alt="Illustration of how to derive the margin" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 136: Illustration of how to derive the margin<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Consider any two points on the two margins, e.g., the <span class="math inline">\(\boldsymbol{x}_A\)</span> and <span class="math inline">\(\boldsymbol{x}_B\)</span> in Figure <a href="remarks-5.html#fig:f7-margin-proj">136</a>. The <em>margin width</em> is equal to the projection of the vector <span class="math inline">\(\overrightarrow{A B} = \boldsymbol{x}_B - \boldsymbol{x}_A\)</span> on the direction <span class="math inline">\(\boldsymbol{w}\)</span>, which is</p>
<p><span class="math display" id="eq:7-marginpre">\[\begin{equation} 
    \text{margin } = \frac{ (\boldsymbol{x}_B - \boldsymbol{x}_A) \cdot \vec{\boldsymbol{w}}}{\|\boldsymbol{w}\|}.
\tag{81}
\end{equation}\]</span></p>
<p>It is known that</p>
<p><span class="math display">\[ \boldsymbol{w}^{T} \boldsymbol{x}_B + b =1, \]</span></p>
<p>and</p>
<p><span class="math display">\[ \boldsymbol{w}^{T} \boldsymbol{x}_A + b = -1. \]</span></p>
<p>Thus, Eq. <a href="remarks-5.html#eq:7-marginpre">(81)</a> is rewritten as</p>
<p><span class="math display" id="eq:7-margin">\[\begin{equation} 
    \text{margin } = \frac{2}{\|\boldsymbol{w}\|}.
\tag{82}
\end{equation}\]</span></p>
</div>
<div id="why-the-nonzero-alpha_n-are-the-support-vectors" class="section level3 unnumbered">
<h3>Why the nonzero <span class="math inline">\(\alpha_n\)</span> are the support vectors</h3>
<p>Theoretically, to understand why the nonzero <span class="math inline">\(\alpha_n\)</span> are the support vectors, we can use the <em>Karush–Kuhn–Tucker (KKT) conditions</em><label for="tufte-sn-196" class="margin-toggle sidenote-number">196</label><input type="checkbox" id="tufte-sn-196" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">196</span> Bertsekas, D., <em>Nonlinear Programming: 3rd Edition</em>, Athena Scientific, 2016.</span>. Based on the <em>complementary slackness</em> as one of the KKT conditions, the following equations must hold</p>
<p><span class="math display">\[ 
\alpha_{n}\left[y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1\right]=0 \text {, for } n=1,2, \dots, N.
\]</span></p>
<p>Thus, for any data point <span class="math inline">\(\boldsymbol{x}_n\)</span>, it is either</p>
<p><span class="math display">\[ 
\alpha_{n} = 0 \text {, and } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1 \neq 0;
\]</span></p>
<p>or</p>
<p><span class="math display">\[ 
\alpha_{n} \neq 0 \text {, and } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1 = 0.
\]</span></p>
<p>Revisiting Eq. <a href="support-vector-machine.html#eq:7-5regions">(58)</a> or Figure <a href="support-vector-machine.html#fig:f7-5regions">116</a>, we know that only the support vectors have <span class="math inline">\(\alpha_{n} \neq 0\)</span> and <span class="math inline">\(y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1 = 0\)</span>.</p>
<!-- % A model is determined by $\boldsymbol{w}$ and $b$. In order for the idea of the *maximum margin* to work, the *margin* of the model has to be a function of the model parameters, $\boldsymbol{w}$ and $b$. This is not necessary always granted in reality. First, let's assume that the *margin* could be characterized by the model parameters. Then, let's concern a simpler problem first. Look at Figure~\@ref(fig:f7-3), and derive the perpendicular distance^[Denoted as $\|A N\|$] between the point $A$ with the line^[The line's mathematical model is $\boldsymbol{w}^{T} \boldsymbol{x}+b=0$]. -->
<!-- % To facilitate the derivation, let's further identify two points, $N$ and $B$, on the line.  -->
<!-- % It is known that -->
<!-- % \begin{equation} -->
<!-- %   \|A N\|=\|A B\| \cos \theta.   -->
<!-- %   (\#eq:7-marginAN) -->
<!-- % \end{equation} -->
<!-- % And by definition, we know that: -->
<!-- % \begin{equation}  -->
<!-- %     \cos \theta = \frac{\overrightarrow{A B} \cdot \vec{\boldsymbol{w}}}{\|A B\|\|\boldsymbol{w}\|}, -->
<!-- %     (\#eq:7-costheta) -->
<!-- % \end{equation} -->
<!-- % \noindent where $\overrightarrow{A B}$ is defined as^[$\boldsymbol{x}_{a}$ and  -->
<!-- % $\boldsymbol{x}_{b}$ are the coordinates of the two data points $A$ and $B$, respectively.]: -->
<!-- % \begin{equation} -->
<!-- %     \overrightarrow{A B} = \boldsymbol{x}_{a}-\boldsymbol{x}_{b}. -->
<!-- %     (\#eq:7-arrowAB) -->
<!-- % \end{equation} -->
<!-- % We plug Eq.~\@ref(eq:7-costheta) in Eq.~\@ref(eq:7-marginAN) and get: -->
<!-- % \begin{equation} -->
<!-- %     \|A N\| = \frac{\overrightarrow{A B} \cdot \vec{\boldsymbol{w}}}{\|\boldsymbol{w}\|}. -->
<!-- %     (\#eq:7-marginAN2) -->
<!-- % \end{equation} -->
<!-- % We plug Eq.~\@ref(eq:7-arrowAB) in Eq.~\@ref(eq:7-marginAN2) and get: -->
<!-- % \[  -->
<!-- % \|A N\| = \frac{\overrightarrow{A B} \cdot \vec{\boldsymbol{w}}}{\|\boldsymbol{w}\|}=\frac{\boldsymbol{w}^{T}\left(\boldsymbol{x}_{a}-\boldsymbol{x}_{b}\right)}{\|\boldsymbol{w}\|}. -->
<!-- % \] -->
<!-- % Recall that the data point $B$ is a data point on the line $\boldsymbol{w}^{T} \boldsymbol{x}+b=0$,  -->
<!-- % \begin{equation} -->
<!-- %     \boldsymbol{w}^{T} \boldsymbol{x}_{b} = -b. -->
<!-- % \end{equation} -->
<!-- % We can finally derive the mathematical expression of $\|A N\|$ in terms of $\boldsymbol{w}$ and $b$: -->
<!-- % \begin{equation} -->
<!-- %     \|A N\|=\frac{\boldsymbol{w}^{T}\left(\boldsymbol{x}_{a}-\boldsymbol{x}_{\boldsymbol{b}}\right)}{\|\boldsymbol{w}\|}=\frac{\boldsymbol{w}^{T} \boldsymbol{x}_{\boldsymbol{a}}+b}{\|\boldsymbol{w}\|}. -->
<!-- %    (\#eq:7-ANfinal) -->
<!-- % \end{equation} -->
<!-- % This is a major step towards the development of the *maximum margin* for SVM. To see that, let's apply this conclusion Eq.~\@ref(eq:7-ANfinal) on Figure \@ref(fig:f7-2) to obtain Figure \@ref(fig:f7-4). -->
</div>
<div id="adaboost-algorithm" class="section level3 unnumbered">
<h3>AdaBoost algorithm</h3>
<p>The specifics of the AdaBoost algorithm shown in Figure <a href="ensemble-learning.html#fig:f7-AdaBoost">123</a> are described below.</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> Input: <span class="math inline">\(N\)</span> data points, <span class="math inline">\(\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{N}, y_{N}\right)\)</span>.</p></li>
<li><p> Initialization: Initialize equal weights for all data points <span class="math display">\[ \boldsymbol{w}_{0}=\left(\frac{1}{N}, \ldots, \frac{1}{N}\right).\]</span></p></li>
<li><p> At iteration <span class="math inline">\(t\)</span>:</p></li>
</ul>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> Step 1: Build model <span class="math inline">\(h_t\)</span> on the dataset with weights <span class="math inline">\(\boldsymbol{w}_{t-1}\)</span>.</p></li>
<li><p> Step 2: Calculate errors using <span class="math inline">\(h_t\)</span> <span class="math display">\[ \epsilon_{t}=\sum_{n=1}^{N} w_{t, n}\left\{h_{t}\left(x_{n}\right) \neq y_{n}\right\}.\]</span></p></li>
<li><p> Step 3: Update weights of the data points <span class="math display">\[
  \boldsymbol{w}_{t+1, i}=\frac{w_{t, i}}{Z_{t}} \times \left\{\begin{array}{c}{e^{-\alpha_{t}} \text { if } h_{t}\left(x_{n}\right)=y_{n}} \\ {e^{\alpha_{t}} \text { if } h_{t}\left(x_{n}\right) \neq y_{n}}.\end{array} \right.\]</span> Here, <span class="math display">\[Z_{t} \text { is a normalization factor so that } \sum_{n=1}^{N} w_{t+1, n}=1,\]</span> and <span class="math display">\[ \alpha_{t}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{t}}{\epsilon_{t}}\right).\]</span></p></li>
</ul>
<p><!-- end{itemize} --></p>
<ul>
<li><p> Iterations: Repeat Step 1 to Step 3 for <span class="math inline">\(T\)</span> times, to get <span class="math inline">\(h_1\)</span>, <span class="math inline">\(h_2\)</span>, <span class="math inline">\(h_3\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(h_T\)</span>.</p></li>
<li><p> Output: <span class="math display">\[ H(x)=\operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} h_{t}(x)\right).\]</span></p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>When all the base models are trained, the aggregation of these models in predicting on a data instance <span class="math inline">\(\boldsymbol{x}\)</span> is a weighted sum of base models</p>
<p><span class="math display">\[
h(\boldsymbol{x})=\sum_{i} \gamma_{i} h_{i}(\boldsymbol{x}),
\]</span></p>
<p>where the weight <span class="math inline">\(\gamma_{i}\)</span> is proportional to the accuracy of <span class="math inline">\(h_{i}(x)\)</span> on the training dataset.</p>
</div>
</div>
<p style="text-align: center;">
<a href="ensemble-learning.html"><button class="btn btn-default">Previous</button></a>
<a href="exercises-5.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
