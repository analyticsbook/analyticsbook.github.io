<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Tree models | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Tree models | Data Analytics">

<title>Tree models | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="tree-models" class="section level2 unnumbered">
<h2>Tree models</h2>
<p></p>
<div id="rationale-and-formulation-1" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>While the linear regression model is a typical data modeling method, the decision tree model represents a typical method in the category of algorithmic modeling<label for="tufte-sn-35" class="margin-toggle sidenote-number">35</label><input type="checkbox" id="tufte-sn-35" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">35</span> The two types of modeling cultures are discussed in Table <a href="overview.html#tab:t2-1">1</a>.</span>. The linear regression model, given its many origins and implications, builds a model based on a mathematical characterization of the <em>data-generating mechanism</em> , which emphasizes an analytic understanding of the underlying system and how the data is generated from this system<label for="tufte-sn-36" class="margin-toggle sidenote-number">36</label><input type="checkbox" id="tufte-sn-36" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">36</span> I.e., Eq. <a href="regression-models.html#eq:2-simLR-fx">(3)</a> explains how <span class="math inline">\(y\)</span> is impacted by <span class="math inline">\(x\)</span>, and Eq. <a href="regression-models.html#eq:2-simLR-eps">(4)</a> explains how the rest of <span class="math inline">\(y\)</span> is impacted by a random force. This is illustrated in Figure <a href="tree-models.html#fig:f2-lr-datamodel">16</a>.</span>. This pursuit of “mechanism” is sometimes too much to ask for if we know little about the physics but only the data, since understanding the mechanism of a problem needs experimental science and profound insights. And this pursuit of “mechanism” limits the applicability of a data modeling method when the data don’t seem to follow the data-generating mechanism <em>prescribed</em> by the model.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-lr-datamodel"></span>
<img src="graphics/2_lr_datamodel.png" alt="The *data-generating mechanism* of a simple linear regression model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 16: The <em>data-generating mechanism</em> of a simple linear regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>For example, Table <a href="tree-models.html#tab:t2-3">3</a> shows a dataset that has <span class="math inline">\(6\)</span> observations, with two predictors, <em>Weather</em> and <em>Day of week (Dow)</em>, and an outcome variable, <em>Play</em>. Assume that this is a dataset collected by a causal dog walker whose routine includes a sports field.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-3">Table 3: </span>Example of a dataset where a decision tree has a home game</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Weather</th>
<th align="left">Dow (day of weak)</th>
<th align="left">Play</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">Rainy</td>
<td align="left">Saturday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">Sunny</td>
<td align="left">Saturday</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">Windy</td>
<td align="left">Tuesday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">Sunny</td>
<td align="left">Saturday</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">Sunny</td>
<td align="left">Monday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">Windy</td>
<td align="left">Saturday</td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p></p>
<p>It is hard to imagine that, for this dataset, how we can denote the two predictors as <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and connect it with the outcome variable <span class="math inline">\(y\)</span> in the form of Eq. <a href="regression-models.html#eq:2-multiLR">(13)</a>, i.e.,</p>
<!-- \ArrowBetweenLines [\Downarrow] -->
<p><span class="math display">\[\begin{equation*}
    \begin{aligned}
    &amp;\text{*Yes*} = \beta_0 + \beta_1 \text{*Rainy*} + \beta_2 \text{*Tuesday*} + \epsilon?
    \end{aligned}
\end{equation*}\]</span></p>
<p>For this dataset, decision tree is a natural fit. As shown in Figure <a href="tree-models.html#fig:f2-11">17</a>, a decision tree contains a <strong>root node</strong> , <strong>inner nodes</strong> , and <strong>decision nodes</strong> (i.e., the shaded leaf nodes of the tree in Figure <a href="tree-models.html#fig:f2-11">17</a>). For any data point to reach its prediction, it starts from the root node, follows the <strong>splitting rules</strong> alongside the arcs to travel through inner nodes, then finally reaches a decision node. For example, consider the data point “<em>Weather = Sunny, Dow = Saturday</em>,” it starts with the root node, “<em>Weather = Sunny?</em>” then goes to inner node “<em>Dow = Saturday?</em>” then reaches the decision node as the left child node of the inner node “<em>Dow = Saturday?</em>” So the decision is “<em>Play = Yes</em>.”</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-11"></span>
<img src="graphics/2_11.png" alt="Example of a decision tree model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 17: Example of a decision tree model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Compare with data modeling methods that hope to build a characterization of the data-generating mechanism , algorithmic modeling methods such as the decision tree mimic <em>heuristics</em> in human reasoning. It is challenging, while unnecessary, to write up a model of algorithmic modeling in mathematical forms as the one shown in Eq. <a href="regression-models.html#eq:2-multiLR">(13)</a>. Algorithmic modeling methods are more <em>semantics-oriented</em>, and more focused on patterns detection and description.</p>
</div>
<div id="theorymethod" class="section level3 unnumbered">
<h3>Theory/Method</h3>
<p>Decision trees could be generated by manual inspection of the data. The one shown in Figure <a href="tree-models.html#fig:f2-11">17</a> could be easily drawn with a few inspection of the 6 data points in Table <a href="tree-models.html#tab:t2-3">3</a>. Automatic algorithms have been developed that can take a dataset as input and generate a decision tree as output. We can see from Figure <a href="tree-models.html#fig:f2-11">17</a> that a key element of a decision tree is the <em>splitting rules</em> that guide a data point to travel through the inner nodes to reach a final decision node (i.e., to reach a decision).</p>
<p>A splitting rule is defined by a variable and the set of values the variable is allowed to take, e.g., in “<em>Weather = Sunny?</em>” “Weather” is the variable and “Sunny” is the set of value. The variable used for splitting is referred to as the <strong>splitting variable</strong> , and the set of value is referred to as the <strong>splitting value</strong> .</p>
<p>We start with the root node. Possible splitting rules are</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> “<em>Weather = Sunny?</em>”</p></li>
<li><p> “<em>Dow = Saturday?</em>”</p></li>
<li><p> “<em>Dow = Monday?</em>”</p></li>
<li><p> “<em>Dow = Tuesday?</em>”</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>Each of the splitting rules will lead to a different root node. Two examples are shown in Figure <a href="tree-models.html#fig:f2-12">18</a>. Which one should we use?</p>
<p></p>
<div class="figure"><span id="fig:f2-12"></span>
<p class="caption marginnote shownote">
Figure 18: Example of two root nodes
</p>
<img src="graphics/2_12.png" alt="Example of two root nodes" width="100%"  />
</div>
<p></p>
<p>To help us decide on which splitting rule is the best, the concepts <strong>entropy</strong> of data and <strong>information gain</strong> (<strong>IG</strong>) are needed.</p>
<p><em>Entropy and information gain (IG).</em> We can use the concept <strong>entropy</strong> to measure the homogeneity of the data points in a node of the decision tree. It is defined as</p>
<p><span class="math display" id="eq:2-entropy">\[\begin{equation}
e = \sum\nolimits_{i=1,\cdots,K}-P_i\log _{2} P_i.
\tag{21}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(K\)</span> represents the number of classes of the data points in the node<label for="tufte-sn-37" class="margin-toggle sidenote-number">37</label><input type="checkbox" id="tufte-sn-37" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">37</span> E.g., in Table <a href="tree-models.html#tab:t2-3">3</a>, there are <span class="math inline">\(K=2\)</span> classes, <em>Yes</em> and <em>No</em>.</span>, and <span class="math inline">\(P_i\)</span> is the proportion of data points that belong to the class <span class="math inline">\(i\)</span>. The entropy <span class="math inline">\(e\)</span> is defined as zero when the data points in the node all belong to one single class<label for="tufte-sn-38" class="margin-toggle sidenote-number">38</label><input type="checkbox" id="tufte-sn-38" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">38</span> What is more deterministic than this case?</span>. And <span class="math inline">\(e = 1\)</span> is the maximum value for the entropy of a dataset, i.e., try an example with two classes, where <span class="math inline">\(P_1 = 0.5\)</span> and <span class="math inline">\(P_2 = 0.5\)</span>.<label for="tufte-sn-39" class="margin-toggle sidenote-number">39</label><input type="checkbox" id="tufte-sn-39" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">39</span> What is more uncertain than this case?</span></p>
<p>A node that consists of data points that are dominated by one class (i.e., entropy is small) is ready to be made a decision node. If it still has a large entropy, splitting it into two child nodes could help reduce the entropy. Thus, to further split a node, we look for the best splitting rule that can maximize the entropy reduction. This entropy reduction can be measured by <strong>IG</strong>, which is the difference of entropy of the parent node and the average entropy of the two child nodes weighted by their number of data points. It is defined as</p>
<p><span class="math display" id="eq:2-IG">\[\begin{equation}
IG = e_s - \sum\nolimits_{i=1,2} w_i e_i.
\tag{22}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(e_s\)</span> is the entropy of the parent node, <span class="math inline">\(e_i\)</span> is the entropy of the child node <span class="math inline">\(i\)</span>, and <span class="math inline">\(w_i\)</span> is the number of data points in the child node <span class="math inline">\(i\)</span> divided by the number of data points in the parent node.</p>
<p>For example, for the left tree in Figure <a href="tree-models.html#fig:f2-12">18</a>, using the definition of entropy in Eq. <a href="tree-models.html#eq:2-entropy">(21)</a>, the entropy of the root node is calculated as</p>
<p><span class="math display">\[-\frac{4}{6} \log _2 \frac{4}{6} - \frac{2}{6}\log _2 \frac{2}{6}=0.92.\]</span></p>
<p>The entropy of the left child node (“<em>Weather = Sunny</em>”) is</p>
<p><span class="math display">\[-\frac{2}{3} \log _2 \frac{2}{3} - \frac{1}{3} \log _2 \frac{1}{3}=0.92.\]</span></p>
<p>The entropy of the right child node (“<em>Weather != Sunny</em>”) is <span class="math inline">\(0\)</span> since all three data points (ID = <span class="math inline">\(1,3,6\)</span>) belong to the same class.</p>
<p>Then, using the definition of IG in Eq. <a href="tree-models.html#eq:2-IG">(22)</a>, the IG for the splitting rule “<em>Weather = Sunny</em>” is</p>
<p><span class="math display">\[IG = 0.92 - \frac{3}{6} \times 0.92 - \frac{3}{6} \times 0=0.46.\]</span></p>
<p>For the tree in Figure <a href="tree-models.html#fig:f2-12">18</a> (right), the entropy of the left child node (“<em>Dow = Saturday</em>”) is</p>
<p><span class="math display">\[-\frac{2}{4} \log _2 \frac{2}{4} - \frac{2}{4} \log _2 \frac{2}{4} = 1.\]</span></p>
<p>The entropy of the right child node (“<em>Dow != Saturday</em>”) is <span class="math inline">\(0\)</span> since the two data points (ID = 3,5) belong to the same class.</p>
<p>Thus, the IG for the splitting rule “<em>Dow = Saturday</em>” is</p>
<p><span class="math display">\[IF=0.92-\frac{4}{6} \times 1 - \frac{2}{6} \times 0=0.25.\]</span></p>
<p>As the IG for the splitting rule “<em>Weather = Sunny</em>” is higher, the left tree in Figure <a href="tree-models.html#fig:f2-12">18</a> is a better choice to start the tree.</p>
<p><em>Recursive partitioning.</em> The splitting process discussed above could be repeatedly used, until there is no further need to split a node, i.e., the node contains data points from one single class, which is ideal and almost would never happen in reality; or the node has reached the minimum number of data points<label for="tufte-sn-40" class="margin-toggle sidenote-number">40</label><input type="checkbox" id="tufte-sn-40" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">40</span> It is common to assign a minimum number of data points to prevent the tree-growing algorithm to generate too tiny leaf nodes. This is to prevent “<strong>overfitting</strong>” . Elaborated discussion of overfitting will be provided in <strong>Chapter 5</strong>.</span>. This repetitive splitting process is called <strong>recursive partitioning</strong>.</p>
<p>For instance, the left child node in the tree shown in Figure <a href="tree-models.html#fig:f2-12">18</a> (left) with data points (ID = <span class="math inline">\(2,4,5\)</span>) still has two classes, and can be further split by selecting the next best splitting rule. The right child node has only one class and becomes a decision node labeled with the decision “<em>Play = No</em>.”</p>
<p>This greedy approach, like other greedy optimization approaches, is easy to use. One limitation of greedy approches is that they may find <strong>local optimal</strong> solutions instead of <strong>global optimal</strong> solutions. The optimal choice we made in choosing between the two alternatives in Figure <a href="tree-models.html#fig:f2-12">18</a> is a <em>local</em> optimal choice, and all later nodes of the final tree model are impacted by our decision made on the root node. The <em>optimal root node</em> doesn’t necessarily lead to the <em>optimal tree</em><label for="tufte-sn-41" class="margin-toggle sidenote-number">41</label><input type="checkbox" id="tufte-sn-41" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">41</span> In other words, an optimal tree is the optimal one among all the possible trees, so an optimal root node won’t necessarily lead to an optimal tree.</span>.</p>
<p>An illustration of the risk of getting stuck in a local optimal solution of greedy optimization approaches is shown in Figure <a href="tree-models.html#fig:f2-localoptimal">19</a>. Where the algorithm gets started matters to where it ends up. For this reason, decision tree algorithms are often sensitive to data, i.e., it is not uncommon that a slight change of the dataset may cause a considerable change of the topology of the decision tree.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-localoptimal"></span>
<img src="graphics/2_localoptimal.png" alt="A greedy optimization approach starts its adventure from an **initial solution**. Here, $x_1$, $x_2$, $x_3$ are different initial solutions of $3$ usages of the optimization approach, and $3$ *optimal* solutions are found, while only one of them is *globally optimal*." width="250px"  />
<!--
<p class="caption marginnote">-->Figure 19: A greedy optimization approach starts its adventure from an <strong>initial solution</strong>. Here, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span> are different initial solutions of <span class="math inline">\(3\)</span> usages of the optimization approach, and <span class="math inline">\(3\)</span> <em>optimal</em> solutions are found, while only one of them is <em>globally optimal</em>.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Tree pruning.</em> To enhance the robustness of the decision tree learned by data-driven approaches such as the recursive partitioning, <strong>pruning</strong> methods could be used to cut down some unstable or insignificant branches. There are <strong>pre-pruning</strong> and <strong>post-pruning</strong> methods. Pre-pruning stops growing a tree when a pre-defined criterion is met. For example, one can set the <strong>depth of a tree</strong> (i.e., the depth of a node is the number of edges from the node to the tree’s root node; the depth of a tree is the maximum depth of its leaf nodes), or the minimum number of data points at the leaf nodes. These approaches need prior knowledge, and they may not necessarily reflect the characteristics of the particular dataset. More data-dependent approaches can be used. For example, we may set a minimum IG threshold to stop growing a tree when the IG is below the threshold. This may cause another problem, i.e., a small IG at an internal node does not necessarily mean its potential child nodes can only have smaller IG values. Therefore, pre-pruning can cause over-simplified trees and thus <strong>underfitted</strong> tree models. In other words, it may be too cautious.</p>
<p>In contrast, post-pruning prunes a tree after it is fully grown. A fully grown model aggressively spans the tree, i.e., by setting the depth of the tree as a large number. To pursue a fully grown tree is to mitigate the risk of underfit. The cost is that it may overfit the data, so post-pruning is needed. Post-pruning starts from the bottom of the tree. If removing an inner node (together with all the descendant nodes) does not increase the error <em>significantly</em>, then it should be pruned. The question is how to evaluate the significance of the increase of error<label for="tufte-sn-42" class="margin-toggle sidenote-number">42</label><input type="checkbox" id="tufte-sn-42" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">42</span> Interested readers may find the discussion in the Remarks section useful.</span>.</p>
<p>We will refer readers to <strong>Chapter 5</strong> for understanding more about concepts such as <strong>empirical error</strong> and <strong>generalization error</strong> . Understanding the difference between them is a key step towards maturity in data analytics. Like the difference between <em>money</em> and <em>currency</em>, the difference will be obvious to you as long as you have seen the difference.</p>
<p><em>Extensions and other considerations.</em></p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> In our data example in Table <a href="tree-models.html#tab:t2-3">3</a> we only have categorical variables, so candidate splitting rules could be defined relatively easier. For a continuous variable, one approach to identify candidate splitting rules is to order the observed values first, and then, use the average of each pair of consecutive values for splitting.</p></li>
<li><p> If the outcome variable is continuous, we can use the variance of the outcome variable to measure the “entropy” of a node, i.e.,</p></li>
</ul>
<p><span class="math display">\[v= \sum\nolimits_{n=1}\nolimits^N \left(\bar y - y_n\right)^2 ,\]</span></p>
<p>where <span class="math inline">\(y_{n=1,\cdots,N}\)</span> are the values of the outcome variable in the node, and <span class="math inline">\(\bar y\)</span> is the average of the outcome variable. And the information gain can be calculated similarly.</p>
<ul>
<li> Both pre-pruning and post-pruning are useful in practices, and it is hard to say which one is better than the other. There is a belief that post-pruning can often outperform pre-pruning. A better procedure is to use <strong>cross-validation</strong>.<label for="tufte-sn-43" class="margin-toggle sidenote-number">43</label><input type="checkbox" id="tufte-sn-43" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">43</span> Details are given in <strong>Chapter 5</strong>.</span> A popular pre-pruning parameter used in the R package <code>rpart</code> is <code>cp</code>, i.e., it sets a value such that all splits need to improve the IG by at least a factor of <code>cp</code> to be approved. This pre-pruning strategy works well in many applications.</li>
</ul>
<p><!-- end{itemize} --></p>
</div>
<div id="r-lab-1" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline.</em> We use <code>DX_bl</code> as the outcome variable that is binary<label for="tufte-sn-44" class="margin-toggle sidenote-number">44</label><input type="checkbox" id="tufte-sn-44" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">44</span> In <code>DX_bl</code>, <code>0</code> denotes normal subjects; <code>1</code> denotes diseased subjects.</span>. We use other variables (except <code>ID</code>, <code>TOTAL13</code> and <code>MMSCORE</code>) to predict <code>DX_bl</code>.</p>
<p><strong>Step 1</strong> loads the needed R packages and data into the workspace.</p>
<p></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="tree-models.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Key package for decision tree in R: </span></span>
<span id="cb19-2"><a href="tree-models.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rpart (for building the tree); </span></span>
<span id="cb19-3"><a href="tree-models.html#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># rpart.plot (for drawing the tree)</span></span>
<span id="cb19-4"><a href="tree-models.html#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb19-5"><a href="tree-models.html#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb19-6"><a href="tree-models.html#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb19-7"><a href="tree-models.html#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="tree-models.html#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb19-9"><a href="tree-models.html#cb19-9" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb19-10"><a href="tree-models.html#cb19-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb19-11"><a href="tree-models.html#cb19-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span></code></pre></div>
<p>
</p>
<p><strong>Step 2</strong> is about data preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="tree-models.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb20-2"><a href="tree-models.html#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and </span></span>
<span id="cb20-3"><a href="tree-models.html#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Y vector (outcome variable)</span></span>
<span id="cb20-4"><a href="tree-models.html#cb20-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb20-5"><a href="tree-models.html#cb20-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl</span>
<span id="cb20-6"><a href="tree-models.html#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="tree-models.html#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The following code makes sure the variable &quot;DX_bl&quot; </span></span>
<span id="cb20-8"><a href="tree-models.html#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># is a &quot;factor&quot;.</span></span>
<span id="cb20-9"><a href="tree-models.html#cb20-9" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, Y) </span>
<span id="cb20-10"><a href="tree-models.html#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This line is to &quot;factorize&quot; the variable &quot;DX_bl&quot;.</span></span>
<span id="cb20-11"><a href="tree-models.html#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># It denotes &quot;0&quot; as &quot;c0&quot; and &quot;1&quot; as &quot;c1&quot;,</span></span>
<span id="cb20-12"><a href="tree-models.html#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co"># to highlight the fact that</span></span>
<span id="cb20-13"><a href="tree-models.html#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;DX_bl&quot; is a factor variable, not a numerical variable</span></span>
<span id="cb20-14"><a href="tree-models.html#cb20-14" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Y) <span class="co"># as.factor is to convert any variable</span></span>
<span id="cb20-15"><a href="tree-models.html#cb20-15" aria-hidden="true" tabindex="-1"></a>                  <span class="co"># into the format as &quot;factor&quot; variable.</span></span>
<span id="cb20-16"><a href="tree-models.html#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="tree-models.html#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb20-18"><a href="tree-models.html#cb20-18" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb20-19"><a href="tree-models.html#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb20-20"><a href="tree-models.html#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="tree-models.html#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># generate the same random sequence</span></span>
<span id="cb20-22"><a href="tree-models.html#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb20-23"><a href="tree-models.html#cb20-23" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb20-24"><a href="tree-models.html#cb20-24" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb20-25"><a href="tree-models.html#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb20-26"><a href="tree-models.html#cb20-26" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-15"></span>
<img src="graphics/2_15.png" alt="The unpruned decision tree to predict `DX_bl`" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 20: The unpruned decision tree to predict <code>DX_bl</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 3</strong> is to use the <code>rpart()</code> function in the R package <code>rpart</code> to build the decision tree.</p>
<p></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="tree-models.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; use rpart to build the decision tree.</span></span>
<span id="cb21-2"><a href="tree-models.html#cb21-2" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data.train)</span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> is to use the <code>prp()</code> function to plot the decision tree<label for="tufte-sn-45" class="margin-toggle sidenote-number">45</label><input type="checkbox" id="tufte-sn-45" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">45</span> <code>prp()</code> is a capable function. It has many arguments to specify the details of how the tree should be drawn. Use <code>help(prp)</code> to see details.</span></p>
<p></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="tree-models.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; draw the tree</span></span>
<span id="cb22-2"><a href="tree-models.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree, <span class="at">nn.cex =</span> <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p>And the decision tree is shown in Figure <a href="tree-models.html#fig:f2-15">20</a>.</p>
<p><strong>Step 5</strong> is to prune the tree using the R function <code>prune()</code>. Remember that the parameter <code>cp</code> controls the model complexity<label for="tufte-sn-46" class="margin-toggle sidenote-number">46</label><input type="checkbox" id="tufte-sn-46" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">46</span> A larger <code>cp</code> leads to a less complex tree.</span>.</p>
<p>Let us try <code>cp</code> <span class="math inline">\(= 0.03\)</span>. This leads to a decision tree as shown in Figure <a href="tree-models.html#fig:f2-16">21</a>.</p>
<p></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="tree-models.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; prune the tree</span></span>
<span id="cb23-2"><a href="tree-models.html#cb23-2" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">prune</span>(tree,<span class="at">cp=</span><span class="fl">0.03</span>)</span>
<span id="cb23-3"><a href="tree-models.html#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree,<span class="at">nn.cex=</span><span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-16"></span>
<img src="graphics/2_16.png" alt="The pruned decision tree model to predict `DX_bl` of the AD data with `cp = 0.03`" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 21: The pruned decision tree model to predict <code>DX_bl</code> of the AD data with <code>cp = 0.03</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 6</strong> is to evaluate the trained model by predicting the testing data.</p>
<p></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="tree-models.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict using your tree model</span></span>
<span id="cb24-2"><a href="tree-models.html#cb24-2" aria-hidden="true" tabindex="-1"></a>pred.tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree, data.test, <span class="at">type=</span><span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p></p>
<p>And we can evaluate the prediction performance using error rate.</p>
<p></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="tree-models.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The following line calculates the prediction error</span></span>
<span id="cb25-2"><a href="tree-models.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rate (a number from 0 to 1) for a binary classification problem</span></span>
<span id="cb25-3"><a href="tree-models.html#cb25-3" aria-hidden="true" tabindex="-1"></a>err.tree <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(pred.tree <span class="sc">!=</span></span>
<span id="cb25-4"><a href="tree-models.html#cb25-4" aria-hidden="true" tabindex="-1"></a>                           data.test<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.tree)</span>
<span id="cb25-5"><a href="tree-models.html#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) which(pred.tree != data$DX_bl) identifies the locations</span></span>
<span id="cb25-6"><a href="tree-models.html#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">#    of the incorrect predictions;</span></span>
<span id="cb25-7"><a href="tree-models.html#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) length(any vector) returns the length of that vector;</span></span>
<span id="cb25-8"><a href="tree-models.html#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) thus, the ratio of incorrect prediction over the total</span></span>
<span id="cb25-9"><a href="tree-models.html#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">#    prediction is the prediction error</span></span>
<span id="cb25-10"><a href="tree-models.html#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(err.tree)</span></code></pre></div>
<p></p>
</div>
</div>
<p style="text-align: center;">
<a href="regression-models.html"><button class="btn btn-default">Previous</button></a>
<a href="remarks.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
