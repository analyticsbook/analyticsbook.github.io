<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Remarks | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Remarks | Data Analytics">

<title>Remarks | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="remarks-4" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="derivation-of-the-em-algorithm" class="section level3 unnumbered">
<h3>Derivation of the EM algorithm</h3>
<p>The aforementioned two-step iterative algorithm (i.e., as outlined in Figure <a href="clustering.html#fig:f6-cluster-cycle">108</a>) illustrates how the <strong>EM Algorithm</strong> works. We have assumed that the two-step iterative algorithm would converge. Luckily, it had been proved that the EM Algorithm generally would converge<label for="tufte-sn-152" class="margin-toggle sidenote-number">152</label><input type="checkbox" id="tufte-sn-152" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">152</span> Wu, J., <em>On the Convergence Properties of the EM Algorithm</em>, The Annals of Statistics, Volume 11, Number 1, Pages 95-103, 1983.</span>.</p>
<!-- % Let's start with the Gaussian mixture model (GMM), that has been one of the most popular clustering model. GMM assumes that the data come from not just one distribution but a few. As shown in Figure \@ref(fig:f6-13) , the data is sampled from a mix of 4 distributions.  -->
<!-- % ```{r eval=FALSE,tidy=FALSE} -->
<!-- % # Simulate a clustering structure -->
<!-- % X <- c(rnorm(200, 0, 1), rnorm(200, 10,2), rnorm(200,20,1), rnorm(200,40, 2)) -->
<!-- % Y <- c(rnorm(800, 0, 1)) -->
<!-- % plot(X,Y, ylim = c(-5, 5), pch = 19, col = "gray25") -->
<!-- % ``` -->
<!-- % \begin{marginfigure} -->
<!-- %  \centering -->
<!-- %  \includegraphics{6_13.png} -->
<!-- %  \caption{A mixture of four Gaussian distributions} -->
<!-- %  \label{fig:6-13} -->
<!-- % \end{marginfigure} -->
<p>The task of the EM algorithm is to learn the unknown parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span> from a given dataset. The <span class="math inline">\(\boldsymbol{\Theta}\)</span> includes</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> [1.] The parameters of the <span class="math inline">\(M\)</span> Gaussian distributions: <span class="math inline">\(\left\{\boldsymbol{\mu}_{m}, \boldsymbol{\Sigma}_{m}, m=1,2, \ldots, M\right\}\)</span>.</p></li>
<li><p> [2.] The probability vector <span class="math inline">\(\boldsymbol{\pi}\)</span> that includes the elements <span class="math inline">\(\left\{\pi_{m}, m=1,2, \ldots, M\right\}\)</span>.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>Don’t forget the binary indicator variable for each data point, denoted as <span class="math inline">\(z_{n m}\)</span>: <span class="math inline">\(z_{n m}=1\)</span> indicates that the data point <span class="math inline">\(x_{n}\)</span> was sampled from the <span class="math inline">\(m^{th}\)</span> cluster<label for="tufte-sn-153" class="margin-toggle sidenote-number">153</label><input type="checkbox" id="tufte-sn-153" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">153</span> The reason that <span class="math inline">\(z_{n m}\)</span> is not included in <span class="math inline">\(\boldsymbol{\Theta}\)</span>, as it could be seen later, after the presentation of the EM algorithm, is that <span class="math inline">\(z_{n m}\)</span> provides a bridge to facilitate the learning of <span class="math inline">\(\boldsymbol{\Theta}\)</span>. They are not essential parameters of the model, although they are useful to facilitate the estimation of the parameters of the model. Entities like <span class="math inline">\(z_{n m}\)</span> are often called <strong>latent variables</strong> instead of <em>parameters</em>.</span>.</p>
<p><em>The Likelihood Function.</em> To learn these parameters from data, like in the logistic regression model, we derive a likelihood function to connect the data and parameters. For GMM, we cannot write <span class="math inline">\(p\left(\boldsymbol{x}_{n} | \boldsymbol{\Theta}\right)\)</span> directly. But it is possible to write <span class="math inline">\(p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right)\)</span> directly<label for="tufte-sn-154" class="margin-toggle sidenote-number">154</label><input type="checkbox" id="tufte-sn-154" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">154</span> That is what <span class="math inline">\(z_{n m}\)</span> is needed for.</span></p>
<p><span class="math display" id="eq:6-likelihood-xn">\[\begin{equation}
    p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) = \prod_{m=1}^{M}\left[p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right) p\left(z_{n m}=1\right)\right]^{z_{n m}}.
\tag{35}
\end{equation}\]</span></p>
<p>We apply <em>log</em> on Eq. <a href="remarks-4.html#eq:6-likelihood-xn">(35)</a> and get the log-likelihood function in Eq <a href="remarks-4.html#eq:6-loglike-xn">(36)</a><label for="tufte-sn-155" class="margin-toggle sidenote-number">155</label><input type="checkbox" id="tufte-sn-155" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">155</span> Note that, by definition, <span class="math inline">\(\pi_m = p\left(z_{n m}=1\right)\)</span>.</span></p>
<p><span class="math display" id="eq:6-loglike-xn">\[\begin{equation}
    \log p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) = \sum_{m=1}^{M}\left[z_{n m} \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right)+z_{n m} \log \pi_{m}\right]. 
\tag{36}
\end{equation}\]</span></p>
<p>It is known that<label for="tufte-sn-156" class="margin-toggle sidenote-number">156</label><input type="checkbox" id="tufte-sn-156" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">156</span> I.e., by the definition of multivariate normal distribution; interested readers may see the <strong>Appendix</strong> of this book for a brief review. Here, the constant term <span class="math inline">\((2 \pi)^{-p / 2}\)</span> in the density function of the multivariate normal distribution is ignored, so “<span class="math inline">\(\propto\)</span>” is used instead of “<span class="math inline">\(=\)</span>.”</span></p>
<p><span class="math display" id="eq:6-like-mnv-xn">\[\begin{equation}
    p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right) \propto \left|\boldsymbol{\Sigma}_{m}\right|^{-1 / 2} \exp \left\{-\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\right\}.
\tag{37}
\end{equation}\]</span></p>
<p>Plug Eq. <a href="remarks-4.html#eq:6-like-mnv-xn">(37)</a> into Eq. <a href="remarks-4.html#eq:6-loglike-xn">(36)</a>, we get</p>
<p><span class="math display" id="eq:6-loglike-xn2">\[\begin{equation}
\begin{gathered}
    \log p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) \propto \\
    \sum_{m=1}^{M}\left[z_{n m} \left( - \frac{1}{2}\log \left|\boldsymbol{\Sigma}_{m}\right|  -\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)+\right. z_{n m} \log \pi_{m} \right].
\end{gathered}
\tag{38}
\end{equation}\]</span></p>
<p>As there are <span class="math inline">\(N\)</span> data points, the complete log-likelihood function is defined as</p>
<p><span class="math display" id="eq:6-complete-loglike">\[\begin{equation}
    l(\boldsymbol{\Theta}) = \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\Theta}) = \log \prod_{n=1}^{N} p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right).
\tag{39}
\end{equation}\]</span></p>
<p>With Eq. <a href="remarks-4.html#eq:6-loglike-xn2">(38)</a>, Eq. <a href="remarks-4.html#eq:6-complete-loglike">(39)</a> can be rewritten as</p>
<p><span class="math display" id="eq:6-complete-loglike2">\[\begin{equation}
\begin{gathered}
    l(\boldsymbol{\Theta}) \propto \\
   \sum_{n=1}^{N} \sum_{m=1}^{M}\left[z_{n m} \left( - \frac{1}{2}\log \left|\boldsymbol{\Sigma}_{m}\right|  -\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)+\right. z_{n m} \log \pi_{m} \right].
\end{gathered}
\tag{40}
\end{equation}\]</span></p>
<p>Now we have an <em>explicit</em> form of <span class="math inline">\(l(\boldsymbol{\Theta})\)</span>, based on which we use an optimization algorithm to search for the best estimate of <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<p>Recall that <span class="math inline">\(z_{n m}\)</span> is unknown. Here comes the <em>initialization</em> again. Following the idea we have implemented in the data example shown in Table <a href="clustering.html#tab:t6-example">25</a>, we propose the following strategy:</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Initialization. Either initialize <span class="math inline">\(\left\{z_{nm}, n=1,2, \ldots, N; m=1,2, \ldots, M\right\}\)</span> or <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p></li>
<li><p> E-step. We can estimate <span class="math inline">\(z_{n m}\)</span> if we have known <span class="math inline">\(\boldsymbol{\Theta}\)</span> (i.e., given <span class="math inline">\(\boldsymbol{\Theta}\)</span>), the best estimate of <span class="math inline">\(z_{n m}\)</span> is the expectation of <span class="math inline">\(z_{n m}\)</span> where the expectation is taken regarding the distribution <span class="math inline">\(p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)\)</span> (i.e., denoted as <span class="math inline">\(\left\langle Z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}\)</span>). By definition, we have</p></li>
</ul>
<p><span class="math display" id="eq:6-Eznm">\[\begin{equation}
\begin{gathered}
    \left\langle z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}=1\cdot p\left(z_{n m}=1 | \boldsymbol{x}_{n},{\boldsymbol{\Theta}}\right)+0 \cdot p\left(z_{n m}=0 | \boldsymbol{x}_{n}, {\boldsymbol{\Theta}}\right).
\end{gathered}
\tag{41}
\end{equation}\]</span></p>
<p>It is known that</p>
<p><span class="math display" id="eq:6-znm">\[\begin{equation}
    p\left(z_{n m}=1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)=\frac{p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right) \pi_{m}}{\sum_{k=1}^{M} p\left(\boldsymbol{x}_{n} | z_{n k}=1, \boldsymbol{\Theta}\right) \pi_{k}}.
\tag{42}
\end{equation}\]</span></p>
<p>Thus,</p>
<p><span class="math display" id="eq:6-Eznm2">\[\begin{equation}
\begin{gathered}
    \left\langle z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}= \frac{p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right) \pi_{m}}{\sum_{k=1}^{M}  p\left(\boldsymbol{x}_{n} | z_{n k}=1, \boldsymbol{\Theta}\right) \pi_{k}}.
\end{gathered}
\tag{43}
\end{equation}\]</span></p>
<ul>
<li> M-step. Then, we derive the expectation of <span class="math inline">\(l(\boldsymbol{\Theta})\)</span> regarding the distribution <span class="math inline">\(p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)\)</span></li>
</ul>
<p><span class="math display" id="eq:6-likehihood-eznm">\[\begin{equation}
\begin{gathered}
    \langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}=\sum_{n=1}^{N} \sum_{m=1}^{M}\left[\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)+\right. \\
    \left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \log \pi_{m} ].
\end{gathered}   
\tag{44}
\end{equation}\]</span></p>
<p>And we optimize Eq. <a href="remarks-4.html#eq:6-likehihood-eznm">(44)</a> for <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<ul>
<li> Repeat the E-step and M-step. With the updated <span class="math inline">\(\boldsymbol{\Theta}\)</span>, we go back to the estimate of <span class="math inline">\(z_{n m}\)</span> using Eq. <a href="remarks-4.html#eq:6-Eznm2">(43)</a>, and then, feed the new estimate of <span class="math inline">\(z_{n m}\)</span> into Eq. <a href="remarks-4.html#eq:6-likehihood-eznm">(44)</a>, and solve for <span class="math inline">\(\boldsymbol{\Theta}\)</span> again. Repeat these iterations, until all the parameters in the iterations don’t change significantly<label for="tufte-sn-157" class="margin-toggle sidenote-number">157</label><input type="checkbox" id="tufte-sn-157" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">157</span> Usually, we define a tolerance, e.g., the difference between two consecutive estimates of <span class="math inline">\(\boldsymbol{\Theta}\)</span> is numerically bounded, such as <span class="math inline">\(10^{-4}\)</span>.</span>.</li>
</ul>
<p><!-- end{enumerate} --></p>
<p><em>More about the M-step.</em> To estimate the parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span>, in the M-step we use the First Derivative Test again and take derivatives of <span class="math inline">\(\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> (i.e., as shown in Eq. <a href="remarks-4.html#eq:6-likehihood-eznm">(44)</a>) regarding <span class="math inline">\(\boldsymbol{\Theta}\)</span> and put the derivatives equal to zero.</p>
<p>For <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span>, we have</p>
<p><span class="math display" id="eq:6-diff-mu">\[\begin{equation}
    \frac{\partial\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}}{\partial \boldsymbol{\mu}_{m}}=\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\mu}_{m}}=\boldsymbol{0}.
\tag{45}
\end{equation}\]</span></p>
<p>Based on Eq. <a href="remarks-4.html#eq:6-like-mnv-xn">(37)</a>, we can derive</p>
<p><span class="math display" id="eq:6-diff-mu2">\[\begin{equation}
    \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\mu}_{m}}=
    -\frac{1}{2} \frac{\partial\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)}{\partial \boldsymbol{\mu}_{m}}=\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}.
\tag{46}
\end{equation}\]</span></p>
<p>Putting the result of Eq. <a href="remarks-4.html#eq:6-diff-mu2">(46)</a> into Eq. <a href="remarks-4.html#eq:6-diff-mu">(45)</a>, we can estimate <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span> by solving Eq. <a href="remarks-4.html#eq:6-diff-mu">(45)</a></p>
<p><span class="math display" id="eq:6-solution-mu">\[\begin{equation}
    \boldsymbol{\mu}_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \boldsymbol{x}_{n}}{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}.
\tag{47}
\end{equation}\]</span></p>
<p>Similarly, we take derivatives of <span class="math inline">\(\langle l({\boldsymbol{\Theta}})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> regarding <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> and put the derivatives equal to zero</p>
<p><span class="math display" id="eq:6-diff-sigma">\[\begin{equation}
    \frac{\partial\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}}{\partial \boldsymbol{\Sigma}_{m}}=\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\Sigma}_{m}}=\boldsymbol{O}.
\tag{48}
\end{equation}\]</span></p>
<p>Based on Eq. <a href="remarks-4.html#eq:6-like-mnv-xn">(37)</a>, we can derive</p>
<p><span class="math display" id="eq:6-diff-sigma2">\[\begin{equation}
\begin{gathered}
    \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\Sigma}_{m}} =  \\
    \frac{1}{2} \frac{\partial\left\{\left|\boldsymbol{\Sigma}_{m}\right|^{-1 / 2}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\right\}}{\partial \boldsymbol{\Sigma}_{m}}=\frac{1}{2}\left[\boldsymbol{\Sigma}_{m}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right].
\end{gathered}
\tag{49}
\end{equation}\]</span></p>
<p>Plug Eq. <a href="remarks-4.html#eq:6-diff-sigma2">(49)</a> into Eq. <a href="remarks-4.html#eq:6-diff-sigma">(48)</a>, we have</p>
<p><span class="math display" id="eq:6-diff-sigma3">\[\begin{equation}
    \sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1 | \boldsymbol{X}, \boldsymbol{\Theta}\right)}\left[\boldsymbol{\Sigma}_{m}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right]=\boldsymbol{O}.
\tag{50}
\end{equation}\]</span></p>
<p>Solving Eq. <a href="remarks-4.html#eq:6-diff-sigma3">(50)</a>, we estimate <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> as</p>
<p><span class="math display" id="eq:6-solution-sigma">\[\begin{equation}
    \boldsymbol{\Sigma}_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1| \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \left[\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right]}{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1| \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}.
\tag{51}
\end{equation}\]</span></p>
<p>Lastly, to estimate <span class="math inline">\(\pi_{m}\)</span>, recall that <span class="math inline">\(\pi_m\)</span> is the percentage of the data points in the whole mix that come from the <span class="math inline">\(m^{th}\)</span> distribution, and <span class="math inline">\(\pi_m = p\left(z_{n m}=1\right)\)</span>, we can estimate <span class="math inline">\(\pi_{m}\)</span> as</p>
<p><span class="math display" id="eq:6-solution-pi">\[\begin{equation}
    \pi_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}{N}.
\tag{52}
\end{equation}\]</span></p>
</div>
<div id="convergence-of-the-em-algorithm" class="section level3 unnumbered">
<h3>Convergence of the EM Algorithm</h3>
<p>Readers may have found that Eq. <a href="remarks-4.html#eq:6-complete-loglike2">(40)</a> gives us the form of <span class="math inline">\(\log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\)</span>, that is what is denoted as <span class="math inline">\(l(\boldsymbol{\Theta})\)</span>. But, since <span class="math inline">\(\boldsymbol{Z}\)</span> is the latent variable and not part of the parameters, the objective function of the GMM model should be</p>
<p><span class="math display" id="eq:6-EMobj1">\[\begin{equation}
    \log p(\boldsymbol{X}| \boldsymbol{\Theta})=\log \int p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\Theta}) d \boldsymbol{Z}.
\tag{53}
\end{equation}\]</span></p>
<p>But this is not what has been done in the EM algorithm. Instead, the EM algorithm solves for Eq. <a href="remarks-4.html#eq:6-likehihood-eznm">(44)</a>, that is essentially</p>
<p><span class="math display" id="eq:6-EMobj2">\[\begin{equation}
    \langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})} =
    \int \log p(\boldsymbol{X}, \boldsymbol{Z} ; \boldsymbol{\Theta}) p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta}) d \boldsymbol{Z}.
\tag{54}
\end{equation}\]</span></p>
<p>How does the solving of Eq. <a href="remarks-4.html#eq:6-EMobj2">(54)</a> help the solving of Eq. <a href="remarks-4.html#eq:6-EMobj1">(53)</a>?</p>
<p>The power of the EM algorithm draws on <strong>Jensen’s inequality</strong> . Let <span class="math inline">\(f\)</span> be a convex function defined on an interval <span class="math inline">\(I\)</span>. If <span class="math inline">\(x_{1}, x_{2}, \ldots x_{n} \in I \text { and } \gamma_{1}, \gamma_{2}, \ldots \gamma_{n} \geq0\)</span> with <span class="math inline">\(\sum_{i=1}^{n} \gamma_{i}=1\)</span>, then based on Jensen’s inequality, it is known that <span class="math inline">\(f\left(\sum_{i=1}^{n} \gamma_{i} x_{i}\right) \leq \sum_{i=1}^{n} \gamma_{i} f\left(x_{i}\right)\)</span>. Let’s apply this result to analyze the EM algorithm.</p>
<p>First, notice that</p>
<p><span class="math display">\[
\log p(\boldsymbol{X} | \boldsymbol \Theta)=\log \int p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}
\]</span></p>
<p><span class="math display">\[
=\log \int Q(\boldsymbol{Z}) \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
\]</span></p>
<p>Here, <span class="math inline">\(Q(\boldsymbol{Z})\)</span> is any distribution of <span class="math inline">\(\boldsymbol{Z}\)</span>. In the EM algorithm</p>
<p><span class="math display">\[Q(\boldsymbol{Z})=p(\boldsymbol{Z} | \boldsymbol{X}, \Theta).\]</span></p>
<p>Using Jensen’s inequality here, we have</p>
<p><span class="math display">\[
\log \int Q(\boldsymbol{Z}) \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}
\]</span></p>
<p><span class="math display">\[
\geq \int Q(\boldsymbol{Z}) \log \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
\]</span></p>
<p>Since</p>
<p><span class="math display">\[
\int Q(\boldsymbol{Z}) \log \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
\]</span></p>
<p><span class="math display">\[
=\int Q(\boldsymbol{Z}) \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}-\int Q(\boldsymbol{Z}) Q(\boldsymbol{Z}) d \boldsymbol{Z},
\]</span></p>
<p>and <span class="math inline">\(\int Q(\boldsymbol{Z}) Q(\boldsymbol{Z}) d \boldsymbol{Z}\)</span> is quadratic and thus non-negative,</p>
<p>our final result is</p>
<p><span class="math display" id="eq:6-JI">\[\begin{equation}
    \log p(\boldsymbol{X} | \boldsymbol \Theta) \geq \int Q(\boldsymbol{Z}) \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}.
\tag{55}
\end{equation}\]</span></p>
<p>When we set <span class="math inline">\(Q(\boldsymbol{Z}) = p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, Eq. <a href="remarks-4.html#eq:6-JI">(55)</a> is rewritten as</p>
<p><span class="math display" id="eq:6-JI2">\[\begin{equation}
    \log p(\boldsymbol{X} | \boldsymbol \Theta) \geq \langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}.
\tag{56}
\end{equation}\]</span></p>
<p>Eq. <a href="remarks-4.html#eq:6-JI2">(56)</a> reveals that <span class="math inline">\(\langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> is the <strong>lower bound</strong> of <span class="math inline">\(\log p(\boldsymbol{X} | \boldsymbol \Theta)\)</span>. Thus, maximization of <span class="math inline">\(\langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> can only increase the value of <span class="math inline">\(\log p(\boldsymbol{X} | \boldsymbol \Theta)\)</span>. This is why solving Eq. <a href="remarks-4.html#eq:6-EMobj2">(54)</a> helps the solving of Eq. <a href="remarks-4.html#eq:6-EMobj1">(53)</a>. This is the foundation of the effectiveness of the EM algorithm. The EM algorithm is often used to solve for problems that involve latent variables. Note that, <span class="math inline">\(Q(\boldsymbol{Z})\)</span> could be any distribution rather than <span class="math inline">\(p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, and Eq. <a href="remarks-4.html#eq:6-JI">(55)</a> still holds. In applications where we could not explicitly derive <span class="math inline">\(p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, a surrogate distribution is used for <span class="math inline">\(Q(\boldsymbol{Z})\)</span>. This variant of the EM algorithm is called the <em>variational inference</em><label for="tufte-sn-158" class="margin-toggle sidenote-number">158</label><input type="checkbox" id="tufte-sn-158" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">158</span> A good starting point to know more about variational inference within a context of GMM, see: David, B., Kucukelbir, A. and McAuliffe, J., <em>Variational Inference: A Review for Statisticians</em>, Journal of the American Statistical Association, Volume 112, Number 518, Pages 859-877, 2017.</span>.</p>
</div>
<div id="clustering-by-random-forest" class="section level3 unnumbered">
<h3>Clustering by random forest</h3>
<p>Many clustering algorithms have been developed. The random forest model can be used for clustering as well. This is a byproduct utility of a random forest model. One advantage of using random forest for clustering is that it can cluster data points with mixed types of variables. To conduct clustering in random forests is to extract the distance information between data points that have been learned by the random forest model. There are multiple ways to do so. For example, one approach<label for="tufte-sn-159" class="margin-toggle sidenote-number">159</label><input type="checkbox" id="tufte-sn-159" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">159</span> Shi, T. and Horvath, S., <em>Unsupervised learning with random forest predictors.</em> Journal of Computational and Graphical Statistics, Volume 15, Issue 1, Pages 118-138, 2006.</span> that has been implemented in the R package <code>randomForests</code> is to generate a synthetic dataset with the same size as the original dataset, e.g., randomly generate the measurements of each variable using its empirical marginal distribution. The original dataset is taken as one class, while the synthetic dataset is taken as another class. Since the random forest model is used to classify the two classes, it will stress on the difference between the two datasets, which is, the variable dependency that is embedded in the original dataset but lost in the synthetic dataset because of the way the synthetic dataset is generated. Hence, each tree will be enriched with splitting variables that are dependent on other variables. After the random forest model is built, a distance between any pair of two data points can be calculated based on the frequency of this pair of data points existing in the same nodes of the random forest model. With this distance information, distance-based clustering algorithms such as the <em>hierarchical clustering</em> or <em>K-means clustering</em><label for="tufte-sn-160" class="margin-toggle sidenote-number">160</label><input type="checkbox" id="tufte-sn-160" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">160</span> E.g., both could be implemented using the R package <code>cluster</code>.</span> algorithms can be applied to detect the clusters.</p>
<p>In the following example, we generate a dataset with two clusters. The clusters produced from the random forest model are shown in Figure <a href="remarks-4.html#fig:f6-16">112</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-16"></span>
<img src="graphics/6_16.png" alt="Clusters produced by the random forest model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 112: Clusters produced by the random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We then use the following R code to apply a random forest model on this dataset to find the clusters. It can be seen that the clusters are reasonably recovered by the random forest model.</p>
<p></p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="remarks-4.html#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>(<span class="at">all =</span> <span class="cn">TRUE</span>))</span>
<span id="cb142-2"><a href="remarks-4.html#cb142-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb142-3"><a href="remarks-4.html#cb142-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb142-4"><a href="remarks-4.html#cb142-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb142-5"><a href="remarks-4.html#cb142-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb142-6"><a href="remarks-4.html#cb142-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb142-7"><a href="remarks-4.html#cb142-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb142-8"><a href="remarks-4.html#cb142-8" aria-hidden="true" tabindex="-1"></a>ndata <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb142-9"><a href="remarks-4.html#cb142-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-10"><a href="remarks-4.html#cb142-10" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb142-11"><a href="remarks-4.html#cb142-11" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>), sigma)</span>
<span id="cb142-12"><a href="remarks-4.html#cb142-12" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">2</span>), sigma)</span>
<span id="cb142-13"><a href="remarks-4.html#cb142-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data1, data2)</span>
<span id="cb142-14"><a href="remarks-4.html#cb142-14" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(data)</span>
<span id="cb142-15"><a href="remarks-4.html#cb142-15" aria-hidden="true" tabindex="-1"></a>prox <span class="ot">&lt;-</span> rf<span class="sc">$</span>proximity</span>
<span id="cb142-16"><a href="remarks-4.html#cb142-16" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">pam</span>(prox, <span class="dv">2</span>)</span>
<span id="cb142-17"><a href="remarks-4.html#cb142-17" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(data)</span>
<span id="cb142-18"><a href="remarks-4.html#cb142-18" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>cluster <span class="ot">&lt;-</span> <span class="fu">as.character</span>(clusters<span class="sc">$</span>clustering)</span>
<span id="cb142-19"><a href="remarks-4.html#cb142-19" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> V1, <span class="at">y =</span> V2, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb142-20"><a href="remarks-4.html#cb142-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Data points&#39;</span>)</span></code></pre></div>
<p></p>
</div>
<div id="clustering-based-prediction-models" class="section level3 unnumbered">
<h3>Clustering-based prediction models</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-clusterwisepred"></span>
<img src="graphics/6_clusterwisepred.png" alt="Clustering-based prediction models" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 113: Clustering-based prediction models<!--</p>-->
<!--</div>--></span>
</p>
<p>
<!-- %[width=0.6\textwidth] --></p>
<p>As we have mentioned, clustering is a flexible concept. And it could be used in a combination of methods. Figure <a href="remarks-4.html#fig:f6-clusterwisepred">113</a> illustrates the basic idea of <em>clustering-based prediction models</em>. It applies a clustering algorithm first on the data and then builds a model for each cluster. As a data analytics strategy, we could combine different clustering algorithms and prediction models that are appropriate for an application context. There are also integrated algorithms that have articulated this strategy on the formulation level. For example, the <em>Treed Regression</em> method<label for="tufte-sn-161" class="margin-toggle sidenote-number">161</label><input type="checkbox" id="tufte-sn-161" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">161</span> Alexander, W. and Grimshaw, S., <em>Treed regression.</em> Journal of Computational and Graphical Statistics, Volume 5, Issue 2, Pages 156-175, 1996.</span>
is one example that proposed to build a tree to stratify the dataset first, and then, create regression models on the leaf nodes—here, each leaf node is a cluster. Similarly, the <em>logistic model trees</em><label for="tufte-sn-162" class="margin-toggle sidenote-number">162</label><input type="checkbox" id="tufte-sn-162" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">162</span> Landwehr, N., Hall, M. and Frank, E. <em>Logistic model trees.</em> Machine Learning, Volume 59, Issue 1, Pages 161–205, 2004.</span> also use a tree model to cluster data points into different leaf nodes and build different logistic regression model for each leaf node. Motivated by this line of thought, more models have been developed with different combination of tree models and prediction models (or other types of statistical models) on the leaf nodes<label for="tufte-sn-163" class="margin-toggle sidenote-number">163</label><input type="checkbox" id="tufte-sn-163" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">163</span> Gramacy, R. and Lee, H. <em>Bayesian treed Gaussian process models with an application to computer modeling.</em> Journal of American Statistical Association, Volume 103, Issue 483, Pages 1119-1130, 2008.</span>.<label for="tufte-sn-164" class="margin-toggle sidenote-number">164</label><input type="checkbox" id="tufte-sn-164" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">164</span> Liu, H., Chen, X., Lafferty, J. and Wasserman, L. <em>Graph-valued regression.</em> In the Proceeding of Advances in Neural Information Processing Systems 23 (NIPS), 2010.</span></p>
</div>
</div>
<p style="text-align: center;">
<a href="clustering.html"><button class="btn btn-default">Previous</button></a>
<a href="exercises-4.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
