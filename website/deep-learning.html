<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Deep learning | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Deep learning | Data Analytics">

<title>Deep learning | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="deep-learning" class="section level2 unnumbered">
<h2>Deep learning</h2>
<p>To know more about “deep learning” , we need to start with its name. The word “deep” is ambiguous but expressive, undetermined but significant. This inviting gesture may have a dazzling effect, but it is based on a specific reason: a deep neural network model is truly deep in terms of its architecture—from input variables to output variables there are many layers in between. Other than that, it is not different from other models in this book. The basic framework of learning as shown in Figure <a href="overview.html#fig:f2-1">2</a> and Eq. <a href="overview.html#eq:ch2-genericmodel">(1)</a> in <strong>Chapter 2</strong> still holds true for deep learning.</p>
<p>The word “deep” doesn’t imply that other models we have learned so far are not deep. Many models have been studied in great depth, such as the linear models<label for="tufte-sn-245" class="margin-toggle sidenote-number">245</label><input type="checkbox" id="tufte-sn-245" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">245</span> Anderson, T. W., <em>An Introduction to Multivariate Statistical Analysis</em>, Wiley, 3rd edition, 2003.</span> and the learning theory developed for the support vector machine<label for="tufte-sn-246" class="margin-toggle sidenote-number">246</label><input type="checkbox" id="tufte-sn-246" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">246</span> Vapnik, V., <em>The Nature of Statistical Learning Theory</em>, Springer, 2000.</span>. In this chapter, we will refer to deep learning, specifically to those neural network (NN) models that have many hidden layers, because for NN models we could take the word “deep” at face value—if a model looks deep, it is a deep model. This superficiality, however, builds on a solid foundation<label for="tufte-sn-247" class="margin-toggle sidenote-number">247</label><input type="checkbox" id="tufte-sn-247" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">247</span> E.g., the Universal approximation theorem; please refer to Hornik, K., Approximation Capabilities of Multilayer Feedforward Networks, <em>Neural Networks</em>, Volume 4, Issue 2, Pages 251-257, 1991.</span>: a neural network with a more complex architecture means a more complex form for <span class="math inline">\(f(x)\)</span> in Eq. <a href="overview.html#eq:ch2-genericmodel">(1)</a>. In other words, this is an attractive proposal, since it suggests we can easily build up depth and capacity of the model by merely increasing its visual complexity. And there have been tools that allow users to drag ready-made modules and piece them together to create the architecture of the NN model they’d like to build, and automatically translate the architecture into its mathematical form and carry out the computational tasks for model training and prediction<label for="tufte-sn-248" class="margin-toggle sidenote-number">248</label><input type="checkbox" id="tufte-sn-248" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">248</span> E.g., TensorFlow <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>.</span>.</p>
<div id="rationale-and-formulation-16" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><em>An architecture means a function.</em> We have mentioned in <strong>Chapter 2</strong> that the data modeling methods seek explicit forms of <span class="math inline">\(f(x)\)</span> in Eq. <a href="overview.html#eq:ch2-genericmodel">(1)</a>, while algorithmic modeling methods seek implicit forms. Deep models bend the two. It is like an algorithmic modeling method that you don’t need to write up the specific form of <span class="math inline">\(f(x)\)</span>, while on the other hand, in theory you could write up <span class="math inline">\(f(x)\)</span> after you have had the architecture<label for="tufte-sn-249" class="margin-toggle sidenote-number">249</label><input type="checkbox" id="tufte-sn-249" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">249</span> In this sense, it is also like the kernel trick used in the SVM model. Remember that in <strong>Chapter 7</strong> we have seen that by using the kernel function in SVM, an implicit transformation of the variables is achieved, and we usually do not know what is the explicit form of <span class="math inline">\(\phi(x)\)</span> the SVM model encodes, but in theory there is such a form of <span class="math inline">\(\phi(x)\)</span>.</span>.</p>
<p>The architecture of a NN model could be quite expressive, i.e., Figure <a href="deep-learning.html#fig:f10-nn-architecture">175</a> shows an architecture of a neural network model with one layer that is flexible enough to include existing models such as the linear regression model, logistic regression model, and SVM, as shown in Table <a href="deep-learning.html#tab:t10-NNexample">53</a>.</p>
<p></p>
<div class="figure"><span id="fig:f10-nn-architecture"></span>
<p class="caption marginnote shownote">
Figure 175: Architecture of a simple neural network model. The figure is drawn using Alex LeNail’s online tool: <a href="http://alexlenail.me/NN-SVG/index.html">http://alexlenail.me/NN-SVG/index.html</a>.
</p>
<img src="graphics/10_nn_architecture.png" alt="Architecture of a simple neural network model. The figure is drawn using Alex LeNail's online tool: [http://alexlenail.me/NN-SVG/index.html](http://alexlenail.me/NN-SVG/index.html)." width="100%"  />
</div>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-NNexample">Table 53: </span>Expression of some models using the architecture of a one-layer neural network in Figure <a href="deep-learning.html#fig:f10-nn-architecture">175</a></span><!--</caption>--></p>
<table>
<colgroup>
<col width="17%" />
<col width="28%" />
<col width="54%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Model</strong></th>
<th align="left"><strong>Activation Function <span class="math inline">\(\Phi\)</span></strong></th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear regression</td>
<td align="left">Linear: <span class="math inline">\(\Phi(z)=z\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\left(y-\sum_{i=1}^{p} w_{i} x_{i}\right)^2\)</span></td>
</tr>
<tr class="even">
<td align="left">Logistic regression</td>
<td align="left">Sigmoid: <span class="math inline">\(\Phi(z)=\frac{1}{1-e^{-z}}\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\log(1+\exp[-y\sum_{i=1}^{p} w_{i} x_{i}])\)</span></td>
</tr>
<tr class="odd">
<td align="left">Support vector machine</td>
<td align="left">Null: <span class="math inline">\(\Phi(z)=z\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\max(0,1-y\sum_{i=1}^{p} w_{i} x_{i})\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The NN structure shown in Figure <a href="deep-learning.html#fig:f10-nn-architecture">175</a> is a basic form of NN architecture that is called the <strong>perceptron</strong> . As a basic form, it is a module that could be repeatedly used in different kinds of composition, e.g., in parallel, concatenation, or in a sequence. The basic forms are also called architectural primitives or foundational building blocks. Most deep architectures are built by combining these architectural primitives. Figures <a href="deep-learning.html#fig:f10-nn-composition">176</a> and <a href="deep-learning.html#fig:f10-nn-composition2">177</a> show two examples. There have been many of those basic forms developed. Softwares such as TensorFlow build on this concept by allowing users to use graphic user interface (GUI) to compose the architecture of their deep networks using these building blocks<label for="tufte-sn-250" class="margin-toggle sidenote-number">250</label><input type="checkbox" id="tufte-sn-250" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">250</span> For introduction of TensorFlow, readers may check out this book: Ramsundar, B. and Zadeh, R. <em>TensorFlow for Deep Learning: from Linear Regression to Reinforcement Learning</em>, O’Reilly Media, 2017.</span>.</p>
<p>As we have mentioned, for NN models there are theories showing that if a model looks deep, it is a deep model. The universal approximation theorem has shown that a NN model with one hidden layer could characterize all smooth functions. While there is no guarantee that in practice adding more layers will always be better, the theoretical results did imply that is the right direction.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-nn-composition"></span>
<img src="graphics/10_nn_composition.png" alt="Build more complicated NN models with a basic form" width="100%"  />
<p class="caption marginnote shownote">
Figure 176: Build more complicated NN models with a basic form
</p>
</div>
<p></p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-nn-composition2"></span>
<img src="graphics/10_nn_composition2.png" alt="Build deeper NN models with basic forms and activation functions" width="100%"  />
<p class="caption marginnote shownote">
Figure 177: Build deeper NN models with basic forms and activation functions
</p>
</div>
<p></p>
<p>Recall the XOR problem introduced in <strong>Chapter 7</strong> as shown in Figure <a href="support-vector-machine.html#fig:f7-8">121</a>. With a slight modification of the problem to facilitate the presentation here, the dataset has <span class="math inline">\(4\)</span> data points</p>
<p><span class="math display">\[ 
\begin{array}{l}{\boldsymbol{x}_{1}=(0,0), y_{1}=0}; \\ {\boldsymbol{x}_{2}=(0,1), y_{2}=1}; \\ {\boldsymbol{x}_{3}=(1,0), y_{3}=1} ;\\ {\boldsymbol{x}_{4}=(1,1), y_{4}=0.}\end{array}
\]</span></p>
<p>This is a typical nonlinear problem. A NN model with one hidden layer as shown in Figure <a href="deep-learning.html#fig:f10-xor-nn">178</a> could solve this problem.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-xor-nn"></span>
<img src="graphics/10_xor_nn.png" alt="Architecture of a neural network with a hidden layer" width="100%"  />
<p class="caption marginnote shownote">
Figure 178: Architecture of a neural network with a hidden layer
</p>
</div>
<p></p>
<p>For instance, for <span class="math inline">\(\boldsymbol{x}_{1}=(0,0)\)</span>, from the input layer to the first node (i.e., the upper one) in the hidden layer, we have</p>
<p><span class="math display">\[
0 \times 1  + 0 \times 1 + 1 \times 0 = 0.
\]</span></p>
<p>The value <span class="math inline">\(0\)</span> provides the input for the activation function at the hidden node, and we have <span class="math inline">\(\Phi(0) = \max (0,0) = 0\)</span>.</p>
<p>From the input layer to the second node (i.e., the lower one) in the hidden layer, we have</p>
<p><span class="math display">\[
0 \times 1  + 0 \times 1 + 1 \times -1 = -1.
\]</span></p>
<p>The value <span class="math inline">\(-1\)</span> provides the input for the activation function at the hidden node, and we have <span class="math inline">\(\Phi(-1) = \max (0,-1) = 0\)</span>.</p>
<p>Then, from the hidden layer to the output layer, we have</p>
<p><span class="math display">\[
1 \times 0 - 2 \times 0 = 0.
\]</span></p>
<p>Using the activation function at the output layer, <span class="math inline">\(\Phi(z) = z\)</span>, the final prediction correctly predicts</p>
<p><span class="math display">\[
y = 0.
\]</span></p>
<p>We can follow the same process and see that the two-layer NN as shown in Figure <a href="deep-learning.html#fig:f10-xor-nn">178</a> could solve the XOR problem.</p>
<p><em>How to read a deep net.</em> Roughly speaking, there are three major efforts in developing deep learning models: to create basic forms, to design architectural principles or composition rules, and to design learning algorithms that can robustly and efficiently learn the parameters of the deep model using data<label for="tufte-sn-251" class="margin-toggle sidenote-number">251</label><input type="checkbox" id="tufte-sn-251" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">251</span> A deep NN model has massive parameters, so learning these parameters from data had been a challenge in the past. Some contributed the recent revitalization of deep learning—as the neural network model had its “rise and fall” in the past decades—to a range of optimization tricks such as pretraining and dropout, the growth of computing power, and the availability of Big Data, all enabled the data-driven learning of a giant collection of parameters of a deep NN model.</span>. Practical application of deep models is to make the network deeper by stacking these basic forms following some composition rules. From this perspective, it is not a surprise to see why it was quoted, “For reason in this sense is nothing but reckoning, that is adding and subtracting …”<label for="tufte-sn-252" class="margin-toggle sidenote-number">252</label><input type="checkbox" id="tufte-sn-252" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">252</span> Hobbes, T., Leviathan. 1651.</span>, to explain the logic of designing neural networks in Raul Rojas’s book<label for="tufte-sn-253" class="margin-toggle sidenote-number">253</label><input type="checkbox" id="tufte-sn-253" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">253</span> Rojas. R., <em>Neural Networks: a Systematic Introduction</em>. Springer, 1996.</span>.</p>
<p>We can take a look at the convolutional neural networks (CNN) as an example. The CNN is one popular deep NN model and is often used for learning from image data. Its architecture consists of a few basic forms and composition rules that are particularly developed for images.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-cnn-image"></span>
<img src="graphics/10_cnn_structure.png" alt="Architecture of a CNN model" width="100%"  />
<p class="caption marginnote shownote">
Figure 179: Architecture of a CNN model
</p>
</div>
<p></p>
<p>The CNN architecture shown in Figure <a href="deep-learning.html#fig:f10-cnn-image">179</a> has two parts. The first part (i.e., everything before the last <span class="math inline">\(3\)</span> layers) is to translate the image data into vectorized form and provides the input for the second part (i.e., the last <span class="math inline">\(3\)</span> layers) that is a NN as we have discussed earlier. One basic form of CNN is the convolutional layer. The basic purpose of a convolutional layer is to transform the image into a feature map, as shown in Figure <a href="deep-learning.html#fig:f10-conv-op">180</a>.</p>
<p></p>
<div class="figure"><span id="fig:f10-conv-op"></span>
<p class="caption marginnote shownote">
Figure 180: A convolutional layer aggregates spatially correlated information as a feature extraction process
</p>
<img src="graphics/10_conv_op.png" alt="A convolutional layer aggregates spatially correlated information as a feature extraction process" width="100%"  />
</div>
<p></p>
<p>Suppose that <span class="math inline">\(w_1=1\)</span>, <span class="math inline">\(w_2=2\)</span>, <span class="math inline">\(w_3=2\)</span>, <span class="math inline">\(w_4=1\)</span> in Figure <a href="deep-learning.html#fig:f10-conv-op">180</a>; Figure <a href="deep-learning.html#fig:f10-conv-layer">181</a> further shows the computational details of how the convolutional layer works.</p>
<p></p>
<div class="figure"><span id="fig:f10-conv-layer"></span>
<p class="caption marginnote shownote">
Figure 181: How the convolutional layer works.
</p>
<img src="graphics/10_conv_layer.png" alt="How the convolutional layer works." width="100%"  />
</div>
<p></p>
<p>The convolutional layer is good at exploiting the spatial structure<label for="tufte-sn-254" class="margin-toggle sidenote-number">254</label><input type="checkbox" id="tufte-sn-254" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">254</span> I.e., if the entities that are close to each other are semantically related, it is a spatial structure.</span> in its input data. Because of this, CNN is particularly useful for learning from image data, since for images the pixels close to one another are usually semantically related.</p>
<p>The max pooling layer is another basic form of CNN. Figure <a href="deep-learning.html#fig:f10-max-pool">182</a> shows how it works. The max pooling looks too simple an idea, but it works remarkably well. The real mystery when we look at a “simple” idea like this is why it was the max pooling that stood out among many other “simple” ideas. But there has been no conclusive theory to explain it<label for="tufte-sn-255" class="margin-toggle sidenote-number">255</label><input type="checkbox" id="tufte-sn-255" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">255</span> To quote Andrew Ng in his online course for convolutional neural networks (<a href="https://www.coursera.org/learn/convolutional-neural-networks">https://www.coursera.org/learn/convolutional-neural-networks</a>): <em>“… the main reason people use max pooling is because it’s been found in a lot of experiments to work well … I don’t know of anyone who fully knows if that is the real underlying reason.”</em></span>.</p>
<p>But one can compare the max pooling with the convolutional layer. One difference is that the parameters of a convolutional layer is learned from data, making it an adaptive and flexible form to a particular problem. The max pooling, however, is a fixed nonlinear transformation without parameters to learn. In other words, it has no computational cost. No wonder it is believed that one main function of the max pooling is to reduce the number of parameters of the deep NN model and to alleviate the computational cost. This would relieve some computational burden since a deep NN model has a massive number of parameters to be learned from data. Another aspect we should think of is that max pooling is good for image data. It may help increase the robustness of the model against translation invariance, i.e., to recognize an object, say, a cat, in an image, we need the algorithm to be resilient to the potential variation on angle or distance or any other factors that cause scale issues. Max pooling only keeps the “max” and discards the rest.</p>
<p></p>
<div class="figure"><span id="fig:f10-max-pool"></span>
<p class="caption marginnote shownote">
Figure 182: How the max pooling layer works
</p>
<img src="graphics/10_max_pool.png" alt="How the max pooling layer works" width="100%"  />
</div>
<p></p>
<p>One can add as many convolutional layers or max pooling layers as needed when designing a CNN model, and the convolutional layers and the max pooling layer could be alternatively arranged as a pipeline to extract features from the image data, e.g., in Figure <a href="deep-learning.html#fig:f10-cnn-image">179</a>, there are <span class="math inline">\(2\)</span> convolutional layers and <span class="math inline">\(1\)</span> max pooling layer. It has been found in many cases that for the CNN to be successful, it needs to be made quite deep. For this reason, some consider the deep NN models a different species from NN models.</p>
</div>
<div id="r-lab-15" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline for NN.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get the dataset into R and organize it in required format.</p>
<p></p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="deep-learning.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb205-2"><a href="deep-learning.html#cb205-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-3"><a href="deep-learning.html#cb205-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb205-4"><a href="deep-learning.html#cb205-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb205-5"><a href="deep-learning.html#cb205-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/KR.csv&quot;</span>)</span>
<span id="cb205-6"><a href="deep-learning.html#cb205-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb205-7"><a href="deep-learning.html#cb205-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(data)</span></span>
<span id="cb205-8"><a href="deep-learning.html#cb205-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-9"><a href="deep-learning.html#cb205-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb205-10"><a href="deep-learning.html#cb205-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb205-11"><a href="deep-learning.html#cb205-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data<span class="sc">$</span>x</span>
<span id="cb205-12"><a href="deep-learning.html#cb205-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>y</span>
<span id="cb205-13"><a href="deep-learning.html#cb205-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-14"><a href="deep-learning.html#cb205-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data </span></span>
<span id="cb205-15"><a href="deep-learning.html#cb205-15" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data) <span class="sc">*</span> <span class="dv">4</span><span class="sc">/</span><span class="dv">5</span>) )</span>
<span id="cb205-16"><a href="deep-learning.html#cb205-16" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb205-17"><a href="deep-learning.html#cb205-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data </span></span>
<span id="cb205-18"><a href="deep-learning.html#cb205-18" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates a list of models. For a NN model, important decisions are made on the design of the architecture, e.g., how many hidden layers and how many nodes in each hidden layer. For example, here, we create three NN models, all have one hidden layer but a different number of hidden nodes.</p>
<p></p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="deep-learning.html#cb206-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb206-2"><a href="deep-learning.html#cb206-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb206-3"><a href="deep-learning.html#cb206-3" aria-hidden="true" tabindex="-1"></a><span class="co"># NN model with one hidden layer and different # of nodes</span></span>
<span id="cb206-4"><a href="deep-learning.html#cb206-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb206-5"><a href="deep-learning.html#cb206-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model1: neuralnet(y~x, data=data, hidden=c(3)) </span></span>
<span id="cb206-6"><a href="deep-learning.html#cb206-6" aria-hidden="true" tabindex="-1"></a><span class="co"># model2: neuralnet(y~x, data=data, hidden=c(5)) </span></span>
<span id="cb206-7"><a href="deep-learning.html#cb206-7" aria-hidden="true" tabindex="-1"></a><span class="co"># model3: neuralnet(y~x, data=data, hidden=c(8)) </span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses cross-validation to evaluate the candidate models to identify the best model.</p>
<p></p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="deep-learning.html#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; cross-validation for model evaluation </span></span>
<span id="cb207-2"><a href="deep-learning.html#cb207-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-3"><a href="deep-learning.html#cb207-3" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">=</span> <span class="dv">10</span> <span class="co"># number of folds</span></span>
<span id="cb207-4"><a href="deep-learning.html#cb207-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the sample size, N, of the dataset</span></span>
<span id="cb207-5"><a href="deep-learning.html#cb207-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(data.train)[<span class="dv">1</span>] </span>
<span id="cb207-6"><a href="deep-learning.html#cb207-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-7"><a href="deep-learning.html#cb207-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-8"><a href="deep-learning.html#cb207-8" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb207-9"><a href="deep-learning.html#cb207-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(neuralnet)</span>
<span id="cb207-10"><a href="deep-learning.html#cb207-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-11"><a href="deep-learning.html#cb207-11" aria-hidden="true" tabindex="-1"></a><span class="co"># cv_mse records the prediction error for each fold</span></span>
<span id="cb207-12"><a href="deep-learning.html#cb207-12" aria-hidden="true" tabindex="-1"></a>cv_mse <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb207-13"><a href="deep-learning.html#cb207-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb207-14"><a href="deep-learning.html#cb207-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># In each iteration of the n_folds iterations</span></span>
<span id="cb207-15"><a href="deep-learning.html#cb207-15" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb207-16"><a href="deep-learning.html#cb207-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is the testing data, from the ith fold</span></span>
<span id="cb207-17"><a href="deep-learning.html#cb207-17" aria-hidden="true" tabindex="-1"></a>  data.test.cv <span class="ot">&lt;-</span> data.train[test_i, ]  </span>
<span id="cb207-18"><a href="deep-learning.html#cb207-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Then, the remaining data form the training data</span></span>
<span id="cb207-19"><a href="deep-learning.html#cb207-19" aria-hidden="true" tabindex="-1"></a>  data.train.cv <span class="ot">&lt;-</span> data.train[<span class="sc">-</span>test_i, ] </span>
<span id="cb207-20"><a href="deep-learning.html#cb207-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the neural network model with one hidden layer of 3</span></span>
<span id="cb207-21"><a href="deep-learning.html#cb207-21" aria-hidden="true" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(y<span class="sc">~</span>x, <span class="at">data=</span>data, <span class="at">hidden=</span><span class="fu">c</span>(<span class="dv">3</span>)) </span>
<span id="cb207-22"><a href="deep-learning.html#cb207-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb207-23"><a href="deep-learning.html#cb207-23" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">compute</span> (model1, data.test.cv)  </span>
<span id="cb207-24"><a href="deep-learning.html#cb207-24" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb207-25"><a href="deep-learning.html#cb207-25" aria-hidden="true" tabindex="-1"></a>  model1<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb207-26"><a href="deep-learning.html#cb207-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get the true y values for the testing data</span></span>
<span id="cb207-27"><a href="deep-learning.html#cb207-27" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test.cv<span class="sc">$</span>y </span>
<span id="cb207-28"><a href="deep-learning.html#cb207-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mean((true_y - y_hat)^2): mean squared error (MSE). </span></span>
<span id="cb207-29"><a href="deep-learning.html#cb207-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The smaller this error, the better your model is</span></span>
<span id="cb207-30"><a href="deep-learning.html#cb207-30" aria-hidden="true" tabindex="-1"></a>  cv_mse[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb207-31"><a href="deep-learning.html#cb207-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb207-32"><a href="deep-learning.html#cb207-32" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_mse)</span></code></pre></div>
<p></p>
<p>The result is shown below</p>
<p></p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="deep-learning.html#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.09439574 # Model1</span></span>
<span id="cb208-2"><a href="deep-learning.html#cb208-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.04433521 # Model2</span></span>
<span id="cb208-3"><a href="deep-learning.html#cb208-3" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.1142009  # Model3</span></span></code></pre></div>
<p></p>
<p>Obviously, <code>model2</code> achieves the lowest prediction error.</p>
<p></p>
<div class="figure"><span id="fig:f10-visual-3nn"></span>
<p class="caption marginnote shownote">
Figure 183: Visualization of the three fitted models and the data
</p>
<img src="graphics/10_visual_3nn.png" alt="Visualization of the three fitted models and the data" width="100%"  />
</div>
<p></p>
<p>We can also visually examine the fitness of the three models in Figure <a href="deep-learning.html#fig:f10-visual-3nn">183</a> to see how well the three models fit the data.</p>
<p></p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="deep-learning.html#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use visual inspection to assist the model selection. </span></span>
<span id="cb209-2"><a href="deep-learning.html#cb209-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-3"><a href="deep-learning.html#cb209-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb209-4"><a href="deep-learning.html#cb209-4" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model1, data.train)  </span>
<span id="cb209-5"><a href="deep-learning.html#cb209-5" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb209-6"><a href="deep-learning.html#cb209-6" aria-hidden="true" tabindex="-1"></a>model1<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb209-7"><a href="deep-learning.html#cb209-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb209-8"><a href="deep-learning.html#cb209-8" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model2, data.train)  </span>
<span id="cb209-9"><a href="deep-learning.html#cb209-9" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb209-10"><a href="deep-learning.html#cb209-10" aria-hidden="true" tabindex="-1"></a>model2<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb209-11"><a href="deep-learning.html#cb209-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb209-12"><a href="deep-learning.html#cb209-12" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model3, data.train) </span>
<span id="cb209-13"><a href="deep-learning.html#cb209-13" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb209-14"><a href="deep-learning.html#cb209-14" aria-hidden="true" tabindex="-1"></a>model3<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb209-15"><a href="deep-learning.html#cb209-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-16"><a href="deep-learning.html#cb209-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-17"><a href="deep-learning.html#cb209-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data.train, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb209-18"><a href="deep-learning.html#cb209-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model1<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb209-19"><a href="deep-learning.html#cb209-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model2<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb209-20"><a href="deep-learning.html#cb209-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model3<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb209-21"><a href="deep-learning.html#cb209-21" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;NN (3 hidden nodes)&quot;</span>, </span>
<span id="cb209-22"><a href="deep-learning.html#cb209-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;NN (5 hidden nodes)&quot;</span>, <span class="st">&quot;NN (8 hidden nodes)&quot;</span>), </span>
<span id="cb209-23"><a href="deep-learning.html#cb209-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;black&quot;</span>), </span>
<span id="cb209-24"><a href="deep-learning.html#cb209-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="dv">32</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure"><span id="fig:f10-visual-finalnn"></span>
<p class="caption marginnote shownote">
Figure 184: Visualization of the architecture of the final model
</p>
<img src="graphics/10_visual_finalnn.png" alt="Visualization of the architecture of the final model" width="100%"  />
</div>
<p></p>
<p><strong>Step 5</strong> builds the final model. Figure <a href="deep-learning.html#fig:f10-visual-finalnn">184</a> shows the architecture of the final model.</p>
<p></p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="deep-learning.html#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, build your final model</span></span>
<span id="cb210-2"><a href="deep-learning.html#cb210-2" aria-hidden="true" tabindex="-1"></a>nn.final <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(y<span class="sc">~</span>x, <span class="at">data=</span>data.train, <span class="at">hidden=</span><span class="fu">c</span>(<span class="dv">5</span>)) <span class="co"># </span></span>
<span id="cb210-3"><a href="deep-learning.html#cb210-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nn.final) <span class="co"># Draw the architecture of the NN model</span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the final model for prediction.</p>
<p></p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="deep-learning.html#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Evaluate the prediction performance of your model</span></span>
<span id="cb211-2"><a href="deep-learning.html#cb211-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb211-3"><a href="deep-learning.html#cb211-3" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(nn.final, data.test)  </span>
<span id="cb211-4"><a href="deep-learning.html#cb211-4" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result </span>
<span id="cb211-5"><a href="deep-learning.html#cb211-5" aria-hidden="true" tabindex="-1"></a><span class="co"># get the true y values for the testing data</span></span>
<span id="cb211-6"><a href="deep-learning.html#cb211-6" aria-hidden="true" tabindex="-1"></a>true_y <span class="ot">&lt;-</span> data.test<span class="sc">$</span>y  </span>
<span id="cb211-7"><a href="deep-learning.html#cb211-7" aria-hidden="true" tabindex="-1"></a><span class="co"># mean((true_y - y_hat)^2): mean squared error (MSE). </span></span>
<span id="cb211-8"><a href="deep-learning.html#cb211-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The smaller this error, the better your model is</span></span>
<span id="cb211-9"><a href="deep-learning.html#cb211-9" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb211-10"><a href="deep-learning.html#cb211-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(mse)</span></code></pre></div>
<p></p>
<p><em>The 6-Step R Pipeline for CNN.</em> Before starting the pipeline, let’s first install the Keras package.</p>
<p></p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="deep-learning.html#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;devtools&quot;</span>) <span class="co"># install devtools</span></span>
<span id="cb212-2"><a href="deep-learning.html#cb212-2" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;rstudio/keras&quot;</span>) <span class="co"># install Keras</span></span></code></pre></div>
<p></p>
<p><strong>Step 1</strong> and <strong>Step 2</strong> get the MNIST handwritten digit dataset into R and process the data in required format. The goal is to classify a handwritten number into one of the <span class="math inline">\(10\)</span> classes (from <span class="math inline">\(0\)</span> to <span class="math inline">\(9\)</span>).</p>
<p></p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="deep-learning.html#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read digits classification data </span></span>
<span id="cb213-2"><a href="deep-learning.html#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb213-3"><a href="deep-learning.html#cb213-3" aria-hidden="true" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb213-4"><a href="deep-learning.html#cb213-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-5"><a href="deep-learning.html#cb213-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb213-6"><a href="deep-learning.html#cb213-6" aria-hidden="true" tabindex="-1"></a><span class="co"># code adapted from </span></span>
<span id="cb213-7"><a href="deep-learning.html#cb213-7" aria-hidden="true" tabindex="-1"></a><span class="co"># keras.rstudio.com/articles/examples/mnist_cnn.html</span></span>
<span id="cb213-8"><a href="deep-learning.html#cb213-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Input image dimensions</span></span>
<span id="cb213-9"><a href="deep-learning.html#cb213-9" aria-hidden="true" tabindex="-1"></a>img_rows <span class="ot">&lt;-</span> <span class="dv">28</span></span>
<span id="cb213-10"><a href="deep-learning.html#cb213-10" aria-hidden="true" tabindex="-1"></a>img_cols <span class="ot">&lt;-</span> <span class="dv">28</span></span>
<span id="cb213-11"><a href="deep-learning.html#cb213-11" aria-hidden="true" tabindex="-1"></a>num_classes <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb213-12"><a href="deep-learning.html#cb213-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-13"><a href="deep-learning.html#cb213-13" aria-hidden="true" tabindex="-1"></a><span class="co"># The data, shuffled and split between training and testing sets</span></span>
<span id="cb213-14"><a href="deep-learning.html#cb213-14" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb213-15"><a href="deep-learning.html#cb213-15" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb213-16"><a href="deep-learning.html#cb213-16" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb213-17"><a href="deep-learning.html#cb213-17" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y</span>
<span id="cb213-18"><a href="deep-learning.html#cb213-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-19"><a href="deep-learning.html#cb213-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Redefine  dimension of train/test inputs</span></span>
<span id="cb213-20"><a href="deep-learning.html#cb213-20" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_train, </span>
<span id="cb213-21"><a href="deep-learning.html#cb213-21" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(<span class="fu">nrow</span>(x_train), img_rows, img_cols, <span class="dv">1</span>))</span>
<span id="cb213-22"><a href="deep-learning.html#cb213-22" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_test, </span>
<span id="cb213-23"><a href="deep-learning.html#cb213-23" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(<span class="fu">nrow</span>(x_test), img_rows, img_cols, <span class="dv">1</span>))</span>
<span id="cb213-24"><a href="deep-learning.html#cb213-24" aria-hidden="true" tabindex="-1"></a>input_shape <span class="ot">&lt;-</span> <span class="fu">c</span>(img_rows, img_cols, <span class="dv">1</span>)</span>
<span id="cb213-25"><a href="deep-learning.html#cb213-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-26"><a href="deep-learning.html#cb213-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform RGB values into [0,1] range</span></span>
<span id="cb213-27"><a href="deep-learning.html#cb213-27" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> x_train <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb213-28"><a href="deep-learning.html#cb213-28" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> x_test <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb213-29"><a href="deep-learning.html#cb213-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-30"><a href="deep-learning.html#cb213-30" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&#39;x_train_shape:&#39;</span>, <span class="fu">dim</span>(x_train), <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb213-31"><a href="deep-learning.html#cb213-31" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">nrow</span>(x_train), <span class="st">&#39;train samples</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb213-32"><a href="deep-learning.html#cb213-32" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">nrow</span>(x_test), <span class="st">&#39;test samples</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb213-33"><a href="deep-learning.html#cb213-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-34"><a href="deep-learning.html#cb213-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert class vectors to binary class matrices</span></span>
<span id="cb213-35"><a href="deep-learning.html#cb213-35" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_train, num_classes)</span>
<span id="cb213-36"><a href="deep-learning.html#cb213-36" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_test, num_classes)</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates different models. In deep learning, parameters that are determined before training a model are called <strong>hyperparameters</strong> . Hyperparameters for a CNN include number of layers, number of nodes for a layer, kernel size of a convolution layer<label for="tufte-sn-256" class="margin-toggle sidenote-number">256</label><input type="checkbox" id="tufte-sn-256" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">256</span> E.g., in Figure <a href="deep-learning.html#fig:f10-conv-layer">181</a> the kernel size is <span class="math inline">\(2\)</span>.</span>, etc. Here we create three models with different kernel sizes for the convolution layers.</p>
<p></p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="deep-learning.html#cb214-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb214-2"><a href="deep-learning.html#cb214-2" aria-hidden="true" tabindex="-1"></a>define_model <span class="ot">&lt;-</span> <span class="cf">function</span>(kernel_size){</span>
<span id="cb214-3"><a href="deep-learning.html#cb214-3" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb214-4"><a href="deep-learning.html#cb214-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convolution layer 1</span></span>
<span id="cb214-5"><a href="deep-learning.html#cb214-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">8</span>, </span>
<span id="cb214-6"><a href="deep-learning.html#cb214-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">kernel_size =</span> <span class="fu">c</span>(kernel_size,kernel_size), </span>
<span id="cb214-7"><a href="deep-learning.html#cb214-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb214-8"><a href="deep-learning.html#cb214-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">input_shape =</span> input_shape) <span class="sc">%&gt;%</span> </span>
<span id="cb214-9"><a href="deep-learning.html#cb214-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pooling layer 1</span></span>
<span id="cb214-10"><a href="deep-learning.html#cb214-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb214-11"><a href="deep-learning.html#cb214-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convolution layer 2</span></span>
<span id="cb214-12"><a href="deep-learning.html#cb214-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">16</span>, </span>
<span id="cb214-13"><a href="deep-learning.html#cb214-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">kernel_size =</span> <span class="fu">c</span>(kernel_size,kernel_size), </span>
<span id="cb214-14"><a href="deep-learning.html#cb214-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb214-15"><a href="deep-learning.html#cb214-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pooling layer 2</span></span>
<span id="cb214-16"><a href="deep-learning.html#cb214-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb214-17"><a href="deep-learning.html#cb214-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dense layers</span></span>
<span id="cb214-18"><a href="deep-learning.html#cb214-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb214-19"><a href="deep-learning.html#cb214-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb214-20"><a href="deep-learning.html#cb214-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> num_classes, <span class="at">activation =</span> <span class="st">&#39;softmax&#39;</span>)</span>
<span id="cb214-21"><a href="deep-learning.html#cb214-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb214-22"><a href="deep-learning.html#cb214-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compile model</span></span>
<span id="cb214-23"><a href="deep-learning.html#cb214-23" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb214-24"><a href="deep-learning.html#cb214-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> loss_categorical_crossentropy,</span>
<span id="cb214-25"><a href="deep-learning.html#cb214-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="fu">optimizer_adadelta</span>(),</span>
<span id="cb214-26"><a href="deep-learning.html#cb214-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb214-27"><a href="deep-learning.html#cb214-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb214-28"><a href="deep-learning.html#cb214-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(model)</span>
<span id="cb214-29"><a href="deep-learning.html#cb214-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb214-30"><a href="deep-learning.html#cb214-30" aria-hidden="true" tabindex="-1"></a><span class="co"># define three models</span></span>
<span id="cb214-31"><a href="deep-learning.html#cb214-31" aria-hidden="true" tabindex="-1"></a>model_kernel_1 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">2</span>)</span>
<span id="cb214-32"><a href="deep-learning.html#cb214-32" aria-hidden="true" tabindex="-1"></a>model_kernel_2 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">3</span>)</span>
<span id="cb214-33"><a href="deep-learning.html#cb214-33" aria-hidden="true" tabindex="-1"></a>model_kernel_3 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">5</span>)</span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses cross-validation to evaluate the candidate models to identify the best model.</p>
<p></p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="deep-learning.html#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Use cross-validation for model evaluation</span></span>
<span id="cb215-2"><a href="deep-learning.html#cb215-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-3"><a href="deep-learning.html#cb215-3" aria-hidden="true" tabindex="-1"></a><span class="co"># set upfunction for evaluating accuracy</span></span>
<span id="cb215-4"><a href="deep-learning.html#cb215-4" aria-hidden="true" tabindex="-1"></a>cv_accuracy <span class="ot">&lt;-</span> <span class="cf">function</span>(n_folds, kernel_size,x_train,y_train){</span>
<span id="cb215-5"><a href="deep-learning.html#cb215-5" aria-hidden="true" tabindex="-1"></a>  N <span class="ot">&lt;-</span> <span class="fu">dim</span>(x_train)[<span class="dv">1</span>] <span class="co"># the sample size, N, of the dataset</span></span>
<span id="cb215-6"><a href="deep-learning.html#cb215-6" aria-hidden="true" tabindex="-1"></a>  folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb215-7"><a href="deep-learning.html#cb215-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb215-8"><a href="deep-learning.html#cb215-8" aria-hidden="true" tabindex="-1"></a>  accuracy_v <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb215-9"><a href="deep-learning.html#cb215-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb215-10"><a href="deep-learning.html#cb215-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set up training and testing data</span></span>
<span id="cb215-11"><a href="deep-learning.html#cb215-11" aria-hidden="true" tabindex="-1"></a>    test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k)</span>
<span id="cb215-12"><a href="deep-learning.html#cb215-12" aria-hidden="true" tabindex="-1"></a>    x.train.cv <span class="ot">&lt;-</span> x_train[<span class="sc">-</span>test_i,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>] </span>
<span id="cb215-13"><a href="deep-learning.html#cb215-13" aria-hidden="true" tabindex="-1"></a>    x.test.cv <span class="ot">&lt;-</span> x_train[test_i,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>]   </span>
<span id="cb215-14"><a href="deep-learning.html#cb215-14" aria-hidden="true" tabindex="-1"></a>    y.train.cv <span class="ot">&lt;-</span> y_train[<span class="sc">-</span>test_i,,drop<span class="ot">=</span><span class="cn">FALSE</span> ] </span>
<span id="cb215-15"><a href="deep-learning.html#cb215-15" aria-hidden="true" tabindex="-1"></a>    y.test.cv <span class="ot">&lt;-</span> y_train[test_i,,drop<span class="ot">=</span><span class="cn">FALSE</span> ]</span>
<span id="cb215-16"><a href="deep-learning.html#cb215-16" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb215-17"><a href="deep-learning.html#cb215-17" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">define_model</span>(kernel_size)</span>
<span id="cb215-18"><a href="deep-learning.html#cb215-18" aria-hidden="true" tabindex="-1"></a>    model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb215-19"><a href="deep-learning.html#cb215-19" aria-hidden="true" tabindex="-1"></a>      x_train, y_train, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb215-20"><a href="deep-learning.html#cb215-20" aria-hidden="true" tabindex="-1"></a>      <span class="at">epochs =</span> <span class="dv">2</span>,<span class="at">validation_split =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> <span class="dv">0</span></span>
<span id="cb215-21"><a href="deep-learning.html#cb215-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb215-22"><a href="deep-learning.html#cb215-22" aria-hidden="true" tabindex="-1"></a>    scores <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(</span>
<span id="cb215-23"><a href="deep-learning.html#cb215-23" aria-hidden="true" tabindex="-1"></a>    x.test.cv, y.test.cv, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb215-24"><a href="deep-learning.html#cb215-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb215-25"><a href="deep-learning.html#cb215-25" aria-hidden="true" tabindex="-1"></a>    accuracy_v <span class="ot">&lt;-</span> <span class="fu">c</span>(accuracy_v, scores[<span class="dv">2</span>])</span>
<span id="cb215-26"><a href="deep-learning.html#cb215-26" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb215-27"><a href="deep-learning.html#cb215-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(accuracy_v)</span>
<span id="cb215-28"><a href="deep-learning.html#cb215-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb215-29"><a href="deep-learning.html#cb215-29" aria-hidden="true" tabindex="-1"></a><span class="co"># get average accuracy for each model</span></span>
<span id="cb215-30"><a href="deep-learning.html#cb215-30" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_1 <span class="ot">&lt;-</span> </span>
<span id="cb215-31"><a href="deep-learning.html#cb215-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">2</span>,x_train,y_train)</span>
<span id="cb215-32"><a href="deep-learning.html#cb215-32" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_1))</span>
<span id="cb215-33"><a href="deep-learning.html#cb215-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-34"><a href="deep-learning.html#cb215-34" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_2 <span class="ot">&lt;-</span> </span>
<span id="cb215-35"><a href="deep-learning.html#cb215-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">3</span>,x_train,y_train)</span>
<span id="cb215-36"><a href="deep-learning.html#cb215-36" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_2))</span>
<span id="cb215-37"><a href="deep-learning.html#cb215-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-38"><a href="deep-learning.html#cb215-38" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_3 <span class="ot">&lt;-</span> </span>
<span id="cb215-39"><a href="deep-learning.html#cb215-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">5</span>,x_train,y_train)</span>
<span id="cb215-40"><a href="deep-learning.html#cb215-40" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_3))</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="deep-learning.html#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9680667 # Model1</span></span>
<span id="cb216-2"><a href="deep-learning.html#cb216-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9742167 # Model2</span></span>
<span id="cb216-3"><a href="deep-learning.html#cb216-3" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9760833  # Model3</span></span></code></pre></div>
<p></p>
<p><strong>Step 5</strong> builds the final model based on all the training data.</p>
<p></p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="deep-learning.html#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, build your final model</span></span>
<span id="cb217-2"><a href="deep-learning.html#cb217-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-3"><a href="deep-learning.html#cb217-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">define_model</span>(<span class="dv">5</span>)</span>
<span id="cb217-4"><a href="deep-learning.html#cb217-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb217-5"><a href="deep-learning.html#cb217-5" aria-hidden="true" tabindex="-1"></a>      x_train, y_train, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb217-6"><a href="deep-learning.html#cb217-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">epochs =</span> <span class="dv">2</span>,<span class="at">validation_split =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> <span class="dv">0</span></span>
<span id="cb217-7"><a href="deep-learning.html#cb217-7" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the final model for prediction.</p>
<p></p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="deep-learning.html#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Evaluate the prediction performance of your model</span></span>
<span id="cb218-2"><a href="deep-learning.html#cb218-2" aria-hidden="true" tabindex="-1"></a>scores <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(</span>
<span id="cb218-3"><a href="deep-learning.html#cb218-3" aria-hidden="true" tabindex="-1"></a>    x_test, y_test, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb218-4"><a href="deep-learning.html#cb218-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(scores[<span class="dv">2</span>])</span></code></pre></div>
<p></p>
<p>To visualize the process of how this CNN model works, the following R code is used to visualize the output from each layer, shown in Figure <a href="deep-learning.html#fig:f10-cnn-activations">185</a>.</p>
<p></p>
<div class="figure"><span id="fig:f10-cnn-activations"></span>
<p class="caption marginnote shownote">
Figure 185: Visualize the outputs from all layers of the CNN model
</p>
<img src="graphics/10_visual_7_activations.png" alt="Visualize the outputs from all layers of the CNN model" width="100%"  />
</div>
<p></p>
<p></p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="deep-learning.html#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize output for a layer</span></span>
<span id="cb219-2"><a href="deep-learning.html#cb219-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-3"><a href="deep-learning.html#cb219-3" aria-hidden="true" tabindex="-1"></a><span class="co"># use the first image from testing data</span></span>
<span id="cb219-4"><a href="deep-learning.html#cb219-4" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> x_test[<span class="dv">1</span>,,,]</span>
<span id="cb219-5"><a href="deep-learning.html#cb219-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.raster</span>(img))</span>
<span id="cb219-6"><a href="deep-learning.html#cb219-6" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> x_test[<span class="dv">1</span>,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb219-7"><a href="deep-learning.html#cb219-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-8"><a href="deep-learning.html#cb219-8" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to plot an image</span></span>
<span id="cb219-9"><a href="deep-learning.html#cb219-9" aria-hidden="true" tabindex="-1"></a>plot_image <span class="ot">&lt;-</span> <span class="cf">function</span>(channel) {</span>
<span id="cb219-10"><a href="deep-learning.html#cb219-10" aria-hidden="true" tabindex="-1"></a>    rotate <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">t</span>(<span class="fu">apply</span>(x, <span class="dv">2</span>, rev))</span>
<span id="cb219-11"><a href="deep-learning.html#cb219-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">image</span>(<span class="fu">rotate</span>(channel), <span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">asp =</span> <span class="dv">1</span>, </span>
<span id="cb219-12"><a href="deep-learning.html#cb219-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">col =</span> <span class="fu">gray.colors</span>(<span class="dv">12</span>))</span>
<span id="cb219-13"><a href="deep-learning.html#cb219-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb219-14"><a href="deep-learning.html#cb219-14" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the testing image</span></span>
<span id="cb219-15"><a href="deep-learning.html#cb219-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_image</span>(  <span class="dv">1</span> <span class="sc">-</span> img[<span class="dv">1</span>,,,]   )</span>
<span id="cb219-16"><a href="deep-learning.html#cb219-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-17"><a href="deep-learning.html#cb219-17" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the output from the second layer </span></span>
<span id="cb219-18"><a href="deep-learning.html#cb219-18" aria-hidden="true" tabindex="-1"></a>layer_number <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb219-19"><a href="deep-learning.html#cb219-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-20"><a href="deep-learning.html#cb219-20" aria-hidden="true" tabindex="-1"></a><span class="co"># print layer name</span></span>
<span id="cb219-21"><a href="deep-learning.html#cb219-21" aria-hidden="true" tabindex="-1"></a>layer_name <span class="ot">&lt;-</span> model<span class="sc">$</span>layers[[layer_number]]<span class="sc">$</span>name</span>
<span id="cb219-22"><a href="deep-learning.html#cb219-22" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(layer_name)</span>
<span id="cb219-23"><a href="deep-learning.html#cb219-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-24"><a href="deep-learning.html#cb219-24" aria-hidden="true" tabindex="-1"></a>layer_outputs <span class="ot">&lt;-</span> <span class="fu">lapply</span>(model<span class="sc">$</span>layers[layer_number], </span>
<span id="cb219-25"><a href="deep-learning.html#cb219-25" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">function</span>(layer) layer<span class="sc">$</span>output)</span>
<span id="cb219-26"><a href="deep-learning.html#cb219-26" aria-hidden="true" tabindex="-1"></a>activation_model <span class="ot">&lt;-</span> <span class="fu">keras_model</span>(<span class="at">inputs =</span> model<span class="sc">$</span>input, </span>
<span id="cb219-27"><a href="deep-learning.html#cb219-27" aria-hidden="true" tabindex="-1"></a>                                <span class="at">outputs =</span> layer_outputs)</span>
<span id="cb219-28"><a href="deep-learning.html#cb219-28" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the outputs from the layer for the image</span></span>
<span id="cb219-29"><a href="deep-learning.html#cb219-29" aria-hidden="true" tabindex="-1"></a>layer_activation <span class="ot">&lt;-</span> activation_model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(img)</span>
<span id="cb219-30"><a href="deep-learning.html#cb219-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-31"><a href="deep-learning.html#cb219-31" aria-hidden="true" tabindex="-1"></a><span class="co"># check dimension</span></span>
<span id="cb219-32"><a href="deep-learning.html#cb219-32" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">dim</span>(layer_activation))</span>
<span id="cb219-33"><a href="deep-learning.html#cb219-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-34"><a href="deep-learning.html#cb219-34" aria-hidden="true" tabindex="-1"></a><span class="co"># number of features</span></span>
<span id="cb219-35"><a href="deep-learning.html#cb219-35" aria-hidden="true" tabindex="-1"></a>n_features <span class="ot">&lt;-</span> <span class="fu">dim</span>(layer_activation)[[<span class="dv">4</span>]] </span>
<span id="cb219-36"><a href="deep-learning.html#cb219-36" aria-hidden="true" tabindex="-1"></a><span class="co"># image width</span></span>
<span id="cb219-37"><a href="deep-learning.html#cb219-37" aria-hidden="true" tabindex="-1"></a>image_size <span class="ot">&lt;-</span> <span class="fu">dim</span>(layer_activation)[[<span class="dv">2</span>]] </span>
<span id="cb219-38"><a href="deep-learning.html#cb219-38" aria-hidden="true" tabindex="-1"></a><span class="co"># number of columns and images per column </span></span>
<span id="cb219-39"><a href="deep-learning.html#cb219-39" aria-hidden="true" tabindex="-1"></a><span class="co"># (each column plots an image)</span></span>
<span id="cb219-40"><a href="deep-learning.html#cb219-40" aria-hidden="true" tabindex="-1"></a>n_cols <span class="ot">&lt;-</span> n_features </span>
<span id="cb219-41"><a href="deep-learning.html#cb219-41" aria-hidden="true" tabindex="-1"></a>images_per_col <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co">#</span></span>
<span id="cb219-42"><a href="deep-learning.html#cb219-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-43"><a href="deep-learning.html#cb219-43" aria-hidden="true" tabindex="-1"></a><span class="co"># plot n_cols of images</span></span>
<span id="cb219-44"><a href="deep-learning.html#cb219-44" aria-hidden="true" tabindex="-1"></a>op <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(n_cols, images_per_col), </span>
<span id="cb219-45"><a href="deep-learning.html#cb219-45" aria-hidden="true" tabindex="-1"></a>            <span class="at">mai =</span> <span class="fu">rep_len</span>(<span class="dv">0</span>, <span class="dv">4</span>)) </span>
<span id="cb219-46"><a href="deep-learning.html#cb219-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-47"><a href="deep-learning.html#cb219-47" aria-hidden="true" tabindex="-1"></a><span class="co"># plot each image</span></span>
<span id="cb219-48"><a href="deep-learning.html#cb219-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (col <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span>(n_cols<span class="dv">-1</span>)) {</span>
<span id="cb219-49"><a href="deep-learning.html#cb219-49" aria-hidden="true" tabindex="-1"></a>        col_ix <span class="ot">&lt;-</span> col <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb219-50"><a href="deep-learning.html#cb219-50" aria-hidden="true" tabindex="-1"></a>        channel_image <span class="ot">&lt;-</span> layer_activation[<span class="dv">1</span>,,,col_ix]</span>
<span id="cb219-51"><a href="deep-learning.html#cb219-51" aria-hidden="true" tabindex="-1"></a>      <span class="fu">plot_image</span>(<span class="dv">1</span><span class="sc">-</span>channel_image)</span>
<span id="cb219-52"><a href="deep-learning.html#cb219-52" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
</div>
</div>
<p style="text-align: center;">
<a href="overview-8.html"><button class="btn btn-default">Previous</button></a>
<a href="intrees.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
