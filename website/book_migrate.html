<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Data Analytics">

<title>Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a>
<ul>
<li><a href="#who-will-benefit-from-this-book">Who will benefit from this book?</a></li>
<li><a href="#overview-of-a-data-analytics-pipeline">Overview of a data analytics pipeline</a></li>
<li><a href="#topics-in-a-nutshell">Topics in a nutshell</a>
<ul>
<li><a href="#data-models-i.e.-regression-based-techniques">Data models (i.e., regression-based techniques)</a></li>
<li><a href="#algorithmic-models-i.e.-tree-based-techniques">Algorithmic models (i.e., tree-based techniques)</a></li>
</ul></li>
</ul></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#regression-models">Regression models</a>
<ul>
<li><a href="#rationale-and-formulation">Rationale and formulation</a></li>
<li><a href="#theory-and-method">Theory and method</a></li>
<li><a href="#r-lab">R Lab</a></li>
</ul></li>
<li><a href="#tree-models">Tree models</a>
<ul>
<li><a href="#rationale-and-formulation-1">Rationale and formulation</a></li>
<li><a href="#theorymethod">Theory/Method</a></li>
<li><a href="#r-lab-1">R Lab</a></li>
</ul></li>
<li><a href="#remarks">Remarks</a>
<ul>
<li><a href="#statistical-model-vs.-causal-model">Statistical model vs. causal model</a></li>
<li><a href="#design-of-experiments">Design of experiments</a></li>
<li><a href="#the-pessimistic-error-estimation-in-post-pruning">The pessimistic error estimation in post-pruning</a></li>
</ul></li>
<li><a href="#exercises">Exercises</a></li>
</ul></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a>
<ul>
<li><a href="#overview-1">Overview</a></li>
<li><a href="#logistic-regression-model">Logistic regression model</a>
<ul>
<li><a href="#rationale-and-formulation-2">Rationale and formulation</a></li>
<li><a href="#theory-and-method-1">Theory and method</a></li>
<li><a href="#r-lab-2">R Lab</a></li>
</ul></li>
<li><a href="#ranking-problem-by-pairwise-comparison">Ranking problem by pairwise comparison</a>
<ul>
<li><a href="#rationale-and-formulation-3">Rationale and formulation</a></li>
<li><a href="#theory-and-method-2">Theory and method</a></li>
</ul></li>
<li><a href="#statistical-process-control-using-decision-tree">Statistical process control using decision tree</a>
<ul>
<li><a href="#rationale-and-formulation-4">Rationale and formulation</a></li>
<li><a href="#r-lab-3">R Lab</a></li>
</ul></li>
<li><a href="#remarks-1">Remarks</a>
<ul>
<li><a href="#more-about-the-logistic-function">More about the logistic function</a></li>
<li><a href="#does-the-logistic-function-make-sense-an-eda-approach">Does the logistic function make sense? — An EDA approach</a></li>
<li><a href="#regression-vs.-tree-models">Regression vs. tree models</a></li>
<li><a href="#can-we-use-a-tree-model-for-regression">Can we use a tree model for regression?</a></li>
</ul></li>
<li><a href="#exercises-1">Exercises</a>
<ul>
<li><a href="#data-analysis">Data analysis</a></li>
</ul></li>
</ul></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a>
<ul>
<li><a href="#overview-2">Overview</a></li>
<li><a href="#how-bootstrap-works">How bootstrap works</a>
<ul>
<li><a href="#rationale-and-formulation-5">Rationale and formulation</a></li>
<li><a href="#theory-and-method-3">Theory and method</a></li>
<li><a href="#r-lab-4">R Lab</a></li>
</ul></li>
<li><a href="#random-forests">Random forests</a>
<ul>
<li><a href="#rationale-and-formulation-6">Rationale and formulation</a></li>
<li><a href="#theory-and-method-4">Theory and method</a></li>
<li><a href="#r-lab-5">R Lab</a></li>
</ul></li>
<li><a href="#remarks-2">Remarks</a>
<ul>
<li><a href="#the-gini-index-versus-the-entropy">The Gini index versus the entropy</a></li>
<li><a href="#why-random-forests-work">Why random forests work</a></li>
<li><a href="#variable-importance-by-random-forests">Variable importance by random forests</a></li>
<li><a href="#partial-dependency-plot">Partial dependency plot</a></li>
</ul></li>
<li><a href="#exercises-2">Exercises</a></li>
</ul></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a>
<ul>
<li><a href="#overview-3">Overview</a></li>
<li><a href="#cross-validation">Cross-validation</a>
<ul>
<li><a href="#rationale-and-formulation-7">Rationale and formulation</a></li>
<li><a href="#theorymethod-1">Theory/Method</a></li>
<li><a href="#r-lab-6">R Lab</a></li>
</ul></li>
<li><a href="#out-of-bag-error-in-random-forests">Out-of-bag error in random forests</a>
<ul>
<li><a href="#rationale-and-formulation-8">Rationale and formulation</a></li>
<li><a href="#theory-and-method-5">Theory and method</a></li>
<li><a href="#r-lab-7">R Lab</a></li>
</ul></li>
<li><a href="#remarks-3">Remarks</a>
<ul>
<li><a href="#the-law-of-learning-errors">The “law” of learning errors</a></li>
<li><a href="#a-larger-view-of-model-selection-and-validation">A larger view of <em>model selection and validation</em></a></li>
<li><a href="#the-confusion-matrix">The confusion matrix</a></li>
<li><a href="#the-roc-curve">The ROC curve</a></li>
</ul></li>
<li><a href="#exercises-3">Exercises</a></li>
</ul></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a>
<ul>
<li><a href="#overview-4">Overview</a></li>
<li><a href="#diagnosis-in-regression">Diagnosis in regression</a>
<ul>
<li><a href="#residual-analysis">Residual analysis</a></li>
<li><a href="#multicollinearity">Multicollinearity</a></li>
</ul></li>
<li><a href="#diagnosis-in-random-forests">Diagnosis in random forests</a>
<ul>
<li><a href="#residual-analysis-1">Residual analysis</a></li>
</ul></li>
<li><a href="#clustering">Clustering</a>
<ul>
<li><a href="#rationale-and-formulation-9">Rationale and formulation</a></li>
<li><a href="#theory-and-method-6">Theory and method</a></li>
<li><a href="#formal-definition-of-the-gmm">Formal definition of the GMM</a></li>
<li><a href="#r-lab-8">R Lab</a></li>
</ul></li>
<li><a href="#remarks-4">Remarks</a>
<ul>
<li><a href="#derivation-of-the-em-algorithm">Derivation of the EM algorithm</a></li>
<li><a href="#convergence-of-the-em-algorithm">Convergence of the EM Algorithm</a></li>
<li><a href="#clustering-by-random-forest">Clustering by random forest</a></li>
<li><a href="#clustering-based-prediction-models">Clustering-based prediction models</a></li>
</ul></li>
<li><a href="#exercises-4">Exercises</a></li>
</ul></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a>
<ul>
<li><a href="#overview-5">Overview</a></li>
<li><a href="#support-vector-machine">Support vector machine</a>
<ul>
<li><a href="#rationale-and-formulation-10">Rationale and formulation</a></li>
<li><a href="#theory-and-method-7">Theory and method</a></li>
<li><a href="#r-lab-9">R Lab</a></li>
</ul></li>
<li><a href="#ensemble-learning">Ensemble learning</a>
<ul>
<li><a href="#rationale-and-formulation-11">Rationale and formulation</a></li>
<li><a href="#analysis-of-the-decision-tree-random-forests-and-adaboost">Analysis of the decision tree, random forests, and AdaBoost</a></li>
<li><a href="#r-lab-10">R Lab</a></li>
</ul></li>
<li><a href="#remarks-5">Remarks</a>
<ul>
<li><a href="#is-svm-a-more-complex-model">Is SVM a more complex model?</a></li>
<li><a href="#is-svm-a-neural-network-model">Is SVM a neural network model?</a></li>
<li><a href="#derivation-of-the-margin">Derivation of the margin</a></li>
<li><a href="#why-the-nonzero-alpha_n-are-the-support-vectors">Why the nonzero <span class="math inline">\(\alpha_n\)</span> are the support vectors</a></li>
<li><a href="#adaboost-algorithm">AdaBoost algorithm</a></li>
</ul></li>
<li><a href="#exercises-5">Exercises</a></li>
</ul></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a>
<ul>
<li><a href="#overview-6">Overview</a></li>
<li><a href="#lasso">LASSO</a>
<ul>
<li><a href="#rationale-and-formulation-12">Rationale and formulation</a></li>
<li><a href="#the-shooting-algorithm">The shooting algorithm</a></li>
<li><a href="#a-small-data-example">A small data example</a></li>
<li><a href="#r-lab-11">R Lab</a></li>
</ul></li>
<li><a href="#principal-component-analysis">Principal component analysis</a>
<ul>
<li><a href="#rationale-and-formulation-13">Rationale and formulation</a></li>
<li><a href="#theory-and-method-8">Theory and method</a></li>
<li><a href="#a-small-data-example-1">A small data example</a></li>
<li><a href="#r-lab-12">R Lab</a></li>
</ul></li>
<li><a href="#remarks-6">Remarks</a>
<ul>
<li><a href="#why-lasso-uses-the-l1-norm">Why LASSO uses the L<sub>1</sub> norm</a></li>
<li><a href="#the-myth-of-pca">The myth of PCA</a></li>
</ul></li>
<li><a href="#exercises-6">Exercises</a></li>
</ul></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a>
<ul>
<li><a href="#overview-7">Overview</a></li>
<li><a href="#kernel-regression-model">Kernel regression model</a>
<ul>
<li><a href="#rationale-and-formulation-14">Rationale and formulation</a></li>
<li><a href="#theory-and-method-9">Theory and method</a></li>
<li><a href="#r-lab-13">R Lab</a></li>
</ul></li>
<li><a href="#conditional-variance-regression-model">Conditional variance regression model</a>
<ul>
<li><a href="#rationale-and-formulation-15">Rationale and formulation</a></li>
<li><a href="#theory-and-method-10">Theory and method</a></li>
<li><a href="#r-lab-14">R Lab</a></li>
</ul></li>
<li><a href="#remarks-7">Remarks</a>
<ul>
<li><a href="#experiment">Experiment</a></li>
<li><a href="#linear-regression-as-a-kernel-regression-model">Linear regression as a kernel regression model</a></li>
<li><a href="#more-about-heteroscedasticity">More about heteroscedasticity</a></li>
</ul></li>
<li><a href="#exercises-7">Exercises</a></li>
</ul></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a>
<ul>
<li><a href="#overview-8">Overview</a></li>
<li><a href="#deep-learning">Deep learning</a>
<ul>
<li><a href="#rationale-and-formulation-16">Rationale and formulation</a></li>
<li><a href="#r-lab-15">R Lab</a></li>
</ul></li>
<li><a href="#intrees">inTrees</a>
<ul>
<li><a href="#rationale-and-formulation-17">Rationale and formulation</a></li>
<li><a href="#theory-and-method-11">Theory and method</a></li>
<li><a href="#r-lab-16">R Lab</a></li>
</ul></li>
<li><a href="#remarks-8">Remarks</a>
<ul>
<li><a href="#images-text-and-audio">Images, text, and audio</a></li>
<li><a href="#a-key-is-made-to-unlock-but-what-is-the-lock">A key is made to unlock, but what is the lock?</a></li>
<li><a href="#decay-and-relative-decay">Decay and relative decay</a></li>
</ul></li>
<li><a href="#exercises-8">Exercises</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a>
<ul>
<li><a href="#the-normal-distribution">The Normal Distribution</a></li>
<li><a href="#matrix-operations">Matrix Operations</a></li>
<li><a href="#optimization">Optimization</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="header">
<h1 class="title">Data Analytics</h1>
<h4 class="author"><em>Shuai Huang &amp; Houtao Deng</em></h4>
<h4 class="date"><em>A Small Data Approach</em></h4>
</div>
<div id="preface" class="section level1 unnumbered">
<h1>Preface</h1>
<p>This book is suitable for an introductory course of data analytics to help students understand some main statistical learning models, such as linear regression, logistic regression, tree models and random forests, ensemble learning, sparse learning, principal component analysis, kernel methods including the support vector machine and kernel regression, etc. Data science practice is a process that should be told as a story, rather than a one-time implementation of one single model. This process is a main focus of this book, with many course materials about exploratory data analysis, residual analysis, and flowcharts to develop and validate models and data pipelines.</p>
<p>There are <span class="math inline">\(10\)</span> chapters. Except for Chapter 1, which gives an overview of the book, each chapter will introduce two or three techniques. For each technique, we will highlight the intuition and rationale behind it. We then articulate the intuition, use math to formulate the learning problem, and present the full version of the analytic formulation. We use R to implement the technique on both simulated or real-world datasets, present the analysis process (together with R code), show the dynamics in the analysis process, and comment on the results. Some Remarks are also made at the end of each chapter to enhance understanding of the techniques, reveal their different natures by other perspectives, reveal their limitations, and mention existing remedies to overcome these limitations.</p>
<p>There are three unique aspects to this book.</p>
<p>First, instructors will find many small datasets (i.e., consisting of 5—10 data points of 2—4 variables) in this book for models to be manually implemented by students using step-by-step process. The idea is to let students work out pencil solutions and then compare them with results obtained from established R packages. For example, a dataset with <span class="math inline">\(3\)</span> data points and <span class="math inline">\(2\)</span> predictors is used to illustrate how the shooting algorithm of LASSO could be implemented both on paper and in the R package <code>glmnet</code>. Another example is that, to understand the concept of the support vector machine (SVM), we use a dataset with <span class="math inline">\(4\)</span> data points and <span class="math inline">\(2\)</span> predictors to illustrate how the dual formulation of SVM could be solved manually. Furthermore, by this small dataset we help students see the connection between the computational algorithm with the geometric pattern of the data, i.e., the correspondence between the numeric solution with the so-called support vectors clearly visible in the scatterplot of the data.</p>
<p>Second, instructors will find graphical illustrations to explain some methods to students. These angles exploit connections between the methods; for example, the SVM is illustrated as a neural network; the kernel regression is introduced as a departure from the mindset of global models; and the logistic regression model is introduced as a few creative twists of the modeling process to apply the linear method for a binary classification problem, etc. On a larger scale, the connection between classic statistical models with machine learning algorithms is illustrated by focusing on the understanding of the iterative nature of the computational algorithms enabled by computers. We help students develop an eye for a method’s connection with other models that only appear to be different. This understanding will help us know a method’s strength and limitations, the importance of the context, and the assumptions we have carried in our data analysis.</p>
<p>Third, it is important for students to understand the storytelling component of data science. Data scientists tell stories every day. A story conveys a message, and a skillful data scientist must have the experience that the message changes its shape and meaning, depending on which model is used, how the model is tuned, or what part of the data is used. And some models have assumed a particular storytelling mode or structure. For example, we found hypothesis testing is a difficult concept for students to understand its essence, because it is a “negative” reading of data. It is not to translate what the data says, but to seek evidence from data against the null hypothesis we will need to come up with first. Examples as such will be found in the book to help students have a larger and deeper view of what they will learn.</p>

</div>
<div id="acknowledgments" class="section level1 unnumbered">
<h1>Acknowledgments</h1>
<p>The first draft of this book was written in the summer of 2017 to be used as the textbook for a new course about Data Analytics (IND E 498) in the Department of Industrial &amp; Systems Engineering of the University of Washington-Seattle. The course participants were mostly senior undergraduate students and first-year graduate students who provided invaluable comments and feedback to improve the book. The authors also thank Ameer Hamza Shakur, Jingshuo Feng, Prof. Xiangyu Chang and his students for their generous help on some figures, R code, and a range of R/LaTex tools. We also thank the Alzheimer’s Disease Neuroimaging Initiative (ADNI, <a href="https://adni.loni.usc.edu/">https://adni.loni.usc.edu/</a>) for the data used in this book.</p>
<p>In writing this book, we owe great debt to many people who generously share their materials and codes online. During the three-year writing process, we tried our best to acknowledge and cite all the specific resources we have used, and we may still have missed a few. In online communities such as <a href="GitHub.com">GitHub.com</a> and <a href="stackoverflow.com">stackoverflow.com</a> and numerous personal websites/blogs, you can find free resources which can help you quickly start a new project. Most importantly, this book in its current form wouldn’t be possible without R and RStudio (<a href="https://www.rstudio.com">https://www.rstudio.com</a>), <code>bookdown</code> (<a href="https://bookdown.org/">https://bookdown.org/</a>)<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> Xie, Y., <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>, CRC Press, 2019.</span>, and the Tufte-LaTeX Developers.</p>
<p>Last, but not least, the authors would like to take this opportunity to thank their editor, John Kimmel, for his support and encouragement throughout the development of this book. The authors also would like to thank the anonymous reviewers who have given great comments and the project editor Michele Dimont and the copyeditor’s remarkable work to improve the book.</p>

</div>
<div id="chapter-1.-introduction" class="section level1 unnumbered">
<h1>Chapter 1. Introduction</h1>
<div id="who-will-benefit-from-this-book" class="section level2 unnumbered">
<h2>Who will benefit from this book?</h2>
<p>Students who will find this book useful are those who have not systematically learned statistics or machine learning, but have had some exposure to basic statistical knowledge such as normal distribution, hypothesis testing, and are interested in finding data scientist jobs in a variety of areas. And practitioners with or without formal training in data science-related disciplines, who use data science in interdisciplinary areas, will find this book a useful addition. For example, I know a friend who learned statistics in college, more or less as applied mathematics that less emphasized data, computation, and storytelling, had found a remarkable resemblance between many data science methods with some concepts that she learned 20 years ago. She said if she could have a book that helps connect all the dots, and go through the materials with an easy-to-follow programming tool, it would help her move to a new field of work, as she is a physicist who is now working on genetics data.</p>
<p>We feel the same way. We have been working with medical doctors to diagnose surgical site infections using mobile phone images, with healthcare professionals to use hospital data to optimize the care process, with biologists and epidemiologists to understand the natural history of diseases, and with manufacturing companies to build Internet-of-Things, among others. The challenge of interdisciplinary collaboration is to cross boundaries and build new platforms. To embark on this adventure, a flexible understanding of our methods is important, as well as the skill of storytelling.</p>
<p>To help readers develop these skills, the style of the book highlights a combination of two aspects: technical concreteness and holistic thinking<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> The chapters are named using different qualities of holistic thinking in decision-makings, including “Abstraction,” “Recognition,” “Resonance,” “Learning,” “Diagnosis,” “Scalability,” “Pragmatism,” and “Synthesis.”</span>. Holistic thinking is the foundation of how we formulate problems and why we could trust our formulations, knowing that our formulations inevitably are only a partial representation of a real-world problem. Holistic thinking is also the foundation of communication between team members of different backgrounds. With a diverse team, things that make sense intuitively are important to build team-wide trust in decision-making. And technical concreteness is the springboard for us to jump into a higher awareness and understanding of the problem to make holistic decisions.</p>
<p>To begin our journey, first, let’s look at the big picture, the data analytics pipeline.</p>
</div>
<div id="overview-of-a-data-analytics-pipeline" class="section level2 unnumbered">
<h2>Overview of a data analytics pipeline</h2>
<p>A typical data analytics pipeline consists of several major pillars. In the example shown in Figure <a href="#fig:ffig1">1</a>, it has four pillars: sensor and devices, data preprocessing and feature engineering, feature selection and dimension reduction, and modeling and data analysis. While this is not the only way to present the diverse data pipelines in the real world, these pipelines more or less resemble this architecture.</p>
<!-- ```{r, fig.cap='\\label{fig:fig1} Overview of a data analytics pipeline', echo=FALSE, message=FALSE, warning=FALSE,fig.fullwidth=FALSE,fig.margin=FALSE}
knitr::include_graphics("graphics/1.png",dpi = 300)
``` -->
<p></p>
<div class="figure" style="text-align: center"><span id="fig:ffig1"></span>
<p class="caption marginnote shownote">
Figure 1: Overview of a data analytics pipeline
</p>
<img src="graphics/1.png" alt="Overview of a data analytics pipeline" width="80%"  />
</div>
<p></p>
<p>The pipeline starts with a real-world problem, for which we are not sure about the underlying system/mechanism, but we are able to characterize the system by defining some variables. Then, we could develop sensors and devices to acquire measurements of these variables<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> These measurements, we call data, are objective evidences that we can use to explore the statistical principles or mechanistic laws regulating the system behaviors.</span>. Before analyzing the data and building models, there is a step for data preprocessing and feature engineering. For example, some signals acquired by sensors are not interpretable or not easily compatible with human perceptions, such as the signal acquired by MRI scanning machines in the Fourier space. Data preprocessing also refers to removal of outliers or imputation of missing data, detection and removal of redundant features, to name a few. After data preprocessing, we may conduct feature selection and dimension reduction to distill or condense signals in the data and reduce noise. Finally, we conduct modeling and data analysis on the prepared dataset to gain knowledge and build models of the real-world system<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> Prediction models are quite common, but other models for decision-makings, such as system modeling, monitoring, intervention, and control, can be built as well.</span>.</p>
<p>This book focuses on the last two pillars of this pipeline, the modeling, data analysis, feature selection, and dimension reduction methods. But it is helpful to keep in mind the big picture of a data analytics pipeline. Because in practice, it takes a whole pipeline to make things work.</p>
</div>
<div id="topics-in-a-nutshell" class="section level2 unnumbered">
<h2>Topics in a nutshell</h2>
<p>Specific techniques that will be introduced in this book are shown below.</p>
<div id="data-models-i.e.-regression-based-techniques" class="section level3 unnumbered">
<h3>Data models (i.e., regression-based techniques)</h3>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> Chapter 2: Linear regression, least squares estimation, hypothesis testing, R-squared, First Derivative Test, connection with experimental design, data-generating mechanism, history of adventures in understanding errors, exploratory data analysis (EDA)</p></li>
<li><p> Chapter 3: Logistic regression, generalized least squares estimation, iterative reweighted least squares (IRLS) algorithm, ranking (formulated as a regression problem)</p></li>
<li><p> Chapter 4: Bootstrap, data resampling, nonparametric hypothesis testing, nonparametric confidence interval estimation</p></li>
<li><p> Chapter 5: Overfitting and underfitting, limitation of R-squared, training dataset and testing dataset, random sampling, K-fold cross-validation, the confusion matrix, false positive and false negative, the Receiver Operating Characteristics (ROC) curve, the law of errors, how data scientists work with clients</p></li>
<li><p> Chapter 6: Residual analysis, normal Q-Q plot, Cook’s distance, leverage, multicollinearity, heterogeneity, clustering, Gaussian mixture model (GMM), the Expectation-Maximization (EM) algorithm, Jensen Inequality</p></li>
<li><p> Chapter 7: Support Vector Machine (SVM), generalize data versus memorize data, maximum margin, support vectors, model complexity and regularization, primal-dual formulation, quadratic programming, KKT condition, kernel trick, kernel machines, SVM as a neural network model</p></li>
<li><p> Chapter 8: LASSO, sparse learning, <span class="math inline">\(L_1\)</span>-norm and <span class="math inline">\(L_2\)</span>-norm regularization, Ridge regression, feature selection, shooting algorithm, Principal Component Analysis (PCA), eigenvalue decomposition, scree plot</p></li>
<li><p> Chapter 9: Kernel regression as generalization of linear regression model, local smoother regression model, k-nearest neighbor (KNN) regression model, conditional variance regression model, heteroscedasticity, weighted least squares estimation, model extension and stacking</p></li>
<li><p> Chapter 10: Deep learning, neural network, activation function, model primitives, convolution, max pooling, convolutional neural network (CNN)</p></li>
</ul>
<p><!-- end{itemize} --></p>
</div>
<div id="algorithmic-models-i.e.-tree-based-techniques" class="section level3 unnumbered">
<h3>Algorithmic models (i.e., tree-based techniques)</h3>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> Chapter 2: Decision tree, entropy, information gain (IG), node splitting, pre- and post-pruning, empirical error, generalization error, pessimistic error by binomial approximation, greedy recursive splitting</p></li>
<li><p> Chapter 3: System monitoring reformulated as classification problem, real-time contrasts method (RTC), design of monitoring statistics, sliding window, anomaly detection, false alarm</p></li>
<li><p> Chapter 4: Random forest, Gini index, weak classifiers, the probabilistic mechanism about why random forests can create a strong classifier out of many weak classifiers, importance score, partial dependency plot</p></li>
<li><p> Chapter 5: Out-of-bag (OOB) error in random forest</p></li>
<li><p> Chapter 6: Residual analysis, clustering by random forests</p></li>
<li><p> Chapter 7: Ensemble learning, Adaboost, analysis of ensemble learning from statistical, computational, and representational perspectives</p></li>
<li><p> Chapter 10: Automations of pipelines, integration of tree models, feature selection, and regression models in <code>inTrees</code>, random forest as a rule generator, rule extraction, pruning, selection, and summarization, confidence and support of rules, variable interactions, rule-based prediction</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>In this book, we will use lower case letters, e.g., <span class="math inline">\(x\)</span>, to represent scalars, bold face, lower case letters, e.g., <span class="math inline">\(\boldsymbol{x}\)</span>, to represent vectors, and bold face, upper case letters, e.g., <span class="math inline">\(\boldsymbol{X}\)</span>, to represent matrices.</p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
</div>
<div id="chapter-2.-abstraction-regression-tree-models" class="section level1 unnumbered">
<h1>Chapter 2. Abstraction: Regression &amp; Tree Models</h1>
<div id="overview" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 2 is about <em>Abstraction</em>. It concerns how we model and formulate a problem using <em>specific mathematical models</em>. Abstraction is powerful. It begins with identification of a few main entities from the problem, and continues to characterize their relationships. Then we focus on the study of these interconnected entities as a pure mathematical system. Consequences can be analytically established within this abstracted framework, while a phenomenon in a concerned context could be identified as special instances, or manifestations, of the abstracted model. In other words, by making abstraction of a real-world problem, we free ourselves from the application context that is usually unbounded and not well defined.</p>
<p>People often adopt a blackbox view of a real-world problem, as shown in Figure <a href="#fig:f2-1">2</a>. There is one (or more) key performance metrics of the system, called the output variable<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> Denoted as <span class="math inline">\(y\)</span>, e.g., the yield of a chemical process, the mortality rate of an ICU, the GDP of a nation, etc.</span>, and there is a set of input variables<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> Denoted as <span class="math inline">\(x_{1}, x_{2}, \ldots, x_{p}\)</span>; also called predictors, covariates, features, and, sometimes, factors.</span> that may help us predict the output variable. These variables are the <em>few main entities</em> identified from the problem, and how the input variables impact the output variable is <em>one</em> main type of relationship we develop models to characterize.</p>
<!--

 \small

<div class="figure">
<p class="caption">(\#fig:unnamed-chunk-1)\label{fig:2-1} The blackbox nature of many data science problems</p><img src="graphics/2_1.png" alt="\label{fig:2-1} The blackbox nature of many data science problems"  /></div>

 \normalsize
-->
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-1"></span>
<p class="caption marginnote shownote">
Figure 2: The blackbox nature of many data science problems
</p>
<img src="graphics/2_1.png" alt="The blackbox nature of many data science problems" width="80%"  />
</div>
<p></p>
<p>These relationships are usually unknown, due to our lack of understanding of the system. It is not always plausible or economically feasible to develop a Newtonian style characterization of the system<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> I.e., using differential equations.</span>. Thus, statistical models are needed. They collect data from this blackbox system and build models to characterize the relationship between the input variables and the output variable. Generally, there are two cultures for statistical modeling<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> Breiman, L., * Statistical Modeling: The Two Cultures,* Statistical Science, Volume 16, Issue 3, 199-231, 2001.</span>: One is the <strong>data modeling</strong> culture, while another is the <strong>algorithmic modeling</strong> culture. Linear regression models are examples of the <em>data modeling</em> culture; decision tree models are examples of the <em>algorithmic modeling</em> culture.</p>
<p>Two goals are shared by the two cultures: (1) to understand the relationships between the predictors and the output, and (2) to predict the output based on the predictors. The two also share some common criteria to evaluate the success of their models, such as the prediction performance. Another commonality they share is a generic form of their models</p>
<p><span class="math display" id="eq:ch2-genericmodel">\[\begin{equation}
    y=f(\boldsymbol{x})+\epsilon,
\tag{1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f(\boldsymbol{x})\)</span> reflects the <em>signal</em> part of <span class="math inline">\(y\)</span> that can be ascertained by knowing <span class="math inline">\(\boldsymbol{x}\)</span>, and <span class="math inline">\(\epsilon\)</span> reflects the <em>noise</em> part of <span class="math inline">\(y\)</span> that remains uncertain even when we know <span class="math inline">\(x\)</span>. To better illustrate this, we could annotate the model form in Eq. <a href="#eq:ch2-genericmodel">(1)</a> as<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> An interesting book about the antagonism between signal and noise: Silver, N., <em>The Signal and the Noise: Why So Many Predictions Fail–but Some Don’t</em>, Penguin Books, 2015. The author’s prediction model, however, failed to predict Donald Trump’s victory of the 2016 US Election.</span></p>
<p><span class="math display" id="eq:2-genericmodel">\[\begin{equation}
    \underbrace{y}_{data} = \underbrace{f(\boldsymbol{x})}_{signal} + \underbrace{\epsilon}_{noise},
\tag{2}
\end{equation}\]</span></p>
<p>The two cultures differ in their ideas about how to model these two parts. A brief illustration is shown in Table <a href="#tab:t2-1">1</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-1">Table 1: </span>Comparison between the two cultures of models</span><!--</caption>--></p>
<table>
<colgroup>
<col width="12%" />
<col width="19%" />
<col width="33%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left"><span class="math inline">\(f(\boldsymbol{x})\)</span></th>
<th align="left"><span class="math inline">\(\epsilon\)</span></th>
<th align="left">Ideology</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Data Modeling</strong></td>
<td align="left">Explicit form (e.g., linear regression).</td>
<td align="left">Statistical distribution (e.g., Gaussian).</td>
<td align="left">Imply <em>Cause</em> and <em>effect</em>; uncertainty has a structure.</td>
</tr>
<tr class="even">
<td align="left"><strong>Algorithmic Modeling</strong></td>
<td align="left">Implicit form (e.g., tree model).</td>
<td align="left">Rarely modeled as structured uncertainty; taken as meaningless noise.</td>
<td align="left">More focus on prediction; to <em>fit</em> data rather than to <em>explain</em> data.</td>
</tr>
</tbody>
</table>
<p></p>
<p>An illustration of the <em>data modeling</em>, using linear regression model, is shown in Figure <a href="#fig:f2-datamodel">3</a>. To develop such a model, we need efforts in two endeavors: the modeling of the signal, and the modeling of the noise (also called errors). It was probably the modeling of the errors, rather than the modeling of the signal, that eventually established a science: Statistics<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> Errors, as the name suggests, are embarrassment to a theory that claims to be rational. Errors are irrational, like a crack on the smooth surface of rationality. But rationally, if we could find <em>a law of errors</em>, we then find the law of irrationality. With that, once again rationality trumps irrationality, and the crack is sealed.</span>.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f2-datamodel"></span>
<img src="graphics/2_datamodel.png" alt="Illustration of the *ideology* of data modeling, i.e., data is used to calibrate, or, estimate, the parameters of a pre-specified mathematical structure" width="80%"  />
<p class="caption marginnote shownote">
Figure 3: Illustration of the <em>ideology</em> of data modeling, i.e., data is used to calibrate, or, estimate, the parameters of a pre-specified mathematical structure
</p>
</div>
<p></p>
<p>One only needs to take a look at the beautiful form of the normal distribution (and notice the name as well) to have an impression of its grand status as the law of errors. Comparing with other candidate forms that historically were its competitors, this concentrated, symmetrical, round and smooth form seems a more rational form that a law should take, i.e., see Figure <a href="#fig:f2-errorlaws">4</a>.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f2-errorlaws"></span>
<img src="graphics/2_errorlaws.png" alt="Hypothesized laws of errors, including the normal distribution (also called the Gaussian distribution, developed by Gauss in 1809) and some of its old rivalries" width="80%"  />
<p class="caption marginnote shownote">
Figure 4: Hypothesized laws of errors, including the normal distribution (also called the Gaussian distribution, developed by Gauss in 1809) and some of its old rivalries
</p>
</div>
<p></p>
<p>The <span class="math inline">\(\epsilon\)</span> in Eq. <a href="#eq:ch2-genericmodel">(1)</a> is often called the <strong>error term</strong>, noise term, or residual term. <span class="math inline">\(\epsilon\)</span> is usually modeled as a Gaussian distribution with mean as <span class="math inline">\(0\)</span>. The mean has to be <span class="math inline">\(0\)</span>; otherwise, it contradicts with the name <em>error</em>. <span class="math inline">\(f(\boldsymbol{x})\)</span> is also called the model of the mean structure<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> To see that, notice that <span class="math inline">\(\mathrm{E}{(y)} = \mathrm{E}{[f(\boldsymbol{x}) + \epsilon]} = \mathrm{E}{[f(\boldsymbol{x})]} + \mathrm{E}{[\epsilon]}\)</span>. Since <span class="math inline">\(\mathrm{E}{(\epsilon)} = 0\)</span> and <span class="math inline">\(f(\boldsymbol{x})\)</span> is not a random variable, we have <span class="math inline">\(\mathrm{E}{(y)} = f(\boldsymbol{x})\)</span>. Thus, <span class="math inline">\(f(\boldsymbol{x})\)</span> essentially predicts the mean of the output variable.</span>.</p>
</div>
<div id="regression-models" class="section level2 unnumbered">
<h2>Regression models</h2>
<div id="rationale-and-formulation" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>Let’s consider a simple regression model, where there is only one predictor <span class="math inline">\(x\)</span> to predict the outcome <span class="math inline">\(y\)</span>. Linear regression model assumes a linear form of <span class="math inline">\(f(x)\)</span></p>
<p><span class="math display" id="eq:2-simLR-fx">\[\begin{equation}
f(x)=\beta_{0}+\beta_{1} x ,
\tag{3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is called the <strong>intercept</strong>, and <span class="math inline">\(\beta_1\)</span> is called the <strong>slope</strong>. Both are also called <strong>regression coefficients</strong>, or more generally, <strong>parameters</strong>.</p>
<p>And <span class="math inline">\(\epsilon\)</span> is modeled as a normal distribution<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> I.e., could be other types of distributions, but normal distribution is the norm.</span> with mean <span class="math inline">\(0\)</span>,</p>
<p><span class="math display" id="eq:2-simLR-eps">\[\begin{equation}
\epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right),
\tag{4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> is the <strong>variance</strong> of the error.</p>
<p>For any given value of <span class="math inline">\(x\)</span>, we know the model of <span class="math inline">\(y\)</span> is</p>
<p><span class="math display" id="eq:2-simLR-y">\[\begin{equation}
y = \beta_{0}+\beta_{1}x + \epsilon.
\tag{5}
\end{equation}\]</span></p>
<p>As Figure <a href="#fig:f2-lrpred">5</a> reveals, in linear regression model, <span class="math inline">\(y\)</span> is not modeled as a numerical value, but as a distribution. In other words, <span class="math inline">\(y\)</span> itself is treated as a random variable. Its distribution’s mean is modeled by <span class="math inline">\(x\)</span> and the variance is <em>inherited</em> from <span class="math inline">\(\epsilon\)</span>. Knowing the value of <span class="math inline">\(x\)</span> helps us to determine the <em>location</em> of this distribution, but not the <em>shape</em>—the shape is always fixed.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-lrpred"></span>
<p class="caption marginnote shownote">
Figure 5: In a linear regression model, <span class="math inline">\(y\)</span> is modeled as a distribution as well
</p>
<img src="graphics/2_lrpred.png" alt="In a linear regression model, $y$ is modeled as a distribution as well" width="80%"  />
</div>
<p></p>
<p>To make a prediction of <span class="math inline">\(y\)</span> for any given <span class="math inline">\(x\)</span>, <span class="math inline">\(\beta_{0}+\beta_{1}x\)</span> comes as a natural choice. It is too natural that it is often unnoticed or unquestioned. Nonetheless, to predict a random variable, using its mean is the “best” choice, but it is not the only possibility, as Figure <a href="#fig:f2-lrpred">5</a> reveals that <span class="math inline">\(y\)</span> itself is a random variable, and to predict a random variable, we could also use a confidence interval instead of a point estimate. It depends on what you’d like to predict. If the goal is to predict what is the most likely value for <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>, then the best guess is <span class="math inline">\(\beta_{0}+\beta_{1}x\)</span>.<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> An important job for statisticians is to prove some ideas are our best choices, i.e., by showing that these choices are optimal decisions under some specific conditions (accurately defined by mathematical terms). It is often that intuitions come before proofs, so many theories are actually developed retrospectively.</span></p>
<!-- \begin{figure} -->
<!--     \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width=0.95\textwidth]{graphics/2_lrpred.png} -->
<!--    \caption{} -->
<!--    \label{fig:2-lrpred} -->
<!-- \end{figure} -->
<p>There are more assumptions that have been made to enable the model in Eq. <a href="#eq:2-simLR-y">(5)</a>.</p>
<p><!-- begin{itemize} --></p>
<ul>
<li> There is a linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. And this linear relationship remains the same for all the values of <span class="math inline">\(x\)</span>. This is often referred to as a <em>global</em> relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Sometimes this assumption is considered strong, e.g., as shown in Figure <a href="#fig:f2-2"><strong>??</strong></a>, in drug research it is often found that the dose (<span class="math inline">\(x\)</span>) is related to the effect of the drug (<span class="math inline">\(y\)</span>) in a varying manner that depends on the value of <span class="math inline">\(x\)</span>. Still, from Figure <a href="#fig:f2-2"><strong>??</strong></a> we can see that the linear line captures an essential component in the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, providing a good statistical approximation. Regression models that capture <em>locality</em> in the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are introduced in <strong>Chapter 9</strong>.</li>
</ul>
<ul>
<li> The model acknowledges a degree of unpredictability of <span class="math inline">\(y\)</span>. Eq. <a href="#eq:2-simLR-y">(5)</a> indicates that <span class="math inline">\(y\)</span> is generated by a combination of the signal (i.e., <span class="math inline">\(\beta_{0}+\beta_{1}x\)</span>) and the noise (i.e., <span class="math inline">\(\epsilon\)</span>). Since we could never predict noise, we compute a metric called <strong>R-squared</strong> to quantify the predictability of a model</li>
</ul>
<p><span class="math display" id="eq:2-R2">\[\begin{equation}
        \text{R-squared} = \frac{\sigma_{y}^{2}-\sigma_{\varepsilon}^{2}}{\sigma_{y}^{2}}.
\tag{6}
    \end{equation}\]</span>
Here, <span class="math inline">\(\sigma_{y}^{2}\)</span> is the variance of <span class="math inline">\(y\)</span>. The <em>R-squared</em> ranges from <span class="math inline">\(0\)</span> (zero predictability) to <span class="math inline">\(1\)</span> (perfect predictability).</p>
<ul>
<li> The <em>significance</em> of <span class="math inline">\(x\)</span> in predicting <span class="math inline">\(y\)</span>, and the <em>accuracy</em> of <span class="math inline">\(x\)</span> in predicting <span class="math inline">\(y\)</span>, are two different concepts. A predictor <span class="math inline">\(x\)</span> could be inadequate in predicting <span class="math inline">\(y\)</span>, i.e., the R-squared could be as low as <span class="math inline">\(0.1\)</span>, but it still could be statistically significant. In other words, the relation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is not strong, but it is not spurious either. This often happens in social science research and education research projects. Some scenarios are shown in Figure <a href="#fig:f2-signvsaccu">6</a>.</li>
</ul>
<p></p>
<div class="figure fullwidth"><span id="fig:f2-signvsaccu"></span>
<img src="graphics/2_fourtypes.png" alt="Significance vs. accuracy" width="80%"  />
<p class="caption marginnote shownote">
Figure 6: Significance vs. accuracy
</p>
</div>
<p></p>
<ul>
<li> The noise is usually modeled as a normal distribution, but this assumption could be relaxed. A detailed discussion about how to check the normality assumption in data analysis can be found in <strong>Chapter 5</strong>.</li>
</ul>
<p><!-- end{itemize} --></p>
</div>
<div id="theory-and-method" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p><em>Parameter estimation.</em> To estimate a model is to estimate its parameters, i.e., for the model shown in Eq. <a href="#eq:2-simLR-y">(5)</a>, unknown parameters include <span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span>, and <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>. Usually, we estimate the regression coefficients first. Then, as shown in Figure <a href="#fig:f2-datamodel">3</a>, errors could be computed, and further, <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> could be estimated<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> I.e., as a standard practice of sample variance estimation by taking the residuals (i.e., <span class="math inline">\(\epsilon_1\)</span>, <span class="math inline">\(\epsilon_2\)</span> and <span class="math inline">\(\epsilon_3\)</span>) as <em>samples</em> of the population of <em>error</em>.</span>.</p>
<p>A training dataset is collected to estimate the parameters. The basic idea is that the best estimate should lead to a line, as shown in Figure <a href="#fig:f2-datamodel">3</a>, that fits the training data as close as possible. To quantify this quality of fitness of a line, two principles are shown in Figure <a href="#fig:f2-3">7</a>: one based on perpendicular offset (left), while another one based on vertical offset (right). History of statistics has chosen the vertical offset as a more favorable approach, since it leads to tractability in analytic forms<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> When there were no computers yet, analytic tractability was, and still is, held as a sacred quality of a model.</span>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-3"></span>
<p class="caption marginnote shownote">
Figure 7: Two principles to fit a linear regression model: (left) perpendicular offsets; (right) vertical offsets. The distances between the dots (the training data) with the line (the trained model) provide a quantitative metric of how well the model fits the data.
</p>
<img src="graphics/2_3.png" alt="Two principles to fit a linear regression model: (left) perpendicular offsets; (right) vertical offsets. The distances between the dots (the training data) with the line (the trained model) provide a quantitative metric of how well the model fits the data." width="80%"  />
</div>
<p></p>
<p>The principle of minimizing vertical offsets leads to the <strong>least-squares estimation</strong> of linear regression models. We can exercise the least squares estimation using the simple regression model shown in Eq. <a href="#eq:2-simLR-y">(5)</a>. The objective, based on the principle suggested in Figure <a href="#fig:f2-3">7</a> (right), is to find the line that <strong>minimizes</strong> the <strong>sum of the squared</strong> of the vertical derivations of the observed data points from the line.</p>
<p>Suppose that we have collected <span class="math inline">\(N\)</span> data points, denoted as, <span class="math inline">\(\left(x_{n}, y_{n}\right)\)</span> for <span class="math inline">\(n=1,2, \dots, N\)</span>.<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> Data is paired, i.e., <span class="math inline">\(y_{n}\)</span> corresponds to <span class="math inline">\(x_{n}\)</span>.</span> For each data point, i.e., the <span class="math inline">\(n_{th}\)</span> data point, the residual <span class="math inline">\(\epsilon_{n}\)</span> is defined as</p>
<p><span class="math display" id="eq:2-simLR-res">\[\begin{equation}
\epsilon_{n} = y_{n}-\left(\beta_{0}+\beta_{1} x_{n}\right).
\tag{7}
\end{equation}\]</span></p>
<p>Then, we define the sum of the squared of the vertical derivations of the observed data points from the line as</p>
<p><span class="math display" id="eq:2-simLR-LS">\[\begin{equation}
l\left(\beta_{0}, \beta_{1}\right)=\sum_{n=1}^{N}\epsilon_{n}^2.
\tag{8}
\end{equation}\]</span></p>
<p>Plugging Eq. <a href="#eq:2-simLR-res">(7)</a> in Eq. <a href="#eq:2-simLR-LS">(8)</a> we have</p>
<p><span class="math display" id="eq:2-simLR-LS-2">\[\begin{equation}
l\left(\beta_{0}, \beta_{1}\right)=\sum_{n=1}^{N}\left[y_{n}-\left(\beta_{0}+\beta_{1} x_{n}\right)\right]^{2}.
\tag{9}
\end{equation}\]</span></p>
<p>To estimate <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> is to minimize this least squares <strong>loss function</strong> <span class="math inline">\(l\left(\beta_{0}, \beta_{1}\right)\)</span>. This is an <strong>unconstrained continuous optimization</strong> problem. We take derivatives of <span class="math inline">\(l\left(\beta_{0}, \beta_{1}\right)\)</span> regarding the two parameters and set them to be zero, to derive the estimation equations—this is a common practice of the <strong>First Derivative Test</strong>, illustrated in Figure <a href="#fig:f2-1stderivativetest">8</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-1stderivativetest"></span>
<img src="graphics/2_1stderivativetest.png" alt="Illustration of the **First Derivative Test** in optimization, i.e., the optimal solution would lead the first derivative to be zero. It is widely used in statistics and machine learning to find optimal solutions of some model formulations. More applications of this technique can be found in later chapters." width="250px"  />
<!--
<p class="caption marginnote">-->Figure 8: Illustration of the <strong>First Derivative Test</strong> in optimization, i.e., the optimal solution would lead the first derivative to be zero. It is widely used in statistics and machine learning to find optimal solutions of some model formulations. More applications of this technique can be found in later chapters.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display">\[
\frac{\partial l\left(\beta_{0}, \beta_{1}\right)}{\partial \beta_{0}}=-2 \sum_{n=1}^{N}\left[y_{n}-\left(\beta_{0}+\beta_{1} x_{n}\right)\right]=0,
\]</span>
<span class="math display">\[
\frac{\partial l\left(\beta_{0}, \beta_{1}\right)}{\partial \beta_{1}}=-2 \sum_{n=1}^{N} x_{n}\left[y_{n}-\left(\beta_{0}+\beta_{1} x_{n}\right)\right]=0.
\]</span></p>
<p>These two could be rewritten in a more succinct way</p>
<p><span class="math display">\[
\left[ \begin{array}{cc}{N} &amp; {\sum_{n=1}^{N} x_{n}} \\ {\sum_{n=1}^{N} x_{n}} &amp; {\sum_{n=1}^{N} x_{n}^{2}}\end{array}\right] \left[ \begin{array}{c}{\beta_{0}} \\ {\beta_{1}}\end{array}\right]=\left[ \begin{array}{c}{\sum_{n=1}^{N} y_{n}} \\ {\sum_{n=1}^{N} x_{n} y_{n}}\end{array}\right].
\]</span></p>
<p>We solve these two equations and derive the estimators of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>, denoted as <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>, respectively, as</p>
<p><span class="math display" id="eq:2-beta-hat-scalar">\[\begin{equation}
    \begin{aligned}
    &amp;\hat{\beta}_{1}=\frac{\sum_{n=1}^{N}\left(x_{n}-\overline{x}\right)\left(y_{n}-\overline{y}\right)}{\sum_{n=1}^{N} x_{n}^{2}-N \overline{x}^{2}}, \\
    &amp;\hat{\beta}_{0}= \overline{y} - \hat{\beta}_{1} \overline{x}.
    \end{aligned}
\tag{10}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\overline{x}\)</span> and <span class="math inline">\(\overline{y}\)</span> are the sample mean of the two variables, respectively.</p>
<p>There is a structure hidden inside Eq. <a href="#eq:2-beta-hat-scalar">(10)</a>. Note that the estimator <span class="math inline">\(\hat{\beta}_{1}\)</span> can be rewritten as</p>
<p><span class="math display" id="eq:2-beta1hat">\[\begin{equation}
\hat{\beta}_{1}=\frac{\sum_{n=1}^{N}\left(x_{n}-\overline{x}\right)\left(y_{n}-\overline{y}\right)}{N-1} \Big/ \frac{\sum_{n=1}^{N} x_{n}^{2}-N \overline{x}^{2}}{N-1},
\tag{11}
\end{equation}\]</span></p>
<p>and note that the sample variance of <span class="math inline">\(x\)</span> is defined as</p>
<p><span class="math display">\[
\operatorname{var}(x)=\frac{\sum_{n=1}^{N} x_{n}^{2}-N \overline{x}^{2}}{N-1},
\]</span></p>
<p>while the numerator in Eq. <a href="#eq:2-beta1hat">(11)</a> is called the <strong>sample covariance</strong><label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> The covariance is a measure of the joint variability of two random variables. Denoted as <span class="math inline">\(\operatorname{cov}(x, y)\)</span>, the larger the covariance, the stronger the two variables interact.</span>.</p>
<p>Thus, we can <em>re</em>write the estimators of <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{0}\)</span> as</p>
<p><span class="math display" id="eq:2-simLR-LSE">\[\begin{equation}
    \begin{aligned}
    &amp;\hat{\beta}_{1}=\frac{\operatorname{cov}(x, y)}{\operatorname{var}(x)}, \\
    &amp;\hat{\beta}_{0} = \overline{y} - \hat{\beta}_{1} \overline{x}.
    \end{aligned}
\tag{12}
\end{equation}\]</span></p>
<p><em>A small data example.</em> Let’s practice the estimation method using a simple example. The dataset is shown in Table <a href="#tab:t2-1ex">2</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-1ex">Table 2: </span>An example dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x\)</span></th>
<th align="left"><span class="math inline">\(1\)</span></th>
<th align="left"><span class="math inline">\(3\)</span></th>
<th align="left"><span class="math inline">\(3\)</span></th>
<th align="left"><span class="math inline">\(5\)</span></th>
<th align="left"><span class="math inline">\(5\)</span></th>
<th align="left"><span class="math inline">\(6\)</span></th>
<th align="left"><span class="math inline">\(8\)</span></th>
<th align="left"><span class="math inline">\(9\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(y\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(8\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Following Eq. <a href="#eq:2-beta-hat-scalar">(10)</a> we can get <span class="math inline">\(\beta_0 = -1.0714\)</span> and <span class="math inline">\(\beta_1 = 1.2143\)</span>. The R codes to verify your calculation are shown below.</p>
<p></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple example of regression with one predictor</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>),<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>),</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">4</span>),<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>),<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">5</span>),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">7</span>),<span class="fu">c</span>(<span class="dv">9</span>,<span class="dv">8</span>)))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(data) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;Y&quot;</span>,<span class="st">&quot;X&quot;</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(data)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>lm.YX <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X, <span class="at">data =</span> data)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.YX)</span></code></pre></div>
<p></p>
<p><em>Extension to multivariate regression model.</em> Consider a more general case where there are more than one predictor</p>
<p><span class="math display" id="eq:2-multiLR">\[\begin{equation}
    y=\beta_{0}+\sum_{i=1}^{p} \beta_{i} x_{i}+\varepsilon.
\tag{13}
\end{equation}\]</span></p>
<p>To fit this multivariate linear regression model with <span class="math inline">\(p\)</span> predictors, we collect <span class="math inline">\(N\)</span> data points, denoted as</p>
<p><span class="math display">\[
\boldsymbol{y}=\left[ \begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{N}}\end{array}\right], \text {     }  \boldsymbol{X}=\left[ \begin{array}{ccccc}{1} &amp; {x_{11}} &amp; {x_{21}} &amp; {\cdots} &amp; {x_{p 1}} \\ {1} &amp; {x_{12}} &amp; {x_{22}} &amp; {\cdots} &amp; {x_{p 2}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {1} &amp; {x_{1 N}} &amp; {x_{2 N}} &amp; {\cdots} &amp; {x_{p N}}\end{array}\right].
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{y} \in R^{N \times 1}\)</span> denotes for the <span class="math inline">\(N\)</span> measurements of the outcome variable, and <span class="math inline">\(\boldsymbol{X} \in R^{N \times(p+1)}\)</span> denotes for the data matrix that includes the <span class="math inline">\(N\)</span> measurements of the <span class="math inline">\(p\)</span> input variables and the intercept term, <span class="math inline">\(\beta_{0}\)</span>, i.e., the first column of <span class="math inline">\(\boldsymbol{X}\)</span> corresponds to <span class="math inline">\(\beta_{0}\)</span>.<label for="tufte-sn-18" class="margin-toggle sidenote-number">18</label><input type="checkbox" id="tufte-sn-18" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">18</span> Again, the data is paired, i.e., <span class="math inline">\(y_{n}\)</span> corresponds to <span class="math inline">\(\boldsymbol{x}_n\)</span> that is the <span class="math inline">\(n_{th}\)</span> row of the matrix <span class="math inline">\(\boldsymbol{X}\)</span>.</span></p>
<p>To estimate the regression coefficients in Eq. <a href="#eq:2-multiLR">(13)</a>, again, we use the least squares estimation method. The first step is to calculate the sum of the squared of the vertical derivations of the observed data points from “the line”<label for="tufte-sn-19" class="margin-toggle sidenote-number">19</label><input type="checkbox" id="tufte-sn-19" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">19</span> Here, actually, a hyperplane.</span>. Following Eq. <a href="#eq:2-simLR-res">(7)</a>, we can define the residual as</p>
<p><span class="math display" id="eq:2-multiLR-res">\[\begin{equation}
\epsilon_{n} = y_n - \left(\beta_{0}+\sum_{i=1}^{p} \beta_{i} x_{in}\right).
\tag{14}
\end{equation}\]</span></p>
<p>Then, following Eq. <a href="#eq:2-simLR-LS">(8)</a>, the sum of the squared of the vertical derivations of the observed data points from “the line” is</p>
<p><span class="math display" id="eq:2-multiLR-LS">\[\begin{equation}
l\left(\beta_{0}, ...,  \beta_{p}\right)=\sum_{n=1}^{N}\epsilon_{n}^2.
\tag{15}
\end{equation}\]</span></p>
<p>This is again an unconstrained continuous optimization problem, that could be solved by the same procedure we have done for the simple linear regression model. Here, we show how a vector-/matrix-based representation of this derivation process could make things easier.</p>
<p>Let’s write up the regression coefficients and residuals in vector forms as</p>
<p><span class="math display">\[
\boldsymbol{\beta}=\left[ \begin{array}{c}{\beta_{0}} \\ {\beta_{1}} \\ {\vdots} \\ {\beta_{p}}\end{array}\right], \text { and } \boldsymbol{\varepsilon}=\left[ \begin{array}{c}{\varepsilon_{1}} \\ {\varepsilon_{2}} \\ {\vdots} \\ {\varepsilon_{N}}\end{array}\right].
\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol{\beta} \in R^{(p+1) \times 1}\)</span> denotes for the regression parameters and <span class="math inline">\(\boldsymbol{\varepsilon} \in R^{N \times 1}\)</span> denotes for the <span class="math inline">\(N\)</span> residuals which are assumed to follow a normal distribution with mean as zero and variance as <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>.</p>
<p>Then, based on Eq. <a href="#eq:2-multiLR-res">(14)</a>, we rewrite <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> as
<span class="math display">\[
\boldsymbol{\varepsilon} = \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}.
\]</span></p>
<p>Eq. <a href="#eq:2-multiLR-LS">(15)</a> could be rewritten as</p>
<p><span class="math display" id="eq:2-multiLR-LS-matrix">\[\begin{equation}
l(\boldsymbol{\beta})=(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta}).
\tag{16}
\end{equation}\]</span></p>
<p>To estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> is to solve the optimization problem</p>
<p><span class="math display">\[
\min _{\boldsymbol{\beta}}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta}).
\]</span></p>
<p>To solve this problem, we can take the gradients of the objective function regarding <span class="math inline">\(\boldsymbol{\beta}\)</span> and set them to be zero</p>
<p><span class="math display">\[
\frac{\partial(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=0,
\]</span></p>
<p>which gives rise to the equation</p>
<p><span class="math display">\[
\boldsymbol{X}^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})=0.
\]</span></p>
<p>This leads to the <strong>least squares estimator</strong> of <span class="math inline">\(\boldsymbol{\beta}\)</span> as</p>
<p><span class="math display" id="eq:2-multiLR-LSE">\[\begin{equation}
  \widehat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y}.
\tag{17}
\end{equation}\]</span></p>
<p>A resemblance can be easily detected between the estimator in Eq. <a href="#eq:2-multiLR-LSE">(17)</a> with Eq. <a href="#eq:2-simLR-LSE">(12)</a>, by noticing that <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{y}\)</span> reflects the correlation<label for="tufte-sn-20" class="margin-toggle sidenote-number">20</label><input type="checkbox" id="tufte-sn-20" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">20</span> I.e., corresponds to <span class="math inline">\(\operatorname{cov}(x, y)\)</span>.</span> between predictors and output, and <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> reflects the variability<label for="tufte-sn-21" class="margin-toggle sidenote-number">21</label><input type="checkbox" id="tufte-sn-21" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">21</span> I.e., corresponds to <span class="math inline">\(\operatorname{var}(x)\)</span>.</span> of the predictors.</p>
<p>Eq. <a href="#eq:2-multiLR-LSE">(17)</a> may come as a surprise to some readers. The regression coefficients, <span class="math inline">\(\boldsymbol{\beta}\)</span>, by their definition, are supposed to only characterize the relationship between <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(y\)</span>. However, from Eq. <a href="#eq:2-multiLR-LSE">(17)</a>, it is clear that the variability of <span class="math inline">\(\boldsymbol{x}\)</span> matters. This is not a contradiction. <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> are <em>two</em> different entities: <span class="math inline">\(\boldsymbol{\beta}\)</span> is a theoretical concept, while <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is a statistical estimate. Statisticians have established theories<label for="tufte-sn-22" class="margin-toggle sidenote-number">22</label><input type="checkbox" id="tufte-sn-22" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">22</span> E.g, interested readers may read this book: Ravishanker, N. and Dey, D.K., <em>A First Course in Linear Model Theory</em>, Chapman &amp; Hall/CRC, 2001.</span> to study how well <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> estimates <span class="math inline">\(\boldsymbol{\beta}\)</span>. From Eq. <a href="#eq:2-multiLR-LSE">(17)</a>, it is clear that where we observe the linear system<label for="tufte-sn-23" class="margin-toggle sidenote-number">23</label><input type="checkbox" id="tufte-sn-23" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">23</span> I.e., from which <span class="math inline">\(\boldsymbol{x}\)</span> we take measurement of <span class="math inline">\(y\)</span>’s.</span> matters to the modeling of the system. This is one main motivation of the area called the <strong>Design of Experiments</strong> that aims to identify the best locations of <span class="math inline">\(\boldsymbol{x}\)</span> from which we collect observations of the outcome variable, in order to achieve the best parameter estimation results.</p>
<p>By generalizing the result in Figure <a href="#fig:f2-lrpred">5</a> on the multivariate regression, we can see that <span class="math inline">\(\boldsymbol{y}\)</span> is a random vector<label for="tufte-sn-24" class="margin-toggle sidenote-number">24</label><input type="checkbox" id="tufte-sn-24" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">24</span> “MVN” stands for Multivariate Normal Distribution. See <strong>Appendix</strong> for background knowledge on MVN.</span>,</p>
<p><span class="math display">\[\begin{equation}
    \boldsymbol{y} \sim \text{MVN}\left(\boldsymbol{X}^{T}\boldsymbol{\beta},\sigma_{\varepsilon}^{2} \boldsymbol{I}\right).
\end{equation}\]</span></p>
<p>And <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, as shown in Eq. <a href="#eq:2-multiLR-LSE">(17)</a>, is essentially a <em>function</em> of <span class="math inline">\(\boldsymbol{y}\)</span>. Thus, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is a random vector as well. In other words, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> has a distribution. Because of the normality of <span class="math inline">\(\boldsymbol{y}\)</span>, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is also distributed as a normal distribution.</p>
<p>The mean of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is <span class="math inline">\(\boldsymbol{\beta}\)</span>, because</p>
<p>And the covariance matrix of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is</p>
<p>Because</p>
<p><span class="math display">\[
\operatorname{cov}(\boldsymbol{y}) = \sigma_{\varepsilon}^{2} \boldsymbol{I},
\]</span></p>
<p>we have</p>
<p><span class="math display">\[
\operatorname{cov}(\widehat{\boldsymbol{\beta}}) =
\sigma_{\varepsilon}^{2}\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1}.
\]</span></p>
<p>Thus, we have derived that</p>
<p><span class="math display" id="eq:2-betaDist-matrix">\[\begin{equation}
    \boldsymbol{y} \sim \text{MVN}\left(\boldsymbol{X}^{T}\boldsymbol{\beta},\sigma_{\varepsilon}^{2} \boldsymbol{I}\right)  \Rightarrow \widehat{\boldsymbol{\beta}} \sim \text{MVN}\left[\boldsymbol{\beta},\sigma_{\varepsilon}^{2} \left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1}\right].
\tag{18}
\end{equation}\]</span></p>
<!-- \begin{equation} -->
<!--     \boldsymbol{y} \sim \text{MVN}\left(\boldsymbol{X}^{T}\boldsymbol{\beta}},\sigma_{\varepsilon}^{2} \boldsymbol{I}\right)  \Rightarrow \widehat{\boldsymbol{\beta}} \sim \text{MVN}\left(\boldsymbol{\beta}},\sigma_{\varepsilon}^{2}\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1}\right). -->
<!-- \end{equation} -->
<p>For each individual parameter <span class="math inline">\(\beta_i\)</span>, we can infer that</p>
<p><span class="math display" id="eq:2-betaDist">\[\begin{equation}
    \hat{\beta}_{i} \sim N\left(\beta_{i}, \frac{\sigma_{\varepsilon}^{2}}{\boldsymbol{x}_{i}^T \boldsymbol{x}_{i}}\right)
\tag{19}
\end{equation}\]</span></p>
<p><em>Hypothesis testing of regression parameters.</em> Eq. <a href="#eq:2-betaDist">(19)</a> lays the foundation for developing hypothesis testing of the regression parameters.</p>
<p>A hypothesis testing begins with a null hypothesis, e.g.,</p>
<p><span class="math display">\[
H_{0} : \beta_{i}=0.
\]</span></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-4"></span>
<img src="graphics/2_4.png" alt="The distribution of $\hat{\beta}_{i}$ " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 9: The distribution of <span class="math inline">\(\hat{\beta}_{i}\)</span> <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>If the null hypothesis is true, then based on Eq. <a href="#eq:2-betaDist">(19)</a>, we have</p>
<p><span class="math display" id="eq:2-betaDist0">\[\begin{equation}
    \hat{\beta}_{i} \sim N\left(0, \frac{\sigma_{\varepsilon}^{2}}{\boldsymbol{x}_{i}^T \boldsymbol{x}_{i}}\right).
\tag{20}
\end{equation}\]</span></p>
<p>This distribution is shown in Figure <a href="#fig:f2-4">9</a>. It is a graphical display of the possibilities of the values of <span class="math inline">\(\hat{\beta}_{i}\)</span> that we may observe, <em>if</em> <span class="math inline">\(H_{0}\)</span> is true.</p>
<p>Then we can derive further implications. Based on Figure <a href="#fig:f2-4">9</a>, we could define a range of <span class="math inline">\(\hat{\beta}_{i}\)</span> that we believe as most plausible<label for="tufte-sn-25" class="margin-toggle sidenote-number">25</label><input type="checkbox" id="tufte-sn-25" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">25</span> Note that I use the word “plausible” instead of “possible.” Any value is always <em>possible</em>, according to Eq. <a href="#eq:2-betaDist0">(20)</a>. But the <em>possibility</em> is not equally distributed, as shown in Figure <a href="#fig:f2-4">9</a>. Some values are more possible than others.</span>. In other words, if the null hypothesis is true, then it is normal to see <span class="math inline">\(\hat{\beta}_{i}\)</span> in this range. This thought leads to Figure <a href="#fig:f2-5">10</a>. This is <em>what is supposed to be</em>, if the null hypothesis is true. And any value outside of this range is considered as a result of rare chance, noise, or abnormality. We define a level of probability that represents our threshold of rare chance. We coin this threshold level as <span class="math inline">\(\alpha\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-5"></span>
<img src="graphics/2_5.png" alt="The framework of hypothesis testing" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 10: The framework of hypothesis testing<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>With the threshold level <span class="math inline">\(\alpha\)</span>, we conclude that any value of <span class="math inline">\(\hat{\beta}_{i}\)</span> that falls outside of the range is unlikely. If we see <span class="math inline">\(\hat{\beta}_{i}\)</span> falls outside of the range, we reject the null hypothesis <span class="math inline">\(H_{0}\)</span>, based on the conflict between “<em>what is supposed to be</em>” and “<em>what happened to be</em>.”<label for="tufte-sn-26" class="margin-toggle sidenote-number">26</label><input type="checkbox" id="tufte-sn-26" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">26</span> I.e., what we have assumed in <span class="math inline">\(H_{0}\)</span> is <em>what is supposed to be</em>, and what we have observed in data is <em>what happened to be</em>.</span> This framework is shown in Figure <a href="#fig:f2-5">10</a>.</p>
<p>Hypothesis testing is a decision made with risks. We may be wrong: even if the null hypothesis is true, there is still a small probability, <span class="math inline">\(\alpha\)</span>, that we may observe <span class="math inline">\(\hat{\beta}_{i}\)</span> falls outside of the range. But this is not a blind risk. It is a <em>different kind of risk</em>: we have scientifically derived the risk, understood it well, and accepted the risk as a cost.</p>
</div>
<div id="r-lab" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>In this section, we illustrate step-by-step a pipeline of R codes to use the linear regression model in real-world data analysis. Real-world data analysis is challenging. The <em>real-world</em> means objectivity, but the <em>real-worldliness</em> suggests subjectivity. The purpose of the R codes in this book serves a similar function as a diving coach who dives into the water to show how the action should be done, but the <em>real-worldliness</em> can only be felt if you also dive into the water and feel the thrill by yourself. Our data analysis examples try to preserve a certain degree of the <em>real-worldliness</em> that embodies both statistical regularities and realistic irregularities<label for="tufte-sn-27" class="margin-toggle sidenote-number">27</label><input type="checkbox" id="tufte-sn-27" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">27</span> Prof. George Box once said, “<em>all models are wrong, some are useful</em>.”</span>. Only the challenge in many real applications is that the boundary between the statistical regularities and realistic irregularities is unclear and undefined.</p>
<p>Having said that, making informed decisions by drawing from rigorous theories, while at the same time, maintaining a critical attitude about theory, are both needed in practices of data analytics.</p>
<p>Here, our data is from a study of Alzheimer’s disease<label for="tufte-sn-28" class="margin-toggle sidenote-number">28</label><input type="checkbox" id="tufte-sn-28" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">28</span> Data were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (<a href="http://adni.loni.usc.edu">http://adni.loni.usc.edu</a>). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer’s disease (AD).</span> that collected some demographics, genetic, and neuroimaging variables from hundreds of subjects. The goal of this dataset is to use these predictors to predict some outcome variables, e.g., one is called the Mini-Mental State Examination (<code>MMSCORE</code>), which is a clinical score for determining Alzheimer’s disease. It ranges from <span class="math inline">\(1\)</span> to <span class="math inline">\(30\)</span>, while <span class="math inline">\(25\)</span> to <span class="math inline">\(30\)</span> is normal, <span class="math inline">\(20\)</span> to <span class="math inline">\(24\)</span> suggests mild dementia, <span class="math inline">\(13\)</span> to <span class="math inline">\(20\)</span> suggests moderate dementia, and less than <span class="math inline">\(12\)</span> indicates severe dementia.</p>
<p><em>The 5-Step R Pipeline.</em> We start with a pipeline of conducting linear regression analysis in R with 5 steps. Please keep in mind that these 5 steps are not a fixed formula: it is a selection of the authors to make it simple.</p>
<p><strong>Step 1</strong> loads the data into the R work environment.</p>
<p></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(AD)</span></span></code></pre></div>
<p></p>
<p><strong>Step 2</strong> is for data preprocessing. This is a standard chunk of code, and it will be used again in future chapters. As this is the first time we see it, here, let’s break it into several pieces. The first piece is to create your <code>X</code> matrix (predictors) and <code>Y</code> vector (outcome variable). The use of <code>X</code> for predictors and <code>Y</code> for outcome are common practice.</p>
<p></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove variable DX_bl</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> AD[ , <span class="sc">-</span><span class="fu">which</span>(<span class="fu">names</span>(AD) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>))] </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick up the first 15 variables for predictors</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick up the variable MMSCORE for outcome</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span></code></pre></div>
<p></p>
<p>Then, we make a <code>data.frame</code> to enclose both the predictors and outcome variable together. Many R functions presume the data are <em>packaged</em> in this way.</p>
<p></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span></code></pre></div>
<p></p>
<p>Then, we split the data into two parts<label for="tufte-sn-29" class="margin-toggle sidenote-number">29</label><input type="checkbox" id="tufte-sn-29" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">29</span> Usually, there is a client who splits the data for you, sends you the training data only, and withholds the testing data. When you submit your model trained on the training data, the client could verify your model using the testing data. Here, even the dataset we are working on is already the training data, we still split this nominal training data into halves and use one half as the actual training data and the other half as the testing data. Why do we do so? Please see <strong>Chapter 5</strong>.</span>. We name the two parts as <em>training data</em> and <em>testing data</em>, respectively. The training data is to fit the model. The testing data is excluded from the model training: it will be used to test the model after the final model has been selected using the training data solely.</p>
<p></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># generate the same random sequence</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> builds up a linear regression model. We use the <code>lm()</code> function to fit the regression model<label for="tufte-sn-30" class="margin-toggle sidenote-number">30</label><input type="checkbox" id="tufte-sn-30" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">30</span> Use <code>lm()</code> for more information.</span>.</p>
<p></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; Use lm() function to build a full </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model with all predictors</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> ., <span class="at">data =</span> data.train)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD)</span></code></pre></div>
<p></p>
<p>The result is shown in below</p>
<p></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~ ., data = data.train)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="do">## -6.3662 -0.8555  0.1540  1.1241  4.2517 </span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 17.93920    2.38980   7.507 1.16e-12 ***</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          0.02212    0.01664   1.329 0.185036    </span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER    -0.11141    0.22077  -0.505 0.614280    </span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     0.16943    0.03980   4.257 2.96e-05 ***</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          0.65003    0.17836   3.645 0.000328 ***</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45        -1.10136    0.62510  -1.762 0.079348 .  </span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV      7.66067    1.68395   4.549 8.52e-06 ***</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="do">## e2_1        -0.26059    0.36036  -0.723 0.470291    </span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="do">## e4_1        -0.42123    0.24192  -1.741 0.082925 .  </span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3818361    0.24991    0.21449   1.165 0.245120    </span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="do">## rs744373    -0.25192    0.20787  -1.212 0.226727    </span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="do">## rs11136000  -0.23207    0.21836  -1.063 0.288926    </span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="do">## rs610932    -0.11403    0.21906  -0.521 0.603179    </span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3851179    0.16251    0.21402   0.759 0.448408    </span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3764650    0.47607    0.24428   1.949 0.052470 .  </span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444   -0.34550    0.20559  -1.681 0.094149 .  </span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.63 on 242 degrees of freedom</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.3395, Adjusted R-squared:  0.2986 </span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 8.293 on 15 and 242 DF,  p-value: 3.575e-15</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> is model selection. There are many variables that are not significant, i.e., their <em>p-values</em> are larger than <span class="math inline">\(0.05\)</span>. The <code>step()</code> function is used for automatic model selection<label for="tufte-sn-31" class="margin-toggle sidenote-number">31</label><input type="checkbox" id="tufte-sn-31" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">31</span> Use <code>help(step)</code> for more information.</span>, i.e., it implements a brute-force approach to identify the best combinations of variables in a linear regression model.</p>
<p></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; use step() to automatically delete </span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># all the insignificant variables</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic model selection</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>lm.AD.reduced <span class="ot">&lt;-</span> <span class="fu">step</span>(lm.AD, <span class="at">direction=</span><span class="st">&quot;backward&quot;</span>, <span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<p></p>
<p>And the final model the <code>step()</code> function identifies is</p>
<p></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Step:  AIC=259.92</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="do">## MMSCORE ~ PTEDUCAT + FDG + AV45 + HippoNV + e4_1 + rs744373 + </span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="do">##     rs3764650 + rs3865444</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="do">##             Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;                   658.95 259.92                      </span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs744373   1     6.015 664.96 260.27  2.2728  0.132934    </span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="do">## - AV45       1     7.192 666.14 260.72  2.7176  0.100511    </span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="do">## - e4_1       1     8.409 667.36 261.19  3.1774  0.075882 .  </span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3865444  1     8.428 667.38 261.20  3.1848  0.075544 .  </span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3764650  1    10.228 669.18 261.90  3.8649  0.050417 .  </span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="do">## - FDG        1    40.285 699.24 273.23 15.2226  0.000123 ***</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="do">## - PTEDUCAT   1    44.191 703.14 274.67 16.6988 5.913e-05 ***</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="do">## - HippoNV    1    53.445 712.40 278.04 20.1954 1.072e-05 ***</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span></code></pre></div>
<p></p>
<p>It can be seen that the predictors that are kept in the <em>final model</em> are all significant. Also, the <code>R-squared</code> is <span class="math inline">\(0.3228\)</span> using the <span class="math inline">\(8\)</span> selected predictors. This is not bad comparing with the <code>R-squared</code>, <span class="math inline">\(0.3395\)</span>, when all the <span class="math inline">\(15\)</span> predictors are used (we call this model the <em>full model</em>).</p>
<p>We compare the full model with the final model using the F-test that is implemented in <code>anova()</code>.</p>
<p></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm.AD.reduced,lm.AD)</span></code></pre></div>
<p></p>
<p>The returned result, shown below, implies that it is statistically indistinguishable between the two models (<em>p-value</em> of the F-test is <span class="math inline">\(0.529\)</span>). The model <code>lm.AD.reduced</code> provides an equally good explanation of the data as the full model does, but <code>lm.AD.reduced</code> is more economic. The principle of <strong>Occam’s razor</strong><label for="tufte-sn-32" class="margin-toggle sidenote-number">32</label><input type="checkbox" id="tufte-sn-32" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">32</span> “<em>Other things being equal, simpler explanations are generally better than more complex ones</em>,” is the basic idea of Occam’s razor. Albert Einstein was also quoted with a similar expression: “<em>Everything should be made as simple as possible, but no simpler</em>.”</span> would consider the model <code>lm.AD.reduced</code> more in favor.</p>
<p></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Analysis of Variance Table</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Model 1: MMSCORE ~ PTEDUCAT + FDG + AV45 + HippoNV +  </span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="do">##     e4_1 + rs744373 + rs3764650 + rs3865444</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Model 2: MMSCORE ~ AGE + PTGENDER + PTEDUCAT + FDG + AV45 +  </span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     HippoNV + e2_1 + e4_1 + rs3818361 + rs744373 + rs11136000 +  </span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     rs610932 + rs3851179 + rs3764650 + rs3865444</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="do">##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 1    249 658.95                           </span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 2    242 642.73  7    16.218 0.8723  0.529</span></span></code></pre></div>
<p></p>
<p><strong>Step 5</strong> makes prediction. We can use the function <code>predict()</code><label for="tufte-sn-33" class="margin-toggle sidenote-number">33</label><input type="checkbox" id="tufte-sn-33" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">33</span> <code>predict(obj, data)</code></span> which is a function you can find in many R packages. It usually has two main arguments: <code>obj</code> is the model, and <code>data</code> is the data points you want to predict on. Note that, here, we test the model (that was trained on training data) on the testing data. After gathering the predictions, we use the function <code>cor()</code> to measure how close are the predictions with the true outcome values of the testing data. The higher the correlation, the better the predictions.</p>
<p></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; Predict using your linear regession model</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>pred.lm <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm.AD.reduced, data.test)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For regression model, you can use correlation to measure </span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># how close your predictions with the true outcome </span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># values of the data points</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(pred.lm, data.test<span class="sc">$</span>MMSCORE)</span></code></pre></div>
<p></p>
<p><em>Beyond the 5-Step Pipeline.</em> The <strong>Exploratory Data Analysis</strong> (<strong>EDA</strong>) is a practical toolbox that consists of many interesting and insightful methods and tools, mostly empirical and graphical. The idea of EDA was promoted by some statisticians<label for="tufte-sn-34" class="margin-toggle sidenote-number">34</label><input type="checkbox" id="tufte-sn-34" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">34</span> E.g., John W. Tukey was a statistician whose career was known to be an advocate of EDA. See his book: <em>Exploratory Data Analysis</em>, Addison-Wesley Publishing Co., 1977.</span>. The EDA could be used before and after we have built the model. For example, a common practice of EDA is to draw the scatterplots to see how potentially the predictors can predict the outcome variable.</p>
<p></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatterplot matrix to visualize the relationship</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># between outcome variable with continuous predictors</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;GGally&quot;)</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># draw the scatterplots and also empirical</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># shapes of the distributions of the variables</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggpairs</span>(AD[,<span class="fu">c</span>(<span class="dv">16</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)],</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">upper =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="st">&quot;points&quot;</span>),</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">lower =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="st">&quot;cor&quot;</span>))</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(<span class="fu">factor</span>(PTGENDER),</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>      MMSCORE, <span class="at">data =</span> AD,<span class="at">geom=</span><span class="fu">c</span>(<span class="st">&quot;boxplot&quot;</span>), <span class="at">fill =</span> <span class="fu">factor</span>(PTGENDER))</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f2-7">11</a> presents the continuous predictors.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-7"></span>
<p class="caption marginnote shownote">
Figure 11: Scatterplots of the continuous predictors versus outcome variable
</p>
<img src="graphics/2_7.png" alt="Scatterplots of the continuous predictors versus outcome variable" width="80%"  />
</div>
<p></p>
<p>For the other predictors which are binary, we can use a boxplot, which is shown in Figure <a href="#fig:f2-8">12</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-8"></span>
<p class="caption marginnote shownote">
Figure 12: Boxplots of the binary predictors versus outcome variable
</p>
<img src="graphics/2_8.png" alt="Boxplots of the binary predictors versus outcome variable" width="80%"  />
</div>
<p></p>
<p>In what follows we show another case of EDA.</p>
<p>Consider the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code>, and find a graphical way to investigate if the predictor, <code>AGE</code>, mediates the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code>. One way to do so is to color the data points in the scatterplot (i.e., the color corresponds to the numerical scale of <code>AGE</code>). The following R codes generate Figure <a href="#fig:f2-9">13</a>.</p>
<p></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># How to detect interaction terms</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># by exploratory data analysis (EDA)</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(AD, <span class="fu">aes</span>(<span class="at">x =</span> PTEDUCAT, <span class="at">y =</span> MMSCORE))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">colour=</span>AGE), <span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># p &lt;- p + geom_smooth(method = &quot;auto&quot;)</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;MMSE versus PTEDUCAT&quot;</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-9"></span>
<img src="graphics/2_9.png" alt="Scatterplots of `MMSCORE` versus `PTEDUCAT`" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 13: Scatterplots of <code>MMSCORE</code> versus <code>PTEDUCAT</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>It looks like that the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code> indeed changes according to different levels of <code>AGE</code>. While this is subtle, we change the strategy and draw two more figures, i.e., we draw the same scatterplot on two levels of <code>AGE</code>, i.e., <code>AGE &lt; 60</code> and <code>AGE &gt; 80</code>. The following R codes generate Figure <a href="#fig:f2-10">14</a>.</p>
<p></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(AD[<span class="fu">which</span>(AD<span class="sc">$</span>AGE <span class="sc">&lt;</span> <span class="dv">60</span>),], </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">x =</span> PTEDUCAT, <span class="at">y =</span> MMSCORE))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;MMSE versus PTEDUCAT when AGE &lt; 60&quot;</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(AD[<span class="fu">which</span>(AD<span class="sc">$</span>AGE <span class="sc">&gt;</span> <span class="dv">80</span>),], </span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">x =</span> PTEDUCAT, <span class="at">y =</span> MMSCORE))</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;MMSE versus PTEDUCAT when AGE &gt; 80&quot;</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-10"></span>
<p class="caption marginnote shownote">
Figure 14: Scatterplots of <code>MMSCORE</code> versus <code>PTEDUCAT</code> when (left) <code>AGE &lt; 60</code> or (right) <code>AGE &gt; 80</code>
</p>
<img src="graphics/2_10_1.png" alt="Scatterplots of `MMSCORE` versus `PTEDUCAT` when (left) `AGE &lt; 60`  or (right) `AGE &gt; 80`" width="49%" height="49%"  /><img src="graphics/2_10_2.png" alt="Scatterplots of `MMSCORE` versus `PTEDUCAT` when (left) `AGE &lt; 60`  or (right) `AGE &gt; 80`" width="49%" height="49%"  />
</div>
<p></p>
<p>Figure <a href="#fig:f2-10">14</a> shows that the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code> changes dramatically according to different levels of <code>AGE</code>. In other words, it means that the way the predictor <code>PTEDUCAT</code> impacts the outcome <code>MMSCORE</code> is not simply additive as a regular linear regression model would suggest. Rather, the relationship between the two is modified by <code>AGE</code>. This discovery suggests a different mechanism underlying the three variables, as demonstrated in Figure <a href="#fig:f2-lr-interact">15</a>.</p>
<p>We then add an interaction term into the regression model</p>
<p></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the multiple linear regression model </span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with an interaction term: AGE*PTEDUCAT</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>lm.AD.int <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> AGE <span class="sc">+</span> PTGENDER <span class="sc">+</span> PTEDUCAT </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                  <span class="sc">+</span> AGE<span class="sc">*</span>PTEDUCAT, <span class="at">data =</span> AD)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.int)</span></code></pre></div>
<p></p>
<p>We can see that this interaction term is significant.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-lr-interact"></span>
<img src="graphics/2_lr_interact.png" alt="Different data-generating mechanisms: (left) additive relationships between predictors and outcome; (right) additive relationships and interaction" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 15: Different data-generating mechanisms: (left) additive relationships between predictors and outcome; (right) additive relationships and interaction<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~ AGE + PTGENDER </span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="do">##     + PTEDUCAT + AGE * PTEDUCAT,</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     data = AD)</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.2571 -0.9204  0.5156  1.4219  4.2975</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  40.809411   5.500441   7.419 4.93e-13 ***</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          -0.202043   0.074087  -2.727  0.00661 **</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER     -0.470951   0.187143  -2.517  0.01216 *</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     -0.642352   0.336212  -1.911  0.05662 .</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE:PTEDUCAT  0.011083   0.004557   2.432  0.01534 *</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.052 on 512 degrees of freedom</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.07193,    Adjusted R-squared:  0.06468</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  9.92 on 4 and 512 DF,  p-value: 9.748e-08</span></span></code></pre></div>
<p></p>
</div>
</div>
<div id="tree-models" class="section level2 unnumbered">
<h2>Tree models</h2>
<p></p>
<div id="rationale-and-formulation-1" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>While the linear regression model is a typical data modeling method, the decision tree model represents a typical method in the category of algorithmic modeling<label for="tufte-sn-35" class="margin-toggle sidenote-number">35</label><input type="checkbox" id="tufte-sn-35" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">35</span> The two types of modeling cultures are discussed in Table <a href="#tab:t2-1">1</a>.</span>. The linear regression model, given its many origins and implications, builds a model based on a mathematical characterization of the <em>data-generating mechanism</em>, which emphasizes an analytic understanding of the underlying system and how the data is generated from this system<label for="tufte-sn-36" class="margin-toggle sidenote-number">36</label><input type="checkbox" id="tufte-sn-36" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">36</span> I.e., Eq. <a href="#eq:2-simLR-fx">(3)</a> explains how <span class="math inline">\(y\)</span> is impacted by <span class="math inline">\(x\)</span>, and Eq. <a href="#eq:2-simLR-eps">(4)</a> explains how the rest of <span class="math inline">\(y\)</span> is impacted by a random force. This is illustrated in Figure <a href="#fig:f2-lr-datamodel">16</a>.</span>. This pursuit of “mechanism” is sometimes too much to ask for if we know little about the physics but only the data, since understanding the mechanism of a problem needs experimental science and profound insights. And this pursuit of “mechanism” limits the applicability of a data modeling method when the data don’t seem to follow the data-generating mechanism <em>prescribed</em> by the model.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-lr-datamodel"></span>
<img src="graphics/2_lr_datamodel.png" alt="The *data-generating mechanism* of a simple linear regression model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 16: The <em>data-generating mechanism</em> of a simple linear regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>For example, Table <a href="#tab:t2-3">3</a> shows a dataset that has <span class="math inline">\(6\)</span> observations, with two predictors, <em>Weather</em> and <em>Day of week (Dow)</em>, and an outcome variable, <em>Play</em>. Assume that this is a dataset collected by a causal dog walker whose routine includes a sports field.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-3">Table 3: </span>Example of a dataset where a decision tree has a home game</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Weather</th>
<th align="left">Dow (day of weak)</th>
<th align="left">Play</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">Rainy</td>
<td align="left">Saturday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">Sunny</td>
<td align="left">Saturday</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">Windy</td>
<td align="left">Tuesday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">Sunny</td>
<td align="left">Saturday</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">Sunny</td>
<td align="left">Monday</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">Windy</td>
<td align="left">Saturday</td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p></p>
<p>It is hard to imagine that, for this dataset, how we can denote the two predictors as <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and connect it with the outcome variable <span class="math inline">\(y\)</span> in the form of Eq. <a href="#eq:2-multiLR">(13)</a>, i.e.,</p>
<!-- \ArrowBetweenLines [\Downarrow] -->
<p><span class="math display">\[\begin{equation*}
    \begin{aligned}
    &amp;\text{*Yes*} = \beta_0 + \beta_1 \text{*Rainy*} + \beta_2 \text{*Tuesday*} + \epsilon?
    \end{aligned}
\end{equation*}\]</span></p>
<p>For this dataset, decision tree is a natural fit. As shown in Figure <a href="#fig:f2-11">17</a>, a decision tree contains a <strong>root node</strong>, <strong>inner nodes</strong>, and <strong>decision nodes</strong> (i.e., the shaded leaf nodes of the tree in Figure <a href="#fig:f2-11">17</a>). For any data point to reach its prediction, it starts from the root node, follows the <strong>splitting rules</strong> alongside the arcs to travel through inner nodes, then finally reaches a decision node. For example, consider the data point “<em>Weather = Sunny, Dow = Saturday</em>,” it starts with the root node, “<em>Weather = Sunny?</em>” then goes to inner node “<em>Dow = Saturday?</em>” then reaches the decision node as the left child node of the inner node “<em>Dow = Saturday?</em>” So the decision is “<em>Play = Yes</em>.”</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-11"></span>
<img src="graphics/2_11.png" alt="Example of a decision tree model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 17: Example of a decision tree model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Compare with data modeling methods that hope to build a characterization of the data-generating mechanism, algorithmic modeling methods such as the decision tree mimic <em>heuristics</em> in human reasoning. It is challenging, while unnecessary, to write up a model of algorithmic modeling in mathematical forms as the one shown in Eq. <a href="#eq:2-multiLR">(13)</a>. Algorithmic modeling methods are more <em>semantics-oriented</em>, and more focused on patterns detection and description.</p>
</div>
<div id="theorymethod" class="section level3 unnumbered">
<h3>Theory/Method</h3>
<p>Decision trees could be generated by manual inspection of the data. The one shown in Figure <a href="#fig:f2-11">17</a> could be easily drawn with a few inspection of the 6 data points in Table <a href="#tab:t2-3">3</a>. Automatic algorithms have been developed that can take a dataset as input and generate a decision tree as output. We can see from Figure <a href="#fig:f2-11">17</a> that a key element of a decision tree is the <em>splitting rules</em> that guide a data point to travel through the inner nodes to reach a final decision node (i.e., to reach a decision).</p>
<p>A splitting rule is defined by a variable and the set of values the variable is allowed to take, e.g., in “<em>Weather = Sunny?</em>” “Weather” is the variable and “Sunny” is the set of value. The variable used for splitting is referred to as the <strong>splitting variable</strong>, and the set of value is referred to as the <strong>splitting value</strong>.</p>
<p>We start with the root node. Possible splitting rules are</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> “<em>Weather = Sunny?</em>”</p></li>
<li><p> “<em>Dow = Saturday?</em>”</p></li>
<li><p> “<em>Dow = Monday?</em>”</p></li>
<li><p> “<em>Dow = Tuesday?</em>”</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>Each of the splitting rules will lead to a different root node. Two examples are shown in Figure <a href="#fig:f2-12">18</a>. Which one should we use?</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f2-12"></span>
<p class="caption marginnote shownote">
Figure 18: Example of two root nodes
</p>
<img src="graphics/2_12.png" alt="Example of two root nodes" width="80%"  />
</div>
<p></p>
<p>To help us decide on which splitting rule is the best, the concepts <strong>entropy</strong> of data and <strong>information gain</strong> (<strong>IG</strong>) are needed.</p>
<p><em>Entropy and information gain (IG).</em> We can use the concept <strong>entropy</strong> to measure the homogeneity of the data points in a node of the decision tree. It is defined as</p>
<p><span class="math display" id="eq:2-entropy">\[\begin{equation}
e = \sum\nolimits_{i=1,\cdots,K}-P_i\log _{2} P_i.
\tag{21}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(K\)</span> represents the number of classes of the data points in the node<label for="tufte-sn-37" class="margin-toggle sidenote-number">37</label><input type="checkbox" id="tufte-sn-37" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">37</span> E.g., in Table <a href="#tab:t2-3">3</a>, there are <span class="math inline">\(K=2\)</span> classes, <em>Yes</em> and <em>No</em>.</span>, and <span class="math inline">\(P_i\)</span> is the proportion of data points that belong to the class <span class="math inline">\(i\)</span>. The entropy <span class="math inline">\(e\)</span> is defined as zero when the data points in the node all belong to one single class<label for="tufte-sn-38" class="margin-toggle sidenote-number">38</label><input type="checkbox" id="tufte-sn-38" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">38</span> What is more deterministic than this case?</span>. And <span class="math inline">\(e = 1\)</span> is the maximum value for the entropy of a dataset, i.e., try an example with two classes, where <span class="math inline">\(P_1 = 0.5\)</span> and <span class="math inline">\(P_2 = 0.5\)</span>.<label for="tufte-sn-39" class="margin-toggle sidenote-number">39</label><input type="checkbox" id="tufte-sn-39" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">39</span> What is more uncertain than this case?</span></p>
<p>A node that consists of data points that are dominated by one class (i.e., entropy is small) is ready to be made a decision node. If it still has a large entropy, splitting it into two child nodes could help reduce the entropy. Thus, to further split a node, we look for the best splitting rule that can maximize the entropy reduction. This entropy reduction can be measured by <strong>IG</strong>, which is the difference of entropy of the parent node and the average entropy of the two child nodes weighted by their number of data points. It is defined as</p>
<p><span class="math display" id="eq:2-IG">\[\begin{equation}
IG = e_s - \sum\nolimits_{i=1,2} w_i e_i.
\tag{22}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(e_s\)</span> is the entropy of the parent node, <span class="math inline">\(e_i\)</span> is the entropy of the child node <span class="math inline">\(i\)</span>, and <span class="math inline">\(w_i\)</span> is the number of data points in the child node <span class="math inline">\(i\)</span> divided by the number of data points in the parent node.</p>
<p>For example, for the left tree in Figure <a href="#fig:f2-12">18</a>, using the definition of entropy in Eq. <a href="#eq:2-entropy">(21)</a>, the entropy of the root node is calculated as</p>
<p><span class="math display">\[-\frac{4}{6} \log _2 \frac{4}{6} - \frac{2}{6}\log _2 \frac{2}{6}=0.92.\]</span></p>
<p>The entropy of the left child node (“<em>Weather = Sunny</em>”) is</p>
<p><span class="math display">\[-\frac{2}{3} \log _2 \frac{2}{3} - \frac{1}{3} \log _2 \frac{1}{3}=0.92.\]</span></p>
<p>The entropy of the right child node (“<em>Weather != Sunny</em>”) is <span class="math inline">\(0\)</span> since all three data points (ID = <span class="math inline">\(1,3,6\)</span>) belong to the same class.</p>
<p>Then, using the definition of IG in Eq. <a href="#eq:2-IG">(22)</a>, the IG for the splitting rule “<em>Weather = Sunny</em>” is</p>
<p><span class="math display">\[IG = 0.92 - \frac{3}{6} \times 0.92 - \frac{3}{6} \times 0=0.46.\]</span></p>
<p>For the tree in Figure <a href="#fig:f2-12">18</a> (right), the entropy of the left child node (“<em>Dow = Saturday</em>”) is</p>
<p><span class="math display">\[-\frac{2}{4} \log _2 \frac{2}{4} - \frac{2}{4} \log _2 \frac{2}{4} = 1.\]</span></p>
<p>The entropy of the right child node (“<em>Dow != Saturday</em>”) is <span class="math inline">\(0\)</span> since the two data points (ID = 3,5) belong to the same class.</p>
<p>Thus, the IG for the splitting rule “<em>Dow = Saturday</em>” is</p>
<p><span class="math display">\[IF=0.92-\frac{4}{6} \times 1 - \frac{2}{6} \times 0=0.25.\]</span></p>
<p>As the IG for the splitting rule “<em>Weather = Sunny</em>” is higher, the left tree in Figure <a href="#fig:f2-12">18</a> is a better choice to start the tree.</p>
<p><em>Recursive partitioning.</em> The splitting process discussed above could be repeatedly used, until there is no further need to split a node, i.e., the node contains data points from one single class, which is ideal and almost would never happen in reality; or the node has reached the minimum number of data points<label for="tufte-sn-40" class="margin-toggle sidenote-number">40</label><input type="checkbox" id="tufte-sn-40" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">40</span> It is common to assign a minimum number of data points to prevent the tree-growing algorithm to generate too tiny leaf nodes. This is to prevent “<strong>overfitting</strong>.” Elaborated discussion of overfitting will be provided in <strong>Chapter 5</strong>.</span>. This repetitive splitting process is called <strong>recursive partitioning</strong>.</p>
<p>For instance, the left child node in the tree shown in Figure <a href="#fig:f2-12">18</a> (left) with data points (ID = <span class="math inline">\(2,4,5\)</span>) still has two classes, and can be further split by selecting the next best splitting rule. The right child node has only one class and becomes a decision node labeled with the decision “<em>Play = No</em>.”</p>
<p>This greedy approach, like other greedy optimization approaches, is easy to use. One limitation of greedy approches is that they may find <strong>local optimal</strong> solutions instead of <strong>global optimal</strong> solutions. The optimal choice we made in choosing between the two alternatives in Figure <a href="#fig:f2-12">18</a> is a <em>local</em> optimal choice, and all later nodes of the final tree model are impacted by our decision made on the root node. The <em>optimal root node</em> doesn’t necessarily lead to the <em>optimal tree</em><label for="tufte-sn-41" class="margin-toggle sidenote-number">41</label><input type="checkbox" id="tufte-sn-41" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">41</span> In other words, an optimal tree is the optimal one among all the possible trees, so an optimal root node won’t necessarily lead to an optimal tree.</span>.</p>
<p>An illustration of the risk of getting stuck in a local optimal solution of greedy optimization approaches is shown in Figure <a href="#fig:f2-localoptimal">19</a>. Where the algorithm gets started matters to where it ends up. For this reason, decision tree algorithms are often sensitive to data, i.e., it is not uncommon that a slight change of the dataset may cause a considerable change of the topology of the decision tree.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-localoptimal"></span>
<img src="graphics/2_localoptimal.png" alt="A greedy optimization approach starts its adventure from an **initial solution**. Here, $x_1$, $x_2$, $x_3$ are different initial solutions of $3$ usages of the optimization approach, and $3$ *optimal* solutions are found, while only one of them is *globally optimal*." width="250px"  />
<!--
<p class="caption marginnote">-->Figure 19: A greedy optimization approach starts its adventure from an <strong>initial solution</strong>. Here, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span> are different initial solutions of <span class="math inline">\(3\)</span> usages of the optimization approach, and <span class="math inline">\(3\)</span> <em>optimal</em> solutions are found, while only one of them is <em>globally optimal</em>.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Tree pruning.</em> To enhance the robustness of the decision tree learned by data-driven approaches such as the recursive partitioning, <strong>pruning</strong> methods could be used to cut down some unstable or insignificant branches. There are <strong>pre-pruning</strong> and <strong>post-pruning</strong> methods. Pre-pruning stops growing a tree when a pre-defined criterion is met. For example, one can set the <strong>depth of a tree</strong> (i.e., the depth of a node is the number of edges from the node to the tree’s root node; the depth of a tree is the maximum depth of its leaf nodes), or the minimum number of data points at the leaf nodes. These approaches need prior knowledge, and they may not necessarily reflect the characteristics of the particular dataset. More data-dependent approaches can be used. For example, we may set a minimum IG threshold to stop growing a tree when the IG is below the threshold. This may cause another problem, i.e., a small IG at an internal node does not necessarily mean its potential child nodes can only have smaller IG values. Therefore, pre-pruning can cause over-simplified trees and thus <strong>underfitted</strong> tree models. In other words, it may be too cautious.</p>
<p>In contrast, post-pruning prunes a tree after it is fully grown. A fully grown model aggressively spans the tree, i.e., by setting the depth of the tree as a large number. To pursue a fully grown tree is to mitigate the risk of underfit. The cost is that it may overfit the data, so post-pruning is needed. Post-pruning starts from the bottom of the tree. If removing an inner node (together with all the descendant nodes) does not increase the error <em>significantly</em>, then it should be pruned. The question is how to evaluate the significance of the increase of error<label for="tufte-sn-42" class="margin-toggle sidenote-number">42</label><input type="checkbox" id="tufte-sn-42" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">42</span> Interested readers may find the discussion in the Remarks section useful.</span>.</p>
<p>We will refer readers to <strong>Chapter 5</strong> for understanding more about concepts such as <strong>empirical error</strong> and <strong>generalization error</strong>. Understanding the difference between them is a key step towards maturity in data analytics. Like the difference between <em>money</em> and <em>currency</em>, the difference will be obvious to you as long as you have seen the difference.</p>
<p><em>Extensions and other considerations.</em></p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> In our data example in Table <a href="#tab:t2-3">3</a> we only have categorical variables, so candidate splitting rules could be defined relatively easier. For a continuous variable, one approach to identify candidate splitting rules is to order the observed values first, and then, use the average of each pair of consecutive values for splitting.</p></li>
<li><p> If the outcome variable is continuous, we can use the variance of the outcome variable to measure the “entropy” of a node, i.e.,</p></li>
</ul>
<p><span class="math display">\[v= \sum\nolimits_{n=1}\nolimits^N \left(\bar y - y_n\right)^2 ,\]</span></p>
<p>where <span class="math inline">\(y_{n=1,\cdots,N}\)</span> are the values of the outcome variable in the node, and <span class="math inline">\(\bar y\)</span> is the average of the outcome variable. And the information gain can be calculated similarly.</p>
<ul>
<li> Both pre-pruning and post-pruning are useful in practices, and it is hard to say which one is better than the other. There is a belief that post-pruning can often outperform pre-pruning. A better procedure is to use <strong>cross-validation</strong><label for="tufte-sn-43" class="margin-toggle sidenote-number">43</label><input type="checkbox" id="tufte-sn-43" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">43</span> Details are given in <strong>Chapter 5</strong>.</span>. A popular pre-pruning parameter used in the R package <code>rpart</code> is <code>cp</code>, i.e., it sets a value such that all splits need to improve the IG by at least a factor of <code>cp</code> to be approved. This pre-pruning strategy works well in many applications.</li>
</ul>
<p><!-- end{itemize} --></p>
</div>
<div id="r-lab-1" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline.</em> We use <code>DX_bl</code> as the outcome variable that is binary<label for="tufte-sn-44" class="margin-toggle sidenote-number">44</label><input type="checkbox" id="tufte-sn-44" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">44</span> In <code>DX_bl</code>, <code>0</code> denotes normal subjects; <code>1</code> denotes diseased subjects.</span>. We use other variables (except <code>ID</code>, <code>TOTAL13</code> and <code>MMSCORE</code>) to predict <code>DX_bl</code>.</p>
<p><strong>Step 1</strong> loads the needed R packages and data into the workspace.</p>
<p></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Key package for decision tree in R: </span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rpart (for building the tree); </span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># rpart.plot (for drawing the tree)</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span></code></pre></div>
<p>
</p>
<p><strong>Step 2</strong> is about data preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and </span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Y vector (outcome variable)</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The following code makes sure the variable &quot;DX_bl&quot; </span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># is a &quot;factor&quot;.</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, Y) </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This line is to &quot;factorize&quot; the variable &quot;DX_bl&quot;.</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># It denotes &quot;0&quot; as &quot;c0&quot; and &quot;1&quot; as &quot;c1&quot;,</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co"># to highlight the fact that</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;DX_bl&quot; is a factor variable, not a numerical variable</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Y) <span class="co"># as.factor is to convert any variable</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>                  <span class="co"># into the format as &quot;factor&quot; variable.</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># generate the same random sequence</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-15"></span>
<img src="graphics/2_15.png" alt="The unpruned decision tree to predict `DX_bl`" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 20: The unpruned decision tree to predict <code>DX_bl</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 3</strong> is to use the <code>rpart()</code> function in the R package <code>rpart</code> to build the decision tree.</p>
<p></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; use rpart to build the decision tree.</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data.train)</span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> is to use the <code>prp()</code> function to plot the decision tree<label for="tufte-sn-45" class="margin-toggle sidenote-number">45</label><input type="checkbox" id="tufte-sn-45" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">45</span> <code>prp()</code> is a capable function. It has many arguments to specify the details of how the tree should be drawn. Use <code>help(prp)</code> to see details.</span></p>
<p></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; draw the tree</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree, <span class="at">nn.cex =</span> <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p>And the decision tree is shown in Figure <a href="#fig:f2-15">20</a>.</p>
<p><strong>Step 5</strong> is to prune the tree using the R function <code>prune()</code>. Remember that the parameter <code>cp</code> controls the model complexity<label for="tufte-sn-46" class="margin-toggle sidenote-number">46</label><input type="checkbox" id="tufte-sn-46" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">46</span> A larger <code>cp</code> leads to a less complex tree.</span>.</p>
<p>Let us try <code>cp</code> <span class="math inline">\(= 0.03\)</span>. This leads to a decision tree as shown in Figure <a href="#fig:f2-16">21</a>.</p>
<p></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; prune the tree</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">prune</span>(tree,<span class="at">cp=</span><span class="fl">0.03</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree,<span class="at">nn.cex=</span><span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-16"></span>
<img src="graphics/2_16.png" alt="The pruned decision tree model to predict `DX_bl` of the AD data with `cp = 0.03`" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 21: The pruned decision tree model to predict <code>DX_bl</code> of the AD data with <code>cp = 0.03</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 6</strong> is to evaluate the trained model by predicting the testing data.</p>
<p></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict using your tree model</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>pred.tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree, data.test, <span class="at">type=</span><span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p></p>
<p>And we can evaluate the prediction performance using error rate.</p>
<p></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The following line calculates the prediction error</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rate (a number from 0 to 1) for a binary classification problem</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>err.tree <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(pred.tree <span class="sc">!=</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>                           data.test<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.tree)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) which(pred.tree != data$DX_bl) identifies the locations</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">#    of the incorrect predictions;</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) length(any vector) returns the length of that vector;</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) thus, the ratio of incorrect prediction over the total</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">#    prediction is the prediction error</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(err.tree)</span></code></pre></div>
<p></p>
</div>
</div>
<div id="remarks" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="statistical-model-vs.-causal-model" class="section level3 unnumbered">
<h3>Statistical model vs. causal model</h3>
<p>People unconsciously interprets a regression model as a causal model. When an unconscious interpretation is stated, it seems absurd and untrue, but it is fair to say that the line between a statistical model and a causal model is often blurred. We cannot blame ourselves for falling for this temptation before we have had a chance to see it through a critical lens, since both models share the same representation: an asymmetric form where predictors are on one side of the equation and the outcome is on the other side. Plus, the concept of <em>significance</em> is no less confusing: a common misinterpretation is to treat the <em>statistical significance</em> of a predictor as evidence of <em>causal significance</em> in the application context. The fact is that statistical significance doesn’t imply that the relationship between the predictor and the outcome variable is causal.</p>
<p>To see this, in what follows we will show an example that the statistical significance of a variable would disappear when some other variables are added into the model. Still using the AD dataset, we fit a regression model using the variable <code>AGE</code> only.</p>
<p></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>lm.AD.age <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> AGE, <span class="at">data =</span> AD)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.age)</span></code></pre></div>
<p></p>
<p>And the result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~  AGE, data = AD)</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.7020 -0.9653  0.6948  1.6182  2.5447</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 30.44147    0.94564  32.191   &lt;2e-16 ***</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         -0.03333    0.01296  -2.572   0.0104 *</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.11 on 515 degrees of freedom</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.01268,    Adjusted R-squared:  0.01076</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 6.614 on 1 and 515 DF,  p-value: 0.0104</span></span></code></pre></div>
<p></p>
<p>The predictor, <code>AGE</code>, is significant since its <em>p-value</em> is <span class="math inline">\(0.0104\)</span>.</p>
<p>Now let’s include more demographics variables into the model.</p>
<p></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the multiple linear regression model </span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with more than one predictor</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>lm.AD.demo <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span>  AGE <span class="sc">+</span> PTGENDER <span class="sc">+</span> PTEDUCAT,</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> AD)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.demo)</span></code></pre></div>
<p></p>
<p>And the result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE ~ AGE + </span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="do">##    PTGENDER + PTEDUCAT, data = AD)</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.4290 -0.9766  0.5796  1.4252  3.4539</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 27.70377    1.11131  24.929  &lt; 2e-16 ***</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         -0.02453    0.01282  -1.913   0.0563 .</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER    -0.43356    0.18740  -2.314   0.0211 *</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     0.17120    0.03432   4.988 8.35e-07 ***</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.062 on 513 degrees of freedom</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.0612, Adjusted R-squared:  0.05571</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 11.15 on 3 and 513 DF,  p-value: 4.245e-07</span></span></code></pre></div>
<p></p>
<p>Now we can see that the predictor <code>AGE</code> is on the boardline of significance with a <em>p-value</em> <span class="math inline">\(0.0563\)</span>. The other predictors, <code>PTGENDER</code> and <code>PTEDUCAT</code>, are significant. The reason that the predictor <code>AGE</code> is now no longer significant is an interesting phenomenon, but it is not unusual in practice that a significant predictor becomes insignificant when other variables are included or excluded^[This is because of the statistical dependence of the estimation of the predictors. Remember that <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are two different entities. In the ground truth the two regression coefficients, <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span>, may be independent with each other, but <span class="math inline">\(\hat{\beta}_i\)</span> and <span class="math inline">\(\hat{\beta}_j\)</span> could still be correlated.</p>
<p>As we have known that</p>
<p><span class="math display">\[
\operatorname{cov}(\widehat{\boldsymbol{\beta}})=\sigma_{\epsilon}^{2}\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1},
\]</span></p>
<p>as long as <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> is not an identity matrix, the estimators of the regression parameters are dependent in a complicated and data-dependant way. Due to this reason, we need to be cautious about how to interpret the estimated regression parameters, as they are interrelated constructs.].</p>
<p>One strategy to mitigate this problem is to explore your data from every possible angle, and try out different model formulations. The goal of your data analysis is not to get a final conclusive model that dictates the rest of the analysis process. The data analysis is an exploratory and dynamic process, i.e., as you see, the dynamic interplay of the variables, how they impact each others’ significance in predicting the outcome, is something you could only obtain by analyzing the data in an exploratory and dynamic way. The fact that a model fits the data well and passes the significance test only means that there is nothing significant in the data that is found to be against the model. The goodness-of-fit of the data doesn’t mean that the data says this model is the only causal model and other models are impossible.</p>
</div>
<div id="design-of-experiments" class="section level3 unnumbered">
<h3>Design of experiments</h3>
<p>Related to this issue of “statistical model vs. causal model,” the design of experiments (DOE) is a discipline which provides systematic data collection procedures to render the regression model as a causal model. How this could be done demands a lengthy discussion and illustration<label for="tufte-sn-47" class="margin-toggle sidenote-number">47</label><input type="checkbox" id="tufte-sn-47" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">47</span> Interested readers may start with this book: Goos, P. and Jones, B., <em>Optimal Design of Experiments: A Case Study Approach</em>, Wiley, 2011.</span>. Here, we briefly review its foundation to see why it has the connection with a linear regression model.</p>
<p>We have seen in Eq. <a href="#eq:2-betaDist-matrix">(18)</a> that the uncertainty of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> comes from two sources, the noise in the data that is encoded in <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span>, and the structure of <span class="math inline">\(\boldsymbol{X}\)</span>. <span class="math inline">\(\sigma_{\epsilon}^{2}\)</span> reflects essential uncertainty inherent in the system, but <span class="math inline">\(\boldsymbol{X}\)</span> is about how we collect the data. Thus, experimental design methods seek to optimize the structure of <span class="math inline">\(\boldsymbol{X}\)</span> such that the uncertainty of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> could be minimized.</p>
<p>For example, suppose that there are three predictors. Let’s consider the following structure of <span class="math inline">\(\boldsymbol{X}\)</span></p>
<p><span class="math display">\[
\boldsymbol{X}=\left[ \begin{array}{lll}{1} &amp; {0} &amp; {0} \\ {0} &amp; {1} &amp; {0} \\ {0} &amp; {0} &amp; {1} \end{array}\right].
\]</span></p>
<p>It can be seen that, with this structure, the variance of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is<label for="tufte-sn-48" class="margin-toggle sidenote-number">48</label><input type="checkbox" id="tufte-sn-48" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">48</span> <span class="math inline">\(\boldsymbol{I}\)</span> is the identity matrix. Here, <span class="math inline">\(\boldsymbol{I}_3 = \left[ \begin{array}{lll}{1} &amp; {0} &amp; {0} \\ {0} &amp; {1} &amp; {0} \\ {0} &amp; {0} &amp; {1} \end{array}\right].\)</span></span></p>
<p><span class="math display">\[cov(\hat{\boldsymbol{\beta}})=\sigma_{\epsilon}^2\boldsymbol{I}_3.\]</span></p>
<p>In other words, we can draw two main observations. First, the estimations of the regression parameters are now independent, given that their correlations are zero. Second, the variances of the estimated regression parameters are the same. Because of these two traits, this data matrix <span class="math inline">\(\boldsymbol X\)</span> is ideal and adopted in DOE to create <em>factorial designs</em>. For a linear regression model built on a dataset with such a data matrix, adding or deleting variables from the regression model will not result in changes of the estimations of other parameters.</p>
</div>
<div id="the-pessimistic-error-estimation-in-post-pruning" class="section level3 unnumbered">
<h3>The pessimistic error estimation in post-pruning</h3>
<p>Let’s look at the tree in Figure <a href="#fig:f2-13">22</a>. It has one root node, one inner node, and three leaf nodes. The target for tree pruning, for this example, is the inner node. In other words, should we prune the inner node and its subsequent child nodes?</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-13"></span>
<img src="graphics/2_13.png" alt="An example of tree pruning using pessimistic error" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 22: An example of tree pruning using pessimistic error<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We have mentioned that if the improvement on error is not significant, we should prune the node. Let’s denote the <strong>empirical error rate</strong><label for="tufte-sn-49" class="margin-toggle sidenote-number">49</label><input type="checkbox" id="tufte-sn-49" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">49</span> Empirical error is derived based on the training data.</span> as <span class="math inline">\(\hat e\)</span>. The reason we give the notation a <em>hat</em> is because it is only an estimate of an underlying parameter, the true error <span class="math inline">\(e\)</span>. <span class="math inline">\(\hat e\)</span> is usually smaller than <span class="math inline">\(e\)</span>, and thus, it is considered to be optimistic. To create a fairer estimate of <span class="math inline">\(e\)</span>, the <strong>pessimistic error estimation</strong> approach is used for tree pruning.</p>
<p>The pessimistic error estimation, like a regression model, builds on a hypothesized data-generating mechanism. Here, the <em>data</em> is the <em>errors</em> we observed from the training data. A data point can be either correctly or wrongly classified, and we can view the probability of being wrongly classified as a Bernoulli trial, while the parameter of this Bernoulli trial, commonly denoted as <span class="math inline">\(p\)</span>, is <span class="math inline">\(e\)</span>. If we denote the total number of errors we have observed on the <span class="math inline">\(n\)</span> data points as <span class="math inline">\(d\)</span>, we can derive that <span class="math inline">\(d\)</span> is distributed as a binomial distribution. We can write this data-generating mechanism as</p>
<p><span class="math display">\[
d \sim Bino\left(n, e\right).
\]</span></p>
<p>Since <span class="math inline">\(n\)</span> is usually large, we can use the normal approximation for the binomial distribution</p>
<p><span class="math display">\[
d \sim N\left(ne, ne(1-e)\right).
\]</span></p>
<p>As <span class="math inline">\(\hat e = d/n\)</span>, we have</p>
<p><span class="math display">\[
\hat e \sim N\left(e, \frac{e(1-e)}{n}\right).
\]</span></p>
<p>Skipping further derivations (more assumptions are imposed, indeed, to derive the following conclusion), we can derive the confidence interval of <span class="math inline">\(e\)</span> as</p>
<p><span class="math display">\[\hat e - z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} \leq e \leq \hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}}.\]</span></p>
<p>The upper bound of the interval, <span class="math inline">\(\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}}\)</span>, is named as the <em>pessimistic error</em>. The tree pruning methods that use the <em>pessimistic error</em> are motivated by a conservative perspective.</p>
<p>The pessimistic error depends on three values: <span class="math inline">\(\alpha\)</span>, which is often set to be <span class="math inline">\(0.25\)</span> so that <span class="math inline">\(z_{\alpha/2}=1.15\)</span>; <span class="math inline">\(\hat e\)</span>, which is the training error rate; and <span class="math inline">\(n\)</span>, which is the number of data points at the node<label for="tufte-sn-50" class="margin-toggle sidenote-number">50</label><input type="checkbox" id="tufte-sn-50" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">50</span> The pessimistic error is larger with a smaller <span class="math inline">\(n\)</span>, an estimation method that accounts for the sample size.</span>.</p>
<p>Now let’s revisit Figure <a href="#fig:f2-13">22</a>.</p>
<p>First, let’s derive the pessimistic errors for the two child nodes of the inner node. The empirical error rate for the left child node is <span class="math inline">\(\hat e = \frac{9}{19}=0.4737\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.4737 + 1.15\sqrt{\frac{0.4737(1-0.4737)}{19}}=0.605.\]</span></p>
<p>With this error rate, for a node with <span class="math inline">\(19\)</span> data points, the total misclassified data points can be <span class="math inline">\(mp=0.605\times 19=11.5\)</span>.</p>
<p>For the right child node, the empirical error rate is <span class="math inline">\(\hat e = \frac{9}{20}=0.45\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.45 + 1.15\sqrt{\frac{0.45(1-0.45)}{20}}=0.578.\]</span></p>
<p>With this error rate, for a node with <span class="math inline">\(20\)</span> data points, the total misclassified data points can be <span class="math inline">\(mp=0.578\times 20=11.56\)</span>.</p>
<p>Thus, if we keep this branch, the total misclassified data points would be <span class="math inline">\(mp=11.5+11.56=23.06\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-14"></span>
<img src="graphics/2_14.png" alt="The pruned tree of Figure \@ref(fig:f2-13)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 23: The pruned tree of Figure <a href="#fig:f2-13">22</a><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now let’s evaluate the alternative: to cut the branch. This means the inner node will become a decision node, as shown in Figure <a href="#fig:f2-14">23</a>. We will label the new decision node as C1, since <span class="math inline">\(20\)</span> of the included data points are labeled as C1, while <span class="math inline">\(19\)</span> are labeled as C2. The empirical error rate <span class="math inline">\(e\)</span> is <span class="math inline">\(\hat e = \frac{19}{39}=0.4871\)</span>. For the pessimistic error, we can get that</p>
<p><span class="math display">\[\hat{e} +z_{\alpha/2} \sqrt{\frac{\hat{e}(1-\hat{e})}{n}} = 0.4871 + 1.15\sqrt{\frac{0.4871(1-0.4871)}{39}}=0.579.\]</span></p>
<p>With this error rate, for a dataset with 39 data points, the total misclassified data points can be <span class="math inline">\(mp=0.579\times 39=22.59\)</span>. This is what would happen if we prune the tree. As <span class="math inline">\(22.59 &lt; 23.06\)</span>, pruning is a better decision.</p>
<p>The pruned tree is shown in Figure <a href="#fig:f2-14">23</a>. A complete post-pruning method will continue to consider further pruning: now consider pruning the child nodes of the root node. Following the process outlined above, the would-be misclassified data points based on the pessimistic error rate at the root node is <span class="math inline">\(22.92\)</span>, and the total misclassified instances based on the pessimistic error rate from its child nodes is <span class="math inline">\(22.59+0=22.59\)</span>. Pruning the child nodes would lead to increased error. Thus, no further pruning is needed: the child nodes are kept and the final tree consists of three nodes.</p>
</div>
</div>
<div id="exercises" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-HW-lr">Table 4: </span>Dataset for building a linear regression model</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-0.15\)</span></td>
<td align="left"><span class="math inline">\(-0.48\)</span></td>
<td align="left"><span class="math inline">\(0.46\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-0.72\)</span></td>
<td align="left"><span class="math inline">\(-0.54\)</span></td>
<td align="left"><span class="math inline">\(-0.37\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.91\)</span></td>
<td align="left"><span class="math inline">\(-0.27\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0.61\)</span></td>
<td align="left"><span class="math inline">\(1.59\)</span></td>
<td align="left"><span class="math inline">\(1.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(-1.11\)</span></td>
<td align="left"><span class="math inline">\(0.34\)</span></td>
<td align="left"><span class="math inline">\(-0.11\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Here let’s consider the dataset in Table <a href="#tab:t2-HW-lr">4</a>. Let’s build a linear regression model, i.e.,
<span class="math display">\[
  y = \beta_{0}+\beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
  \]</span>
and
<span class="math display">\[
  \epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right).
  \]</span>
and calculate the regression parameters <span class="math inline">\(\beta_{0},\beta_{1},\beta_{2}\)</span> manually.</p></li>
<li><p> Follow up the data on Q1. Use the R pipeline to build the linear regression model. Compare the result from R and the result by your manual calculation.</p></li>
<li><p> Read the following output in R.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data)</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="do">##       Min        1Q    Median        3Q       Max </span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="do">## -0.239169 -0.065621  0.005689  0.064270  0.310456 </span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  0.009124   0.010473   0.871    0.386    </span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="do">## x1           1.008084   0.008696 115.926   &lt;2e-16 ***</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="do">## x2           0.494473   0.009130  54.159   &lt;2e-16 ***</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="do">## x3           0.012988   0.010055   1.292    0.200    </span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="do">## x4          -0.002329   0.009422  -0.247    0.805    </span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 0.1011 on 95 degrees of freedom</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.9942, Adjusted R-squared:  0.994 </span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  4079 on 4 and 95 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<ul>
<li><p> (a) Write the fitted regression model. (b) Identify the significant variables. (c) What is the R-squared of this model? Does the model fit the data well? (d) What would you recommend as the next step in data analysis?</p></li>
<li><p> Consider the dataset in Table <a href="#tab:t2-hw-dt">5</a>. Build a decision tree model by manual calculation. To simplify the process, let’s only try three alternatives for the splits: <span class="math inline">\(x_1\geq0.59\)</span>, <span class="math inline">\(x_1\geq0.37\)</span>, and <span class="math inline">\(x_2\geq0.35\)</span>.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t2-hw-dt">Table 5: </span>Dataset for building a decision tree</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.22\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0.58\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0.57\)</span></td>
<td align="left"><span class="math inline">\(0.28\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0.41\)</span></td>
<td align="left"><span class="math inline">\(0.43\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(0.6\)</span></td>
<td align="left"><span class="math inline">\(0.29\)</span></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(0.12\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(0.25\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(8\)</span></td>
<td align="left"><span class="math inline">\(0.32\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Follow up on the dataset in Q5. Use the R pipeline for building a decision tree model. Compare the result from R and the result by your manual calculation.</p></li>
<li><p> Use the <code>mtcars</code> dataset in R, select the variable <code>mpg</code> as the outcome variable and other variables as predictors, run the R pipeline for linear regression, and summarize your findings.</p></li>
<li><p> Use the <code>mtcars</code> dataset in R, select the variable <code>mpg</code> as the outcome variable and other variables as predictors, run the R pipeline for decision tree, and summarize your findings. Another dataset is to use the <code>iris</code> dataset, select the variable <code>Species</code> as the outcome variable (i.e., to build a classification tree).</p></li>
<li><p> Design a simulated experiment to evaluate the effectiveness of the <code>lm()</code> in R. For instance, you can simulate <span class="math inline">\(100\)</span> samples from a linear regression model with <span class="math inline">\(2\)</span> variables,
<span class="math display">\[
  y = \beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
  \]</span>
where <span class="math inline">\(\beta_{1} = 1\)</span>, <span class="math inline">\(\beta_{2} = 1\)</span>, and
<span class="math display">\[
  \epsilon \sim N\left(0, 1\right).
  \]</span>
You can simulate <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> using the standard normal distribution <span class="math inline">\(N\left(0, 1\right)\)</span>. Run <code>lm()</code> on the simulated data, and see how close the fitted model is with the true model.</p></li>
<li><p> Follow up on the experiment in Q9. Let’s add two more variables <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_4\)</span> into the dataset but still generate <span class="math inline">\(100\)</span> samples from a linear regression model from the same underlying model
<span class="math display">\[
  y = \beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
  \]</span>
where <span class="math inline">\(\beta_{1} = 1\)</span>, <span class="math inline">\(\beta_{2} = 1\)</span>, and
<span class="math display">\[
  \epsilon \sim N\left(0, 1\right).
  \]</span>
In other words, <span class="math inline">\(x_3\)</span> and <span class="math inline">\(x_4\)</span> are insignificant variables. You can simulate <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_4\)</span> using the standard normal distribution <span class="math inline">\(N\left(0, 1\right)\)</span>. Run <code>lm()</code> on the simulated data, and see how close the fitted model is with the true model.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-hw-dt"></span>
<img src="graphics/2_hw_dt.png" alt="The true model for simulation experiment in Q12" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 24: The true model for simulation experiment in Q12<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<ul>
<li><p> Follow up on the experiment in Q10. Run <code>rpart()</code> on the simulated data, and see how close the fitted model is with the true model.</p></li>
<li><p> Design a simulated experiment to evaluate the effectiveness of the <code>rpart()</code> in R package <code>rpart</code>. For instance, you can simulate <span class="math inline">\(100\)</span> samples from a tree model as shown in Figure <a href="#fig:f2-hw-dt">24</a>, run <code>rpart()</code> on the simulated data, and see how close the fitted model is with the true model.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<div id="chapter-3.-recognition-logistic-regression-ranking" class="section level1 unnumbered">
<h1>Chapter 3. Recognition: Logistic Regression &amp; Ranking</h1>
<div id="overview-1" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 3 is about <em>Recognition</em>. This is an important skill in real-world practices of data analytics. It is to recognize <em>the same</em> abstracted form embedded in different real-world problems. No matter how different the problem looks, we hope to leverage existing models and solutions that have been proven effective for the forms that are recognizable in the problem. This is why we say the same model/theory could be applied in multiple areas<label for="tufte-sn-51" class="margin-toggle sidenote-number">51</label><input type="checkbox" id="tufte-sn-51" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">51</span> Another practical metaphor is: <em>a model is a hammer, and applications are nails</em>.</span>.</p>
<p>This is not to say that a real-world problem is equivalent to an abstracted problem. A dialectic thinking is needed here to understand the relationship between a real-world problem and its reduced form, an abstracted formulation. On one hand, for a real-world problem to be <em>real-world</em>, it always has something that exceeds the boundary of a reduced form. On the other hand, for a real-world problem to be solvable, it has to have some kinds of forms.</p>
<p>Many operations researchers believe that being able to recognize these abstracted forms holds the key to solve real-world problems effectively<label for="tufte-sn-52" class="margin-toggle sidenote-number">52</label><input type="checkbox" id="tufte-sn-52" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">52</span> Some said, <em>formulation is an art; and a good formulation contributes more than <span class="math inline">\(50\%\)</span> in solving the problem.</em></span>. For some abstracted forms, indeed we have studied them well and are confident to provide a sense of “closure.” It takes a sense of closure to conclude that we have solved a real-world problem, or at least we have reached the best solution as far as our knowledge permits. And we have established criteria to evaluate how well we have solved these abstract forms. Those are the territories where we have surveyed in detail and in depth. If to solve a real-world problem is to battle a dragon in its lair, <em>recognition</em> is all about paving the way for the dragon to follow the bread crumbs so that we can battle it in a familiar battlefield.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="logistic-regression-model" class="section level2 unnumbered">
<h2>Logistic regression model</h2>
<div id="rationale-and-formulation-2" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-lrgoal1"></span>
<img src="graphics/3_lrgoal1.png" alt="Direct application of linear regression on binary outcome, i.e., illustration of Eq. \@ref(eq:3-goal1) on a one-predictor problem where $x$ is the dose of a treatment and $y$ is the binary outcome variable." width="250px"  />
<!--
<p class="caption marginnote">-->Figure 25: Direct application of linear regression on binary outcome, i.e., illustration of Eq. <a href="#eq:3-goal1">(23)</a> on a one-predictor problem where <span class="math inline">\(x\)</span> is the dose of a treatment and <span class="math inline">\(y\)</span> is the binary outcome variable.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Linear regression models are introduced in <strong>Chapter 2</strong> as a tool to predict a continuous response using a few input variables. In some applications, the response variable is a binary variable that denotes two classes. For example, in the AD dataset, we have a variable called <code>DX_bl</code> that encodes the diagnosis information of the subjects, i.e., <code>0</code> denotes <em>normal</em>, while <code>1</code> denotes <em>diseased</em>.</p>
<p>We have learned about linear regression models to connect the input variables with the outcome variable. It is natural to wonder if the linear regression framework could still be useful here. If we write the regression equation</p>
<p><span class="math display" id="eq:3-goal1">\[\begin{equation}
    \text{The goal: } \underbrace{y}_{\text{Binary}}=\underbrace{\beta_{0}+\sum_{i=1}^{p} \beta_{i} x_{i}+\varepsilon.}_{\text{Continuous and unbounded}}
\tag{23}
\end{equation}\]</span></p>
<p>Something doesn’t make sense. The reason is obvious: the right-hand side of Eq. <a href="#eq:3-goal1">(23)</a> is continuous without bounds, while the left hand side of the equation is a binary variable. A graphical illustration is shown in Figure <a href="#fig:f3-lrgoal1">25</a>. So we have to modify this equation, either the right-hand side or the left-hand side.</p>
<p>Since we want it to be a linear model, it is better not to modify the right-hand side. So look at the left-hand side. Why we have to stick with the natural scale of <span class="math inline">\(y\)</span>? We could certainly work out a more linear-model-friendly scale. For example, instead of predicting <span class="math inline">\(y\)</span>, how about predicting the probability <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span>? If we know <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span>, we can certainly convert it to the scale of <span class="math inline">\(y\)</span>.<label for="tufte-sn-53" class="margin-toggle sidenote-number">53</label><input type="checkbox" id="tufte-sn-53" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">53</span> I.e., if <span class="math inline">\(Pr(y=1|\boldsymbol{x}) \geq 0.5\)</span>, we conclude <span class="math inline">\(y = 1\)</span>; otherwise, <span class="math inline">\(y=0\)</span>.</span></p>
<p>Thus, we consider the following revised goal</p>
<p><span class="math display" id="eq:3-goal2">\[\begin{equation}
    \text{Revised goal: } \underbrace{Pr(y=1|\boldsymbol{x})}_{\text{Continuous but bounded}}=\underbrace{\beta_{0}+\sum_{i=1}^{p} \beta_{i} x_{i}+\varepsilon.}_{\text{Continuous and unbounded}}
\tag{24}
\end{equation}\]</span></p>
<p>Changing our outcome variable from <span class="math inline">\(y\)</span> to <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span> is a good move, since <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span> is on a continuous scale. However, as it is a probability, it has to be in the range of <span class="math inline">\([0,1]\)</span>. We need more modifications to make things work.</p>
<p>If we make a lot of modifications and things barely work, we may have lost the essence. What is the essence of the linear model that we would like to leverage in this binary prediction problem? Interpretability—sure, the linear form seems easy to understand, but as we have pointed out in <strong>Chapter 2</strong>, this interpretability comes with a price, and we need to be cautious when we draw conclusions about the linear model, although there are easy conventions for us to follow. On the other hand, it would sound absurd if we dig into the literature and found there had been no <em>linear model</em> for binary classification problems. Linear model is the baseline of the data analytics enterprise. It is the starting point of our data analytics adventure. That is how important it is.</p>
<p>Back to the business to modify the linear formalism for a binary classification problem. Now our outcome variable is <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span>, and we realize it still doesn’t match with the linear form <span class="math inline">\(\beta_0+\sum_{i=1}^p \beta_i x_i\)</span>. What is the essential task here? If we put the puzzle in a context, it may give us some hints. For example, if our goal is to predict the risk of Alzheimer’s disease for subjects who are aged <span class="math inline">\(65\)</span> years or older, we have known the average risk from recent national statistics is <span class="math inline">\(8.8\%\)</span>. Now if we have a group of individuals who are aged 65 years or older, we could make a risk prediction for them <em>as a group</em>, i.e., 8.8%. But this is not the best we could do for each <em>individual</em>. We could examine an individual’s characteristics such as the gene <em>APOE</em><label for="tufte-sn-54" class="margin-toggle sidenote-number">54</label><input type="checkbox" id="tufte-sn-54" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">54</span> <em>APOE</em> polymorphic alleles play a major role in determining the risk of Alzheimer’s disease (AD): individuals carrying the <span class="math inline">\(\epsilon4\)</span> allele are at increased risk of AD compared with those carrying the more common <span class="math inline">\(\epsilon3\)</span> allele, whereas the <span class="math inline">\(\epsilon2\)</span> allele decreases risk.</span> and see if an individual has higher (or lower) risk than the average. Now comes the inspiration: what if we can <em>rank</em> the risk of the individuals based on their characteristics, can it help with the final goal that is to predict the outcome variable <span class="math inline">\(y\)</span>?</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-lrgoal2"></span>
<img src="graphics/3_lrgoal2.png" alt="Application of the **logistic function** on binary outcome" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 26: Application of the <strong>logistic function</strong> on binary outcome<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now we look closer into the idea of a linear form, and we realize it is more useful in <em>ranking</em> the possibilities rather than directly being eligible probabilities.</p>
<p><span class="math display" id="eq:3-goal3">\[\begin{equation}
    \text{Revised goal: } Pr(y=1|\boldsymbol{x})\propto\beta_0+\sum_{i=1}^p\, \beta_i x_i.
\tag{25}
\end{equation}\]</span></p>
<p>In other words, a linear form can make a comparison of two inputs, say, <span class="math inline">\(\boldsymbol{x}_i\)</span> and <span class="math inline">\(\boldsymbol{x}_j\)</span>, and evaluates which one leads to a higher probability of <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span>.</p>
<p>It is fine that we use the linear form to generate numerical values that rank the subjects. We just need one more step to transform those ranks into probabilities. Statisticians have found that the <strong>logistic function</strong> is suitable here for the transformation</p>
<p><span class="math display" id="eq:3-pry1">\[\begin{equation}
    Pr(y=1|\boldsymbol{x}) = \frac{1}{1+e^{-\left(\beta_0+\sum\nolimits_{i=1}\nolimits^{p}\, \beta_i x_i\right)}}.
\tag{26}
\end{equation}\]</span></p>
<p>Figure <a href="#fig:f3-lrgoal2">26</a> shows that the <em>logistic function</em> indeed provides a better fit of the data than the linear function as shown in Figure <a href="#fig:f3-lrgoal1">25</a>.</p>
<p>Eq. <a href="#eq:3-pry1">(26)</a> can be rewritten as Eq. <a href="#eq:3-logitR">(27)</a></p>
<p><span class="math display" id="eq:3-logitR">\[\begin{equation}
    \log {\frac{Pr(y=1|\boldsymbol{x})}{1-Pr(y=1|\boldsymbol{x})}}=\beta_0+\sum\nolimits_{i=1}\nolimits^{p}\beta_i x_i.
\tag{27}
\end{equation}\]</span></p>
<p>This is the so-called <strong>logistic regression</strong> model. The name stems from the transformation of <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span> used here, i.e., the <span class="math inline">\(\log \frac{Pr(y=1|\boldsymbol{x})}{1-Pr(y=1|\boldsymbol{x})}\)</span>, which is the logistic transformation that has been widely used in many areas such as physics and signal processing.</p>
<p>Note that we have mentioned that we can predict <span class="math inline">\(y=1\)</span> if <span class="math inline">\(Pr(y=1|\boldsymbol{x})\geq0.5\)</span>, and <span class="math inline">\(y=0\)</span> if <span class="math inline">\(Pr(y=1|\boldsymbol{x})&lt;0.5\)</span>. While <span class="math inline">\(0.5\)</span> seems naturally a cut-off value here, it is not necessarily optimal in every application. We could use the techniques discussed in <strong>Chapter 5</strong> such as cross-validation to decide what is the optimal cut-off value in practice.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-discretize"></span>
<img src="graphics/3_discretize.png" alt="Illustration of the discretization process, e.g., two categories ($0.0-0.1$ and $0.1-0.2$) of $x$ are shown" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 27: Illustration of the discretization process, e.g., two categories (<span class="math inline">\(0.0-0.1\)</span> and <span class="math inline">\(0.1-0.2\)</span>) of <span class="math inline">\(x\)</span> are shown<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Visual inspection of data.</em> How do we know that our data could be characterized using a logistic function?</p>
<p>We can <em>discretize</em> the predictor <span class="math inline">\(x\)</span> in Figure <a href="#fig:f3-lrgoal2">26</a> into a few categories, compute the empirical estimate of <span class="math inline">\(Pr(y=1|x)\)</span> in each category, and create a new data table. This procedure is illustrated in Figure <a href="#fig:f3-discretize">27</a>.</p>
<p>Suppose that we discretize the data in Figure <a href="#fig:f3-lrgoal2">26</a> and obtain the result as shown in Table <a href="#tab:t3-goal3">6</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t3-goal3">Table 6: </span>Example of a result after discretization</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Level of <span class="math inline">\(x\)</span></th>
<th align="left"><span class="math inline">\(1\)</span></th>
<th align="left"><span class="math inline">\(2\)</span></th>
<th align="left"><span class="math inline">\(3\)</span></th>
<th align="left"><span class="math inline">\(4\)</span></th>
<th align="left"><span class="math inline">\(5\)</span></th>
<th align="left"><span class="math inline">\(6\)</span></th>
<th align="left"><span class="math inline">\(7\)</span></th>
<th align="left"><span class="math inline">\(8\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(Pr(y=1\)</span>|<span class="math inline">\(x)\)</span></td>
<td align="left"><span class="math inline">\(0.00\)</span></td>
<td align="left"><span class="math inline">\(0.04\)</span></td>
<td align="left"><span class="math inline">\(0.09\)</span></td>
<td align="left"><span class="math inline">\(0.20\)</span></td>
<td align="left"><span class="math inline">\(0.59\)</span></td>
<td align="left"><span class="math inline">\(0.89\)</span></td>
<td align="left"><span class="math inline">\(0.92\)</span></td>
<td align="left"><span class="math inline">\(0.99\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Then, we revise the scale of the <span class="math inline">\(y\)</span>-axis of Figure <a href="#fig:f3-lrgoal2">26</a> to be <span class="math inline">\(Pr(y=1|x)\)</span>, and create Figure <a href="#fig:f3-lrgoal3">28</a>. It could be seen that the empirical curve does fit the form of Eq. <a href="#eq:3-pry1">(26)</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-lrgoal3"></span>
<img src="graphics/3_lrgoal3.png" alt="Revised scale of the $y$-axis of Figure \@ref(fig:f3-lrgoal2), i.e., illustration of Eq. \@ref(eq:3-pry1) " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 28: Revised scale of the <span class="math inline">\(y\)</span>-axis of Figure <a href="#fig:f3-lrgoal2">26</a>, i.e., illustration of Eq. <a href="#eq:3-pry1">(26)</a> <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
</div>
<div id="theory-and-method-1" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>We collect data to estimate the regression parameters of the logistic regression in Eq. <a href="#eq:3-logitR">(27)</a>. Denote the sample size as <span class="math inline">\(N\)</span>. <span class="math inline">\(\boldsymbol{y} \in R^{N \times 1}\)</span> denotes the <span class="math inline">\(N\)</span> measurements of the outcome variable, and <span class="math inline">\(\boldsymbol{X} \in R^{N \times (p+1)}\)</span> denotes the data matrix that includes the <span class="math inline">\(N\)</span> measurements of the <span class="math inline">\(p\)</span> input variables plus the dummy variable for the intercept coefficient <span class="math inline">\(\beta_0\)</span>. As in a linear regression model, <span class="math inline">\(\boldsymbol{\beta}\)</span> is the column vector form of the regression parameters.</p>
<p><em>The likelihood function.</em> The <strong>likelihood function</strong> evaluates how well a given set of parameters fit the data<label for="tufte-sn-55" class="margin-toggle sidenote-number">55</label><input type="checkbox" id="tufte-sn-55" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">55</span> The <em>least squares</em> loss function we derived in <strong>Chapter 2</strong> could also be derived based on the likelihood function of a linear regression model.</span>. The likelihood function has a specific definition, i.e., the conditional probability of the data conditional on the given set of parameters. Here, the dataset is <span class="math inline">\(D = \left \{\boldsymbol{X}, \boldsymbol{y} \right\}\)</span>, so the likelihood function is defined as <span class="math inline">\(Pr(D | \boldsymbol{\beta})\)</span>. It could be broken down into <span class="math inline">\(N\)</span> components<label for="tufte-sn-56" class="margin-toggle sidenote-number">56</label><input type="checkbox" id="tufte-sn-56" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">56</span> Note that, it is assumed that <span class="math inline">\(D\)</span> consists of <span class="math inline">\(N\)</span> independent data points.</span></p>
<p><span class="math display">\[
Pr(D | \boldsymbol{\beta}) = \prod\nolimits_{n=1}\nolimits^{N}Pr(\boldsymbol{x}_n, {y_n} | \boldsymbol{\beta}).
\]</span></p>
<p>For data point <span class="math inline">\((\boldsymbol{x}_n, {y_n})\)</span>, the conditional probability <span class="math inline">\(Pr(\boldsymbol{x}_n, {y_n} | \boldsymbol{\beta})\)</span> is</p>
<div style="page-break-after: always;"></div>
<p><span class="math display">\[\begin{equation}
    Pr(\boldsymbol{x}_n, {y_n} | \boldsymbol{\beta})=\begin{cases}
    p(\boldsymbol{x}_n), &amp; if \, y_n = 1 \\
    1-p(\boldsymbol{x}_n), &amp; if \, y_n = 0. \\
    \end{cases}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(p(\boldsymbol{x}_n) = Pr(y=1|\boldsymbol{x})\)</span>.</p>
<p>A succinct form to represent these two scenarios together is</p>
<p><span class="math display">\[Pr(\boldsymbol{x}_n, {y_n} | \boldsymbol{\beta}) = p(\boldsymbol{x}_n)^{y_n}\left[1-p(\boldsymbol{x}_n)\right]^{1-y_n}.\]</span></p>
<p>Then we can generalize this to all the <span class="math inline">\(N\)</span> data points, and derive the complete likelihood function as</p>
<p><span class="math display">\[
Pr(D | \boldsymbol{\beta})=\prod\nolimits_{n=1}\nolimits^{N}p(\boldsymbol{x}_n)^{y_n}\left[1-p(\boldsymbol{x}_n)\right]^{1-y_n}.
\]</span></p>
<p>It is common to write up its log-likelihood function, defined as <span class="math inline">\(l(\boldsymbol \beta) = \log Pr(D | \boldsymbol{\beta})\)</span>, to turn products into sums</p>
<p><span class="math display">\[l(\boldsymbol \beta)=\sum\nolimits_{n=1}\nolimits^N\, \left \{ y_n \log p(\boldsymbol{x}_n)+(1-y_n)\log [1-p(\boldsymbol{x}_n)]\right\}.\]</span></p>
<p>By plugging in the definition of <span class="math inline">\(p(\boldsymbol{x}_n)\)</span>, this could be further transformed into</p>
<p><span class="math display" id="eq:3-likelihood">\[\begin{equation}
\begin{split}
    l(\boldsymbol \beta) = \sum\nolimits_{n=1}\nolimits^N -\log \left(1+e^{\beta_0+\sum\nolimits_{i=1}\nolimits^p\, \beta_i x_{ni}} \right) - \\ \sum\nolimits_{n=1}\nolimits^N y_n(\beta_0+\sum\nolimits_{i=1}\nolimits^p\, \beta_i x_{ni}).
\end{split}
\tag{28}
\end{equation}\]</span></p>
<p>Note that, for any probabilistic model<label for="tufte-sn-57" class="margin-toggle sidenote-number">57</label><input type="checkbox" id="tufte-sn-57" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">57</span> A probabilistic model has a joint distribution for all the random variables concerned in the model. Interested readers can read this comprehensive book: Koller, D. and Friedman, N., <em>Probabilistic Graphical Models: Principles and Techniques</em>, The MIT Press, 2009.</span>, we could derive the likelihood function in one way or another, in a similar fashion as we have done for the logistic regression model.</p>
<p><em>Algorithm.</em> Eq. <a href="#eq:3-likelihood">(28)</a> provides the objective function of a maximization problem, i.e., the parameter that maximizes <span class="math inline">\(l(\boldsymbol \beta)\)</span> is the best parameter. Theoretically, we could use the First Derivative Test to find the optimal solution. The problem here is that there is no closed-form solution found if we directly apply the First Derivative Test.</p>
<p>Instead, the <strong>Newton-Raphson algorithm</strong> is commonly used to optimize the log-likelihood function of the logistic regression model. It is an iterative algorithm that starts from an <strong>initial solution</strong>, continues to seek updates of the current solution using the following formula</p>
<p><span class="math display" id="eq:3-newtonmethod">\[\begin{equation}
\boldsymbol \beta^{new} = \boldsymbol \beta^{old} - (\frac{\partial^2 l(\boldsymbol \beta)}{\partial \boldsymbol \beta \partial \boldsymbol \beta^T})^{-1} \frac{\partial l(\boldsymbol \beta)}{\partial \boldsymbol \beta}.
\tag{29}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\frac{\partial l(\boldsymbol \beta)}{\partial \boldsymbol \beta}\)</span> is the <strong>gradient</strong> of the current solution, that points to the <strong>direction</strong> following which we should increment the current solution to improve on the objective function. On the other hand, how far we should go along this direction is decided by the <strong>step size</strong> factor, defined as <span class="math inline">\((\frac{\partial^2 l(\boldsymbol \beta)}{\partial \boldsymbol \beta \partial \boldsymbol \beta^T})^{-1}\)</span>. Theoretical results have shown that this formula could converge to the optimal solution. An illustration is given in Figure <a href="#fig:f3-RWalgor">29</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-RWalgor"></span>
<p class="caption marginnote shownote">
Figure 29: Illustration of the gradient-based optimization algorithms that include the Newton-Raphson algorithm as an example. An algorithm starts from an initial solution (e.g., <span class="math inline">\(x_0\)</span> and <span class="math inline">\(x_0&#39;\)</span> are two examples of initial solutions in the figure), uses the gradient to find the <strong>direction</strong>, and moves the solution along that direction with the computed <strong>step size</strong>, until it finds the optimal solution <span class="math inline">\(x^*\)</span>.
</p>
<img src="graphics/3_RWalgor.png" alt="Illustration of the gradient-based optimization algorithms that include the Newton-Raphson algorithm as an example. An algorithm starts from an initial solution (e.g., $x_0$ and $x_0'$ are two examples of initial solutions in the figure), uses the gradient to find the **direction**, and moves the solution along that direction with the computed **step size**, until it finds the optimal solution $x^*$." width="80%"  />
</div>
<p></p>
<p>The Newton-Raphson algorithm presented in Eq. <a href="#eq:3-newtonmethod">(29)</a> is general. To apply it in a logistic regression model, since we have an explicit form of <span class="math inline">\(l(\boldsymbol \beta)\)</span>, we can derive the gradient and step size as shown below</p>
<p><span class="math display">\[\begin{align*}
    \frac{\partial l(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &amp;= \sum\nolimits_{n=1}^{N}\boldsymbol{x}_n\left[y_n -p(\boldsymbol{x}_n)\right], \\
    \frac{\partial^2 l(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^T} &amp;= -\sum\nolimits_{n=1}^N \boldsymbol{x}_n\boldsymbol{x}_n^T p(\boldsymbol{x}_n)\left[1-p(\boldsymbol{x}_n)\right].
\end{align*}\]</span></p>
<p>A certain structure can be revealed if we rewrite it in matrix form<label for="tufte-sn-58" class="margin-toggle sidenote-number">58</label><input type="checkbox" id="tufte-sn-58" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">58</span> <span class="math inline">\(\boldsymbol{p}(\boldsymbol{x})\)</span> is a <span class="math inline">\(N\times1\)</span> column vector of <span class="math inline">\(p(\boldsymbol{x}_n)\)</span>, and <span class="math inline">\(\boldsymbol{W}\)</span> is a <span class="math inline">\(N\times N\)</span> diagonal matrix with the <span class="math inline">\(n^{th}\)</span> diagonal element as <span class="math inline">\(p(\boldsymbol{x}_n )\left[1-p(\boldsymbol{x}_n)\right]\)</span>.</span></p>
<p><span class="math display" id="eq:3-gradient">\[\begin{align}
    \frac{\partial l(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = \boldsymbol{X}^T\left[\boldsymbol{y}-\boldsymbol{p}(\boldsymbol{x})\right],  \\
    \frac{\partial^2 l(\boldsymbol{\beta})}{\boldsymbol{\beta} \boldsymbol{\beta}^T} = -\boldsymbol{X}^T\boldsymbol{W}\boldsymbol{X}.
    \tag{30}
\end{align}\]</span></p>
<p>Plugging Eq. <a href="#eq:3-gradient">(30)</a> into the updating formula as shown in Eq. <a href="#eq:3-newtonmethod">(29)</a>, we can derive a specific formula for logistic regression</p>
<p><span class="math display" id="eq:3-betaupdate">\[\begin{align}
    &amp;\boldsymbol{\beta}^{new} = \boldsymbol{\beta}^{old} + (\boldsymbol{X}^T\boldsymbol{WX})^{-1}\boldsymbol{X}^T\left[ \boldsymbol{y}-\boldsymbol{p}(\boldsymbol{x}) \right], \\
    &amp; = (\boldsymbol{X}^T\boldsymbol{WX})^{-1}\boldsymbol{X}^T\boldsymbol{W} \left(\boldsymbol{X}\boldsymbol{\beta}^{old}+\boldsymbol{W}^{-1} \left[ \boldsymbol{y} - \boldsymbol{p}(\boldsymbol{x})\right] \right), \\
    &amp; = (\boldsymbol{X}^T\boldsymbol{WX})^{-1}\boldsymbol{X}^T\boldsymbol{Wz}.
\tag{31}
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol{z}=\boldsymbol{X}\boldsymbol{\beta}^{old} + \boldsymbol{W}^{-1}(\boldsymbol{y} - \boldsymbol{p}(\boldsymbol{x}))\)</span>.</p>
<p>Putting all these together, a complete flow of the algorithm is shown below</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Initialize <span class="math inline">\(\boldsymbol{\beta}.\)</span><label for="tufte-sn-59" class="margin-toggle sidenote-number">59</label><input type="checkbox" id="tufte-sn-59" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">59</span> I.e., use random values for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</span></p></li>
<li><p> Compute <span class="math inline">\(\boldsymbol{p}(\boldsymbol{x}_n)\)</span> by its definition: <span class="math inline">\(\boldsymbol{p}(\boldsymbol{x}_n )=\frac{1}{1+e^{-(\beta_0+\sum_{i=1}^p\, \beta_i x_{ni})}}\)</span> for <span class="math inline">\(n=1,2,\ldots,N\)</span>.</p></li>
<li><p> Compute the diagonal matrix <span class="math inline">\(\boldsymbol{W}\)</span>, with the <span class="math inline">\(n^{th}\)</span> diagonal element as <span class="math inline">\(\boldsymbol{p}\left(\boldsymbol{x}_{n}\right)\left[1-\boldsymbol{p}\left(\boldsymbol{x}_{n}\right)\right]\)</span> for <span class="math inline">\(n=1,2,…,N\)</span>.</p></li>
<li><p> Set <span class="math inline">\(\boldsymbol{z}\)</span> as <span class="math inline">\(= \boldsymbol{X} \boldsymbol{\beta}+\boldsymbol{W}^{-1}[\boldsymbol{y}-\boldsymbol{p}(\boldsymbol{x})]\)</span>.</p></li>
<li><p> Set <span class="math inline">\(\boldsymbol{\beta} = \left(\boldsymbol{X}^{T} \boldsymbol{W X}\right)^{-1} \boldsymbol{X}^{T} \boldsymbol{W} \boldsymbol{z}\)</span>.</p></li>
<li><p> If the stopping criteria<label for="tufte-sn-60" class="margin-toggle sidenote-number">60</label><input type="checkbox" id="tufte-sn-60" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">60</span> A common stopping criteria is to evaluate the difference between two consecutive solutions, i.e., if the Euclidean distance between the two vectors, <span class="math inline">\(\boldsymbol{\beta}^{new}\)</span> and <span class="math inline">\(\boldsymbol{\beta}^{old}\)</span>, is less than <span class="math inline">\(10^{-4}\)</span>, then it is considered no difference and the algorithm stops.</span> is met, stop; otherwise, go back to step 2.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p><em>Generalized least squares estimator.</em> The estimation formula as shown in Eq. <a href="#eq:3-betaupdate">(31)</a> resembles the generalized least squares (GLS) estimator of a regression model, where each data point <span class="math inline">\((\boldsymbol{x}_n,y_n)\)</span> is associated with a weight <span class="math inline">\(w_n\)</span>. This insight revealed by the Newton-Raphson algorithm suggests a new perspective to look at the logistic regression model. The updating formula shown in Eq. <a href="#eq:3-betaupdate">(31)</a> suggests that, in each iteration of parameter updating, we actually solve a weighted regression model as</p>
<p><span class="math display">\[\boldsymbol{\beta}^{new} \leftarrow \mathop{\arg\min}_{\boldsymbol{\beta}} (\boldsymbol{z}-\boldsymbol{X}\boldsymbol \beta)^T\boldsymbol{W}(\boldsymbol{z}-\boldsymbol{X}\boldsymbol{\beta}).\]</span></p>
<p>For this reason, the algorithm we just introduced is also called the <strong>Iteratively Reweighted Least Squares</strong> (<strong>IRLS</strong>) algorithm. <span class="math inline">\(\boldsymbol{z}\)</span> is referred to as the <strong>adjusted response</strong>.</p>
</div>
<div id="r-lab-2" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>In the AD dataset, the variable <code>DX_bl</code> encodes the diagnosis information, i.e.,<code>0</code> denotes <em>normal</em> while <code>1</code> denotes <em>diseased</em>. We build a logistic regression model using <code>DX_bl</code> as the outcome variable.</p>
<!-- % ^[Using numbers to encode categorical variables is not the best way, but it is not uncommon. You may consider encoding them in a safer way, i.e., convert "1" to be "c1" and "0" to be "c0". Is this an act of paranoid? No. There could be millions of small problems like this, and if you don't seal these little devils in bottles when you build up your analytics pipeline --- you will witness their enormous power. \\ Whatever can go wrong, will go wrong --- Murphy's law.] -->
<p><em>The 7-Step R Pipeline.</em>
<strong>Step 1</strong> is to import data into R.</p>
<p></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(AD)</span></span></code></pre></div>
<p></p>
<p><strong>Step 2</strong> is for data preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>DX_bl</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The following code makes sure the variable &quot;DX_bl&quot; is a &quot;factor&quot;. </span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># It denotes &quot;0&quot; as &quot;c0&quot; and &quot;1&quot; as &quot;c1&quot;, to highlight the fact </span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># that &quot;DX_bl&quot; is a factor variable, not a numerical variable.</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, Y)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co"># as.factor is to convert any variable into the </span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="co"># format as &quot;factor&quot; variable.</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Y) </span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># generate the same random sequence</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> is to use the function <code>glm()</code> to build a logistic regression model<label for="tufte-sn-61" class="margin-toggle sidenote-number">61</label><input type="checkbox" id="tufte-sn-61" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">61</span> Type<code>help(glm)</code> in R Console to learn more of the function.</span>.</p>
<p></p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; Use glm() function to build a full model </span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with all predictors</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>logit.AD.full <span class="ot">&lt;-</span> <span class="fu">glm</span>(DX_bl<span class="sc">~</span>., <span class="at">data =</span> data.train,</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.AD.full)</span></code></pre></div>
<p></p>
<p>And the result is shown below</p>
<p></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = DX_bl ~ ., family = &quot;binomial&quot;, data = data.train)</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Deviance Residuals: </span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min       1Q   Median       3Q      Max  </span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.4250  -0.3645  -0.0704   0.2074   3.1707  </span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  43.97098    7.83797   5.610 2.02e-08 ***</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          -0.07304    0.03875  -1.885  0.05945 .  </span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER      0.48668    0.46682   1.043  0.29716    </span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     -0.24907    0.08714  -2.858  0.00426 ** </span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          -3.28887    0.59927  -5.488 4.06e-08 ***</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45          2.09311    1.36020   1.539  0.12385    </span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV     -38.03422    6.16738  -6.167 6.96e-10 ***</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="do">## e2_1          0.90115    0.85564   1.053  0.29225    </span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="do">## e4_1          0.56917    0.54502   1.044  0.29634    </span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3818361    -0.47249    0.45309  -1.043  0.29703    </span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="do">## rs744373      0.02681    0.44235   0.061  0.95166    </span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a><span class="do">## rs11136000   -0.31382    0.46274  -0.678  0.49766    </span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="do">## rs610932      0.55388    0.49832   1.112  0.26635    </span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3851179    -0.18635    0.44872  -0.415  0.67793    </span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3764650    -0.48152    0.54982  -0.876  0.38115    </span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444     0.74252    0.45761   1.623  0.10467    </span></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 349.42  on 257  degrees of freedom</span></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 139.58  on 242  degrees of freedom</span></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 171.58</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 7</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> is to use the <code>step()</code> function for model selection.</p>
<!-- % ^[1) direction="backward" was used in the example of linear regression model; here, we use  direction="both", which means that we start with the full model, then sequentially both remove insignificant variables and also recruit new variables (from the ones that have been removed previously - why? remember that the fact that a variable is significant also depends on what are other variables on the model already); 2) trace = 0 is to disable the presentation of showing all the models that have been evaluated along the process. If you want to see the process, simply set trace = 1 or not specify this argument (by default, trace = 1 in the \textcolor[rgb]{0,0,1}{step}() function).] -->
<p></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; use step() to automatically delete </span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co"># all the insignificant</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># variables</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Also means, automatic model selection</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>logit.AD.reduced <span class="ot">&lt;-</span> <span class="fu">step</span>(logit.AD.full, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>,</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>                         <span class="at">trace =</span> <span class="dv">0</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.AD.reduced)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = DX_bl ~ AGE + PTEDUCAT + FDG + AV45 + HippoNV + </span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="do">##     rs3865444, family = &quot;binomial&quot;, data = data.train)</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Deviance Residuals: </span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="do">##      Min        1Q    Median        3Q       Max  </span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.38957  -0.42407  -0.09268   0.25092   2.73658  </span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  42.68795    7.07058   6.037 1.57e-09 ***</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          -0.07993    0.03650  -2.190  0.02853 *  </span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     -0.22195    0.08242  -2.693  0.00708 ** </span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          -3.16994    0.55129  -5.750 8.92e-09 ***</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45          2.62670    1.18420   2.218  0.02655 *  </span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV     -36.22215    5.53083  -6.549 5.79e-11 ***</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444     0.71373    0.44290   1.612  0.10707    </span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 349.42  on 257  degrees of freedom</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 144.62  on 251  degrees of freedom</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 158.62</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 7</span></span></code></pre></div>
<p></p>
<p>You may have noticed that some variables included in this model are actually not significant.</p>
<p><strong>Step 4</strong> compares the final model selected by the <code>step()</code> function with the full model.</p>
<p></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 continued</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(logit.AD.reduced,logit.AD.full,<span class="at">test =</span> <span class="st">&quot;LRT&quot;</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The argument, test = &quot;LRT&quot;, means that the p-value </span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># is derived via the Likelihood Ratio Test (LRT).</span></span></code></pre></div>
<p></p>
<p>And we can see that the two models are not statistically different, i.e., <em>p-value</em> is <span class="math inline">\(0.8305\)</span>.</p>
<p><strong>Step 5</strong> is to evaluate the overall significance of the final model<label for="tufte-sn-62" class="margin-toggle sidenote-number">62</label><input type="checkbox" id="tufte-sn-62" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">62</span> <strong>Step 4</strong> compares two models. <strong>Step 5</strong> tests if a model has a lack-of-fit with data. A model could be better than another, but it is possible that both of them fit the data poorly.</span>.</p>
<p></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; test the significance of the logistic model</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Test residual deviance for lack-of-fit </span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (if &gt; 0.10, little-to-no lack-of-fit)</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>dev.p.val <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(logit.AD.reduced<span class="sc">$</span>deviance,</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>                        logit.AD.reduced<span class="sc">$</span>df.residual)</span></code></pre></div>
<p></p>
<p>And it can be seen that the model shows no lack-of-fit as the <em>p-value</em> is <span class="math inline">\(1\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>dev.p.val</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> is to use your final model for prediction. We can do so using the <code>predict()</code> function.</p>
<!-- % ^[A few comments: 1) \textcolor[rgb]{0,0,1}{predict}() is a function that you can find in many R packages. R package developers usually write such a function, in the form as predict(obj, data), where "obj" is the object of the created model by that package, and "data" is the data points you want to predict on. 2) While in many cases this is not needed, sometimes you do need to specify the argument "type". Here, you can type \textcolor[rgb]{0,0,1}{help(predict)} in R Console and see it has several options, e.g., type = c("link", "response", "terms"). We use the default here, i.e., type = "link", which means that we expect the outcome "y\_hat" are the values from the linear equation part of the logistic regression model. In this way, "y\_hat" are the intermediate values that are not the final prediction. To convert it into final prediction, which should be binary, we can use "0" as the cut-off value, i.e., if y\_hat < 0, we name it as one class, and if y\_hat > 0, it is another class. Notice that the cut-off value as 0 is default, but not necessary an optimal value in every application.] -->
<p></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict on test data using your </span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic regression model</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit.AD.reduced, data.test)</span></code></pre></div>
<p></p>
<p><strong>Step 7</strong> is to evaluate the prediction performance of the final model.</p>
<p></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7 -&gt; Evaluate the prediction performance of </span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># your logistic regression model</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (1) Three main metrics for classification: Accuracy, </span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sensitivity (1- False Positive), </span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Specificity (1 - False Negative)</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>y_hat2 <span class="ot">&lt;-</span> y_hat</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>y_hat2[<span class="fu">which</span>(y_hat <span class="sc">&gt;</span> <span class="dv">0</span>)] <span class="ot">=</span> <span class="st">&quot;c1&quot;</span> </span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Since y_hat here is the values from the linear equation </span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># part of the logistic regression model, by default, </span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co"># we should use 0 as a cut-off value (only by default, </span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co"># not optimal though), i.e., if y_hat &lt; 0, we name it </span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="co"># as one class, and if y_hat &gt; 0, it is another class.</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>y_hat2[<span class="fu">which</span>(y_hat <span class="sc">&lt;</span> <span class="dv">0</span>)] <span class="ot">=</span> <span class="st">&quot;c0&quot;</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret) </span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co"># confusionMatrix() in the package &quot;caret&quot; is a powerful </span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="co"># function to summarize the prediction performance of a </span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="co"># classification model, reporting metrics such as Accuracy, </span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Sensitivity (1- False Positive), </span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Specificity (1 - False Negative), to name a few.</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">table</span>(y_hat2, data.test<span class="sc">$</span>DX_bl))</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a><span class="co"># (2) ROC curve is another commonly reported metric for </span></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="co"># classification models</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC) </span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a><span class="co"># pROC has the roc() function that is very useful here</span></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">roc</span>(data.test<span class="sc">$</span>DX_bl, y_hat),</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">main=</span><span class="st">&quot;ROC Curve&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-Step7-ROC"></span>
<img src="graphics/3_Step7_ROC.png" alt="The ROC curve of the final model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 30: The ROC curve of the final model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Results are shown below. We haven’t discussed the <strong>ROC curve</strong> yet, which will be a main topic in <strong>Chapter 5</strong>. At this moment, remember that a model with a ROC curve that has a larger <strong>Area Under the Curve</strong> (<strong>AUC</strong>) is a better model. And a model whose ROC curve ties with the diagonal straight line (as shown in Figure <a href="#fig:f3-Step7-ROC">30</a>) is equivalent with random guess.</p>
<p></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="do">## y_hat2  c0  c1</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="do">##     c0 117  29</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="do">##     c1  16  97</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="do">##                Accuracy : 0.8263          </span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="do">##                  95% CI : (0.7745, 0.8704)</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5135          </span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : &lt; 2e-16         </span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Kappa : 0.6513          </span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.07364         </span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="do">##             Sensitivity : 0.8797          </span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="do">##             Specificity : 0.7698          </span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8014          </span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.8584          </span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a><span class="do">##              Prevalence : 0.5135          </span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a><span class="do">##          Detection Rate : 0.4517          </span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5637          </span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8248          </span></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : c0 </span></span></code></pre></div>
<p></p>
<p><em>Model uncertainty.</em> The <span class="math inline">\(95 \%\)</span> confidence interval (CI) of the regression coefficients can be derived, as shown below</p>
<p></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="do">## coefficients and 95% CI</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(<span class="at">coef =</span> <span class="fu">coef</span>(logit.AD.reduced), <span class="fu">confint</span>(logit.AD.reduced))</span></code></pre></div>
<p></p>
<p>Results are</p>
<p></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="do">##                     coef       2.5 %       97.5 %</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  42.68794758  29.9745022  57.88659748</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE          -0.07993473  -0.1547680  -0.01059348</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     -0.22195425  -0.3905105  -0.06537066</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          -3.16994212  -4.3519800  -2.17636447</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45          2.62670085   0.3736259   5.04703489</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV     -36.22214822 -48.1671093 -26.35100122</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444     0.71373441  -0.1348687   1.61273264</span></span></code></pre></div>
<p></p>
<p><em>Prediction uncertainty.</em> As in linear regression, we could derive the variance of the estimated regression coefficients <span class="math inline">\(\operatorname{var}(\hat{\boldsymbol{\beta}})\)</span>; then, since <span class="math inline">\(\boldsymbol{\hat{y}} = \boldsymbol{X} \hat{\boldsymbol{\beta}}\)</span>, we can derive <span class="math inline">\(\operatorname{var}(\boldsymbol{\hat{y}})\)</span><label for="tufte-sn-63" class="margin-toggle sidenote-number">63</label><input type="checkbox" id="tufte-sn-63" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">63</span> The linearity assumption between <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(y\)</span> enables the explicit characterization of this chain of uncertainty propagation.</span>. Skipping the technical details, the <span class="math inline">\(95\%\)</span> CI of the predictions are obtained using the R code below</p>
<p></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remark: how to obtain the 95% CI of the predictions</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit.AD.reduced, data.test, <span class="at">type =</span> <span class="st">&quot;link&quot;</span>, </span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">se.fit =</span> <span class="cn">TRUE</span>) </span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># se.fit = TRUE, is to get the standard error in the predictions, </span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co"># which is necessary information for us to construct </span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="co"># the 95% CI of the predictions</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>data.test<span class="sc">$</span>fit    <span class="ot">&lt;-</span> y_hat<span class="sc">$</span>fit</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>data.test<span class="sc">$</span>se.fit <span class="ot">&lt;-</span> y_hat<span class="sc">$</span>se.fit</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co"># We can readily convert this information into the 95% CIs </span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co"># of the predictions (the way these 95% CIs are </span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co"># derived are again, only in approximated sense).</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co"># CI for fitted values</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> <span class="fu">within</span>(data.test, {</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co"># added &quot;fitted&quot; to make predictions at appended temp values</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>fitted    <span class="ot">=</span> <span class="fu">exp</span>(fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(fit))</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>fit.lower <span class="ot">=</span> <span class="fu">exp</span>(fit <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> </span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>                                          <span class="fu">exp</span>(fit <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit))</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>fit.upper <span class="ot">=</span> <span class="fu">exp</span>(fit <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> </span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>                                          <span class="fu">exp</span>(fit <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit))</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div>
<p></p>
<p><em>Odds ratio.</em> The <strong>odds ratio</strong> (<strong>OR</strong>) quantifies the strength of the association between two events, <em>A</em> and <em>B</em>. It is defined as the ratio of the odds of <em>A</em> in the presence of <em>B</em> and the odds of <em>A</em> in the absence of <em>B</em>, or equivalently due to symmetry.</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> If the <em>OR</em> equals <span class="math inline">\(1\)</span>, <em>A</em> and <em>B</em> are independent;</p></li>
<li><p> If the <em>OR</em> is greater than <span class="math inline">\(1\)</span>, the presence of one event increases the odds of the other event;</p></li>
<li><p> If the <em>OR</em> is less than <span class="math inline">\(1\)</span>, the presence of one event reduces the odds of the other event.</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>A regression coefficient of a logistic regression model can be converted into an <em>odds ratio</em>, as done in the following codes.</p>
<p></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="do">## odds ratios and 95% CI</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(logit.AD.reduced), </span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>          <span class="fu">confint</span>(logit.AD.reduced)))</span></code></pre></div>
<p></p>
<p>The odds ratios and their <span class="math inline">\(95\%\)</span> CIs are</p>
<p></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="do">##                       OR        2.5 %       97.5 %</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 3.460510e+18 1.041744e+13 1.379844e+25</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         9.231766e-01 8.566139e-01 9.894624e-01</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT    8.009520e-01 6.767113e-01 9.367202e-01</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG         4.200603e-02 1.288128e-02 1.134532e-01</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="do">## AV45        1.382807e+01 1.452993e+00 1.555605e+02</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="do">## HippoNV     1.857466e-16 1.205842e-21 3.596711e-12</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="do">## rs3865444   2.041601e+00 8.738306e-01 5.016501e+00</span></span></code></pre></div>
<p></p>
<p><em>Exploratory Data Analysis (EDA).</em> EDA essentially conceptualizes the analysis process as a dynamic one, sometimes with a playful tone<label for="tufte-sn-64" class="margin-toggle sidenote-number">64</label><input type="checkbox" id="tufte-sn-64" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">64</span> And it is probably because of this conceptual framework, EDA happens to use a lot of figures to explore the data. Figures are rich in information, some are not easily generalized into abstract numbers.</span> EDA could start with something simple. For example, we can start with a smaller model rather than throw everything into the analysis.</p>
<p>Let’s revisit the data analysis done in the <em>7-step R pipeline</em> and examine a simple logistic regression model with only one predictor, <code>FDG</code>.</p>
<p></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression model with FDG</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>logit.AD.FDG <span class="ot">&lt;-</span> <span class="fu">glm</span>(DX_bl <span class="sc">~</span> FDG, <span class="at">data =</span> AD, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.AD.FDG)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = DX_bl   FDG, family = &quot;binomial&quot;, data = AD)</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Deviance Residuals:</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min       1Q   Median       3Q      Max</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.4686  -0.8166  -0.2758   0.7679   2.7812</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error z value Pr(&gt;|z|)</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  18.3300     1.7676   10.37   &lt;2e-16 ***</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="do">## FDG          -2.9370     0.2798  -10.50   &lt;2e-16 ***</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 711.27  on 516  degrees of freedom</span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 499.00  on 515  degrees of freedom</span></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 503</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p></p>
<p>It can be seen that the predictor <code>FDG</code> is significant, as the <em>p-value</em> is <span class="math inline">\(&lt;2e-16\)</span> that is far less than <span class="math inline">\(0.05\)</span>. On the other hand, although there is no <em>R-Squared</em> in the logistic regression model, we could observe that, out of the total deviance of <span class="math inline">\(711.27\)</span>, <span class="math inline">\(711.27 - 499.00 = 212.27\)</span> could be explained by <code>FDG</code>.</p>
<p>This process could be repeated for every variable in order to have a sense of what are their <em>marginal</em> contributions in explaining away the variation in the outcome variable. This practice, which seems dull, is not always associated with an immediate reward. But it is not uncommon in practice, particularly when we have seen in <strong>Chapter 2</strong> that, in regression models, the regression coefficients are interdependent, the regression models are not causal models, and, when you throw variables into the model, they may generate interactions just like chemicals, etc. Looking at your data from every possible angle is useful to conduct data <em>analytics</em>.</p>
<p>Back to the simple model that only uses one variable, <code>FDG</code>. To understand better how well it predicts the outcome, we can draw figures to visualize the predictions. First, let’s get the predictions and their <span class="math inline">\(95\%\)</span> CI values.</p>
<p></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>logit.AD.FDG <span class="ot">&lt;-</span> <span class="fu">glm</span>(DX_bl <span class="sc">~</span>   FDG, <span class="at">data =</span> data.train,</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit.AD.FDG, data.test, <span class="at">type =</span> <span class="st">&quot;link&quot;</span>,</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>data.test<span class="sc">$</span>fit    <span class="ot">&lt;-</span> y_hat<span class="sc">$</span>fit</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>data.test<span class="sc">$</span>se.fit <span class="ot">&lt;-</span> y_hat<span class="sc">$</span>se.fit</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># CI for fitted values</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> <span class="fu">within</span>(data.test, {</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co"># added &quot;fitted&quot; to make predictions at appended temp values</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>  fitted    <span class="ot">=</span> <span class="fu">exp</span>(fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(fit))</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>  fit.lower <span class="ot">=</span> <span class="fu">exp</span>(fit <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(fit <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>                                                    se.fit))</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>  fit.upper <span class="ot">=</span> <span class="fu">exp</span>(fit <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se.fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(fit <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>                                                    se.fit))</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-pred-FDG-boxplot"></span>
<img src="graphics/3_pred_FDG_boxplot.png" alt="Boxplots of the *predicted probabilities of diseased*, i.e., the $Pr(y=1|\boldsymbol{x})$" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 31: Boxplots of the <em>predicted probabilities of diseased</em>, i.e., the <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We then draw Figure <a href="#fig:f3-pred-FDG-boxplot">31</a> using the following script.</p>
<p></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use Boxplot to evaluate the prediction performance</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">qplot</span>(<span class="fu">factor</span>(data.test<span class="sc">$</span>DX_bl), data.test<span class="sc">$</span>fit, <span class="at">data =</span> data.test,</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">geom=</span><span class="fu">c</span>(<span class="st">&quot;boxplot&quot;</span>), <span class="at">fill =</span> <span class="fu">factor</span>(data.test<span class="sc">$</span>DX_bl)) <span class="sc">+</span> </span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>      <span class="fu">labs</span>(<span class="at">fill=</span><span class="st">&quot;Dx_bl&quot;</span>) <span class="sc">+</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">25</span>))</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f3-pred-FDG-boxplot">31</a> indicates that the model can separate the two classes significantly (while not being good enough). It gives us a global presentation of the prediction. We can draw another figure, Figure <a href="#fig:f3-1">32</a>, to examine more details, i.e., look into the “local” parts of the predictions to see where we can improve.</p>
<p></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>newData <span class="ot">&lt;-</span> data.test[<span class="fu">order</span>(data.test<span class="sc">$</span>FDG),]</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>newData<span class="sc">$</span>DX_bl <span class="ot">=</span> <span class="fu">as.numeric</span>(newData<span class="sc">$</span>DX_bl)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>newData<span class="sc">$</span>DX_bl[<span class="fu">which</span>(newData<span class="sc">$</span>DX_bl<span class="sc">==</span><span class="dv">1</span>)] <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>newData<span class="sc">$</span>DX_bl[<span class="fu">which</span>(newData<span class="sc">$</span>DX_bl<span class="sc">==</span><span class="dv">2</span>)] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>newData<span class="sc">$</span>DX_bl <span class="ot">=</span> <span class="fu">as.numeric</span>(newData<span class="sc">$</span>DX_bl)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(newData, <span class="fu">aes</span>(<span class="at">x =</span> FDG, <span class="at">y =</span> DX_bl))</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="co"># predicted curve and point-wise 95\% CI</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> FDG, <span class="at">ymin =</span> fit.lower,</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>                         <span class="at">ymax =</span> fit.upper), <span class="at">alpha =</span> <span class="fl">0.2</span>)</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> FDG, <span class="at">y =</span> fitted), <span class="at">colour=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="co"># fitted values</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> fitted), <span class="at">size=</span><span class="dv">2</span>, <span class="at">colour=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a><span class="co"># observed values</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Probability&quot;</span>) <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">18</span>))</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;Observed and predicted probability of disease&quot;</span>)</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-1"></span>
<img src="graphics/3_1.png" alt="Predicted probabilities (the red curve) with their 95\% CIs (the gray area) versus observed outcomes in data (the dots above and below)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 32: Predicted probabilities (the red curve) with their 95% CIs (the gray area) versus observed outcomes in data (the dots above and below)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="#fig:f3-1">32</a> shows that the model captures the relationship between <code>FDG</code> with <code>DX_bl</code> with a smooth logit curve, and the prediction confidences are fairly small (evidenced by the tight 95% CIs). On the other hand, it is also obvious that the single-predictor model does well on the two ends of the probability range (i.e., close to <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>), but not in the middle range where data points from the two classes could not be clearly separated.</p>
<p>We can add more predictors to enhance its prediction power. To decide on which predictors we should include, we can visualize the relationships between the predictors with the outcome variable. For example, continuous predictors could be presented in <strong>Boxplot</strong> to see if the distribution of the continuous predictor is different across the two classes, i.e., if it is different, it means the predictor could help separate the two classes. The following R codes generate Figure <a href="#fig:f3-2">33</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-2"></span>
<p class="caption marginnote shownote">
Figure 33: Boxplots of the continuous predictors in the two classes
</p>
<img src="graphics/3_2.png" alt="Boxplots of the continuous predictors in the two classes" width="80%"  />
</div>
<p></p>
<p></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;reshape2&quot;)</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(reshape2)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>data.train<span class="sc">$</span>ID <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(data.train)[<span class="dv">1</span>])</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>AD.long <span class="ot">&lt;-</span> <span class="fu">melt</span>(data.train[,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">16</span>,<span class="dv">17</span>)],</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">id.vars =</span> <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;DX_bl&quot;</span>))</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data using ggplot</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(AD.long, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">factor</span>(DX_bl), <span class="at">y =</span> value))</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co"># boxplot, size=.75 to stand out behind CI</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">size =</span> <span class="fl">0.75</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co"># points for observed data</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">position =</span> <span class="fu">position_jitter</span>(<span class="at">w =</span> <span class="fl">0.05</span>, <span class="at">h =</span> <span class="dv">0</span>),</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>                    <span class="at">alpha =</span> <span class="fl">0.1</span>)</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="co"># diamond at mean for each group</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">stat_summary</span>(<span class="at">fun =</span> mean, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">shape =</span> <span class="dv">18</span>,</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>                      <span class="at">size =</span> <span class="dv">6</span>, <span class="at">alpha =</span> <span class="fl">0.75</span>, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence limits based on normal distribution</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">stat_summary</span>(<span class="at">fun.data =</span> <span class="st">&quot;mean_cl_normal&quot;</span>,</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>                      <span class="at">geom =</span> <span class="st">&quot;errorbar&quot;</span>, <span class="at">width =</span> .<span class="dv">2</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">facet_wrap</span>(<span class="sc">~</span> variable, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Boxplots of variables by diagnosis (0 - normal; 1 - patient)&quot;</span>)</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f3-2">33</a> shows that some variables, e.g., <code>FDG</code> and <code>HippoNV</code>, could separate the two classes significantly. Some variables, such as <code>AV45</code> and <code>AGE</code>, have less prediction power, but still look promising. Note these observations cautiously, since these figures only show <em>marginal</em> relationship among variables<label for="tufte-sn-65" class="margin-toggle sidenote-number">65</label><input type="checkbox" id="tufte-sn-65" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">65</span> Boxplot is nice but it cannot show synergistic effects among the variables.</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-3"></span>
<img src="graphics/3_3.png" alt="Boxplots of the *predicted probabilities of diseased*, i.e., the $Pr(y=1|\boldsymbol{x})$" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 34: Boxplots of the <em>predicted probabilities of diseased</em>, i.e., the <span class="math inline">\(Pr(y=1|\boldsymbol{x})\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="#fig:f3-3">34</a> shows the boxplot of the predicted probabilities of diseased made by the final model identified in <strong>Step 4</strong> of the <em>7-step R pipeline</em>. This figure is to be compared with Figure <a href="#fig:f3-pred-FDG-boxplot">31</a>. It indicates that the final model is much better than the model that only uses the predictor <code>FDG</code> alone.</p>
</div>
</div>
<div id="ranking-problem-by-pairwise-comparison" class="section level2 unnumbered">
<h2>Ranking problem by pairwise comparison</h2>
<div id="rationale-and-formulation-3" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>In recent years, we have witnessed a growing interest in estimating the ranks of a list of items. This same problem could be found in a variety of applications, such as the online advertisement of products on Amazon or movie recommendation by Netflix. These problems could be analytically summarized as: given a list of items denoted by <span class="math inline">\(\boldsymbol{M}=\left\{M_{1}, M_{2}, \ldots, M_{p}\right\}\)</span>, what is the rank of the items (denoted by <span class="math inline">\(\boldsymbol{\phi}=\left\{\phi_{1}, \phi_{2}, \ldots, \phi_{p}\right\}\)</span>)?<label for="tufte-sn-66" class="margin-toggle sidenote-number">66</label><input type="checkbox" id="tufte-sn-66" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">66</span> Here, <span class="math inline">\(\boldsymbol{\phi}\)</span> is a vector of real values, i.e., the larger the <span class="math inline">\(\phi_i\)</span>, the higher the rank of <span class="math inline">\(M_i\)</span>.</span></p>
<p>To obtain ranking of items, comparison data (either by domain expert or users) is often collected, e.g., a pair of items in <span class="math inline">\(M\)</span>, let’s say, <span class="math inline">\(M_i\)</span> and <span class="math inline">\(M_j\)</span>, will be pushed to the expert/user who conducts the comparison to see if <span class="math inline">\(M_i\)</span> is better than <span class="math inline">\(M_j\)</span>; then, a score, denoted as <span class="math inline">\(y_k\)</span>, will be returned, i.e., a positive <span class="math inline">\(y_k\)</span> indicates that the expert/user supports that <span class="math inline">\(M_i\)</span> is better than <span class="math inline">\(M_j\)</span>, while a negative <span class="math inline">\(y_k\)</span> indicates the opposite. Note that the larger the <span class="math inline">\(y_k\)</span>, the stronger the support.</p>
<p>Denote the expert/user data as <span class="math inline">\(\boldsymbol y\)</span>, which is a vector and consists of the set of pairwise comparisons. The question is to estimate the ranking <span class="math inline">\(\boldsymbol \phi\)</span> based on <span class="math inline">\(\boldsymbol y\)</span>.</p>
</div>
<div id="theory-and-method-2" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>It looks like an unfamiliar problem, but a surprise recognition was made in the paper<label for="tufte-sn-67" class="margin-toggle sidenote-number">67</label><input type="checkbox" id="tufte-sn-67" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">67</span> Osting, B., Brune, C. and Osher, S. <em>Enhanced statistical rankings via targeted data collection</em>. Proceedings of the 30 International Conference on Machine Learning (ICML), 2013.</span> that the underlying statistical model is a linear regression model. This indicates that we can use the rich array of methods in linear regression framework to solve many problems in ranking.</p>
<p>To see that, first, we need to make explicit the relationship between the parameter to be estimated (<span class="math inline">\(\boldsymbol \phi\)</span>) and the data (<span class="math inline">\(\boldsymbol y\)</span>). For the <span class="math inline">\(k^{th}\)</span> comparison that involves items <span class="math inline">\(M_i\)</span> and <span class="math inline">\(M_j\)</span>, we could assume that <span class="math inline">\(y_k\)</span> is distributed as</p>
<p><span class="math display" id="eq:3-rank-y">\[\begin{equation}
y_{k} \sim N\left(\phi_{i}-\phi_{j}, \sigma^{2} / w_{k}\right).
\tag{32}
\end{equation}\]</span></p>
<p>This assumes that if the item <span class="math inline">\(M_i\)</span> is more (or less) important than the item <span class="math inline">\(M_j\)</span>, we will expect to see positive (or negative) values of <span class="math inline">\(y_k\)</span>. <span class="math inline">\(\sigma^2\)</span> encodes the overall accuracy level of the expert/user knowledge<label for="tufte-sn-68" class="margin-toggle sidenote-number">68</label><input type="checkbox" id="tufte-sn-68" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">68</span> More knowledgeable expert/user will have smaller <span class="math inline">\(\sigma^2\)</span>.</span>. Expert/user could also provide their confidence level on a particular comparison, encoded in <span class="math inline">\(w_k\)</span><label for="tufte-sn-69" class="margin-toggle sidenote-number">69</label><input type="checkbox" id="tufte-sn-69" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">69</span> When this information is lacking, we could simply assume <span class="math inline">\(w_k=1\)</span> for all the comparison data.</span>.</p>
<p>Following this line, we illustrate how we could represent the comparison data in a more compact matrix form. This is shown in Figure <a href="#fig:f3-5">35</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-5"></span>
<p class="caption marginnote shownote">
Figure 35: The data structure and its analytic formulation underlying the pairwise comparison. Each node is an item in <span class="math inline">\(M\)</span>, while each arc represents a comparison of two items
</p>
<img src="graphics/3_5.png" alt="The data structure and its analytic formulation underlying the pairwise comparison. Each node is an item in $M$, while each arc represents a comparison of two items" width="80%"  />
</div>
<p></p>
<p>The matrix <span class="math inline">\(\boldsymbol B\)</span> shown in Figure <a href="#fig:f3-5">35</a> is defined as</p>
<p><span class="math display">\[\boldsymbol{B}_{k j}=\left\{\begin{array}{cc}{1} &amp; {\text { if } j=h e a d(k)} \\ {-1} &amp; {\text { if } j=\operatorname{tail}(k)} \\ {0} &amp; {\text { otherwise }}\end{array}\right.\]</span></p>
<p>Here, <span class="math inline">\(j=tail(k)\)</span> if the <span class="math inline">\(k^{th}\)</span> comparison is asked in the form as “if <span class="math inline">\(M_i\)</span> is better than <span class="math inline">\(M_j\)</span>” (i.e., denoted as <span class="math inline">\(M_i\rightarrow M_j\)</span>); otherwise, <span class="math inline">\(j=head(k)\)</span> for a question asked in the form as <span class="math inline">\(M_j\rightarrow M_i\)</span>.</p>
<p>Based on the definition of <span class="math inline">\(\boldsymbol B\)</span>, we rewrite Eq. <a href="#eq:3-rank-y">(32)</a> as</p>
<p><span class="math display" id="eq:3-rank-y-reg">\[\begin{equation}
    y_k = \sum_{i=1}^{p} \phi_{i} \boldsymbol{B}_{ki} + \varepsilon_k,
\tag{33}
\end{equation}\]</span></p>
<p>where the distribution of <span class="math inline">\(\epsilon_k\)</span> is</p>
<p><span class="math display" id="eq:3-rank-y-reg-eps">\[\begin{equation}
\epsilon_k \sim N\left(0, \sigma^{2}/w_k \right).
\tag{34}
\end{equation}\]</span></p>
<p>Putting Eq. <a href="#eq:3-rank-y-reg">(33)</a> in matrix form, we can derive that
<span class="math display">\[\boldsymbol{y} \sim N\left(\boldsymbol{B} \boldsymbol{\phi}, \sigma^{2} \boldsymbol{W}^{-1}\right).\]</span></p>
<p>where <span class="math inline">\(\boldsymbol W\)</span> is the diagonal matrix of elements <span class="math inline">\(w_k\)</span> for <span class="math inline">\(k=1,2,…,K\)</span>.</p>
<p>Using the framework developed in <strong>Chapter 2</strong>,<label for="tufte-sn-70" class="margin-toggle sidenote-number">70</label><input type="checkbox" id="tufte-sn-70" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">70</span> Here, the estimation of <span class="math inline">\(\boldsymbol \phi\)</span> is a generalized least squares problem.</span> we could derive the estimator of <span class="math inline">\(\boldsymbol \phi\)</span> as</p>
<p><span class="math display">\[\widehat{\boldsymbol{\phi}}=\left(\boldsymbol{B}^{T} \boldsymbol{W} \boldsymbol{B}\right)^{-1} \boldsymbol{B}^{T} \boldsymbol{W} \boldsymbol{y}.\]</span></p>
<!-- % The recognition of the linear regression formulation underling the ranking problem brings more insights and operational possibilities to solve the problem better. For example, as many design of experiments techniques have been developed for optimal data collection, while most are based on the linear regression framework, these techniques could find relevance in this ranking problem, e.g., what new comparison data we should collect to optimize statistical accuracy and efficiency given limited budget? As shown in the paper, an E-optimal design method could be introduced here to optimally decide on which new comparison should be conducted. As this process involves Bayesian statistics, optimal design, and optimization, interested readers are encouraged to read the paper. -->
</div>
</div>
<div id="statistical-process-control-using-decision-tree" class="section level2 unnumbered">
<h2>Statistical process control using decision tree</h2>
<p>A fundamental problem in <strong>statistical process control</strong> (<strong>SPC</strong>) is illustrated in Figure <a href="#fig:f3-spcintro">36</a>: given a sequence of observations of a variable that <em>represents the temporal variability of a process</em>, is this process <em>stable</em>?</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-spcintro"></span>
<img src="graphics/3_spcintro.png" alt="A fundamental problem in statistical process control" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 36: A fundamental problem in statistical process control<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>SPC is built on a creative application of the statistical distribution theory. A distribution model represents a <em>stable process</em>—that is the main premise of SPC—while also allots a calculated proportion to <em>chance outliers</em>. An illustration is shown in Figure <a href="#fig:f3-spcintro2">37</a> (left).</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-spcintro2"></span>
<p class="caption marginnote shownote">
Figure 37: (Left) The use of a distribution model to represent a <em>stable process</em>; and (right) the basic idea of a control chart
</p>
<img src="graphics/3_spcintro2.png" alt="(Left) The use of a distribution model to represent a *stable process*; and (right) the basic idea of a control chart" width="80%"  />
</div>
<p></p>
<p>A further invention of SPC is to convert a distribution model, a static object, into a temporal chart, the so-called <strong>control chart</strong>, as shown in Figure <a href="#fig:f3-spcintro2">37</a> (right). A <em>control chart</em> has the upper and lower <em>control limits</em>, and a <em>center line</em>. It is interesting to note that Figure <a href="#fig:f3-spcintro2">37</a> (left) also provides a graphical illustration of how <em>hypothesis testing</em> works, and Figure <a href="#fig:f3-spcintro2">37</a> (right) illustrates the concept of <em>control chart</em>. The two build on the same foundation and differ in perspectives: one is horizontal and the other vertical.</p>
<p>A control chart is used to monitor a process. A reference data is collected to draw the control limits and the center line. Then, new data will be continuously collected over time and drawn in the chart, as shown in Figure <a href="#fig:f3-spcintro4"><strong>??</strong></a>.</p>
<p><code>{r f3-spcintro4, echo=FALSE,out.width="80%", fig.align='center', fig.margin=FALSE, fig.cap=" A control chart, built on a reference data (i.e., $x_1$-$x_8$), is used to monitor future data (i.e., $x_9$-$x_{12}$). An alarm is issued when $x_{12}$ is found to be *out of the control limit*.     " }   knitr::include_graphics('graphics/3_spcintro4.png',dpi = 300)</code></p>
<p>Because of this dependency of SPC on distribution models, a considerable amount of interest has been focused on extending it for applications where the data could not be characterized by a distribution. Along this endeavor, how to leverage decision tree models<label for="tufte-sn-71" class="margin-toggle sidenote-number">71</label><input type="checkbox" id="tufte-sn-71" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">71</span> Remember that the decision tree models can deal with complex datasets such as mixed types of variables, as discussed in <strong>Chapter 2</strong>.</span> for SPC purposes has been an interesting research problem.</p>
<div id="rationale-and-formulation-4" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>One such interesting framework is proposed to cast the process monitoring problem shown in Figure <a href="#fig:f3-spcintro4"><strong>??</strong></a> as a classification problem: the <em>reference data</em> presumably collected from a stable process represents one class, while the <em>online data</em> collected after the reference data represents another class. If the two classes could be significantly separated, an alarm should be issued. Otherwise, if there is no change in the stable process, the two data sets must come from the same distribution, then it will be difficult to classify the two data sets. This will result in a large classification error.</p>
<p>In other words, the classification error is an indicator that we can monitor<label for="tufte-sn-72" class="margin-toggle sidenote-number">72</label><input type="checkbox" id="tufte-sn-72" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">72</span> While process monitoring sounds straightforward, the real challenge sometimes lies in the question about what to monitor, and how.</span>.</p>
<p>Here we introduce the <strong>real-time contrasts</strong> method (<strong>RTC</strong>). The key idea of <em>RTC</em> is to have a <strong>sliding window</strong>, with length of <span class="math inline">\(L\)</span>, that includes the most recent data points to be compared with the reference data. We label the <em>reference data</em> as one class, and the data points in the <em>sliding window</em> as another class. We track the classification error to monitor the process.</p>
<p>We illustrate the <em>RTC</em> method through a simple problem. The collected data for monitoring is shown in Table <a href="#tab:t9-2">7</a>. The reference data is <span class="math inline">\(\{1,2\}\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-2">Table 7: </span>Example of an online dataset with <span class="math inline">\(4\)</span> time points</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><strong>Data ID</strong></th>
<th align="left"><span class="math inline">\(1\)</span></th>
<th align="left"><span class="math inline">\(2\)</span></th>
<th align="left"><span class="math inline">\(3\)</span></th>
<th align="left"><span class="math inline">\(4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Value</strong></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>To monitor the process, we use a window size of <span class="math inline">\(2\)</span>. This means the first monitoring action takes place at the time when the <span class="math inline">\(2^{nd}\)</span> data point is collected. The reference dataset, <span class="math inline">\(\{1,2\}\)</span>, is labeled as class <span class="math inline">\(0\)</span>, and the two online data points, <span class="math inline">\(\{2,1\}\)</span>, are labeled as class <span class="math inline">\(1\)</span>. As these two datasets are identical, the classification error rate is as large as 0.5. No alarm is issued.</p>
<p>At the next time point, the sliding window now includes data points <span class="math inline">\(\{1,3\}\)</span>. A classification rule “<em>if value <span class="math inline">\(\leq 2\)</span>, class <span class="math inline">\(0\)</span>; else, class <span class="math inline">\(1\)</span></em>” would achieve the best classification error rate as <span class="math inline">\(0.25\)</span>. An alarm probably should be issued.</p>
<p>At the next time point, the sliding window includes data points <span class="math inline">\(\{3,3\}\)</span>. The same classification rule “<em>if value <span class="math inline">\(\leq 2\)</span>, class <span class="math inline">\(0\)</span>; else, class <span class="math inline">\(1\)</span></em>” can classify all examples correctly with error rate of <span class="math inline">\(0\)</span>. An alarm should be issued.</p>
<p>We see that the classification error rate is a <strong>monitoring statistic</strong> to guide the triggering of alerts. It is also useful to use the probability estimates of the data points as the <em>monitoring statistic</em>. In other words, the sum of the probability estimates from all data points in the sliding window can be used for monitoring, which is defined as</p>
<p><span class="math display">\[p_{t}=\frac{\sum_{i=1}^{w} \hat{p}_{1}\left(x_{i}\right)}{w}.\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol x_i\)</span> is the <span class="math inline">\(i^{th}\)</span> data point in the sliding window, <span class="math inline">\(w\)</span> is the window size, and <span class="math inline">\(\hat{p}_1(\boldsymbol x_i)\)</span> is the probability estimate of <span class="math inline">\(\boldsymbol x_i\)</span> belonging to class <span class="math inline">\(1\)</span>. At each time point in monitoring, we can obtain a <span class="math inline">\(p_t\)</span>. Following the tradition of control chart, we could chart the time series of <span class="math inline">\(p_t\)</span> and observe the patterns to see if alerts should be triggered.</p>
<!-- % Actually, through in-depth research into this idea of directly using classification error rate as the monitoring statistic, a limitation soon reveals itself. Considering the number of data points in the monitoring window, which is $ L$. The number of possible distinct classification error rate values are actually limited to be $L+1$. This suggests that, while the monitoring statistic should be a continuum, the resolution of the classification error rate to reflect the continuum maybe limited if the window size is too small. This will result in gaps between the monitoring statistics, failing to capture changes that happen in the gaps which are blind zones. -->
<!-- % As a remedy, the probability estimates of the data points can be used to replace the errors of the data points. The probability estimates are continuous indicators, while the errors are binary indicators. Then, the sum of the probability estimates from all data points in the sliding window can be used for monitoring, which is defined as: -->
<!-- % \[p_{t}=\frac{\sum_{i=1}^{w} \hat{p}_{1}\left(x_{i}\right)}{w}.\] -->
<!-- % Here, $\boldsymbol x_i$ is the $i^{th}$data point in the sliding window, $w$ is the window size, and $\hat{p}_1(\boldsymbol x_i)$ is the probability estimate of $\boldsymbol x_i$ belonging to $f_1(\boldsymbol x)$. At each time point in monitoring, we can obtain a $p_t$. Following the tradition of control chart, we could chart the time series of $p_t$ and observe the patterns to see if alerts should be triggered. -->
<!-- % Besides this monitoring capacity, on the other hand, we could also use the classification model for fault diagnosis. Specifically, when the random forest is used for classification, the importance scores from random forests can be used for fault diagnosis. When process is under normal conditions and the classification errors are expected to be high, the importance scores are expected to be equal among process variables as none of them contribute to the classification problem. When process is abnormal, classification errors should be reduced, and the variables responsible for the process abnormality should now have larger importance scores. This gives us the foundation for using random forest for fault diagnosis. -->
<!-- % Note that, under the RTC framework, the size of the sliding window is an important parameter. When the window is too long, the method requires a large number of real-time data in each monitoring epoch, which can delay the identification of abnormal patterns. In contrast, if the window is too short, the classifiers built on the small data sets may be unstable and are prone to more false positives. In our R lab, we will explore this phenomenon further. -->
</div>
<div id="r-lab-3" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We have coded the RTC method into a R function, <code>Monitoring()</code>, as shown below, to give an example about how to write self-defined function in R. This function takes two datasets as input: the first is the reference data, <code>data0</code>, and the second is the online data points, <code>data.real.time</code>. The window size should also be provided in <code>wsz</code>. And we use a classification method named random forest<label for="tufte-sn-73" class="margin-toggle sidenote-number">73</label><input type="checkbox" id="tufte-sn-73" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">73</span> More details are in <strong>Chapter 4</strong>.</span> to build a classifier. The <code>Monitoring()</code> function returns a few monitoring statistics for each online data point, and a score of each variable that represents how likely the variable is responsible for the process change.</p>
<p></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_gray</span>(<span class="at">base_size =</span> <span class="dv">15</span>) )</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co"># define monitoring function. data0: reference data;</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co"># data.real.time: real-time data; wsz: window size</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>Monitoring <span class="ot">&lt;-</span> <span class="cf">function</span>( data0, data.real.time, wsz ){</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>num.data.points <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data.real.time)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>( i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.data.points  ){</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a><span class="co"># at the start of monitoring, when real-time data size is </span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a><span class="co"># smaller than the window size, combine the real-time</span></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a><span class="co"># data points and random samples from the reference data</span></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a><span class="co"># to form a data set of wsz</span></span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(i<span class="sc">&lt;</span>wsz){</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>  ssfr <span class="ot">&lt;-</span> wsz <span class="sc">-</span> i</span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a>  sample.reference <span class="ot">&lt;-</span> data0[<span class="fu">sample</span>(<span class="fu">nrow</span>(data0),</span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>                                 ssfr,<span class="at">replace =</span> <span class="cn">TRUE</span>), ]</span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>  current.real.time.data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(sample.reference,</span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>                            data.real.time[<span class="dv">1</span><span class="sc">:</span>i,,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>}<span class="cf">else</span>{</span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a>  current.real.time.data <span class="ot">&lt;-</span>  data.real.time[(i<span class="sc">-</span>wsz<span class="sc">+</span></span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>                                     <span class="dv">1</span>)<span class="sc">:</span>i,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a>current.real.time.data<span class="sc">$</span>class <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rbind</span>( data0, current.real.time.data )</span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(data) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="dv">1</span><span class="sc">:</span>(<span class="fu">ncol</span>(data)<span class="sc">-</span><span class="dv">1</span>)),</span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;Class&quot;</span>)</span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>Class <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>Class)</span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a><span class="co"># apply random forests to the data</span></span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>my.rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Class <span class="sc">~</span> .,<span class="at">sampsize=</span><span class="fu">c</span>(wsz,wsz), <span class="at">data=</span>data)</span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true" tabindex="-1"></a><span class="co"># get importance score</span></span>
<span id="cb55-40"><a href="#cb55-40" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(importance.mat, <span class="fu">t</span>(my.rf<span class="sc">$</span>importance))</span>
<span id="cb55-41"><a href="#cb55-41" aria-hidden="true" tabindex="-1"></a><span class="co"># get monitoring statistics</span></span>
<span id="cb55-42"><a href="#cb55-42" aria-hidden="true" tabindex="-1"></a>ooblist <span class="ot">&lt;-</span> my.rf[<span class="dv">5</span>]</span>
<span id="cb55-43"><a href="#cb55-43" aria-hidden="true" tabindex="-1"></a>oobcolumn<span class="ot">=</span><span class="fu">matrix</span>(<span class="fu">c</span>(ooblist[[<span class="dv">1</span>]]),<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb55-44"><a href="#cb55-44" aria-hidden="true" tabindex="-1"></a>ooberrornormal<span class="ot">=</span> (oobcolumn[,<span class="dv">3</span>])[<span class="dv">1</span>]</span>
<span id="cb55-45"><a href="#cb55-45" aria-hidden="true" tabindex="-1"></a>ooberrorabnormal<span class="ot">=</span>(oobcolumn[,<span class="dv">3</span>])[<span class="dv">2</span>]</span>
<span id="cb55-46"><a href="#cb55-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-47"><a href="#cb55-47" aria-hidden="true" tabindex="-1"></a>temp<span class="ot">=</span>my.rf[<span class="dv">6</span>]</span>
<span id="cb55-48"><a href="#cb55-48" aria-hidden="true" tabindex="-1"></a>p1vote <span class="ot">&lt;-</span> <span class="fu">mean</span>(temp<span class="sc">$</span>votes[,<span class="dv">2</span>][(<span class="fu">nrow</span>(data0)<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="fu">nrow</span>(data)])</span>
<span id="cb55-49"><a href="#cb55-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-50"><a href="#cb55-50" aria-hidden="true" tabindex="-1"></a>this.stat <span class="ot">&lt;-</span> <span class="fu">c</span>(ooberrornormal,ooberrorabnormal,p1vote)</span>
<span id="cb55-51"><a href="#cb55-51" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(stat.mat, this.stat)</span>
<span id="cb55-52"><a href="#cb55-52" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb55-53"><a href="#cb55-53" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">importance.mat =</span> importance.mat,</span>
<span id="cb55-54"><a href="#cb55-54" aria-hidden="true" tabindex="-1"></a>               <span class="at">stat.mat =</span> stat.mat)</span>
<span id="cb55-55"><a href="#cb55-55" aria-hidden="true" tabindex="-1"></a><span class="fu">return</span>(result)</span>
<span id="cb55-56"><a href="#cb55-56" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p>To demonstrate how to use <code>Monitoring()</code>, let’s consider a <span class="math inline">\(2\)</span>-dimensional process with two variables, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. We simulate the reference data that follow a normal distribution with mean of <span class="math inline">\(0\)</span> and standard deviation of <span class="math inline">\(1\)</span>. The online data come from two distributions: the first <span class="math inline">\(100\)</span> data points are sampled from the same distribution as the reference data, while the second <span class="math inline">\(100\)</span> data points are sampled from another distribution (i.e., the mean of <span class="math inline">\(x_2\)</span> changes to <span class="math inline">\(2\)</span>). We label the reference data with class <span class="math inline">\(0\)</span> and the online data with class <span class="math inline">\(1\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data generation</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># sizes of reference data, real-time data without change, </span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and real-time data with changes</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>length0 <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>length1 <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>length2 <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2-dimension</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>dimension <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="co"># reference data</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( dimension <span class="sc">*</span> length0, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data with no change</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( dimension <span class="sc">*</span> length2, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data different from the reference data in the </span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="co"># second the variable</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>( <span class="at">V1 =</span> <span class="fu">rnorm</span>( <span class="dv">1</span> <span class="sc">*</span> length1, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>                <span class="at">V2 =</span> <span class="fu">rnorm</span>( <span class="dv">1</span> <span class="sc">*</span> length1, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>) )</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to data frame</span></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data0, <span class="at">nrow =</span> length0, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data1, <span class="at">nrow =</span> length2, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> data2 <span class="sc">%&gt;%</span> <span class="fu">as.data.frame</span>()</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a><span class="co"># assign variable names</span></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>( data0 ) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(data0))</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>( data1 ) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(data1))</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>( data2 ) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(data2))</span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a><span class="co"># assign reference data with class 0 and real-time data with class 1</span></span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> data0 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">0</span>)</span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> data1 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">1</span>)</span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> data2 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">1</span>)</span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data consists of normal data and abnormal data</span></span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>data.real.time <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data1,data2)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-11"></span>
<img src="graphics/9_11.png" alt="Scatterplot of the reference dataset and the first $100$ online data points; both data come from the process under normal condition " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 38: Scatterplot of the reference dataset and the first <span class="math inline">\(100\)</span> online data points; both data come from the process under normal condition <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="#fig:f9-11">38</a> shows the scatterplot of the reference dataset and the first <span class="math inline">\(100\)</span> online data points. It can be seen that the two sets of data points are similar.</p>
<p></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>data.plot <span class="ot">&lt;-</span> <span class="fu">rbind</span>( data0, data1 ) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="fu">factor</span>(class))</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data.plot, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">shape =</span> class, <span class="at">color=</span>class)) <span class="sc">+</span> </span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f9-12">39</a> shows the scatterplot of the reference dataset and the second <span class="math inline">\(100\)</span> online data points.</p>
<p></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>data.plot <span class="ot">&lt;-</span> <span class="fu">rbind</span>( data0, data2 ) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="fu">factor</span>(class))</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data.plot, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">shape =</span> class,</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">color=</span>class)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-12"></span>
<img src="graphics/9_12.png" alt="Scatterplot of the reference dataset and the second $100$ online data points that come from the process under abnormal condition" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 39: Scatterplot of the reference dataset and the second <span class="math inline">\(100\)</span> online data points that come from the process under abnormal condition<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now we apply the <em>RTC</em> method. A <em>window size</em> of <span class="math inline">\(10\)</span> is used. The error rates of the two classes and the probability estimates of the data points over time are shown in Figure <a href="#fig:f9-13">40</a> drawn by the following R code.</p>
<p></p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>wsz <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">Monitoring</span>( data0, data.real.time, wsz )</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> result<span class="sc">$</span>stat.mat</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> result<span class="sc">$</span>importance.mat</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot different monitor statistics</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(stat.mat)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>stat.mat<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(stat.mat)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(stat.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;error0&quot;</span>,<span class="st">&quot;error1&quot;</span>,<span class="st">&quot;prob&quot;</span>,<span class="st">&quot;id&quot;</span>)</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> stat.mat <span class="sc">%&gt;%</span> <span class="fu">gather</span>(type, statistics, error0,</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>                                error1,prob)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(stat.mat,<span class="fu">aes</span>(<span class="at">x=</span>id,<span class="at">y=</span>statistics,<span class="at">color=</span>type)) <span class="sc">+</span> </span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-13"></span>
<p class="caption marginnote shownote">
Figure 40: (Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: <em>error0</em> denotes the error rate in Class <span class="math inline">\(0\)</span>, <em>error1</em> denotes the error rate in Class <span class="math inline">\(1\)</span>, and <em>prob</em> denotes the probability estimates of the data points; (right) chart of the importance score of the two variables
</p>
<img src="graphics/9_13.png" alt="(Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: *error0* denotes the error rate in Class $0$, *error1* denotes the error rate in Class $1$, and *prob* denotes the probability estimates of the data points; (right) chart of the importance score of the two variables" width="49%" height="49%"  /><img src="graphics/9_14.png" alt="(Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: *error0* denotes the error rate in Class $0$, *error1* denotes the error rate in Class $1$, and *prob* denotes the probability estimates of the data points; (right) chart of the importance score of the two variables" width="49%" height="49%"  />
</div>
<p></p>
<p>We have known that the process shift happened on <span class="math inline">\(x_2\)</span> after the <span class="math inline">\(100^{th}\)</span> data point—and that is exactly when a good monitoring statistic should signal the process change. Check Figure <a href="#fig:f9-13">40</a> (left) and draw your observation.</p>
<p>As the two classes are separated, we could check which variables are significant. The importance scores of the two variables obtained by the random forest model are shown in Figure <a href="#fig:f9-13">40</a> (right) drawn by the following R code.</p>
<p></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot importance scores for diagnosis</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(importance.mat)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>importance.mat<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(importance.mat)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(importance.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>,<span class="st">&quot;X2&quot;</span>,<span class="st">&quot;id&quot;</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> importance.mat <span class="sc">%&gt;%</span> </span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(variable, importance,X1,X2)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(importance.mat,<span class="fu">aes</span>(<span class="at">x=</span>id,<span class="at">y=</span>importance,</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">color=</span>variable)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f9-13">40</a> (right) shows that the scores of <span class="math inline">\(x_2\)</span> significantly increase after the <span class="math inline">\(100^{th}\)</span> data point. This indicates that <span class="math inline">\(x_2\)</span> is responsible for the process change, which is true.</p>
<p>Let’s consider a <span class="math inline">\(10\)</span>-dimensional dataset with <span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_{10}\)</span>. We still simulate <span class="math inline">\(100\)</span> reference data points of each variable from a normal distribution with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. We use the same distribution to draw the first <span class="math inline">\(100\)</span> online data points. Then, we draw the second <span class="math inline">\(100\)</span> online data points with two variables, <span class="math inline">\(x_9\)</span> and <span class="math inline">\(x_{10}\)</span>, whose means changed from <span class="math inline">\(0\)</span> to <span class="math inline">\(2\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 10-dimensions, with 2 variables being changed from </span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the normal condition</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>dimension <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>wsz <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co"># reference data</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( dimension <span class="sc">*</span> length0, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data with no change</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>( dimension <span class="sc">*</span> length1, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data different from the reference data in the </span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="co"># second the variable</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="fu">rnorm</span>( (dimension <span class="sc">-</span> <span class="dv">2</span>) <span class="sc">*</span> length2, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>            <span class="fu">rnorm</span>( (<span class="dv">2</span>) <span class="sc">*</span> length2, <span class="at">mean =</span> <span class="dv">20</span>, <span class="at">sd =</span> <span class="dv">1</span>))</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to data frame</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data0, <span class="at">nrow =</span> length0, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data1, <span class="at">nrow =</span> length1, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(data2, <span class="at">ncol =</span> <span class="dv">10</span>, <span class="at">byrow =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a><span class="co"># assign reference data with class 0 and real-time data </span></span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a><span class="co"># with class 1</span></span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>data0 <span class="ot">&lt;-</span> data0 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">0</span>)</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> data1 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">1</span>)</span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> data2 <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">class =</span> <span class="dv">1</span>)</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a><span class="co"># real-time data consists of normal data and abnormal data</span></span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>data.real.time <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data1,data2)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-14"></span>
<p class="caption marginnote shownote">
Figure 41: (Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: <em>error0</em> denotes the error rate in Class <span class="math inline">\(0\)</span>, <em>error1</em> denotes the error rate in Class <span class="math inline">\(1\)</span>, and <em>prob</em> denotes the probability estimates of the data points; (right) chart of the importance score of the variables
</p>
<img src="graphics/9_19.png" alt="(Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: *error0* denotes the error rate in Class $0$, *error1* denotes the error rate in Class $1$, and *prob* denotes the probability estimates of the data points; (right) chart of the importance score of the variables" width="49%" height="49%"  /><img src="graphics/9_20.png" alt="(Left) Chart of the monitoring statistics over time. Three monitoring statistics are shown: *error0* denotes the error rate in Class $0$, *error1* denotes the error rate in Class $1$, and *prob* denotes the probability estimates of the data points; (right) chart of the importance score of the variables" width="49%" height="49%"  />
</div>
<p></p>
<p>Figure <a href="#fig:f9-14">41</a> (left) shows that all the monitoring statistics change after the <span class="math inline">\(101^{th}\)</span> time point, and the variables’ scores in Figure <a href="#fig:f9-14">41</a> (right) indicate the change is due to <span class="math inline">\(x_9\)</span> and <span class="math inline">\(x_{10}\)</span>, which is true. The following R codes generated Figure <a href="#fig:f9-14">41</a> (left).</p>
<p></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">Monitoring</span>( data0, data.real.time, wsz )</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> result<span class="sc">$</span>stat.mat</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> result<span class="sc">$</span>importance.mat</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot different monitor statistics</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(stat.mat)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>stat.mat<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(stat.mat)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(stat.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;error0&quot;</span>,<span class="st">&quot;error1&quot;</span>,<span class="st">&quot;prob&quot;</span>,<span class="st">&quot;id&quot;</span>)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>stat.mat <span class="ot">&lt;-</span> stat.mat <span class="sc">%&gt;%</span> <span class="fu">gather</span>(type, statistics, error0,</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>                                error1,prob)</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(stat.mat,<span class="fu">aes</span>(<span class="at">x=</span>id,<span class="at">y=</span>statistics,<span class="at">color=</span>type))<span class="sc">+</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>                                      <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p>The following R codes generated Figure <a href="#fig:f9-14">41</a> (right).</p>
<p></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot importance scores for diagnosis</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(importance.mat)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>importance.mat<span class="sc">$</span>id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(importance.mat)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># colnames(importance.mat) &lt;- c(&quot;X1&quot;,&quot;X2&quot;,&quot;id&quot;)</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>importance.mat <span class="ot">&lt;-</span> importance.mat <span class="sc">%&gt;%</span> </span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(variable, importance,X1<span class="sc">:</span>X10)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>importance.mat<span class="sc">$</span>variable <span class="ot">&lt;-</span> <span class="fu">factor</span>( importance.mat<span class="sc">$</span>variable,</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">levels =</span> <span class="fu">paste0</span>( <span class="st">&quot;X&quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>))</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="co"># levels(importance.mat$variable) &lt;- paste0( &quot;X&quot;, 1:10  )</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(importance.mat,<span class="fu">aes</span>(<span class="at">x=</span>id,<span class="at">y=</span>importance,<span class="at">color=</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>          variable)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>          <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span></code></pre></div>
<p></p>
</div>
</div>
<div id="remarks-1" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="more-about-the-logistic-function" class="section level3 unnumbered">
<h3>More about the logistic function</h3>
<p>Like the linear regression model, Eq. <a href="#eq:3-logitR">(27)</a> seems like <em>one</em> model that explains all the data points<label for="tufte-sn-74" class="margin-toggle sidenote-number">74</label><input type="checkbox" id="tufte-sn-74" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">74</span> We have mentioned that a model with this trait is called a <em>global</em> model.</span>. This observation is good, but we may easily overlook its subtle complexity. As shown in Figure <a href="#fig:f3-lr3regions">42</a>, the logistic regression model is able to encapsulate a complex relationships between <span class="math inline">\(x\)</span> (the dose) with <span class="math inline">\(y\)</span> (the response to treatment) as one succinct mathematical form. This is remarkable, probably unusual, and unmistakably beautiful.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-lr3regions"></span>
<img src="graphics/3_lr3regions.png" alt="The three regions of the logistic function" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 42: The three regions of the logistic function<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>And the regression coefficients flexibly tune the exact shape of the logistic function for each dataset, as shown in Figure <a href="#fig:f3-difflogit">43</a>.</p>
<p>On the other hand, the logistic function is not the only choice. There are some other options, i.e., Chester Ittner Bliss used the <em>cumulative normal distribution function</em> to perform the transformation and called his model the <strong>probit regression</strong> model. There is an interesting discussion of this piece of history in statistics in Chapter 9 of the book<label for="tufte-sn-75" class="margin-toggle sidenote-number">75</label><input type="checkbox" id="tufte-sn-75" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">75</span> Cramer, J.S., <em>Logit Models from Economics and Other Fields</em>, Cambridge University Press, 2003.</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-difflogit"></span>
<img src="graphics/3_difflogit.png" alt="Three examples of the logistic function" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 43: Three examples of the logistic function<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
</div>
<div id="does-the-logistic-function-make-sense-an-eda-approach" class="section level3 unnumbered">
<h3>Does the logistic function make sense? — An EDA approach</h3>
<p>Figure <a href="#fig:f3-lrgoal3">28</a> outlines the main premise of the logistic regression model. It remains unknown whether or not this is a practical assumption. Here, we show how we could evaluate this assumption in a specific dataset. Let’s use the AD dataset and pick up the predictor, <code>HippoNV</code>, and the outcome variable <code>DX_bl</code>.</p>
<p>First, we create a data table like the one shown in Table <a href="#tab:t3-goal3">6</a>. We discretize the continuous variable <code>HippoNV</code> into distinct levels, and compute the prevalence of AD incidences within each level (i.e., the <span class="math inline">\(Pr(y=1|x)\)</span>). The following R code serves this data processing purpose.</p>
<p></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the frequency table in accordance of categorization</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="co"># of HippoNV</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">=</span> <span class="fu">quantile</span>(AD<span class="sc">$</span>HippoNV,<span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.05</span>, <span class="at">to =</span> <span class="fl">0.95</span>,</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>                                          <span class="at">by =</span> <span class="fl">0.05</span>))</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>AD<span class="sc">$</span>HippoNV.category <span class="ot">&lt;-</span> <span class="fu">cut</span>(AD<span class="sc">$</span>HippoNV, <span class="at">breaks=</span><span class="fu">c</span>(<span class="sc">-</span><span class="cn">Inf</span>,</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>                                              temp, <span class="cn">Inf</span>))</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">xtabs</span>(<span class="sc">~</span>DX_bl <span class="sc">+</span> HippoNV.category, </span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data =</span> AD))</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> tempData[<span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">2</span>, <span class="at">to =</span> </span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>                   <span class="dv">2</span><span class="sc">*</span><span class="fu">length</span>(<span class="fu">unique</span>(AD<span class="sc">$</span>HippoNV.category)), </span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>                   <span class="at">by =</span> <span class="dv">2</span>),]</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">xtabs</span>(<span class="sc">~</span>DX_bl <span class="sc">+</span> HippoNV.category, <span class="at">data =</span> AD))</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>tempData<span class="sc">$</span>Total <span class="ot">&lt;-</span> <span class="fu">colSums</span>(<span class="fu">as.matrix</span>(<span class="fu">xtabs</span>(<span class="sc">~</span>DX_bl <span class="sc">+</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>                    HippoNV.category,<span class="at">data =</span> AD)))</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>tempData<span class="sc">$</span>p.hat <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> tempData<span class="sc">$</span>Freq<span class="sc">/</span>tempData<span class="sc">$</span>Total</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>tempData<span class="sc">$</span>HippoNV.category <span class="ot">=</span> <span class="fu">as.numeric</span>(tempData<span class="sc">$</span>HippoNV.category)</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(tempData)</span></code></pre></div>
<p></p>
<p>We use the <code>str()</code> function to visualize the data we have converted: <span class="math inline">\(20\)</span> levels of <code>HippoNV</code> have been created, denoted by the variable <code>HippoNV.category</code>; <code>Total</code> denotes the total number of subjects within each level; and <code>p.hat</code> denotes the proportion of the diseased subjects within each level (i.e., the <span class="math inline">\(Pr(y=1|x)\)</span>).</p>
<p></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(tempData)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="do">## &#39;data.frame&#39;:    20 obs. of  5 variables:</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ DX_bl           : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 ...</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ HippoNV.category: num  1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ Freq            : int  24 25 25 21 22 15 17 17 19 11 ...</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ Total           : num  26 26 26 26 26 25 26 26 26 34 ...</span></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  $ p.hat           : num  0.0769 0.0385 0.0385 0.1923 0.1538</span></span></code></pre></div>
<p></p>
<p>We draw a scatterplot of <code>HippoNV.category</code> versus <code>p.hat</code>, as shown in Figure <a href="#fig:f3-4">44</a>. We also use the <code>loess</code> method, which is a <em>nonparametric smoothing</em> method<label for="tufte-sn-76" class="margin-toggle sidenote-number">76</label><input type="checkbox" id="tufte-sn-76" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">76</span> Related methods will be introduced in <strong>Chapter 9</strong>.</span>, to fit a smooth curve of the scatter data points. Figure <a href="#fig:f3-4">44</a> exhibits a similar pattern as Figure <a href="#fig:f3-lrgoal3">28</a>. This provides an empirical justification of the use of the logistic regression model in this dataset.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-4"></span>
<img src="graphics/3_4.png" alt="The empirical relationship between `HippoNV` and `DX_bl` takes a shape as the logistic function" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 44: The empirical relationship between <code>HippoNV</code> and <code>DX_bl</code> takes a shape as the logistic function<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the scatterplot of HippoNV.category </span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="co"># versus the probability of normal</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tempData, <span class="fu">aes</span>(<span class="at">x =</span> HippoNV.category, <span class="at">y =</span> p.hat))</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">3</span>)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span><span class="st">&quot;Empirically observed probability of normal&quot;</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>              , <span class="at">xlab =</span> <span class="st">&quot;HippoNV&quot;</span>)</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
</div>
<div id="regression-vs.-tree-models" class="section level3 unnumbered">
<h3>Regression vs. tree models</h3>
<p>A decision tree model draws a distinct type of <strong>decision boundary</strong>, as illustrated in Figure <a href="#fig:f3-tree-boundary">45</a>. Think about how a tree is built: at each node, a split is implemented based on <em>one single variable</em>, and in Figure <a href="#fig:f3-tree-boundary">45</a> the classification boundary is either parallel or perpendicular to one axis.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f3-tree-boundary"></span>
<p class="caption marginnote shownote">
Figure 45: Illustration of a decision tree model for a binary classification problem (i.e., the solid circles and empty squares represent data points from two classes), built on two predictors (i.e., <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>); (left) is the scatterplot of the data overlaid with the decision boundary of the decision tree model, which is shown in the (right)
</p>
<img src="graphics/5_simple_tree.png" alt="Illustration of a decision tree model for a binary classification problem (i.e., the solid circles and empty squares represent data points from two classes), built on two predictors (i.e., $x_1$ and $x_2$); (left) is the scatterplot of the data overlaid with the decision boundary of the decision tree model, which is shown in the (right)" width="80%"  />
</div>
<p></p>
<p>This implies that, when applying a decision tree to a dataset with linear relationship between predictors and outcome variables, it may not be an optimal choice. In the following example, we simulate a dataset and apply a decision tree and a logistics regression model to the data, respectively. The training data, and the predicted classes for each data point from the logistic regression and decision models are shown in Figures <a href="#fig:f2-22">46</a>, <a href="#fig:f2-23">47</a> and <a href="#fig:f2-24">48</a>, respectively. It can be seen that the classification boundary from the logistics regression model is linear, while the one from the decision tree is parallel to the axis. Decision tree is not able to capture the linear relationship in the data. The R code for this experiment is shown in below.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-22"></span>
<img src="graphics/2_22.png" alt="Scatterplot of the generated dataset" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 46: Scatterplot of the generated dataset<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-23"></span>
<img src="graphics/2_23.png" alt="Decision boundary captured by a logistic regression model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 47: Decision boundary captured by a logistic regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(rpart)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>ndata <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(ndata, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(ndata, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X1,X2)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">mutate</span>( <span class="at">X12 =</span> <span class="fl">0.5</span> <span class="sc">*</span> (X1 <span class="sc">-</span> X2), <span class="at">Y =</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">ifelse</span>(X12<span class="sc">&gt;=</span><span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>))</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>ix <span class="ot">&lt;-</span> <span class="fu">which</span>( <span class="fu">abs</span>(data<span class="sc">$</span>X12) <span class="sc">&lt;=</span> <span class="fl">0.05</span>)</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>Y[ix] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">runif</span>( <span class="fu">length</span>(ix)) <span class="sc">&lt;</span> <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data  <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>X12) <span class="sc">%&gt;%</span>  <span class="fu">mutate</span>(<span class="at">Y =</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">as.factor</span>(<span class="fu">as.character</span>(Y)))</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data,<span class="fu">aes</span>(<span class="at">x=</span>X1,<span class="at">y=</span>X2,<span class="at">color=</span>Y))<span class="sc">+</span><span class="fu">geom_point</span>()</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>linear_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y <span class="sc">~</span>  ., <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>                                                  <span class="at">data =</span> data)</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>tree_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>( Y <span class="sc">~</span>  ., <span class="at">data =</span> data)</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>pred_linear <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear_model, data,<span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>pred_tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_model, data,<span class="at">type=</span><span class="st">&quot;prob&quot;</span>)[,<span class="dv">1</span>]</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>data_pred <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">pred_linear_class =</span></span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ifelse</span>(pred_linear <span class="sc">&lt;</span><span class="fl">0.5</span>,<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">%&gt;%</span><span class="fu">mutate</span>(<span class="at">pred_linear_class =</span></span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.factor</span>(<span class="fu">as.character</span>(pred_linear_class)))<span class="sc">%&gt;%</span></span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">pred_tree_class =</span> <span class="fu">ifelse</span>( pred_tree <span class="sc">&lt;</span><span class="fl">0.5</span>,<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>( <span class="at">pred_tree_class =</span></span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">as.factor</span>(<span class="fu">as.character</span>(pred_tree_class)))</span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_pred,<span class="fu">aes</span>(<span class="at">x=</span>X1,<span class="at">y=</span>X2,<span class="at">color=</span>pred_linear_class))<span class="sc">+</span></span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_pred,<span class="fu">aes</span>(<span class="at">x=</span>X1,<span class="at">y=</span>X2,<span class="at">color=</span>pred_tree_class))<span class="sc">+</span></span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f2-24"></span>
<img src="graphics/2_24.png" alt="Decision boundary captured by the tree model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 48: Decision boundary captured by the tree model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
</div>
<div id="can-we-use-a-tree-model-for-regression" class="section level3 unnumbered">
<h3>Can we use a tree model for regression?</h3>
<p>The answer is yes. There is nothing preventing us from modifying the tree-learning process as we have presented in <strong>Chapter 2</strong> for predicting continuous outcome. You only need to modify the IG, i.e., to create a similar counterpart for continuous outcomes.</p>
<p>Without going into further technical details, we present the modified 6-step R pipeline for a regression tree.</p>
<p></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># AGE, PTGENDER and PTEDUCAT are used as the </span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictor variables. </span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="co"># MMSCORE (a numeric value) is the outcome.</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: read data into R</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: data preprocessing</span></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: build the tree</span></span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a><span class="co"># for regression problems, use method=&quot;anova&quot;</span></span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>tree_reg <span class="ot">&lt;-</span> <span class="fu">rpart</span>( MMSCORE <span class="sc">~</span>  ., data.train, <span class="at">method=</span><span class="st">&quot;anova&quot;</span>) </span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: draw the tree</span></span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(rpart.plot)</span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree_reg, <span class="at">nn.cex=</span><span class="dv">1</span>)</span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; prune the tree</span></span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a>tree_reg <span class="ot">&lt;-</span> <span class="fu">prune</span>(tree_reg,<span class="at">cp=</span><span class="fl">0.03</span>)</span>
<span id="cb68-31"><a href="#cb68-31" aria-hidden="true" tabindex="-1"></a><span class="fu">prp</span>(tree_reg,<span class="at">nn.cex=</span><span class="dv">1</span>)</span>
<span id="cb68-32"><a href="#cb68-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-33"><a href="#cb68-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict using your tree model</span></span>
<span id="cb68-34"><a href="#cb68-34" aria-hidden="true" tabindex="-1"></a>pred.tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_reg, data.test)</span>
<span id="cb68-35"><a href="#cb68-35" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(pred.tree, data.test<span class="sc">$</span>MMSCORE)</span>
<span id="cb68-36"><a href="#cb68-36" aria-hidden="true" tabindex="-1"></a><span class="co">#For regression model, you can use correlation </span></span>
<span id="cb68-37"><a href="#cb68-37" aria-hidden="true" tabindex="-1"></a><span class="co"># to measure how close are your predictions </span></span>
<span id="cb68-38"><a href="#cb68-38" aria-hidden="true" tabindex="-1"></a><span class="co"># with the true outcome values of the data points</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f3-tree-interaction"></span>
<img src="graphics/3_tree_interaction.png" alt="Decision tree to predict `MMSCORE` using `PTEDUCAT` and `AGE`" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 49: Decision tree to predict <code>MMSCORE</code> using <code>PTEDUCAT</code> and <code>AGE</code><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The learned tree is shown in Figure <a href="#fig:f3-tree-interaction">49</a>. In the EDA analysis shown in <strong>Chapter 2</strong>, it has been shown that the relationship between <code>MMSCORE</code> and <code>PTEDUCAT</code> changes substantially according to different levels of <code>AGE</code>. Here shows the decision tree can also capture the interaction between <code>PTEDUCAT</code>, <code>AGE</code> and <code>MMSCORE</code>.</p>
<!-- % \footnote{Some advanced examples for interested readers: Neal, R. *Bayesian learning for neural networks*, Springer Verlag 1996. \\ Lee, K. and Kim, J. *On the equivalence of linear discriminant analysis and least squares*, AAAI 2005. \\ Ye, J. *Least squares linear discriminant analysis*, ICML 2007. \\ Li, F., Yang, Y. and Xing, E. *From LASSO regression to feature vector machine*, NIPS 2005.} -->
</div>
</div>
<div id="exercises-1" class="section level2 unnumbered">
<h2>Exercises</h2>
<div id="data-analysis" class="section level3 unnumbered">
<h3>Data analysis</h3>
<p><!-- begin{enumerate} --></p>
<ul>
<li> Consider the case that, in building linear regression models, there is a concern that some data points may be more important (or more trustable). For these cases, it is not uncommon to assign a weight to each data point. Denote the weight for the <span class="math inline">\(i^{th}\)</span> data point as <span class="math inline">\(w_i\)</span>. An example is shown in Table <a href="#tab:t3-hw-wls">8</a>, as the last column, e.g., <span class="math inline">\(w_1=1\)</span>, <span class="math inline">\(w_2=2\)</span>, <span class="math inline">\(w_5=3\)</span>.</li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t3-hw-wls">Table 8: </span>Dataset for building a weighted linear regression model</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
<th align="left"><span class="math inline">\(w\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-0.15\)</span></td>
<td align="left"><span class="math inline">\(-0.48\)</span></td>
<td align="left"><span class="math inline">\(0.46\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-0.72\)</span></td>
<td align="left"><span class="math inline">\(-0.54\)</span></td>
<td align="left"><span class="math inline">\(-0.37\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.91\)</span></td>
<td align="left"><span class="math inline">\(-0.27\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0.61\)</span></td>
<td align="left"><span class="math inline">\(1.59\)</span></td>
<td align="left"><span class="math inline">\(1.35\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(-1.11\)</span></td>
<td align="left"><span class="math inline">\(0.34\)</span></td>
<td align="left"><span class="math inline">\(-0.11\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>We still want to estimate the regression parameters in the least-squares framework. Follow the process of the derivation of the least-squares estimator as shown in <strong>Chapter 2</strong>, and propose your new estimator of the regression parameters.</p>
<ul>
<li><p> Follow up the weighted least squares estimator derived in Q1, please calculate the regression parameters of the regression model using the data shown in Table <a href="#tab:t3-hw-wls">8</a>.</p></li>
<li><p> Follow up the dataset in Q1. Use the R pipeline for linear regression on this data (set up the weights in the <code>lm()</code> function). Compare the result from R and the result by your manual calculation.</p></li>
<li><p> Consider the dataset in Table <a href="#tab:t2-hw-dt">5</a>. Use the R pipeline for building a logistic regression model on this data.</p></li>
<li><p> Consider the model fitted in Q4. Suppose that now there are two new data points as shown in Table <a href="#tab:t3-hw-lr-pred">9</a>. Please use the fitted model to predict on these two data points and fill in the table.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t3-hw-lr-pred">Table 9: </span>Two test data points</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(9\)</span></td>
<td align="left"><span class="math inline">\(0.25\)</span></td>
<td align="left"><span class="math inline">\(0.18\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(10\)</span></td>
<td align="left"><span class="math inline">\(0.08\)</span></td>
<td align="left"><span class="math inline">\(1.12\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Use the dataset <code>PimaIndiansDiabetes2</code> in the <code>mlbench</code> R package, run the R pipeline for logistic regression on it, and summarize your findings.</p></li>
<li><p> Follow up on the simulation experiment in Q12 in <strong>Chapter 2</strong>. Apply <code>glm()</code> on the simulated data to build a logistic regression model, and comment on the result.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
</div>
<div id="chapter-4.-resonance-bootstrap-random-forests" class="section level1 unnumbered">
<h1>Chapter 4. Resonance: Bootstrap &amp; Random Forests</h1>
<div id="overview-2" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 4 is about <em>Resonance</em>. It is how we work with computers to exploit its remarkable power in conducting iterations and repetitive tasks which human beings find burdensome to do. This capacity of computers enables applications of modern optimization algorithms which underlie many data analytics models. This capacity also makes it realistic to use statistical techniques that don’t require analytic tractability.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-iteration-hend"></span>
<img src="graphics/4_iteration_hend.png" alt="Iterations from a pendulum" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 50: Iterations from a pendulum<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-iteration-spiral"></span>
<img src="graphics/4_iteration_spiral.png" alt="Iterations that form a spiral" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 51: Iterations that form a spiral<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Decomposing a computational task into repetitive subtasks needs skillful design. The subtasks should be identical in their mathematical forms, so if they differ from each other they only differ in their specific <em>parametric</em> configurations. A subtask should also be easy to solve, and sometimes there is even closed-form solution. These subtasks may or may not be carried out in a sequential manner, but as a whole, they <em>move things forward</em>—solving a problem. Not all repetitions move things forward, e.g., two types of repetitions are shown in Figures <a href="#fig:f4-iteration-hend">50</a> and <a href="#fig:f4-iteration-spiral">51</a>.</p>
<p>Comparing with the <em>divide-and-conquer</em> that is more of a spatial nature, here, repetition is more of a temporal nature, and effective designs of repetitions need the power of <em>resonance</em> between the repetitions. If they are carried out in a sequential manner, they do not depend on each other logically like in a deduction sequence; and if one subtask is only solved suboptimally, as a whole they move forward towards an optimal solution.</p>
<p>A particular invention that has played a critical role in many data analytic applications is the <strong>Bootstrap</strong>. Building on the idea of Bootstrap, Random Forest was invented in 2001,<label for="tufte-sn-77" class="margin-toggle sidenote-number">77</label><input type="checkbox" id="tufte-sn-77" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">77</span> Breiman, L., <em>Random Forests.</em> Machine Learning, Volume 45, Issue 1, Pages 5-32, 2001.</span> after that came countless <strong>Ensemble Learning</strong> methods<label for="tufte-sn-78" class="margin-toggle sidenote-number">78</label><input type="checkbox" id="tufte-sn-78" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">78</span> See <strong>Chapter 7</strong>.</span>.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="how-bootstrap-works" class="section level2 unnumbered">
<h2>How bootstrap works</h2>
<div id="rationale-and-formulation-5" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>There are multiple perspectives to look at Bootstrap. One perspective that has been well studied in the seminar book<label for="tufte-sn-79" class="margin-toggle sidenote-number">79</label><input type="checkbox" id="tufte-sn-79" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">79</span> Efron, B. and Tibshirani, R.J., * An Introduction to the Bootstrap.* Chapman &amp; Hall/CRC, 1993.</span> is to treat Bootstrap as a simulation of the <em>sampling process</em>. As we know, sampling refers to the idea that we could draw samples again and again from the same population. Many statistical techniques make sense only when we put them in the framework of sampling<label for="tufte-sn-80" class="margin-toggle sidenote-number">80</label><input type="checkbox" id="tufte-sn-80" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">80</span> When statistics gained its scientific foundation and a modern appeal, there was also the rise of <em>mass production</em> that introduced human beings into a mechanical world populated with endless repetitive movements of machines and processes, as dramatized in Charlie Chaplin’s movies.</span>. For example, in hypothesis testing, when the Type 1 Error (a.k.a., the <span class="math inline">\(\alpha\)</span>) is set to be <span class="math inline">\(0.05\)</span>, it means that if we are able to repeat for multiple times the data collection, computation of statistics, and hypothesis testing, on average we will reject the null hypothesis <span class="math inline">\(5\%\)</span> of the times even when the null hypothesis is true.</p>
<p>For many statistical models, the analytic <em>tractability</em> of the sampling process lays the foundation to study their behavior and performance. When there were no computers, analytical tractability had been (and still is) one main factor that determines the “fate” of a statistical model—i.e., if we haven’t found an analytical tractable formulation to study the model considering its sampling process, it is hard to convince statisticians that the model is valid<label for="tufte-sn-81" class="margin-toggle sidenote-number">81</label><input type="checkbox" id="tufte-sn-81" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">81</span> E.g., without the possibility (even the possibility is probably only <em>theoretical</em>) of infinite repetition of the same process, the concept <span class="math inline">\(\alpha\)</span> in hypothesis testing will lose its ground of being something tangible.</span>. Nonetheless, there have been good statistical models that have no rigorous mathematical formulations, yet they are effective in applications. It may take years for us to find a mathematical framework to establish these models as rigorous approaches.</p>
<p>As a remedy, Bootstrap exploits the power of computers to simulate the sampling process. Figure <a href="#fig:f4-1">52</a> provides a simple illustration of its idea. The input of a Bootstrap algorithm is a dataset, which provides a <em>representation</em> of the underlying population. As the dataset is considered to be an representational <em>equivalent</em> of the underlying population, <em>sampling from the underlying population</em> could be approximated by <em>resampling from the dataset</em>. As shown in Figure <a href="#fig:f4-1">52</a>, we could generate any number of Bootstrapped datasets by randomly drawing samples (with or without replacement, both have been useful) from the dataset. The idea is simple and effective.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f4-1"></span>
<p class="caption marginnote shownote">
Figure 52: A demonstration of the Bootstrap process
</p>
<img src="graphics/4_1.png" alt="A demonstration of the Bootstrap process" width="80%"  />
</div>
<p></p>
<p><em>The conceptual power of Bootstrap.</em> A good model has a concrete procedure of operations. That is <em>what it does</em>. There is also a conceptual-level perspective that concerns a model regarding <em>what it is</em>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-samplingprocess"></span>
<img src="graphics/4_samplingprocess.png" alt="A demonstration of the sampling process" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 53: A demonstration of the sampling process<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>A sampling process is conventionally—or conveniently—conceived as a static snapshot, as shown in Figure <a href="#fig:f4-samplingprocess">53</a>. The data points are pictured as silhouettes of apples, to highlight the psychological tendency we all share that we too often focus on the samples (the fruit) rather than the sampling process (the tree). This view is not entirely wrong, but it is a reduced view and it is easy for this fact to slip below the level of consciousness. We often take the apples as an absolute fact and forget that they are only historic coincidence: they are a <em>representation</em> of the apple tree (here, corresponds to the concept <em>population</em>); they themselves are not the apple tree.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f4-samplingprocess2"></span>
<p class="caption marginnote shownote">
Figure 54: A demonstration of the dynamic view of Bootstrap. The tree is drawn using <a href="http://mwskirpan.com/FractalTree/">http://mwskirpan.com/FractalTree/</a>.
</p>
<img src="graphics/4_samplingprocess_tree.png" alt="A demonstration of the dynamic view of Bootstrap. The tree is drawn using [http://mwskirpan.com/FractalTree/](http://mwskirpan.com/FractalTree/)." width="80%"  />
</div>
<p></p>
<p>The Bootstrap starts from what we have forgotten. Not to study an apple as an apple, it studies the process of how an apple is created, the process of apple<em>-ing</em>. Bearing this objective in mind, an apple is no longer empirical, now it is a <em>general</em> apple. It bears the genetics of apple that is shared by all apples and conceives the possibility of an apple tree. This is the power of the Bootstrap on the conceptual level. As illustrated in Figure <a href="#fig:f4-samplingprocess2">54</a>, it now animates the static snapshot shown in Figure <a href="#fig:f4-samplingprocess">53</a> and recovers the dynamic process. It creates resonance among the snapshots, and the resonance completes the big picture and enlarges our conceptual view of the problem.</p>
</div>
<div id="theory-and-method-3" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>Let’s consider the estimator of the mean of a normal population. A random variable <span class="math inline">\(X\)</span> follows a normal distribution, i.e., <span class="math inline">\(X \sim N\left(\mu, \sigma^{2}\right)\)</span>. For simplicity, let’s assume that we have known the variance <span class="math inline">\(\sigma^{2}\)</span>. We want to estimate the mean <span class="math inline">\(\mu\)</span>. What we need to do is to randomly draw a few samples from the distribution. Denote these samples as <span class="math inline">\(x_{1}, x_{2}, \ldots, x_{N}\)</span>. To estimate <span class="math inline">\(\mu\)</span> , it seems natural to use the average of the samples, denoted as <span class="math inline">\(\overline{x}=\frac{1}{N} \sum_{n=1}^{N} x_{i}\)</span>. We use <span class="math inline">\(\overline{x}\)</span> as an estimator of <span class="math inline">\(\mu\)</span>, i.e., <span class="math inline">\(\hat{\mu}=\overline{x}\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-two-sampling-process"></span>
<img src="graphics/4_two_sampling_process.png" alt="A sampling process that concerns $x$ (upper) and an enlarged view of the sampling process that concerns both $x$ and $\hat{\mu}$ (bottom)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 55: A sampling process that concerns <span class="math inline">\(x\)</span> (upper) and an enlarged view of the sampling process that concerns both <span class="math inline">\(x\)</span> and <span class="math inline">\(\hat{\mu}\)</span> (bottom)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>A question arises, how good is <span class="math inline">\(\overline{x}\)</span> to be an estimator of <span class="math inline">\(\mu\)</span>?</p>
<p>Obviously, if <span class="math inline">\(\overline{x}\)</span> is numerically close to <span class="math inline">\(\mu\)</span>, it is a good estimator. The problem is, for a particular dataset, we don’t know what is <span class="math inline">\(\mu\)</span>. And even if we know <span class="math inline">\(\mu\)</span>, we need to have criteria to tell us how close is close enough. On top of all these considerations, common sense tells us that <span class="math inline">\(\overline{x}\)</span> itself is a random variable that is subject to uncertainty. To evaluate this uncertainty and get a general sense of how closely <span class="math inline">\(\overline{x}\)</span> estimates <span class="math inline">\(\mu\)</span>, a brute-force approach is to repeat the physical experiment many times. But this is not necessary. In this particular problem, where <span class="math inline">\(X\)</span> follows a normal distribution, we could circumvent the physical burden (i.e., of repeating the experiments) via mathematical derivation. Since <span class="math inline">\(X\)</span> follows a normal distribution, we could derive that <span class="math inline">\(\overline{x}\)</span> is another normal distribution, i.e., <span class="math inline">\(\overline{x} \sim N\left(\mu, \sigma^{2} / N\right)\)</span>. Then we know that <span class="math inline">\(\overline{x}\)</span> is an unbiased estimator, since <span class="math inline">\(E(\overline{x})=\mu\)</span>. Also, we know that the larger the sample size, the better the estimation of <span class="math inline">\(\mu\)</span> by <span class="math inline">\(\overline{x}\)</span>, since the variance of the estimator is <span class="math inline">\(\sigma^{2} / N\)</span>.<label for="tufte-sn-82" class="margin-toggle sidenote-number">82</label><input type="checkbox" id="tufte-sn-82" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">82</span> In this case, the sampling process includes (1) drawing samples from the distribution; and (2) estimating <span class="math inline">\(\mu\)</span> using <span class="math inline">\(\overline{x}\)</span>. Illustration is shown in Figure <a href="#fig:f4-two-sampling-process">55</a>. The <em>sampling process</em> is a flexible concept, depending on what variables are under study.</span></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f4-2"></span>
<p class="caption marginnote shownote">
Figure 56: The (nonparametric) Bootstrap scheme to computationally evaluate the sampling distribution of <span class="math inline">\(\overline{x}\)</span>
</p>
<img src="graphics/4_2.png" alt="The (nonparametric) Bootstrap scheme to computationally evaluate the sampling distribution of $\overline{x}$" width="80%"  />
</div>
<p></p>
<p><em>When Bootstrap is needed.</em> Apparently, knowing the analytic form of <span class="math inline">\(\overline{x}\)</span> is the key in the example shown above to circumvent the physical need of sampling. And the analytic tractability originates from the condition that <span class="math inline">\(X\)</span> follows normal distribution. For many other estimators that we found analytically intractable, Bootstrap provides a computational remedy that enables us to investigate their properties, because it <em>computationally</em> mimics the physical sampling process.</p>
<p>For example, while the distribution of <span class="math inline">\(X\)</span> is unknown, we could follow the Bootstrap scheme illustrated in Figure <a href="#fig:f4-2">56</a> to evaluate the sampling distribution of <span class="math inline">\(\overline{x}\)</span>. For any bootstrapped dataset, we can calculate the <span class="math inline">\(\overline{x}\)</span> and obtain a “sample” of <span class="math inline">\(\overline{x}\)</span>, denoted as <span class="math inline">\(\overline{x}_i\)</span>. Figure <a href="#fig:f4-2">56</a> only illustrates three cases, while in practice usually thousands of bootstrapped datasets are drawn. After we have the “samples” of <span class="math inline">\(\overline{x}\)</span>, we can draw the distribution of <span class="math inline">\(\overline{x}\)</span> and present it as shown in Figure <a href="#fig:f4-2">56</a>. Although we don’t know the distribution’s analytic form, we have its numerical representation stored in a computer.</p>
<p>The Bootstrap scheme illustrated in Figure <a href="#fig:f4-2">56</a> is called <em>nonparametric</em> Bootstrap, since no <em>parametric</em> model is used to mediate the process<label for="tufte-sn-83" class="margin-toggle sidenote-number">83</label><input type="checkbox" id="tufte-sn-83" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">83</span> Put simply, a parametric model is a model with an explicit mathematical form that is calibrated by parameters.</span>. This is not the only way through which we can conduct Bootstrap. For example, a parametric Bootstrap scheme is illustrated in Figure <a href="#fig:f4-3">57</a> to perform the same task, i.e., to study the sampling distribution of <span class="math inline">\(\overline{x}\)</span>. The difference between the nonparametric Bootstrap scheme in Figure <a href="#fig:f4-2">56</a> and the parametric Bootstrap scheme in Figure <a href="#fig:f4-3">57</a> is that, when generating new samples, the nonparametric Bootstrap uses the original dataset as the representation of the underlying population, while the parametric Bootstrap uses a fitted distribution model.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f4-3"></span>
<p class="caption marginnote shownote">
Figure 57:  The (parametric) Bootstrap scheme to computationally evaluate the sampling distribution of <span class="math inline">\(\overline{x}\)</span>
</p>
<img src="graphics/4_3.png" alt=" The (parametric) Bootstrap scheme to computationally evaluate the sampling distribution of $\overline{x}$" width="80%"  />
</div>
<p></p>
<p><em>Bootstrap for regression model.</em> We show another example about how Bootstrap could be used. In <strong>Chapter 2</strong>, we showed that we can derive the explicit distribution of the estimated regression parameters<label for="tufte-sn-84" class="margin-toggle sidenote-number">84</label><input type="checkbox" id="tufte-sn-84" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">84</span> Recall that, to derive this distribution, a few assumptions are needed: the Gaussian assumption of the error term, and the linear assumptions between predictors and outcome variable.</span>. Here, we introduce another approach, based on the idea of Bootstrap, to <em>compute</em> an empirical distribution of the estimated regression parameters.</p>
<p>The first challenge we encounter is the ambiguity of “population” here. Unlike in the parameter estimation examples shown in Figure <a href="#fig:f4-2">56</a>, here, a regression model involves three entities, the predictors <span class="math inline">\(\boldsymbol{x}\)</span>, the outcome variable <span class="math inline">\(y\)</span>, and the error term <span class="math inline">\(\epsilon\)</span>. It depends on how we define the “population” to decide how Bootstrap could be used<label for="tufte-sn-85" class="margin-toggle sidenote-number">85</label><input type="checkbox" id="tufte-sn-85" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">85</span> Remember that, Bootstrap is a computational procedure to mimic the sampling process from a <em>population</em>. So, when you deal with a problem, you need to determine first what is the population.</span>.</p>
<p>A variety of options could be obtained. Below are some examples.</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> [Option 1.] We could simply resample the data points (i.e., the (<span class="math inline">\(\boldsymbol{x}_n\)</span>,<span class="math inline">\(y_n\)</span>) pairs) following the nonparametric Bootstrap scheme shown in Figure <a href="#fig:f4-2">56</a>. For each sampled dataset, we fit a regression model and obtain the fitted regression parameters.</p></li>
<li><p> [Option 2.] We could fix the <span class="math inline">\(\boldsymbol{x}\)</span>, and only sample for <span class="math inline">\(y\)</span>.<label for="tufte-sn-86" class="margin-toggle sidenote-number">86</label><input type="checkbox" id="tufte-sn-86" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">86</span> In this way we implicitly assume that the uncertainty of the dataset mainly comes from <span class="math inline">\(y\)</span>.</span> To sample <span class="math inline">\(y\)</span>, we draw samples using a fitted conditional distribution model <span class="math inline">\(P(y|\boldsymbol{x})\)</span>.<label for="tufte-sn-87" class="margin-toggle sidenote-number">87</label><input type="checkbox" id="tufte-sn-87" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">87</span> We could use kernel density estimation approaches, which are not introduced in this book. Interested readers could explore the R package <code>kdensity</code> and read the monograph by Silverman, B.W., <em>Density Estimation for Statistics and Data Analysis</em>, Chapman &amp; Hall/CRC, 1986. A related method, called <em>Kernel regression</em> model, can be found in <strong>Chapter 9</strong>.</span> Then, for each sampled dataset, we fit a regression model and obtain the fitted parameters.</p></li>
<li><p> [Option 3.] We could simulate new samples of <span class="math inline">\(\boldsymbol{x}\)</span> using the nonparametric Bootstrap method on the samples of <span class="math inline">\(\boldsymbol{x}\)</span> only. Then, for the new samples of <span class="math inline">\(\boldsymbol{x}\)</span>, we draw samples of <span class="math inline">\(y\)</span> using the fitted conditional distribution model <span class="math inline">\(P(y|\boldsymbol{x})\)</span>. This is a combination of the nonparametric and parametric Bootstrap methods. Then, for each sampled dataset, we can fit a regression model and obtain the fitted regression parameters.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>Via either option, we could obtain an empirical distribution of the estimated regression parameter and compute a curve like the one shown in Figure <a href="#fig:f4-2">56</a>. For instance, suppose we repeat this process <span class="math inline">\(10,000\)</span> times. We can obtain <span class="math inline">\(10,000\)</span> sets of estimated regression parameters, then we can use these samples to evaluate the sampling distribution of the regression parameters. We can also see if the parameters are significantly different from <span class="math inline">\(0\)</span>, and derive their <span class="math inline">\(95\%\)</span> confidence intervals.</p>
<p>The three options above are just some examples<label for="tufte-sn-88" class="margin-toggle sidenote-number">88</label><input type="checkbox" id="tufte-sn-88" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">88</span> Options 1 and 3 both define the population as the joint distribution of <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(y\)</span>, but differ in the ways to draw samples. Option 2 defines the population as <span class="math inline">\(P(y|\boldsymbol{x})\)</span> only.</span>. As a more complicated model than simple parametric estimation in distribution fitting, how to conduct Bootstrap on regression models (and other complex models such as time series models or decision tree models) is a challenging problem.</p>
</div>
<div id="r-lab-4" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>4-Step R Pipeline.</em> <strong>Step 1</strong> is to load the dataset into the R workspace.</p>
<p></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span></code></pre></div>
<p></p>
<p><strong>Step 2</strong> is to implement Bootstrap on a model. Here, let’s implement Bootstrap on the parameter estimation problem for normal distribution fitting. We obtain results using both the analytic approach and the Bootstrapped approach, so we could evaluate how well the Bootstrap works.</p>
<p>Specifically, let’s pick up the variable <code>HippoNV</code>, and estimate its mean for the normal subjects<label for="tufte-sn-89" class="margin-toggle sidenote-number">89</label><input type="checkbox" id="tufte-sn-89" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">89</span> Recall that we have both <em>normal</em> and <em>diseased</em> populations in our dataset.</span>. Assuming that the variable <code>HippoNV</code> is distributed as a normal distribution, we could use the <code>fitdistr()</code> function from the R package <code>MASS</code> to estimate the mean and standard derivation, as shown below.</p>
<p></p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Decide on the statistical operation </span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="co"># that you want to &quot;Bootstrap&quot; with</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(MASS)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">fitdistr</span>(AD<span class="sc">$</span>HippoNV, <span class="at">densfun=</span><span class="st">&quot;normal&quot;</span>)  </span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="co"># fitdistr() is a function from the package &quot;MASS&quot;. </span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a><span class="co"># It can fit a range of distributions, e.g., by using the argument, </span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="co"># densfun=&quot;normal&quot;, we fit a normal distribution.</span></span></code></pre></div>
<p></p>
<p>The <code>fitdistr()</code> function returns the estimated parameters together with their standard derivation<label for="tufte-sn-90" class="margin-toggle sidenote-number">90</label><input type="checkbox" id="tufte-sn-90" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">90</span> Here, the standard derivation of the estimated parameters is derived based on the assumption of normality of <code>HippoNV</code>. This is a theoretical result, in contrast with the computational result by Bootstrap shown later.</span>, and the <span class="math inline">\(95\%\)</span> CI of the estimated mean.</p>
<p></p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>fit</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="do">##       mean           sd     </span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.471662891   0.076455789 </span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  (0.003362522) (0.002377662)</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>lower.bound <span class="ot">=</span> fit<span class="sc">$</span>estimate[<span class="dv">1</span>] <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> fit<span class="sc">$</span>sd[<span class="dv">2</span>]</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>upper.bound <span class="ot">=</span> fit<span class="sc">$</span>estimate[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> fit<span class="sc">$</span>sd[<span class="dv">2</span>]</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="do">##   lower.bound   upper.bound     </span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a><span class="do">##    0.4670027     0.4763231 </span></span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> implements the nonparametric Bootstrap scheme<label for="tufte-sn-91" class="margin-toggle sidenote-number">91</label><input type="checkbox" id="tufte-sn-91" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">91</span> The one shown in Figure <a href="#fig:f4-2">56</a>.</span>, as an alternative approach, to obtain the <span class="math inline">\(95\%\)</span> CI of the estimated mean.</p>
<p></p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; draw R bootstrap replicates to </span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="co"># conduct the selected statistical operation</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the vector to store the bootstrapped estimates</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>bs_mean <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, R)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="co"># draw R bootstrap resamples and obtain the estimates</span></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>R) {</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>  resam1 <span class="ot">&lt;-</span> <span class="fu">sample</span>(AD<span class="sc">$</span>HippoNV, <span class="fu">length</span>(AD<span class="sc">$</span>HippoNV), </span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">replace =</span> <span class="cn">TRUE</span>) </span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># resam1 is a bootstrapped dataset. </span></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">fitdistr</span>(resam1 , <span class="at">densfun=</span><span class="st">&quot;normal&quot;</span>)  </span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># store the bootstrapped estimates of the mean</span></span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>  bs_mean[i] <span class="ot">&lt;-</span> fit<span class="sc">$</span>estimate[<span class="dv">1</span>] </span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p>Here, <span class="math inline">\(10,000\)</span> replications are simulated by the Bootstrap method. The <code>bs_mean</code> is a vector of <span class="math inline">\(10,000\)</span> elements to record all the estimated mean parameter in these replications. These <span class="math inline">\(10,000\)</span> estimated parameters could be taken as a set of samples.</p>
<p><strong>Step 4</strong> is to summarize the Bootstrapped samples, i.e., to compute the <span class="math inline">\(95\%\)</span> CI of the estimated mean, as shown below.</p>
<p></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Summarize the results and derive the</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="co"># bootstrap confidence interval (CI) of the parameter</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="co"># sort the mean estimates to obtain quantiles needed</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># to construct the CIs</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>bs_mean.sorted <span class="ot">&lt;-</span> <span class="fu">sort</span>(bs_mean) </span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.025th and 0.975th quantile gives equal-tail bootstrap CI</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>CI.bs <span class="ot">&lt;-</span> <span class="fu">c</span>(bs_mean.sorted[<span class="fu">round</span>(<span class="fl">0.025</span><span class="sc">*</span>R)],</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>                        bs_mean.sorted[<span class="fu">round</span>(<span class="fl">0.975</span><span class="sc">*</span>R<span class="sc">+</span><span class="dv">1</span>)])</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>CI.bs</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="do">##   lower.bound   upper.bound     </span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a><span class="do">##    0.4656406     0.4778276 </span></span></code></pre></div>
<p></p>
<p>It is seen that the <span class="math inline">\(95\%\)</span> CI by Bootstrap is close to the <span class="math inline">\(95\%\)</span> CI in the theoretical result. This shows the validity and efficacy of the Bootstrap method to evaluate the uncertainty of a statistical operation<label for="tufte-sn-92" class="margin-toggle sidenote-number">92</label><input type="checkbox" id="tufte-sn-92" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">92</span> Bootstrap is as good as the theoretical method, but it doesn’t require to know the variable’s distribution. The cost it pays for this robustness is its computational overhead. In this example, the computation is light. For some other cases, the computation could be burdensome, i.e., when the dataset becomes big, or the statistical model itself has been computationally demanding.</span>.</p>
<p><em>Beyond the 4-step Pipeline.</em> While the estimation of the mean of <code>HippoNV</code> is a relatively simple operation, in what follows, we consider a more complex statistical operation, the comparison of the mean parameters of <code>HippoNV</code> across the two classes, <em>normal</em> and <em>diseased</em>.</p>
<p>To do so, the following R code creates a temporary dataset for this purpose.</p>
<p></p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(AD<span class="sc">$</span>HippoNV,AD<span class="sc">$</span>DX_bl)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tempData) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;HippoNV&quot;</span>,<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>tempData<span class="sc">$</span>DX_bl[<span class="fu">which</span>(tempData<span class="sc">$</span>DX_bl<span class="sc">==</span><span class="dv">0</span>)] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Normal&quot;</span>)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>tempData<span class="sc">$</span>DX_bl[<span class="fu">which</span>(tempData<span class="sc">$</span>DX_bl<span class="sc">==</span><span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Diseased&quot;</span>)</span></code></pre></div>
<p></p>
<p>We then use <code>ggplot()</code> to visualize the two distributions by comparing their histograms.</p>
<p></p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tempData,<span class="fu">aes</span>(<span class="at">x =</span> HippoNV, <span class="at">colour=</span>DX_bl))</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..count.., <span class="at">fill=</span>DX_bl),</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">alpha=</span><span class="fl">0.5</span>,<span class="at">position=</span><span class="st">&quot;identity&quot;</span>) </span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p>The result is shown in Figure <a href="#fig:f4-6">58</a>. It could be seen that the two distributions differ from each other. To have a formal evaluation of this impression, the following R code shows how the nonparametric Bootstrap method can be implemented here.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-6"></span>
<img src="graphics/4_6.png" alt=" Histograms of `HippoNV` in the *normal* and *diseased* groups" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 58:  Histograms of <code>HippoNV</code> in the <em>normal</em> and <em>diseased</em> groups<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># draw R bootstrap replicates</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co"># init location for bootstrap samples</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>bs0_mean <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, R)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>bs1_mean <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, R)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="co"># draw R bootstrap resamples and obtain the estimates</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>R) {</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>resam0 <span class="ot">&lt;-</span> <span class="fu">sample</span>(tempData<span class="sc">$</span>HippoNV[<span class="fu">which</span>(tempData<span class="sc">$</span>DX_bl<span class="sc">==</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;Normal&quot;</span>)],<span class="fu">length</span>(tempData<span class="sc">$</span>HippoNV[<span class="fu">which</span>(tempData<span class="sc">$</span>DX_bl<span class="sc">==</span></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;Normal&quot;</span>)]),<span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>fit0 <span class="ot">&lt;-</span> <span class="fu">fitdistr</span>(resam0 , <span class="at">densfun=</span><span class="st">&quot;normal&quot;</span>)  </span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>bs0_mean[i] <span class="ot">&lt;-</span> fit0<span class="sc">$</span>estimate[<span class="dv">1</span>]</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>resam1 <span class="ot">&lt;-</span> <span class="fu">sample</span>(tempData<span class="sc">$</span>HippoNV[<span class="fu">which</span>(tempData<span class="sc">$</span>DX_bl<span class="sc">==</span></span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>             <span class="st">&quot;Diseased&quot;</span>)],</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>         <span class="fu">length</span>(tempData<span class="sc">$</span>HippoNV[<span class="fu">which</span>(tempData<span class="sc">$</span>DX_bl<span class="sc">==</span></span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>             <span class="st">&quot;Diseased&quot;</span>)]),<span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">fitdistr</span>(resam1 , <span class="at">densfun=</span><span class="st">&quot;normal&quot;</span>) </span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>bs1_mean[i] <span class="ot">&lt;-</span> fit1<span class="sc">$</span>estimate[<span class="dv">1</span>]</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a>bs_meanDiff <span class="ot">&lt;-</span> bs0_mean <span class="sc">-</span> bs1_mean</span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a><span class="co"># sort the mean estimates to obtain bootstrap CI</span></span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>bs_meanDiff.sorted <span class="ot">&lt;-</span> <span class="fu">sort</span>(bs_meanDiff)</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.025th and 0.975th quantile gives equal-tail bootstrap CI</span></span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>CI.bs <span class="ot">&lt;-</span> <span class="fu">c</span>(bs_meanDiff.sorted[<span class="fu">round</span>(<span class="fl">0.025</span><span class="sc">*</span>R)],</span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a>           bs_meanDiff.sorted[<span class="fu">round</span>(<span class="fl">0.975</span><span class="sc">*</span>R<span class="sc">+</span><span class="dv">1</span>)])</span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>CI.bs</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-7"></span>
<img src="graphics/4_7.png" alt="Histogram of the estimated mean difference of `HippoNV` in the two groups by Bootstrap with $10,000$ replications " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 59: Histogram of the estimated mean difference of <code>HippoNV</code> in the two groups by Bootstrap with <span class="math inline">\(10,000\)</span> replications <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The <span class="math inline">\(95\%\)</span> CI of the difference of the two mean parameters is</p>
<p></p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>CI.bs</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.08066058 0.10230428</span></span></code></pre></div>
<p></p>
<p>The following R code draws a histogram of the <code>bs_meanDiff</code> to give us visual information about the Bootstrapped estimation of the mean difference, which is shown in Figure <a href="#fig:f4-7">59</a>. The difference is statistically significant.</p>
<p></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot the bootstrap distribution with CI</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="co"># First put data in data.frame for ggplot()</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>dat.bs_meanDiff <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(bs_meanDiff)</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(dat.bs_meanDiff, <span class="fu">aes</span>(<span class="at">x =</span> bs_meanDiff))</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..))</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">alpha=</span><span class="fl">0.1</span>, <span class="at">fill=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_rug</span>()</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="co"># vertical line at CI</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>CI.bs[<span class="dv">1</span>], <span class="at">colour=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">linetype=</span><span class="st">&quot;longdash&quot;</span>)</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>CI.bs[<span class="dv">2</span>], <span class="at">colour=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>                    <span class="at">linetype=</span><span class="st">&quot;longdash&quot;</span>)</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>title <span class="ot">=</span> <span class="st">&quot;Bootstrap distribution of the estimated mean</span></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a><span class="st">         difference of HippoNV between normal and diseased&quot;</span></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span>title)</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p>We can also apply Bootstrap on linear regression. In <strong>Chapter 2</strong> we have fitted a regression model of <code>MMSCORE</code> and presented the analytically derived standard derivation of the estimated regression parameters<label for="tufte-sn-93" class="margin-toggle sidenote-number">93</label><input type="checkbox" id="tufte-sn-93" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">93</span> I.e., as shown in Eq. <a href="#eq:2-betaDist">(19)</a>.</span>. Here, we show that we could use Bootstrap to compute the <span class="math inline">\(95\%\)</span> CI of the regression parameters as well.</p>
<p>First, we fit a linear regression model.</p>
<p></p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a regression model first, for comparison</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(AD<span class="sc">$</span>MMSCORE,AD<span class="sc">$</span>AGE, AD<span class="sc">$</span>PTGENDER, AD<span class="sc">$</span>PTEDUCAT)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tempData) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>,<span class="st">&quot;AGE&quot;</span>,<span class="st">&quot;PTGENDER&quot;</span>,<span class="st">&quot;PTEDUCAT&quot;</span>)</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span>  AGE <span class="sc">+</span> PTGENDER <span class="sc">+</span> PTEDUCAT, <span class="at">data =</span> tempData)</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>sum.lm.AD <span class="ot">&lt;-</span> <span class="fu">summary</span>(lm.AD)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Age is not significant according to the p-value</span></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>std.lm <span class="ot">&lt;-</span> sum.lm.AD<span class="sc">$</span>coefficients[ , <span class="dv">2</span>]</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>lm.AD<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> std.lm[<span class="dv">2</span>]</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>lm.AD<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> std.lm[<span class="dv">2</span>]</span></code></pre></div>
<p></p>
<p>The fitted regression model is</p>
<p></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = MMSCORE   AGE + PTGENDER + PTEDUCAT,</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="do">##                    data = tempData)</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a><span class="do">## -8.4290 -0.9766  0.5796  1.4252  3.4539 </span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 27.70377    1.11131  24.929  &lt; 2e-16 ***</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a><span class="do">## AGE         -0.02453    0.01282  -1.913   0.0563 .  </span></span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a><span class="do">## PTGENDER    -0.43356    0.18740  -2.314   0.0211 *  </span></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a><span class="do">## PTEDUCAT     0.17120    0.03432   4.988 8.35e-07 ***</span></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 </span></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a><span class="do">## &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 2.062 on 513 degrees of freedom</span></span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.0612, Adjusted R-squared:  0.05571 </span></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 11.15 on 3 and 513 DF,  p-value: 4.245e-07</span></span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a><span class="do">##     Lower bound   Upper bound</span></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -0.04966834   0.000600785</span></span></code></pre></div>
<p></p>
<p>Then, we follow Option 1 to conduct Bootstrap for the linear regression model.</p>
<p></p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># draw R bootstrap replicates</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="co"># init location for bootstrap samples</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>bs_lm.AD_demo <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> R, <span class="at">ncol =</span> </span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">length</span>(lm.AD_demo<span class="sc">$</span>coefficients))</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="co"># draw R bootstrap resamples and obtain the estimates</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>R) {</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>  resam_ID <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(tempData)[<span class="dv">1</span>]), <span class="fu">dim</span>(tempData)[<span class="dv">1</span>],</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>                     <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>resam_Data <span class="ot">&lt;-</span> tempData[resam_ID,]</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>  bs.lm.AD_demo <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span>  AGE <span class="sc">+</span> PTGENDER <span class="sc">+</span> PTEDUCAT,</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data =</span> resam_Data)</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>bs_lm.AD_demo[i,] <span class="ot">&lt;-</span> bs.lm.AD_demo<span class="sc">$</span>coefficients</span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p>The <code>bs_lm.AD_demo</code> records the estimated regression parameters in the <span class="math inline">\(10,000\)</span> replications. The following R code shows the <span class="math inline">\(95\%\)</span> CI of <code>AGE</code> by Bootstrap.</p>
<p></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>bs.AGE <span class="ot">&lt;-</span> bs_lm.AD_demo[,<span class="dv">2</span>]</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="co"># sort the mean estimates of AGE to obtain bootstrap CI</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>bs.AGE.sorted <span class="ot">&lt;-</span> <span class="fu">sort</span>(bs.AGE)</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.025th and 0.975th quantile gives equal-tail</span></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="co"># bootstrap CI</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>CI.bs <span class="ot">&lt;-</span> <span class="fu">c</span>(bs.AGE.sorted[<span class="fu">round</span>(<span class="fl">0.025</span><span class="sc">*</span>R)],</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>       bs.AGE.sorted[<span class="fu">round</span>(<span class="fl">0.975</span><span class="sc">*</span>R<span class="sc">+</span><span class="dv">1</span>)])</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>CI.bs</span></code></pre></div>
<p></p>
<p>It is clear that the 95<span class="math inline">\(\%\)</span> CI of <code>AGE</code> includes <span class="math inline">\(0\)</span> in the range. This is consistent with the result by t-test that shows the variable <code>AGE</code> is insignificant (i.e., p-value<span class="math inline">\(=0.0563\)</span>). We can also see that the <span class="math inline">\(95\%\)</span> CI by Bootstrap is close to the <span class="math inline">\(95\%\)</span> CI by theoretical result.</p>
<p></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>CI.bs</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="do">##     Lower bound   Upper bound</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -0.053940482  0.005090523</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-8"></span>
<img src="graphics/4_8.png" alt="Histogram of the estimated regression parameter of `AGE` by Bootstrap with $10,000$ replications" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 60: Histogram of the estimated regression parameter of <code>AGE</code> by Bootstrap with <span class="math inline">\(10,000\)</span> replications<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The following R codes draw a histogram of the Bootstrapped estimation of the regression parameter of <code>AGE</code> to give us a visual examination about the Bootstrapped estimation, which is shown in Figure <a href="#fig:f4-8">60</a>.</p>
<p></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot the bootstrap distribution with CI</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="co"># First put data in data.frame for ggplot()</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>dat.bs.AGE <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(bs.AGE.sorted)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(dat.bs.AGE, <span class="fu">aes</span>(<span class="at">x =</span> bs.AGE))</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..))</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">alpha=</span><span class="fl">0.1</span>, <span class="at">fill=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_rug</span>()</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a><span class="co"># vertical line at CI</span></span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>CI.bs[<span class="dv">1</span>], <span class="at">colour=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">linetype=</span><span class="st">&quot;longdash&quot;</span>)</span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>CI.bs[<span class="dv">2</span>], <span class="at">colour=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>                    <span class="at">linetype=</span><span class="st">&quot;longdash&quot;</span>)</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>title <span class="ot">&lt;-</span> <span class="st">&quot;Bootstrap distribution of the estimated </span></span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a><span class="st">                          regression parameter of AGE&quot;</span></span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> title)</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p>We can also see the <span class="math inline">\(95\%\)</span> CI of <code>PTEDUCAT</code> as shown below, which is between <span class="math inline">\(0.1021189\)</span> and <span class="math inline">\(0.2429209\)</span>. This is also close to the the <span class="math inline">\(95\%\)</span> CI by theoretical result. Also, t-test also shows the variable <code>PTEDUCAT</code> is significant (i.e., p-value is <span class="math inline">\(8.35e-07\)</span>).</p>
<p></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>bs.PTEDUCAT <span class="ot">&lt;-</span> bs_lm.AD_demo[,<span class="dv">4</span>]</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="co"># sort the mean estimates of PTEDUCAT to obtain</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="co"># bootstrap CI</span></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>bs.PTEDUCAT.sorted <span class="ot">&lt;-</span> <span class="fu">sort</span>(bs.PTEDUCAT)</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.025th and 0.975th quantile gives equal-tail</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="co"># bootstrap CI</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>CI.bs <span class="ot">&lt;-</span> <span class="fu">c</span>(bs.PTEDUCAT.sorted[<span class="fu">round</span>(<span class="fl">0.025</span><span class="sc">*</span>R)],</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>           bs.PTEDUCAT.sorted[<span class="fu">round</span>(<span class="fl">0.975</span><span class="sc">*</span>R<span class="sc">+</span><span class="dv">1</span>)])</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>CI.bs</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>CI.bs</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1021189 0.2429209</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-9"></span>
<img src="graphics/4_9.png" alt="Histogram of the estimated regression parameter of `PTEDUCAT` by Bootstrap with $10,000$ replications " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 61: Histogram of the estimated regression parameter of <code>PTEDUCAT</code> by Bootstrap with <span class="math inline">\(10,000\)</span> replications <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The following R code draws a histogram (i.e., Figure <a href="#fig:f4-9">61</a>) of the Bootstrapped estimation of the regression parameter of <code>PTEDUCAT</code>.</p>
<p></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot the bootstrap distribution with CI</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="co"># First put data in data.frame for ggplot()</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>dat.bs.PTEDUCAT <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(bs.PTEDUCAT.sorted)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(dat.bs.PTEDUCAT, <span class="fu">aes</span>(<span class="at">x =</span> bs.PTEDUCAT))</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..))</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">alpha=</span><span class="fl">0.1</span>, <span class="at">fill=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_rug</span>()</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span class="co"># vertical line at CI</span></span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>CI.bs[<span class="dv">1</span>], <span class="at">colour=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">linetype=</span><span class="st">&quot;longdash&quot;</span>)</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>CI.bs[<span class="dv">2</span>], <span class="at">colour=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>                    <span class="at">linetype=</span><span class="st">&quot;longdash&quot;</span>)</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>title <span class="ot">&lt;-</span> <span class="st">&quot;Bootstrap distribution of the estimated regression</span></span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a><span class="st">                                      parameter of PTEDUCAT&quot;</span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> title )</span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
</div>
</div>
<div id="random-forests" class="section level2 unnumbered">
<h2>Random forests</h2>
<div id="rationale-and-formulation-6" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>Randomness has a productive dimension, depending on how you use it. For example, a <strong>random forest</strong> (<strong>RF</strong>) model consists of multiple tree models that are generated by a creative use of two types of randomness. One, each tree is built on a <em>randomly selected</em> set of samples by applying Bootstrap on the original dataset. Two, in building each tree, a <em>randomly selected</em> subset of features is used to choose the best split. Figure <a href="#fig:f4-10">62</a> shows this scheme of random forest.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-10"></span>
<img src="graphics/4_10.png" alt="How random forest uses Bootstrap to grow decision trees" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 62: How random forest uses Bootstrap to grow decision trees<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Random forest has gained superior performances in many applications, and it (together with its variants) has been a winning approach in some data competitions over the past years. While it is not necessary that an aggregation of many models would lead to better performance than its constituting parts, random forest works because of a number of reasons. Here we use an example to show when the random forest, as a sum, is better than its parts (i.e., the decision trees).</p>
<p>The following R code generates a dataset with two predictors and an outcome variable that has two classes. As shown in Figure <a href="#fig:f4-11">63</a> (left), the two classes are separable by a linear boundary.</p>
<p></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This is a script for simulation study</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>(<span class="at">all =</span> <span class="cn">TRUE</span>))</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(rpart)</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(dplyr)</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(randomForest)</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>ndata <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(ndata, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(ndata, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X1, X2)</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">X12 =</span> <span class="fl">0.5</span> <span class="sc">*</span> (X1 <span class="sc">-</span> X2),</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">Y =</span> <span class="fu">ifelse</span>(X12 <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>X12) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">Y =</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">as.factor</span>(<span class="fu">as.character</span>(Y)))</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> X1, <span class="at">y =</span> X2, <span class="at">color =</span> Y)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Data points&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure"><span id="fig:f4-11"></span>
<p class="caption marginnote shownote">
Figure 63: (Left) A linearly separable dataset with two predictors; (middle) the decision boundary of a decision tree model; (right) the decision boundary of a random forest model
</p>
<img src="graphics/4_11.png" alt="(Left) A linearly separable dataset with two predictors; (middle) the decision boundary of a decision tree model; (right) the decision boundary of a random forest model" width="30%"  /><img src="graphics/4_12.png" alt="(Left) A linearly separable dataset with two predictors; (middle) the decision boundary of a decision tree model; (right) the decision boundary of a random forest model" width="30%"  /><img src="graphics/4_13.png" alt="(Left) A linearly separable dataset with two predictors; (middle) the decision boundary of a decision tree model; (right) the decision boundary of a random forest model" width="30%"  />
</div>
<p></p>
<p>Both random forest and decision tree models are applied to the dataset. The classification boundaries of both decision tree and random forest models are shown in Figures <a href="#fig:f4-11">63</a> (middle) and (right), respectively.</p>
<p></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Y <span class="sc">~</span> ., <span class="at">data =</span> data)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>tree_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Y <span class="sc">~</span> ., <span class="at">data =</span> data)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>pred_rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf_model, data, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">1</span>]</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>pred_tree <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_model, data, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">1</span>]</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>data_pred <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">pred_rf_class =</span> <span class="fu">ifelse</span>(pred_rf <span class="sc">&lt;</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">pred_rf_class =</span></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.factor</span>(<span class="fu">as.character</span>(pred_rf_class))) <span class="sc">%&gt;%</span></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pred_tree_class =</span> <span class="fu">ifelse</span>(pred_tree <span class="sc">&lt;</span> </span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">pred_tree_class =</span></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>                     <span class="fu">as.factor</span>(<span class="fu">as.character</span>(pred_tree_class)))</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_pred, <span class="fu">aes</span>(<span class="at">x =</span> X1, <span class="at">y =</span> X2, </span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>                      <span class="at">color =</span> pred_tree_class)) <span class="sc">+</span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Classification boundary from</span></span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a><span class="st">                      a single decision tree&quot;</span>) </span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_pred, <span class="fu">aes</span>(<span class="at">x =</span> X1, <span class="at">y =</span> X2, </span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>                      <span class="at">color =</span> pred_rf_class)) <span class="sc">+</span></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Classification bounday from</span></span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a><span class="st">                      random forests&quot;</span>)</span></code></pre></div>
<p></p>
<p>We can see from Figure <a href="#fig:f4-11">63</a> (middle) that the classification boundary generated by the decision tree model has a difficult to approximate linear boundary. There is an inherent limitation of a tree model to fit smooth boundaries due to its box-shaped nature resulting from its use of rules to segment the data space for making predictions. In contrast, the classification boundary of the random forest model is smoother than the one of the decision tree, and it can provide better approximation of complex and nonlinear classification boundaries.</p>
<p>Having said that, this is not the only reason <em>why</em> the random forest model is remarkable. After all, many models can model linear boundary, and it is actually not the random forests’ strength. The remarkable thing about a random forest is its capacity, as a tree-based model, to actually model linear boundary. It shows its flexibility, adaptability, and learning capacity to characterize complex patterns in a dataset. Let’s see more details to understand how it works.</p>
</div>
<div id="theory-and-method-4" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>Like a decision tree, the learning process of random forests follows the algorithmic modeling framework. It uses an organized set of heuristics, rather than a mathematical characterization. We present the process of building random forest models using a simple example with a small dataset shown in Table <a href="#tab:t4-1">10</a> that has two predictors, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, and an outcome variable with two classes.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t4-1">Table 10: </span>Example of a dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>As shown in Figure <a href="#fig:f4-10">62</a>, each tree is built on a resampled dataset that consists of data instances randomly selected from the original data set<label for="tufte-sn-94" class="margin-toggle sidenote-number">94</label><input type="checkbox" id="tufte-sn-94" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">94</span> I.e., often with the same sample size as the original dataset and is called <strong>sampling with replacement</strong>.</span>. As shown in Figure <a href="#fig:f4-14">64</a>, the first resampled dataset includes data instances (represented by their IDs) <span class="math inline">\(\{1,1,3,4\}\)</span> and is used for building the first tree. The second resampled dataset includes data instances <span class="math inline">\(\{2,3,4,4\}\)</span> and is used for building the second tree. This process repeats until a specific number of trees is built.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-14"></span>
<img src="graphics/4_14.png" alt="Examples of bootstrapped datasets from the dataset shown in Table \@ref(tab:t4-1)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 64: Examples of bootstrapped datasets from the dataset shown in Table <a href="#tab:t4-1">10</a><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The first tree begins with the root node that contains data instances <span class="math inline">\(\{1,1,3,4\}\)</span>. As introduced in <strong>Chapter 2</strong>, we recursively split a node into two child nodes to reduce impurity (i.e., measured by entropy). This greedy recursive splitting process is also used to build each decision tree in a random forest model. A slight variation is that, in the R package <code>randomForest</code>, the <strong>Gini index</strong> is used to measure impurity instead of entropy.</p>
<p>The Gini index for a data set is defined as<label for="tufte-sn-95" class="margin-toggle sidenote-number">95</label><input type="checkbox" id="tufte-sn-95" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">95</span> <span class="math inline">\(C\)</span> is the number of classes of the outcome variable, and <span class="math inline">\(p_{c}\)</span> is the proportion of data instances that come from the class <span class="math inline">\(c\)</span>.</span></p>
<p><span class="math display">\[ 
\operatorname{Gini} =\sum_{c=1}^{C} p_{c}\left(1-p_{c}\right).  
\]</span></p>
<p>The Gini index plays the same role as the entropy (more details could be found in the Remarks section). Similar as the information gain (IG), the <strong>Gini gain</strong> can be defined as</p>
<p><span class="math display">\[\nabla \operatorname{Gini} = \operatorname{Gini}_s - \sum\nolimits_{i=1,\cdots,n} w_i \operatorname{Gini}_i.\]</span></p>
<p>Here, <span class="math inline">\(\operatorname{Gini}_s\)</span> is the Gini index at the node to be split; <span class="math inline">\(w_{i}\)</span> and <span class="math inline">\(\operatorname{Gini}_{i}\)</span>, are the proportion of samples and the Gini index at the <span class="math inline">\(i^{th}\)</span> children node, respectively.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-16-left"></span>
<img src="graphics/4_16_v3.png" alt="root node split using $x_{1}=0$" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 65: root node split using <span class="math inline">\(x_{1}=0\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Let’s go back to the first tree that begins with the root node containing data instances <span class="math inline">\(\{1,1,3,4\}\)</span>. There are three instances that are associated with the class <span class="math inline">\(C0\)</span> (thus, <span class="math inline">\(p_0 = \frac{3}{4}\)</span>), one instance with <span class="math inline">\(C1\)</span> (thus, <span class="math inline">\(p_1 = \frac{1}{4}\)</span>). The Gini index of the root node is calculated as</p>
<p><span class="math display">\[ 
\frac{3}{4} \times \frac{1}{4}+\frac{1}{4} \times \frac{3}{4}=0.375. 
\]</span></p>
<p>To split the root node, candidates of splitting rules are:</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> [Rule 1:] <span class="math inline">\(x_{1}=0 \text { versus } x_{1} \neq 0\)</span>.</p></li>
<li><p> [Rule 2:] <span class="math inline">\(x_{2}=0 \text { versus } x_{2} \neq 0\)</span>.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>The decision tree model introduced in <strong>Chapter 2</strong> would evaluate each of the possible splitting rules, and select the one that yields the maximum Gini gain to split the node. However, for random forests, it randomly selects the variables for splitting a node<label for="tufte-sn-96" class="margin-toggle sidenote-number">96</label><input type="checkbox" id="tufte-sn-96" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">96</span> In general, for a dataset with <span class="math inline">\(p\)</span> variables, <span class="math inline">\(\sqrt{p}\)</span> variables are randomly selected for splitting.</span>. In our example, as there are two variables, we assume that <span class="math inline">\(x_{1}\)</span> is randomly selected for splitting the root node. Thus, <span class="math inline">\(x_{1}=0\)</span> is used for splitting the root node which generates the decision tree model as shown in Figure <a href="#fig:f4-16-left">65</a>.</p>
<!-- \begin{figure*}-->
<!--    \centering-->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi-->
<!--    \subfloat{-->
<!--        \includegraphics{graphics/4_16_v3.png}}-->
<!--    \subfloat{-->
<!--        \includegraphics{graphics/4_17_v5.png}}-->
<!--    \subfloat{-->
<!--        \includegraphics{graphics/4_18_v5.png}}-->
<!--    \caption{(Left) root node split using $x_{1}=0$; (middle) second split using $x_{2}=0$; (right) tree model trained}-->
<!--    \label{fig:4-16}-->
<!-- \end{figure*} -->
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-16-middle"></span>
<img src="graphics/4_17_v5.png" alt="second split using $x_{2}=0$" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 66: second split using <span class="math inline">\(x_{2}=0\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The Gini gain for the split shown in Figure <a href="#fig:f4-16-left">65</a> can be calculated as</p>
<p><span class="math display">\[ 
0.375-0.5 \times 0-0.5 \times 0.5=0.125. 
\]</span></p>
<p>The right node in the tree shown in Figure <a href="#fig:f4-16-left">65</a> has reached a perfect state of homogeneity<label for="tufte-sn-97" class="margin-toggle sidenote-number">97</label><input type="checkbox" id="tufte-sn-97" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">97</span> Which is, in practice, a rare phenomenon.</span>. The left node, however, contains two instances <span class="math inline">\(\{3,4\}\)</span> that are associated with two classes. We further split the left node. Assume that this time <span class="math inline">\(x_{2}\)</span> is randomly selected. The left node can be further split as shown in Figure <a href="#fig:f4-16-middle">66</a>.</p>
<p>All nodes cannot be split further. The final tree model is shown in Figure <a href="#fig:f4-16-right">67</a>, while each leaf node is labeled with the majority class of the instances in the node, such that they become decision nodes.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-16-right"></span>
<img src="graphics/4_18_v5.png" alt="tree model trained" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 67: tree model trained<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Applying the decision tree in Figure <a href="#fig:f4-16-right">67</a> to the <span class="math inline">\(4\)</span> data points as shown in Table <a href="#tab:t4-1">10</a>, we can get the predictions as shown in Table <a href="#tab:t4-1pred">11</a>. The error rate is <span class="math inline">\(25\%\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t4-1pred">Table 11: </span>Example of a dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left">Class</th>
<th align="left">Prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Similarly, the second, third, …, and the <span class="math inline">\(m^{th}\)</span> trees can be built. Usually, in random forest models, tree pruning is not needed. Rather, we use a parameter to control the depth of the tree models to be created (i.e., use the parameter <code>nodesize</code> in <code>randomForest</code>).</p>
<p>When a random forest model is built, to make a prediction for a data point, each tree makes a prediction, then all the predictions are combined; e.g., for continuous outcome variable, the average of the predictions is used as the final prediction; for classification outcome variable, the class that wins majority among all trees is the final prediction.</p>
</div>
<div id="r-lab-5" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 5-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get data into your R work environment and make appropriate preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(AD)</span></span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and Y vector </span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (outcome variable)</span></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>DX_bl</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, Y) </span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Y)  </span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> uses the R package <code>randomForest</code> to build your random forest model.</p>
<p></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; Use randomForest() function to build a </span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="co"># RF model </span></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="co"># with all predictors</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>rf.AD <span class="ot">&lt;-</span> <span class="fu">randomForest</span>( DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data.train, </span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">ntree =</span> <span class="dv">100</span>, <span class="at">nodesize =</span> <span class="dv">20</span>, <span class="at">mtry =</span> <span class="dv">5</span>) </span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Three main arguments to control the complexity </span></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a><span class="co"># of a random forest model</span></span></code></pre></div>
<p></p>
<p>Details for the three arguments in <code>randomForest</code>: The <code>ntree</code> is the number of trees<label for="tufte-sn-98" class="margin-toggle sidenote-number">98</label><input type="checkbox" id="tufte-sn-98" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">98</span> The more trees, the more complex the random forest model is.</span>. The <code>nodesize</code> is the minimum sample size of leaf nodes<label for="tufte-sn-99" class="margin-toggle sidenote-number">99</label><input type="checkbox" id="tufte-sn-99" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">99</span> The larger the sample size in leaf nodes, the less depth of the trees; therefore, the less complex the random forest model is.</span>. The <code>mtry</code> is a parameter to control the degree of randomness when your RF model selects variables to split nodes<label for="tufte-sn-100" class="margin-toggle sidenote-number">100</label><input type="checkbox" id="tufte-sn-100" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">100</span> For classification, the default value of <code>mtry</code> is <span class="math inline">\(\sqrt{p}\)</span>, where <span class="math inline">\(p\)</span> is the number of variables; for regression, the default value of <code>mtry</code> is <span class="math inline">\(p/3\)</span>.</span>.</p>
<p><strong>Step 4</strong> is prediction. We use the <code>predict()</code> function</p>
<p></p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Predict using your RF model</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.AD, data.test,<span class="at">type=</span><span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p></p>
<p><strong>Step 5</strong> evaluates the prediction performance of your model on the testing data.</p>
<p></p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; Evaluate the prediction performance of your RF model</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Three main metrics for classification: Accuracy, </span></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sensitivity (1- False Positive), Specificity (1 - False Negative)</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret) </span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(y_hat, data.test<span class="sc">$</span>DX_bl)</span></code></pre></div>
<p></p>
<p>The result is shown below. It is an information-rich object<label for="tufte-sn-101" class="margin-toggle sidenote-number">101</label><input type="checkbox" id="tufte-sn-101" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">101</span> To learn more about an R object, function, and package, please check out the online documentation that is usually available for an R package that has been released to the public. For example, for the <code>confusionMatrix</code> in the R package <code>caret</code>, check out this link: <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-84/topics/confusionMatrix">https://www.rdocumentation.org/packages/caret/versions/6.0-84/topics/confusionMatrix</a>. Some R packages also come with a journal article published in the <em>Journal of Statistical Software</em>. E.g., for <code>caret</code>, see Kuhn, M., <em>Building predictive models in R using the caret package</em>, Journal of Statistical Software, Volume 28, Issue 5, 2018, <a href="http://www.jstatsoft.org/article/view/v028i05/v28i05.pdf">http://www.jstatsoft.org/article/view/v028i05/v28i05.pdf</a>.</span>.</p>
<p></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction  c0  c1</span></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="do">##         c0 136  31</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a><span class="do">##         c1   4  88</span></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                Accuracy : 0.8649         </span></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                  95% CI : (0.8171, 0.904)</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5405         </span></span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      </span></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Kappa : 0.7232         </span></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 1.109e-05      </span></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a><span class="do">##             Sensitivity : 0.9714         </span></span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a><span class="do">##             Specificity : 0.7395         </span></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8144         </span></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.9565         </span></span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a><span class="do">##              Prevalence : 0.5405         </span></span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a><span class="do">##          Detection Rate : 0.5251         </span></span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.6448         </span></span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8555         </span></span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : c0 </span></span></code></pre></div>
<p></p>
<p>We can also draw the ROC curve</p>
<p></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ROC curve is another commonly reported metric </span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="co"># for classification models</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC) </span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pROC has the roc() function that is very useful here</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.AD, data.test,<span class="at">type=</span><span class="st">&quot;vote&quot;</span>) </span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a><span class="co"># In order to draw ROC, we need the intermediate prediction </span></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a><span class="co"># (before RF model binarize it into binary classification). </span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Thus, by specifying the argument type=&quot;vote&quot;, we can </span></span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a><span class="co"># generate this intermediate prediction. y_hat now has </span></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a><span class="co"># two columns, one corresponds to the ratio of votes the </span></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a><span class="co"># trees assign to one class, and the other column is the </span></span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a><span class="co"># ratio of votes the trees assign to another class.</span></span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>main <span class="ot">=</span> <span class="st">&quot;ROC Curve&quot;</span></span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">roc</span>(data.test<span class="sc">$</span>DX_bl, y_hat[,<span class="dv">1</span>]),</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">main=</span>main)</span></code></pre></div>
<p></p>
<p>And we can have the ROC curve as shown in Figure <a href="#fig:f4-RF-ROC">68</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-RF-ROC"></span>
<img src="graphics/4_RF_ROC.png" alt="ROC curve of the RF model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 68: ROC curve of the RF model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Beyond the 5-Step R Pipeline.</em> Random forests are complex models that have many parameters to be tuned. In <strong>Chapter 2</strong> and <strong>Chapter 3</strong> we have used the <code>step()</code> function for automatic model selection for regression models. Part of the reason this is possible for regression models is that model selection for regression models largely concerns variable selection only. For decision tree and random forest models, the model selection concerns not only variable selection, but also many other aspects, such as the depth of the tree, the number of trees, and the degree of randomness that would be used in model training. This makes the model selection for tree models a craft. An individual’s experience and insights make a difference, and some may find a better model, even the same package is used on the same dataset to build models<label for="tufte-sn-102" class="margin-toggle sidenote-number">102</label><input type="checkbox" id="tufte-sn-102" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">102</span> There is often an impression that a good model is built by a good pipeline, like this 5-step pipeline. This impression is a reductive view, since it only looks at the final stage of data analytics. Like manufacturing, when the process is mature and we are able to see the rationale behind every detail of the manufacturing process, we may lose sight of those alternatives that had been considered, experimented, then discarded (or withheld) for various reasons.</span>.</p>
<p>To see how these parameters impact the models, we conduct some experiments. The number of trees is one of the most important parameters of random forests that we’d like to be tuned well. We can build different random forest models by tuning the parameter <code>ntree</code> in <code>randomForest</code>. For each selection of the number of trees, we first randomly split the dataset into training and testing datasets, then train the model on the training dataset, and evaluate its performance on the testing dataset. This process of data splitting, model training, and testing is repeated <span class="math inline">\(100\)</span> times. We can use boxplots to show the overall performance of the models. Results are shown in Figure <a href="#fig:f4-21">69</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-21"></span>
<img src="graphics/4_21.png" alt="Error v.s. number of trees in a random forest model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 69: Error v.s. number of trees in a random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(randomForest)</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_gray</span>(<span class="at">base_size =</span> <span class="dv">15</span>))</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>target_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">==</span> <span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>data[, target_indx] <span class="ot">&lt;-</span> </span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.factor</span>(<span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, data[, target_indx]))</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>,</span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>                                       <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (itree <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>, <span class="dv">400</span>, <span class="dv">500</span>,</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>    <span class="dv">600</span>, <span class="dv">700</span>)) {</span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data), <span class="fu">floor</span>(<span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">ntree =</span> itree, <span class="at">data =</span></span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>                                        data[train.ix, ])</span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a>pred.test <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[<span class="sc">-</span>train.ix, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>this.err <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(pred.test <span class="sc">!=</span></span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>                data[<span class="sc">-</span>train.ix, ]<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.test)</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">c</span>(itree, this.err))</span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(results) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;num_trees&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(results) <span class="sc">%&gt;%</span></span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">num_trees =</span> <span class="fu">as.character</span>(num_trees))</span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(results<span class="sc">$</span>num_trees) <span class="ot">&lt;-</span> <span class="fu">unique</span>(results<span class="sc">$</span>num_trees)</span>
<span id="cb96-38"><a href="#cb96-38" aria-hidden="true" tabindex="-1"></a>results<span class="sc">$</span>num_trees <span class="ot">&lt;-</span> <span class="fu">factor</span>(results<span class="sc">$</span>num_trees,</span>
<span id="cb96-39"><a href="#cb96-39" aria-hidden="true" tabindex="-1"></a>                            <span class="fu">unique</span>(results<span class="sc">$</span>num_trees))</span>
<span id="cb96-40"><a href="#cb96-40" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> results, <span class="fu">aes</span>(<span class="at">y =</span> error,</span>
<span id="cb96-41"><a href="#cb96-41" aria-hidden="true" tabindex="-1"></a>                <span class="at">x =</span> num_trees)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>It can be seen in Figure <a href="#fig:f4-21">69</a> that when the number of trees is small, particularly, less than <span class="math inline">\(10\)</span>, the improvement on prediction performance of random forest is substantial with trees added. However, the error rates become stable after the number of trees reaches <span class="math inline">\(100\)</span>.</p>
<p>Next, let’s consider the number of features (i.e., use the parameter <code>mtry</code> in the function <code>randomForest</code>). Here, <span class="math inline">\(100\)</span> trees are used. For each number of features, again, following the process we have used in the experiment with the number of trees, we draw the boxplots in Figure <a href="#fig:f4-22">70</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-22"></span>
<img src="graphics/4_22.png" alt="Error v.s. number of features in a random forest model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 70: Error v.s. number of features in a random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>It can be seen that the error rates are not significantly different when the number of features changes.</p>
<p></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(randomForest)</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_gray</span>(<span class="at">base_size =</span> <span class="dv">15</span>))</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a>target_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">==</span> <span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>data[, target_indx] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>        <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, data[, target_indx]))</span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>,</span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>                                       <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a>nFea <span class="ot">&lt;-</span> <span class="fu">ncol</span>(data) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (iFeatures <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nFea) {</span>
<span id="cb97-23"><a href="#cb97-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb97-24"><a href="#cb97-24" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data), <span class="fu">floor</span>(<span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb97-25"><a href="#cb97-25" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">mtry =</span> iFeatures, <span class="at">ntree =</span> <span class="dv">100</span>,</span>
<span id="cb97-26"><a href="#cb97-26" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> data[train.ix,])</span>
<span id="cb97-27"><a href="#cb97-27" aria-hidden="true" tabindex="-1"></a>pred.test <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[<span class="sc">-</span>train.ix, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb97-28"><a href="#cb97-28" aria-hidden="true" tabindex="-1"></a>this.err <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(pred.test <span class="sc">!=</span></span>
<span id="cb97-29"><a href="#cb97-29" aria-hidden="true" tabindex="-1"></a>                 data[<span class="sc">-</span>train.ix, ]<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.test)</span>
<span id="cb97-30"><a href="#cb97-30" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">c</span>(iFeatures, this.err))</span>
<span id="cb97-31"><a href="#cb97-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb97-32"><a href="#cb97-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb97-33"><a href="#cb97-33" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(results) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;num_features&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb97-34"><a href="#cb97-34" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(results) <span class="sc">%&gt;%</span></span>
<span id="cb97-35"><a href="#cb97-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">num_features =</span> <span class="fu">as.character</span>(num_features))</span>
<span id="cb97-36"><a href="#cb97-36" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(results<span class="sc">$</span>num_features) <span class="ot">&lt;-</span> <span class="fu">unique</span>(results<span class="sc">$</span>num_features)</span>
<span id="cb97-37"><a href="#cb97-37" aria-hidden="true" tabindex="-1"></a>results<span class="sc">$</span>num_features <span class="ot">&lt;-</span> <span class="fu">factor</span>(results<span class="sc">$</span>num_features,</span>
<span id="cb97-38"><a href="#cb97-38" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">unique</span>(results<span class="sc">$</span>num_features))</span>
<span id="cb97-39"><a href="#cb97-39" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> results, <span class="fu">aes</span>(<span class="at">y =</span> error,</span>
<span id="cb97-40"><a href="#cb97-40" aria-hidden="true" tabindex="-1"></a>                  <span class="at">x =</span> num_features)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-23"></span>
<img src="graphics/4_23.png" alt="Error v.s. node size in a random forest model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 71: Error v.s. node size in a random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Further, we experiment with the minimum node size (i.e., use the parameter <code>nodesize</code> in the function <code>randomForest</code>), that is, the minimum number of instances at a node. Boxplots of their performances are shown in Figure <a href="#fig:f4-23">71</a>.</p>
<p></p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(randomForest)</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_gray</span>(<span class="at">base_size =</span> <span class="dv">15</span>))</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a>target_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">==</span> <span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>data[, target_indx] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, data[, target_indx]))</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>,</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>                                       <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (inodesize <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>,</span>
<span id="cb98-23"><a href="#cb98-23" aria-hidden="true" tabindex="-1"></a>    <span class="dv">40</span>, <span class="dv">50</span>, <span class="dv">60</span>, <span class="dv">70</span>, <span class="dv">80</span>,<span class="dv">90</span>, <span class="dv">100</span>)) {</span>
<span id="cb98-24"><a href="#cb98-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb98-25"><a href="#cb98-25" aria-hidden="true" tabindex="-1"></a>    train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data), <span class="fu">floor</span>(<span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb98-26"><a href="#cb98-26" aria-hidden="true" tabindex="-1"></a>    rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">ntree =</span> <span class="dv">100</span>, <span class="at">nodesize =</span></span>
<span id="cb98-27"><a href="#cb98-27" aria-hidden="true" tabindex="-1"></a>                     inodesize, <span class="at">data =</span> data[train.ix,])</span>
<span id="cb98-28"><a href="#cb98-28" aria-hidden="true" tabindex="-1"></a>    pred.test <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[<span class="sc">-</span>train.ix, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb98-29"><a href="#cb98-29" aria-hidden="true" tabindex="-1"></a>    this.err <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(pred.test <span class="sc">!=</span></span>
<span id="cb98-30"><a href="#cb98-30" aria-hidden="true" tabindex="-1"></a>                    data[<span class="sc">-</span>train.ix, ]<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.test)</span>
<span id="cb98-31"><a href="#cb98-31" aria-hidden="true" tabindex="-1"></a>    results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">c</span>(inodesize, this.err))</span>
<span id="cb98-32"><a href="#cb98-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># err.rf &lt;- c(err.rf, length(which(pred.test !=</span></span>
<span id="cb98-33"><a href="#cb98-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># data[-train.ix,]$DX_bl))/length(pred.test) )</span></span>
<span id="cb98-34"><a href="#cb98-34" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb98-35"><a href="#cb98-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-36"><a href="#cb98-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-37"><a href="#cb98-37" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(results) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;min_node_size&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb98-38"><a href="#cb98-38" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(results) <span class="sc">%&gt;%</span></span>
<span id="cb98-39"><a href="#cb98-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">min_node_size =</span> <span class="fu">as.character</span>(min_node_size))</span>
<span id="cb98-40"><a href="#cb98-40" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(results<span class="sc">$</span>min_node_size) <span class="ot">&lt;-</span> <span class="fu">unique</span>(results<span class="sc">$</span>min_node_size)</span>
<span id="cb98-41"><a href="#cb98-41" aria-hidden="true" tabindex="-1"></a>results<span class="sc">$</span>min_node_size <span class="ot">&lt;-</span> <span class="fu">factor</span>(results<span class="sc">$</span>min_node_size,</span>
<span id="cb98-42"><a href="#cb98-42" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">unique</span>(results<span class="sc">$</span>min_node_size))</span>
<span id="cb98-43"><a href="#cb98-43" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> results, <span class="fu">aes</span>(<span class="at">y =</span> error,</span>
<span id="cb98-44"><a href="#cb98-44" aria-hidden="true" tabindex="-1"></a>                  <span class="at">x =</span> min_node_size)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<div style="page-break-after: always;"></div>
<p>Figure <a href="#fig:f4-23">71</a> shows that the error rates start to rise when the minimum node size equals 40. And the error rates are not substantially different when the minimum node size is less than 40. All together, these results provide information for us to select models.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-20"></span>
<img src="graphics/4_20.png" alt="Performance of random forest v.s. tree model on the Alzheimer's disease data" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 72: Performance of random forest v.s. tree model on the Alzheimer’s disease data<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>To compare random forest with decision tree, we can also follow a similar process, i.e., half of the dataset is used for training and the other half for testing. This process of splitting data, training the model on training data, and testing the model on testing data is repeated <span class="math inline">\(100\)</span> times, and boxplots of the errors from decision trees and random forests are plotted in Figure <a href="#fig:f4-20">72</a> using the following R code.</p>
<p></p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(randomForest)</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_gray</span>(<span class="at">base_size =</span> <span class="dv">15</span>))</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a>target_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">==</span> <span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>data[, target_indx] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, data[, target_indx]))</span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> </span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a>                   <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>, <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb99-19"><a href="#cb99-19" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb99-20"><a href="#cb99-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-21"><a href="#cb99-21" aria-hidden="true" tabindex="-1"></a>err.tree <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb99-22"><a href="#cb99-22" aria-hidden="true" tabindex="-1"></a>err.rf <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb99-23"><a href="#cb99-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb99-24"><a href="#cb99-24" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data), <span class="fu">floor</span>(<span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb99-25"><a href="#cb99-25" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data[train.ix, ])</span>
<span id="cb99-26"><a href="#cb99-26" aria-hidden="true" tabindex="-1"></a>pred.test <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree, data[<span class="sc">-</span>train.ix, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb99-27"><a href="#cb99-27" aria-hidden="true" tabindex="-1"></a>err.tree <span class="ot">&lt;-</span> <span class="fu">c</span>(err.tree, <span class="fu">length</span>(</span>
<span id="cb99-28"><a href="#cb99-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">which</span>(pred.test <span class="sc">!=</span> data[<span class="sc">-</span>train.ix, ]<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.test))</span>
<span id="cb99-29"><a href="#cb99-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-30"><a href="#cb99-30" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data[train.ix, ])</span>
<span id="cb99-31"><a href="#cb99-31" aria-hidden="true" tabindex="-1"></a>pred.test <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[<span class="sc">-</span>train.ix, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb99-32"><a href="#cb99-32" aria-hidden="true" tabindex="-1"></a>err.rf <span class="ot">&lt;-</span> <span class="fu">c</span>(err.rf, <span class="fu">length</span>(</span>
<span id="cb99-33"><a href="#cb99-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">which</span>(pred.test <span class="sc">!=</span> data[<span class="sc">-</span>train.ix, ]<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.test))</span>
<span id="cb99-34"><a href="#cb99-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb99-35"><a href="#cb99-35" aria-hidden="true" tabindex="-1"></a>err.tree <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">err =</span> err.tree, <span class="at">method =</span> <span class="st">&quot;tree&quot;</span>)</span>
<span id="cb99-36"><a href="#cb99-36" aria-hidden="true" tabindex="-1"></a>err.rf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">err =</span> err.rf, <span class="at">method =</span> <span class="st">&quot;random_forests&quot;</span>)</span>
<span id="cb99-37"><a href="#cb99-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-38"><a href="#cb99-38" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(</span>
<span id="cb99-39"><a href="#cb99-39" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> <span class="fu">rbind</span>(err.tree, err.rf), <span class="fu">aes</span>(<span class="at">y =</span> err, <span class="at">x =</span> method)) <span class="sc">+</span> </span>
<span id="cb99-40"><a href="#cb99-40" aria-hidden="true" tabindex="-1"></a><span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f4-20">72</a> shows that the error rates of decision trees are higher than those for random forests, indicating that random forest is a better model here.</p>
</div>
</div>
<div id="remarks-2" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="the-gini-index-versus-the-entropy" class="section level3 unnumbered">
<h3>The Gini index versus the entropy</h3>
<p>The <em>Gini index</em> plays the same role as the <em>entropy</em>. To see that, the following R code plots the Gini index and the entropy in a binary class problem<label for="tufte-sn-103" class="margin-toggle sidenote-number">103</label><input type="checkbox" id="tufte-sn-103" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">103</span> A binary class problem has two nominal parameters, <span class="math inline">\(p_{1}\)</span> and <span class="math inline">\(p_{2}\)</span>. Because <span class="math inline">\(p_{2}\)</span> equals <span class="math inline">\(1-p_{1}\)</span>, it is actually a one-parameter system, such that we can visualize how the Gini index and the entropy changes according to <span class="math inline">\(p_{1}\)</span>, as shown in Figure <a href="#fig:f4-15">73</a>.</span>. Their similarity is evident as shown in Figure <a href="#fig:f4-15">73</a>.</p>
<p>The following R scripts package the Gini index and entropy as two functions.</p>
<p></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>entropy <span class="ot">&lt;-</span> <span class="cf">function</span>(p_v) {</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (p <span class="cf">in</span> p_v) {</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (p <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>this_term <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>this_term <span class="ot">&lt;-</span> <span class="sc">-</span>p <span class="sc">*</span> <span class="fu">log2</span>(p)</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> e <span class="sc">+</span> this_term</span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a><span class="fu">return</span>(e)</span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>gini <span class="ot">&lt;-</span> <span class="cf">function</span>(p_v) {</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (p <span class="cf">in</span> p_v) {</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (p <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>this.term <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>this.term <span class="ot">&lt;-</span> p <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p)</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> e <span class="sc">+</span> this.term</span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a><span class="fu">return</span>(e)</span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p>The following R script draws Figure <a href="#fig:f4-15">73</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f4-15"></span>
<img src="graphics/4_15.png" alt="Gini index vs. entropy" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 73: Gini index vs. entropy<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>entropy.v <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>gini.v <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>p.v <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (p <span class="cf">in</span> p.v) {</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>entropy.v <span class="ot">&lt;-</span> <span class="fu">c</span>(entropy.v, (<span class="fu">entropy</span>(<span class="fu">c</span>(p, <span class="dv">1</span> <span class="sc">-</span> p))))</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>gini.v <span class="ot">&lt;-</span> <span class="fu">c</span>(gini.v, (<span class="fu">gini</span>(<span class="fu">c</span>(p, <span class="dv">1</span> <span class="sc">-</span> p))))</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p.v, gini.v, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;percentage of class 1&quot;</span>,<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;impurity measure&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>,</span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">cex.axis =</span> <span class="fl">1.5</span>, <span class="at">cex.main =</span> <span class="fl">1.5</span>,<span class="at">cex.sub =</span> <span class="fl">1.5</span>)</span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(p.v, entropy.v, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Entropy&quot;</span>, <span class="st">&quot;Gini index&quot;</span>),</span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="at">cex =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<p></p>
<p>It can be seen in Figure <a href="#fig:f4-15">73</a> that the two impurity measures are similar. Both reach minimum, i.e., <span class="math inline">\(0\)</span>, when all the data instances belong to the same class, and they reach maximum when there are equal numbers of data instances for the two classes. In practice, they produce similar trees.</p>
</div>
<div id="why-random-forests-work" class="section level3 unnumbered">
<h3>Why random forests work</h3>
<p>A random forest model is inspirational because it shows that <em>randomness</em>, usually considered as a troublemaker, has a productive dimension. This seems to be counterintuitive. An explanation has been pointed out in numerous literature that the random forests, together with other models that are called <strong>ensemble learning</strong> models, could make a group of <strong>weak models</strong> come together to form a strong model.</p>
<p>For example, consider a random forest model with <span class="math inline">\(100\)</span> trees. Each tree is a <em>weak model</em> and its accuracy is <span class="math inline">\(0.6\)</span>. Assume that the trees are independent<label for="tufte-sn-104" class="margin-toggle sidenote-number">104</label><input type="checkbox" id="tufte-sn-104" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">104</span> I.e., the predictions of one tree provide no hint to guess the predictions of another tree.</span>, with <span class="math inline">\(100\)</span> trees the probability of the random forest model to predict correctly on any data point is <span class="math inline">\(0.97\)</span>, i.e.,</p>
<p><span class="math display">\[ 
\sum_{k=51}^{100} C(n, k) \times 0.6^{k} \times 0.4^{100-k} = 0.97. 
\]</span></p>
<p>This result is impressive, but don’t forget the <em>assumption of the independence</em> between the trees. This assumption does not hold in reality in a strict sense; i.e., ideally, we hope to have an algorithm that can find many good models that perform well and are all different; but, if we build many models using one dataset, these models would more or less resemble each other. Particularly, when we solely focus on models that can achieve optimal performance, it is often that the identified models end up more or less the same. Responding to this dilemma, randomness (i.e., the use of Bootstrap to randomize choices of data instances and the use of random feature selection for building trees) is introduced into the model-building process to create diversity of the models. The dynamics between the degree of randomness, the performance of each individual model, and their difference, should be handled well. To develop the craft, you may have many practices and focus on driving this dynamics towards a collective good.</p>
</div>
<div id="variable-importance-by-random-forests" class="section level3 unnumbered">
<h3>Variable importance by random forests</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-18"></span>
<img src="graphics/8_18.png" alt="Tree \#1" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 74: Tree #1<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Recall that a random forest model consists of decision trees that are defined by splits on some variables. The splits provide information about variables’ importance. To explain this, consider the data example shown in Table <a href="#tab:t8-3">12</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-3">Table 12: </span>Example of a dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Assume that a random forest model with two trees (i.e., shown in Figures <a href="#fig:f8-18">74</a> and <a href="#fig:f8-19">75</a>) is built on the dataset.</p>
<p>At split 1, the Gini gain for <span class="math inline">\(x_1\)</span> is:</p>
<p><span class="math display">\[0.375-0.5\times0-0.5\times0.5 = 0.125.\]</span></p>
<p>At split 2, the Gini gain for <span class="math inline">\(x_3\)</span> is:</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-19"></span>
<img src="graphics/8_19.png" alt="Tree \#2" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 75: Tree #2<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display">\[0.5-0.5\times0-0.5\times0 = 0.5.\]</span></p>
<p>At split 3, the Gini gain for <span class="math inline">\(x_2\)</span> is</p>
<p><span class="math display">\[0.5-0.25\times0 + 0.75\times0.44 = 0.17.\]</span></p>
<p>At split 4, the Gini gain for <span class="math inline">\(x_3\)</span> is</p>
<p><span class="math display">\[0.44-0.5\times0 + 0.5\times0 = 0.44.\]</span></p>
<p>Table <a href="#tab:t8-scoresheet">13</a> summarizes the contributions of the variables in the splits. The total contribution can be used as the variable’s importance score.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-scoresheet">Table 13: </span>Contributions of the variables in the splits</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID of splits</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.125\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.5\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.17\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.44\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(Total\)</span></td>
<td align="left"><span class="math inline">\(0.125\)</span></td>
<td align="left"><span class="math inline">\(0.17\)</span></td>
<td align="left"><span class="math inline">\(0.94\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The variable’s importance score helps us identify the variables that have strong predictive values. The approach to obtain the variable’s importance score is simple and effective, but it is not perfect. For example, if we revisit the example shown in Table <a href="#tab:t8-3">12</a>, we may notice that <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are identical. What does this suggest to you<label for="tufte-sn-105" class="margin-toggle sidenote-number">105</label><input type="checkbox" id="tufte-sn-105" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">105</span> Interested readers may read this article: Deng, H. and Runger, G., <em>Gene selection with guided regularized random forest</em>, Pattern Recognition, Volume 46, Issue 12, Pages 3483-3489, 2013.</span>?</p>
</div>
<div id="partial-dependency-plot" class="section level3 unnumbered">
<h3>Partial dependency plot</h3>
<p>Variable importance scores indicate whether a variable is informative in predicting the outcome variable. It does not provide information about <em>how</em> the outcome variable is influenced by the variables. <strong>Partial dependency plot</strong> can be used to visualize the relationship between a predictor and the outcome variable, averaged on other predictors.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-7"></span>
<p class="caption marginnote shownote">
Figure 76: Partial dependency plots of variables in random forests
</p>
<img src="graphics/6_7.png" alt="Partial dependency plots of variables in random forests" width="80%"  />
</div>
<p></p>
<p>We draw in Figure <a href="#fig:f6-7">76</a> the partial dependency plots of two variables on the AD dataset. It is clear that the relationships between the outcome variable with both predictors are significant. And the orientation of both relationships is visualized by the plots.</p>
<p></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the partial dependency plots of variables in random forests </span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>randomForest<span class="sc">::</span><span class="fu">partialPlot</span>(rf, data, HippoNV, <span class="st">&quot;1&quot;</span>)</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>randomForest<span class="sc">::</span><span class="fu">partialPlot</span>(rf, data, FDG, <span class="st">&quot;1&quot;</span>)</span></code></pre></div>
<p></p>
</div>
</div>
<div id="exercises-2" class="section level2 unnumbered">
<h2>Exercises</h2>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Continue the example in the 4-step R pipeline R lab in this chapter that estimated the mean and standard derivation of the variable <code>HippoNV</code> of the AD dataset. Use the same R pipeline to evaluate the uncertainty of the estimation of the standard derivation of the variable <code>HippoNV</code> of the AD dataset. Report its <span class="math inline">\(95\%\)</span> CI.</p></li>
<li><p> Use the R pipeline for Bootstrap to evaluate the uncertainty of the estimation of the coefficient of a logistic regression model. Report the <span class="math inline">\(95\%\)</span> CI of the estimated coefficients.</p></li>
</ul>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f4-rf-2trees"></span>
<p class="caption marginnote shownote">
Figure 77: A random forest model with <span class="math inline">\(2\)</span> trees
</p>
<img src="graphics/4_rf_2trees.png" alt="A random forest model with $2$ trees" width="80%"  />
</div>
<p></p>
<ul>
<li> Consider the data in Table <a href="#tab:t4-hw-rf-imp">14</a>. Assume that two trees were built on it, as shown in Figure <a href="#fig:f4-rf-2trees">77</a>. (a) Calculate the Gini index of each node of both trees; and (b) estimate the importance scores of the three variables in this RF model.</li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t4-hw-rf-imp">Table 14: </span>Dataset for Q3</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f4-rf-3trees"></span>
<p class="caption marginnote shownote">
Figure 78: A random forest model with <span class="math inline">\(3\)</span> trees
</p>
<img src="graphics/4_rf_3trees.png" alt="A random forest model with $3$ trees" width="80%"  />
</div>
<p></p>
<ul>
<li> A random forest model with <span class="math inline">\(3\)</span> trees is built, as shown in Figure <a href="#fig:f4-rf-3trees">78</a>. Use this model to predict on the data in Table <a href="#tab:t4-hw-rf-pred">15</a>.</li>
</ul>
<p><!-- end{enumerate} --></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t4-hw-rf-pred">Table 15: </span>Dataset for Q4</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-0.5\)</span></td>
<td align="left"><span class="math inline">\(0.5\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0.8\)</span></td>
<td align="left"><span class="math inline">\(-1.1\)</span></td>
<td align="left"><span class="math inline">\(0.1\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1.2\)</span></td>
<td align="left"><span class="math inline">\(-0.3\)</span></td>
<td align="left"><span class="math inline">\(0.9\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<ul>
<li> Follow up on the simulation experiment in Q9 in Chapter 2. Apply random forest with <span class="math inline">\(100\)</span> trees (by setting <code>ntree = 100</code>) on the simulated data and comment on the result.</li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<div id="chapter-5.-learning-i-cross-validation-oob" class="section level1 unnumbered">
<h1>Chapter 5. Learning (I): Cross-validation &amp; OOB</h1>
<div id="overview-3" class="section level2 unnumbered">
<h2>Overview</h2>
<p>The question of <em>learning</em> in data analytics concerns <em>whether or not the model has learned from the data</em>. To understand this, let’s look at a few dilemmas.</p>
<p><em>Dilemma 1.</em> Let’s consider the prediction of a disease. Hired by the Centers of Disease Control and Prevention (CDC), a data scientist built a prediction model (e.g., a logistic regression model) using tens of thousands of patient’ data collected over several years, and the model’s prediction accuracy was <span class="math inline">\(90\%\)</span>. Isn’t this a good model?</p>
<p>Then we are informed that this is a rare disease, and national statistics has shown that only 0.001<span class="math inline">\(\%\)</span> of the population of the United States have this disease. This contextual knowledge changes our perception of the <span class="math inline">\(90\%\)</span> prediction accuracy dramatically. Consider a trivial model that simply predicts all the cases as negative (i.e., no disease), wouldn’t this trivial model achieve a prediction accuracy as high as <span class="math inline">\(99.999\%\)</span>?<label for="tufte-sn-106" class="margin-toggle sidenote-number">106</label><input type="checkbox" id="tufte-sn-106" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">106</span> <em>Moral of the story:</em> context matters.</span></p>
<p><em>Dilemma 2.</em> Now let’s look at another example. Some studies pointed out that bestsellers could be reasonably predicted by computers based on the book’s content. These studies collected a number of books, some were bestsellers (i.e., based on <em>The New York Times</em> book-selling rank). They extracted features from these books, such as some thematic features and linguistic patterns that could be measured by words use and frequency, and trained prediction models with these features as predictors and the bestseller/non-bestseller as a binary outcome. The model achieved an accuracy between 70% to 80%. This looks promising. The problem is that, e.g., in the dataset, you may see that about 30% of the books were bestsellers.</p>
<p>Statistics show that in recent years there could have been more than one million new books published each year. How many of them would make <em>The New York Times</em> bestseller list? Maybe <span class="math inline">\(0.01\%\)</span> or fewer. So the dataset collected for training the data is not entirely representative of the population. And in fact, this is another rare-disease-like situation<label for="tufte-sn-107" class="margin-toggle sidenote-number">107</label><input type="checkbox" id="tufte-sn-107" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">107</span> <em>Moral of the story:</em> how the dataset is collected also matters.</span>.</p>
<p><em>Dilemma 3.</em> As we have mentioned in <strong>Chapter 2</strong>, the <em>R-squared</em> measures the goodness-of-fit of a regression model. Let’s revisit the definition of the R-squared</p>
<p><span class="math display">\[ 
\text{R-squared} = \frac{\sigma_{y}^{2}-\sigma_{\varepsilon}^{2}}{\sigma_{y}^{2}}.
\]</span></p>
<p>We can see that the denominator is always fixed, no matter how we change the regression model; while the numerator could only decrease if more variables are put into the model, even if these new variables have no relationship with the outcome variable. In other words, with more variables in the model, even if the residuals are not reduced, at worst they remain the same<label for="tufte-sn-108" class="margin-toggle sidenote-number">108</label><input type="checkbox" id="tufte-sn-108" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">108</span> For a more formal discussions about the technical limitations of R-squared, e.g., a good starting point is: Kvalseth, T.O., <em>Cautionary Note about <span class="math inline">\(R^2\)</span></em>, The American Statistician, Volume 39, Issue 4, Pages 279-285, 1985.</span>.</p>
<p>Further, the <em>R-squared</em> is impacted by the variance of the predictors as well. As the regression model is</p>
<p><span class="math display">\[ 
y = \boldsymbol{x} \boldsymbol{\beta} +\epsilon,
\]</span></p>
<p>it is known that the variance of <span class="math inline">\(y\)</span> is
the variance of <span class="math inline">\(\operatorname{var}(y) = \boldsymbol{\beta}^{T} \operatorname{var}(x)\boldsymbol{\beta} + \operatorname{var}(\epsilon)\)</span>. The R-squared can be rewritten as</p>
<p><span class="math display">\[ 
R^{2} = \frac{\boldsymbol{\beta}^{T} \operatorname{var}(\boldsymbol{x})\boldsymbol{\beta}}{\boldsymbol{\beta}^{T} \operatorname{var}(\boldsymbol{x})\boldsymbol{\beta} + \operatorname{var}(\epsilon)}.
\]</span></p>
<p>Thus, the <em>R-squared</em> is not only impacted by how well <span class="math inline">\(\boldsymbol{x}\)</span> can predict <span class="math inline">\(y\)</span>, but also by the variance of <span class="math inline">\(\boldsymbol{x}\)</span> as well<label for="tufte-sn-109" class="margin-toggle sidenote-number">109</label><input type="checkbox" id="tufte-sn-109" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">109</span> <em>Moral of the story:</em> a model’s performance metric could be manipulated under “legal” terms.</span>.</p>
<!-- % Thus, the drawback of using R-squared is that it doesn't account for model complexity. The adjusted R-squared was developed to provide a remedy for this. Some other criteria such as the **AIC**  and **BIC**  were also developed which have a good balance between the model fit (just like R-squared) and model complexity (i.e., how many predictors are used in the model). -->
</div>
<div id="cross-validation" class="section level2 unnumbered">
<h2>Cross-validation</h2>
<div id="rationale-and-formulation-7" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>Performance metrics, such as accuracy and R-squared, are context-dependent (<em>Dilemma 1</em>), data-dependent (<em>Dilemma 2</em>), and vulnerable to conscious or unconscious manipulations (<em>Dilemma 3</em>). These limitations make them <em>relative</em> metrics. They are not the <em>absolutes</em> that we can rely on to evaluate models in a universal fashion in all contexts.</p>
<p>So, what should be the universal and objective criteria to evaluate the learning performance of a model?</p>
<p>To answer this question, we need to understand the concepts, <strong>underfit</strong>, <strong>good fit</strong>, and <strong>overfit</strong>.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f5-1"></span>
<img src="graphics/5_1.png" alt="Three types of model performance" width="80%"  />
<p class="caption marginnote shownote">
Figure 79: Three types of model performance
</p>
</div>
<p></p>
<p>Figure <a href="#fig:f5-1">79</a> shows three models to fit the same dataset that has two classes of data points. The first model is a linear model<label for="tufte-sn-110" class="margin-toggle sidenote-number">110</label><input type="checkbox" id="tufte-sn-110" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">110</span> E.g., <span class="math inline">\(f_1(x)=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}\)</span>.</span> that yields a straight line as the <strong>decision boundary</strong>. Obviously, many data points are misclassified when using a linear decision boundary. Some curvature is needed to bend the decision boundary, so we introduce some second order terms and an interaction term of the two predictors to create another model<label for="tufte-sn-111" class="margin-toggle sidenote-number">111</label><input type="checkbox" id="tufte-sn-111" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">111</span> E.g., <span class="math inline">\(f_2(x)=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\beta_{11} x_{1}^{2}+\beta_{22} x_{2}^{2}+\beta_{12} x_{1} x_{2}\)</span>.</span>. The decision boundary is shown in Figure <a href="#fig:f5-1">79</a> (middle). This improved model still could not classify the two classes completely. More interaction terms<label for="tufte-sn-112" class="margin-toggle sidenote-number">112</label><input type="checkbox" id="tufte-sn-112" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">112</span> E.g., <span class="math inline">\(f_3(\boldsymbol{x})=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\beta_{11} x_{1}^{2}+\beta_{22} x_{2}^{2}+\beta_{12} x_{1} x_{2}+\beta_{112} x_{1}^{2} x_{2}+\beta_{122} x_{1} x_{2}^{2}+\cdots\)</span>.</span> are introduced into the model. The decision boundary is shown in Figure <a href="#fig:f5-1">79</a> (right).</p>
<p>Now <span class="math inline">\(100\%\)</span> prediction accuracy could be achieved. A sense of suspicion should arise: is this <em>too good to be true</em>?</p>
<p>What we have seen in Figure <a href="#fig:f5-1">79</a>, on the positive side, is the capacity we can develop to <em>fit</em> a dataset<label for="tufte-sn-113" class="margin-toggle sidenote-number">113</label><input type="checkbox" id="tufte-sn-113" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">113</span> <em>Fit</em> a dataset is not necessarily model a dataset. Beginners may need time to develop a sense to see the difference between the two.</span>. On the other hand, what is responsible for the sense of suspicion of “too good to be true” is that we didn’t see a <em>validation process</em> at work.</p>
<p>Recall a general assumption of data modeling is<label for="tufte-sn-114" class="margin-toggle sidenote-number">114</label><input type="checkbox" id="tufte-sn-114" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">114</span> I.e., Eq. <a href="#eq:2-genericmodel">(2)</a> in <strong>Chapter 2</strong>.</span></p>
<p><span class="math display">\[\begin{equation*}
    \underbrace{y}_{data} = \underbrace{f(\boldsymbol{x})}_{signal} + \underbrace{\epsilon}_{noise},
\end{equation*}\]</span></p>
<p>where <em>noise</em> is unpredictable. Bearing this framework in mind, we revisit the three models in Figure <a href="#fig:f5-1">79</a>, which from left to right illustrate <strong>underfit</strong>, <strong>good fit</strong>, and <strong>overfit</strong>, respectively. A model called <em>underfitted</em> means it fails to incorporate some pattern of the signal in the dataset. A model called <em>overfitted</em> means it allows the noise to affect the model<label for="tufte-sn-115" class="margin-toggle sidenote-number">115</label><input type="checkbox" id="tufte-sn-115" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">115</span> Noise, by definition, only happens by accident. While the model, by definition, is to generalize the constancy, i.e., the signal, of the data rather than its unrepeatable randomness.</span>. A dataset could be randomly generated, but the <em>mechanism of generating the randomness</em><label for="tufte-sn-116" class="margin-toggle sidenote-number">116</label><input type="checkbox" id="tufte-sn-116" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">116</span> I.e., like a distribution model.</span> is a constancy. The model in the middle panel of Figure <a href="#fig:f5-1">79</a> is able to maintain a balance: it captures the structural constancy in the data to form the model, while resisting the noise and refusing to let them bend its decision boundary.</p>
<p>In summary, Figure <a href="#fig:f5-1">79</a> illustrates that:</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> <strong>Overfit</strong>: Complexity of the model &gt; complexity of the signal;</p></li>
<li><p> <strong>Good fit</strong>: Complexity of the model = complexity of the signal;</p></li>
<li><p> <strong>Underfit</strong>: Complexity of the model &lt; complexity of the signal.</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>In practice, however, the ultimate dilemma is we don’t know what to expect: how much variability in the data comes from the signal or the noise? A sense of proportion always matters, and methods such as cross-validation come to our rescue.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-2"></span>
<img src="graphics/5_2.png" alt="The hold-out method" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 80: The hold-out method<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
</div>
<div id="theorymethod-1" class="section level3 unnumbered">
<h3>Theory/Method</h3>
<p>In what follows, a few approaches that help us to identify the model with <em>good fit</em> (i.e., the one shown in Figure <a href="#fig:f5-1">79</a> (middle), <span class="math inline">\(f_2(x)\)</span>) are introduced. These approaches share the same goal: to train a model on the training data and make sure the learned model would succeed on an <em>unseen</em> testing dataset.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-3"></span>
<img src="graphics/5_3.png" alt="The random sampling method" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 81: The random sampling method<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The first approach is the <strong>hold-out</strong> method. As shown in Figure <a href="#fig:f5-2">80</a>, the hold-out method randomly divides a given dataset into two parts. The model is trained on the training data only, while its performance is evaluated on the testing data. For instance, for the three models shown in Figure <a href="#fig:f5-1">79</a>, each of them will be trained on the training dataset and will have their regression coefficients estimated. Then, the learned models will be evaluated on the testing data. The model that has the best performance on the testing data will be selected as the final model.</p>
<p>Another approach called <strong>random sampling</strong> repeats this random division many times, as shown in Figure <a href="#fig:f5-3">81</a>. Each time, the <em>model training and selection</em> only uses the training dataset, and the model evaluation only uses the testing dataset. The performance of the models on the three experiments could be averaged and the model that has the best average performance is selected.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-4"></span>
<img src="graphics/5_4.png" alt="The K-fold cross-validation method (here, $K=4$)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 82: The K-fold cross-validation method (here, <span class="math inline">\(K=4\)</span>)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The <strong>K-fold cross-validation</strong> is a mix of the <em>random sampling</em> method and the <em>hold-out</em> method. It first divides the dataset into <span class="math inline">\(K\)</span> folds of equal sizes. Then, it trains a model using any combination of <span class="math inline">\(K-1\)</span> folds of the dataset, and tests the model using the remaining one-fold of the dataset. As shown in Figure <a href="#fig:f5-4">82</a>, the model training and testing process is repeated <span class="math inline">\(K\)</span> times. The performance of the models on the <span class="math inline">\(K\)</span> experiments could be averaged and the model that has the best average performance is selected.</p>
<p>These approaches can be used for evaluating a model’s performance in a robust way. They are also useful when we’d like to choose among model types<label for="tufte-sn-117" class="margin-toggle sidenote-number">117</label><input type="checkbox" id="tufte-sn-117" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">117</span> E.g., decision tree vs. linear regression.</span> or model formulations<label for="tufte-sn-118" class="margin-toggle sidenote-number">118</label><input type="checkbox" id="tufte-sn-118" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">118</span> E.g., model 1: <span class="math inline">\(y=\beta_{0}+\beta_{1} x_1\)</span>; vs. model 2: <span class="math inline">\(y=\beta_{0}+\beta_{1} x_1+\beta_{2} x_2\)</span>.</span>. While the model type and the model formulation is settled, for example, suppose that we have determined to use linear regression and the model formulation <span class="math inline">\(y=\beta_{0}+\beta_{1} x_1+\beta_{2} x_2\)</span>, these methods could be used to evaluate the performance of this single model. It is not uncommon that in real data analysis, these cross-validation and sampling methods are used in combination and serve different stages of the analysis process.</p>
</div>
<div id="r-lab-6" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 4-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> are standard procedures to get data into R and further make appropriate preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing </span></span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and Y vector </span></span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a><span class="co"># (outcome variable)</span></span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates a list of models to be evaluated and compared with<label for="tufte-sn-119" class="margin-toggle sidenote-number">119</label><input type="checkbox" id="tufte-sn-119" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">119</span> <em>Linear regression</em>: we often compare models using different predictors; <em>Decision tree</em>: we often compare models with different depths; <em>Random forests</em>: we often compare models with a different number of trees, a different depth of individual trees, or a different number of features to be randomly picked up to split the nodes.</span>.</p>
<p></p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Use linear regression model as an example</span></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="st">&quot;MMSCORE ~ .&quot;</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="st">&quot;MMSCORE ~ AGE + PTEDUCAT + FDG + AV45 + HippoNV +</span></span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a><span class="st">                                                  rs3865444&quot;</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="st">&quot;MMSCORE ~ AGE + PTEDUCAT&quot;</span></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>model4 <span class="ot">&lt;-</span> <span class="st">&quot;MMSCORE ~ FDG + AV45 + HippoNV&quot;</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses the <span class="math inline">\(10\)</span>-fold cross-validation to evaluate the models and find out which one is the best. The R code is shown below and is divided into two parts. The first part uses the <code>sample()</code> function to create random split of the dataset into <span class="math inline">\(10\)</span> folds.</p>
<p></p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Use 10-fold cross-validation to evaluate all models</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="co"># First, let me use 10-fold cross-validation to evaluate the</span></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="co"># performance of model1</span></span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">=</span> <span class="dv">10</span> </span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a><span class="co"># number of fold (the parameter K in K-fold cross validation)</span></span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(data)[<span class="dv">1</span>] <span class="co"># the sample size, N, of the dataset</span></span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a><span class="co"># This randomly creates a labeling vector (1 X N) for </span></span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a><span class="co"># the N samples. For example, here, N = 16, and </span></span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a><span class="co"># I run this function and it returns</span></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a><span class="co"># the value as 5  4  4 10  6  7  6  8  3  2  1  5  3  9  2  1. </span></span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a><span class="co"># That means, the first sample is allocated to the 5th fold,</span></span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a><span class="co"># the 2nd and 3rd samples are allocated to the 4th fold, etc.</span></span></code></pre></div>
<p></p>
<p>The second part shows how we evaluate the models. We only show the code for two models, as the script for evaluating each model is basically the same.</p>
<p></p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate model1</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="co"># cv_mse aims to make records of the mean squared error </span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (MSE) for each fold</span></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>cv_mse <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># In each iteration of the 10 iterations, remember, we use one</span></span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fold of data as the testing data</span></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>  data.train <span class="ot">&lt;-</span> data[<span class="sc">-</span>test_i, ] </span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Then, the remaining 9 folds&#39; data form our training data</span></span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>  data.test <span class="ot">&lt;-</span> data[test_i, ]   </span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is the testing data, from the ith fold</span></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>  lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(model1, <span class="at">data =</span> data.train) </span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the linear model with the training data</span></span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm.AD, data.test)     </span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test<span class="sc">$</span>MMSCORE                  </span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get the true y values for the testing data</span></span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a>  cv_mse[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mean((true_y - y_hat)^2): mean squared error (MSE). </span></span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The smaller this error, the better your model is</span></span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_mse)</span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-26"><a href="#cb107-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, evaluate model2</span></span>
<span id="cb107-27"><a href="#cb107-27" aria-hidden="true" tabindex="-1"></a>cv_mse <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb107-28"><a href="#cb107-28" aria-hidden="true" tabindex="-1"></a><span class="co"># cv_mse aims to make records of the mean squared error (MSE) </span></span>
<span id="cb107-29"><a href="#cb107-29" aria-hidden="true" tabindex="-1"></a><span class="co"># for each fold</span></span>
<span id="cb107-30"><a href="#cb107-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb107-31"><a href="#cb107-31" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb107-32"><a href="#cb107-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># In each iteration of the 10 iterations, remember, </span></span>
<span id="cb107-33"><a href="#cb107-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we use one fold of data as the testing data</span></span>
<span id="cb107-34"><a href="#cb107-34" aria-hidden="true" tabindex="-1"></a>  data.train <span class="ot">&lt;-</span> data[<span class="sc">-</span>test_i, ] </span>
<span id="cb107-35"><a href="#cb107-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Then, the remaining 9 folds&#39; data form our training data</span></span>
<span id="cb107-36"><a href="#cb107-36" aria-hidden="true" tabindex="-1"></a>  data.test <span class="ot">&lt;-</span> data[test_i, ]   </span>
<span id="cb107-37"><a href="#cb107-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is the testing data, from the ith fold</span></span>
<span id="cb107-38"><a href="#cb107-38" aria-hidden="true" tabindex="-1"></a>  lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(model2, <span class="at">data =</span> data.train) </span>
<span id="cb107-39"><a href="#cb107-39" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the linear model with the training data</span></span>
<span id="cb107-40"><a href="#cb107-40" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm.AD, data.test)      </span>
<span id="cb107-41"><a href="#cb107-41" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb107-42"><a href="#cb107-42" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test<span class="sc">$</span>MMSCORE                  </span>
<span id="cb107-43"><a href="#cb107-43" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get the true y values for the testing data</span></span>
<span id="cb107-44"><a href="#cb107-44" aria-hidden="true" tabindex="-1"></a>  cv_mse[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb107-45"><a href="#cb107-45" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mean((true_y - y_hat)^2): mean squared error (MSE). </span></span>
<span id="cb107-46"><a href="#cb107-46" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The smaller this error, the better your model is</span></span>
<span id="cb107-47"><a href="#cb107-47" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb107-48"><a href="#cb107-48" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_mse)</span>
<span id="cb107-49"><a href="#cb107-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-50"><a href="#cb107-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, evaluate model3 ...</span></span>
<span id="cb107-51"><a href="#cb107-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, evaluate model4 ...</span></span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3.17607</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3.12529</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 4.287637</span></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3.337222</span></span></code></pre></div>
<p></p>
<p>We conclude that <code>model2</code> is the best one, as it achieves the minimum mean squared error (MSE).</p>
<p><em>Simulation Experiment.</em> How do we know the cross-validation could identify a good model, i.e., the one that neither overfits nor underfits the data? Let’s design a simulation experiment to study the performance of cross-validation<label for="tufte-sn-120" class="margin-toggle sidenote-number">120</label><input type="checkbox" id="tufte-sn-120" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">120</span> A large portion of the R script in this subsection was modified from <a href="malanor.net">malanor.net</a>, now no longer an active site.</span>.</p>
<p>The purpose of the experiment is two-fold: (1) to show that the cross-validation can help us mitigate the model selection problem, and (2) to show that R is not just a tool for implementing data analysis methods, but also an experimental tool to gain first-hand experience of any method’s practical performance.</p>
<p>Our experiment has a clearly defined metric to measure the complexity of the <em>signal</em>. We resort to the <strong>spline</strong> models<label for="tufte-sn-121" class="margin-toggle sidenote-number">121</label><input type="checkbox" id="tufte-sn-121" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">121</span> A good tutorial:
Eilers, P. and Marx, B., <em>Splines, Knots, and Penalties</em>, Computational statistics, Volume 2, Issue 6, Pages 637-653, 2010.</span> that could be loosely put into the category of regression models, which have a precise mechanism to tune a model’s complexity, i.e., through the parameter of <strong>degree of freedom</strong> (<strong>df</strong>). For simplicity, we simulate a dataset with one predictor and one outcome variable. In R, we use the <code>ns()</code> function to simulate the spline model.</p>
<p>The outcome is a nonlinear curve<label for="tufte-sn-122" class="margin-toggle sidenote-number">122</label><input type="checkbox" id="tufte-sn-122" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">122</span> Here, we use the B-spline basis matrix for natural cubic splines to create a nonlinear curve. This topic is beyond the scope of this book.</span>. We use the degree of freedom (<code>df</code>) parameter in the <code>ns()</code> function to control the complexity of the curve, i.e., the larger the <code>df</code>, the more “nonlinear” the curve. As this curve is the <em>signal</em> of the data, we also simulate noise through a Gaussian distribution using the <code>rnorm()</code> function.</p>
<p></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Write a simulator to generate dataset with one predictor and </span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="co"># one outcome from a polynomial regression model</span></span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>seed <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>)</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(seed)</span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>gen_data <span class="ot">&lt;-</span> <span class="cf">function</span>(n, coef, v_noise) {</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, v_noise)</span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">100</span>))</span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,<span class="fu">ns</span>(x, <span class="at">df =</span> (<span class="fu">length</span>(coef) <span class="sc">-</span> <span class="dv">1</span>)))</span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(X <span class="sc">%*%</span> coef <span class="sc">+</span> eps)</span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a><span class="fu">return</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))   }</span></code></pre></div>
<p></p>
<p>The following R codes generate the scattered grey data points and the true model as shown in Figure <a href="#fig:f5-5">83</a>.</p>
<p></p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;splines&quot;)</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(splines)</span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Loading required package: splines</span></span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate one batch of data, and see how different model</span></span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a><span class="co"># fits with df from 1 to 50</span></span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a>coef <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.68</span>,<span class="fl">0.82</span>,<span class="sc">-</span><span class="fl">0.417</span>,<span class="fl">0.32</span>,<span class="sc">-</span><span class="fl">0.68</span>)</span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true" tabindex="-1"></a>v_noise <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true" tabindex="-1"></a>n_df <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n_df</span>
<span id="cb110-12"><a href="#cb110-12" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_train, coef, v_noise)</span>
<span id="cb110-13"><a href="#cb110-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-14"><a href="#cb110-14" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> tempData[, <span class="st">&quot;x&quot;</span>]</span>
<span id="cb110-15"><a href="#cb110-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> tempData[, <span class="st">&quot;y&quot;</span>]</span>
<span id="cb110-16"><a href="#cb110-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data</span></span>
<span id="cb110-17"><a href="#cb110-17" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> tempData<span class="sc">$</span>x</span>
<span id="cb110-18"><a href="#cb110-18" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">ns</span>(x, <span class="at">df =</span> (<span class="fu">length</span>(coef) <span class="sc">-</span> <span class="dv">1</span>)))</span>
<span id="cb110-19"><a href="#cb110-19" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> tempData<span class="sc">$</span>y</span>
<span id="cb110-20"><a href="#cb110-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-21"><a href="#cb110-21" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, X <span class="sc">%*%</span> coef, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f5-5"></span>
<p class="caption marginnote shownote">
Figure 83: The simulated data from a nonlinear regression model with B-spline basis matrix (<code>df</code>=4), and various fitted models with different degrees of freedom
</p>
<img src="graphics/5_5.png" alt="The simulated data from a nonlinear regression model with B-spline basis matrix (`df`=4), and various fitted models with different degrees of freedom" width="80%"  />
</div>
<p></p>
<p>We then fit the data with a variety of models, starting from <code>df=1</code><label for="tufte-sn-123" class="margin-toggle sidenote-number">123</label><input type="checkbox" id="tufte-sn-123" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">123</span> I.e., corresponds to the linear model.</span> to <code>df=20</code><label for="tufte-sn-124" class="margin-toggle sidenote-number">124</label><input type="checkbox" id="tufte-sn-124" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">124</span> I.e., a very complex model.</span>. The fitted curves are overlaid onto the scattered data points in Figure <a href="#fig:f5-5">83</a>. It can be seen that the linear model obviously underfits the data, as it lacks the flexibility to characterize the complexity of the signal sufficiently. The model that has (<code>df=20</code>) overfits the data, evidenced by its complex shape. It tries too hard to fit the local patterns, i.e., by all the turns and twists of its curve, while the local patterns were mostly induced by noise<label for="tufte-sn-125" class="margin-toggle sidenote-number">125</label><input type="checkbox" id="tufte-sn-125" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">125</span> A model that tries too hard to fit the training data by absorbing its noise into its shape will not perform well on future unseen testing data, since the particular noise in the training data would not appear in the testing data—if a noise repeats itself, it is not noise anymore but signal.</span>.</p>
<p></p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the data using different models with different</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="co"># degrees of freedom (df)</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="fu">t</span>(df), <span class="dv">2</span>, <span class="cf">function</span>(degf) <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">ns</span>(x, <span class="at">df =</span> degf)))</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the models</span></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit[[<span class="dv">1</span>]]), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit[[<span class="dv">4</span>]]), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue4&quot;</span>)</span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a><span class="co"># lines(x, fitted(fit[[10]]), lwd = 3, col = &quot;darkorange&quot;)</span></span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit[[<span class="dv">20</span>]]), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>,</span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;Linear fit (df = 1)&quot;</span>, <span class="st">&quot;Best model (df = 4)&quot;</span>,</span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;Overfitted model (df = 15)&quot;</span>,<span class="st">&quot;Overfitted model (df = 20)&quot;</span>),</span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;dodgerblue4&quot;</span>,</span>
<span id="cb111-14"><a href="#cb111-14" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;forestgreen&quot;</span>), <span class="at">text.width =</span> <span class="dv">32</span>, <span class="at">cex =</span> <span class="fl">0.6</span>)</span></code></pre></div>
<p></p>
<p>Note that, in this example, we have known that the true model has <code>df=4</code>. In reality, we don’t have this knowledge. It is dangerous to keep increasing the model complexity to aggressively pursue better prediction performance <em>on the training data</em>. To see the danger, let’s do another experiment.</p>
<p>First, we use the following R code to generate a testing data from the same distribution of the training data.</p>
<p></p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate test data from the same model</span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>n_test <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>xy_test <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_test, coef, v_noise)</span></code></pre></div>
<p></p>
<p>Then, we fit a set of models from linear (<code>df=1</code>) to (<code>df=20</code>) using the training dataset. And we compute the prediction errors of these models using the training dataset and testing dataset separately. This is done by the following R script.</p>
<p></p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the training and testing errors for each model</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">sapply</span>(fit, <span class="cf">function</span>(obj) <span class="fu">deviance</span>(obj)<span class="sc">/</span><span class="fu">nobs</span>(obj))</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">mapply</span>(<span class="cf">function</span>(obj, degf) <span class="fu">predict</span>(obj, <span class="fu">data.frame</span>(<span class="at">x =</span> </span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>                        xy_test<span class="sc">$</span>x)),fit, df)</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>te <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">as.list</span>(<span class="fu">data.frame</span>(pred)),</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>             <span class="cf">function</span>(y_hat) <span class="fu">mean</span>((xy_test<span class="sc">$</span>y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f5-6"></span>
<p class="caption marginnote shownote">
Figure 84: Prediction errors of the models (from (<code>df</code><span class="math inline">\(=0\)</span>) to (<code>df</code><span class="math inline">\(=20\)</span>)) on the training dataset and testing data
</p>
<img src="graphics/5_6.png" alt="Prediction errors of the models (from (`df`$=0$) to (`df`$=20$)) on the training dataset and testing data" width="80%"  />
</div>
<p></p>
<p>We further present the training and testing errors of the models in Figure <a href="#fig:f5-6">84</a>, by running the R script below.</p>
<div style="page-break-after: always;"></div>
<p></p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the errors</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(df, mse, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">gray</span>(<span class="fl">0.4</span>),</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Prediction error&quot;</span>,</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;The degrees of freedom (logged) of the model&quot;</span>,</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.9</span><span class="sc">*</span><span class="fu">min</span>(mse), <span class="fl">1.1</span><span class="sc">*</span><span class="fu">max</span>(mse)), <span class="at">log =</span> <span class="st">&quot;x&quot;</span>)</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(df, te, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;orange3&quot;</span>)</span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-9"><a href="#cb114-9" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="dv">1</span>], mse[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;palegreen3&quot;</span>, <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb114-10"><a href="#cb114-10" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="dv">1</span>], te[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;palegreen3&quot;</span>, <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb114-11"><a href="#cb114-11" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="fu">which.min</span>(te)], mse[<span class="fu">which.min</span>(te)], <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>,</span>
<span id="cb114-12"><a href="#cb114-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb114-13"><a href="#cb114-13" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="fu">which.min</span>(te)], te[<span class="fu">which.min</span>(te)], <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>,</span>
<span id="cb114-14"><a href="#cb114-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>,<span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb114-15"><a href="#cb114-15" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="dv">15</span>], mse[<span class="dv">15</span>], <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb114-16"><a href="#cb114-16" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="dv">15</span>], te[<span class="dv">15</span>], <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb114-17"><a href="#cb114-17" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;top&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Training error&quot;</span>, <span class="st">&quot;Test error&quot;</span>),</span>
<span id="cb114-18"><a href="#cb114-18" aria-hidden="true" tabindex="-1"></a><span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">gray</span>(<span class="fl">0.4</span>), <span class="st">&quot;orange3&quot;</span>), <span class="at">text.width =</span> <span class="fl">0.3</span>,</span>
<span id="cb114-19"><a href="#cb114-19" aria-hidden="true" tabindex="-1"></a>      <span class="at">cex =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f5-6">84</a> shows that the prediction error on the training dataset keeps decreasing with the increase of the <code>df</code>. This is consistent with our theory, and this only indicates a universal phenomenon that <em>a more complex model can fit the training data better</em>. On the other hand, we could observe that the testing error curve shows a <strong>U-shaped</strong> curve, indicating that an optimal model<label for="tufte-sn-126" class="margin-toggle sidenote-number">126</label><input type="checkbox" id="tufte-sn-126" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">126</span> I.e., the dip location on the U-shaped curve is where the optimal <code>df</code> could be found.</span> exists in this range of the model complexity.</p>
<p>As this is an observation made on one dataset that was randomly generated, we should repeat this experiment multiple times to see if our observation is robust. The following R code repeats this experiment <span class="math inline">\(100\)</span> times and presents the results in Figure <a href="#fig:f5-7">85</a>.</p>
<p></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeat the above experiments in 100 times</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>n_rep <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>coef <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.68</span>,<span class="fl">0.82</span>,<span class="sc">-</span><span class="fl">0.417</span>,<span class="fl">0.32</span>,<span class="sc">-</span><span class="fl">0.68</span>)</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>v_noise <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>n_df <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n_df</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>xy <span class="ot">&lt;-</span> res <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>xy_test <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_test, coef, v_noise)</span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_rep) {</span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>  xy[[i]] <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_train, coef, v_noise)</span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> xy[[i]][, <span class="st">&quot;x&quot;</span>]</span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> xy[[i]][, <span class="st">&quot;y&quot;</span>]</span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a>  res[[i]] <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="fu">t</span>(df), <span class="dv">2</span>,</span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a>              <span class="cf">function</span>(degf) <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">ns</span>(x, <span class="at">df =</span> degf)))</span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-19"><a href="#cb115-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the training and test errors for each model</span></span>
<span id="cb115-20"><a href="#cb115-20" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb115-21"><a href="#cb115-21" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> te <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n_df, <span class="at">ncol =</span> n_rep)</span>
<span id="cb115-22"><a href="#cb115-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_rep) {</span>
<span id="cb115-23"><a href="#cb115-23" aria-hidden="true" tabindex="-1"></a>  mse[, i] <span class="ot">&lt;-</span> <span class="fu">sapply</span>(res[[i]],</span>
<span id="cb115-24"><a href="#cb115-24" aria-hidden="true" tabindex="-1"></a>             <span class="cf">function</span>(obj) <span class="fu">deviance</span>(obj)<span class="sc">/</span><span class="fu">nobs</span>(obj))</span>
<span id="cb115-25"><a href="#cb115-25" aria-hidden="true" tabindex="-1"></a>  pred[[i]] <span class="ot">&lt;-</span> <span class="fu">mapply</span>(<span class="cf">function</span>(obj, degf) <span class="fu">predict</span>(obj,</span>
<span id="cb115-26"><a href="#cb115-26" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">data.frame</span>(<span class="at">x =</span> xy_test<span class="sc">$</span>x)),res[[i]], df)</span>
<span id="cb115-27"><a href="#cb115-27" aria-hidden="true" tabindex="-1"></a>  te[, i] <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">as.list</span>(<span class="fu">data.frame</span>(pred[[i]])),</span>
<span id="cb115-28"><a href="#cb115-28" aria-hidden="true" tabindex="-1"></a>              <span class="cf">function</span>(y_hat) <span class="fu">mean</span>((xy_test<span class="sc">$</span>y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb115-29"><a href="#cb115-29" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb115-30"><a href="#cb115-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-31"><a href="#cb115-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the average training and test errors</span></span>
<span id="cb115-32"><a href="#cb115-32" aria-hidden="true" tabindex="-1"></a>av_mse <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(mse)</span>
<span id="cb115-33"><a href="#cb115-33" aria-hidden="true" tabindex="-1"></a>av_te <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(te)</span>
<span id="cb115-34"><a href="#cb115-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-35"><a href="#cb115-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the errors</span></span>
<span id="cb115-36"><a href="#cb115-36" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(df, av_mse, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">gray</span>(<span class="fl">0.4</span>),</span>
<span id="cb115-37"><a href="#cb115-37" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Prediction error&quot;</span>,</span>
<span id="cb115-38"><a href="#cb115-38" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;The degrees of freedom (logged) of the model&quot;</span>,</span>
<span id="cb115-39"><a href="#cb115-39" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.7</span><span class="sc">*</span><span class="fu">min</span>(mse), <span class="fl">1.4</span><span class="sc">*</span><span class="fu">max</span>(mse)), <span class="at">log =</span> <span class="st">&quot;x&quot;</span>)</span>
<span id="cb115-40"><a href="#cb115-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_rep) {</span>
<span id="cb115-41"><a href="#cb115-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(df, te[, i], <span class="at">col =</span> <span class="st">&quot;lightyellow2&quot;</span>)</span>
<span id="cb115-42"><a href="#cb115-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb115-43"><a href="#cb115-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_rep) {</span>
<span id="cb115-44"><a href="#cb115-44" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(df, mse[, i], <span class="at">col =</span> <span class="fu">gray</span>(<span class="fl">0.8</span>))</span>
<span id="cb115-45"><a href="#cb115-45" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb115-46"><a href="#cb115-46" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(df, av_mse, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">gray</span>(<span class="fl">0.4</span>))</span>
<span id="cb115-47"><a href="#cb115-47" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(df, av_te, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;orange3&quot;</span>)</span>
<span id="cb115-48"><a href="#cb115-48" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="dv">1</span>], av_mse[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;palegreen3&quot;</span>, <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb115-49"><a href="#cb115-49" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="dv">1</span>], av_te[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;palegreen3&quot;</span>, <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb115-50"><a href="#cb115-50" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="fu">which.min</span>(av_te)], av_mse[<span class="fu">which.min</span>(av_te)],</span>
<span id="cb115-51"><a href="#cb115-51" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb115-52"><a href="#cb115-52" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="fu">which.min</span>(av_te)], av_te[<span class="fu">which.min</span>(av_te)],</span>
<span id="cb115-53"><a href="#cb115-53" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb115-54"><a href="#cb115-54" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="dv">20</span>], av_mse[<span class="dv">20</span>], <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb115-55"><a href="#cb115-55" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df[<span class="dv">20</span>], av_te[<span class="dv">20</span>], <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb115-56"><a href="#cb115-56" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;center&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Training error&quot;</span>, <span class="st">&quot;Test error&quot;</span>),</span>
<span id="cb115-57"><a href="#cb115-57" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">gray</span>(<span class="fl">0.4</span>), <span class="st">&quot;darkred&quot;</span>),</span>
<span id="cb115-58"><a href="#cb115-58" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="fl">0.3</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-7"></span>
<img src="graphics/5_7.png" alt="Prediction errors of the models (from `df`$=0$ to `df`$=20$) on the training dataset and testing dataset of $100$ replications. The two highlighted curves represent the mean curves of the $100$ replications of the training and testing error curves, respectively" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 85: Prediction errors of the models (from <code>df</code><span class="math inline">\(=0\)</span> to <code>df</code><span class="math inline">\(=20\)</span>) on the training dataset and testing dataset of <span class="math inline">\(100\)</span> replications. The two highlighted curves represent the mean curves of the <span class="math inline">\(100\)</span> replications of the training and testing error curves, respectively<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Again, we can see that the training error keeps decreasing when the model complexity increases, while the testing error curve has a U-shape. The key to identify the best model complexity is to locate the lowest point on the U-shaped error curve obtained from a testing dataset.</p>
<p>These experiments show that we need a testing dataset to evaluate the model to guide the model selection. Suppose you don’t have a testing dataset. The essence of a testing dataset is that it is not used for training the model. So you create one. What the hold-out, random sampling, and cross-validation approaches really do is use the training dataset to generate an estimate of the error curve that is supposed to be obtained from a testing dataset<label for="tufte-sn-127" class="margin-toggle sidenote-number">127</label><input type="checkbox" id="tufte-sn-127" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">127</span> See Figure <a href="#fig:f5-flowchart">92</a> and its associated text in the Remarks section for more discussion.</span>.</p>
<p>To see that, let’s consider a scenario that there are <span class="math inline">\(200\)</span> samples, and a client has split them into two parts, i.e., a training dataset with <span class="math inline">\(100\)</span> samples and a testing dataset with another <span class="math inline">\(100\)</span> samples. The client only sent the training dataset to us. So we use the <span class="math inline">\(10\)</span>-fold cross-validation on the training dataset, using the following R code, to evaluate the models (<code>df</code> from <span class="math inline">\(0\)</span> to <span class="math inline">\(20\)</span>).</p>
<p></p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation</span></span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(seed)</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>xy <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_train, coef, v_noise)</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> xy<span class="sc">$</span>x</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> xy<span class="sc">$</span>y</span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true" tabindex="-1"></a>fitted_models <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="fu">t</span>(df), <span class="dv">2</span>,</span>
<span id="cb116-10"><a href="#cb116-10" aria-hidden="true" tabindex="-1"></a>         <span class="cf">function</span>(degf) <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">ns</span>(x, <span class="at">df =</span> degf)))</span>
<span id="cb116-11"><a href="#cb116-11" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">sapply</span>(fitted_models,</span>
<span id="cb116-12"><a href="#cb116-12" aria-hidden="true" tabindex="-1"></a>         <span class="cf">function</span>(obj) <span class="fu">deviance</span>(obj)<span class="sc">/</span><span class="fu">nobs</span>(obj))</span>
<span id="cb116-13"><a href="#cb116-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-14"><a href="#cb116-14" aria-hidden="true" tabindex="-1"></a>n_test <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb116-15"><a href="#cb116-15" aria-hidden="true" tabindex="-1"></a>xy_test <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_test, coef, v_noise)</span>
<span id="cb116-16"><a href="#cb116-16" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">mapply</span>(<span class="cf">function</span>(obj, degf)</span>
<span id="cb116-17"><a href="#cb116-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(obj, <span class="fu">data.frame</span>(<span class="at">x =</span> xy_test<span class="sc">$</span>x)),</span>
<span id="cb116-18"><a href="#cb116-18" aria-hidden="true" tabindex="-1"></a>  fitted_models, df)</span>
<span id="cb116-19"><a href="#cb116-19" aria-hidden="true" tabindex="-1"></a>te <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">as.list</span>(<span class="fu">data.frame</span>(pred)),</span>
<span id="cb116-20"><a href="#cb116-20" aria-hidden="true" tabindex="-1"></a>   <span class="cf">function</span>(y_hat) <span class="fu">mean</span>((xy_test<span class="sc">$</span>y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb116-21"><a href="#cb116-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-22"><a href="#cb116-22" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb116-23"><a href="#cb116-23" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> n_train))</span>
<span id="cb116-24"><a href="#cb116-24" aria-hidden="true" tabindex="-1"></a>cv_tmp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n_folds, <span class="at">ncol =</span> <span class="fu">length</span>(df))</span>
<span id="cb116-25"><a href="#cb116-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb116-26"><a href="#cb116-26" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k)</span>
<span id="cb116-27"><a href="#cb116-27" aria-hidden="true" tabindex="-1"></a>  train_xy <span class="ot">&lt;-</span> xy[<span class="sc">-</span>test_i, ]</span>
<span id="cb116-28"><a href="#cb116-28" aria-hidden="true" tabindex="-1"></a>  test_xy <span class="ot">&lt;-</span> xy[test_i, ]</span>
<span id="cb116-29"><a href="#cb116-29" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> train_xy<span class="sc">$</span>x</span>
<span id="cb116-30"><a href="#cb116-30" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> train_xy<span class="sc">$</span>y</span>
<span id="cb116-31"><a href="#cb116-31" aria-hidden="true" tabindex="-1"></a>  fitted_models <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="fu">t</span>(df), <span class="dv">2</span>, <span class="cf">function</span>(degf) <span class="fu">lm</span>(y <span class="sc">~</span></span>
<span id="cb116-32"><a href="#cb116-32" aria-hidden="true" tabindex="-1"></a>                                     <span class="fu">ns</span>(x, <span class="at">df =</span> degf)))</span>
<span id="cb116-33"><a href="#cb116-33" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> test_xy<span class="sc">$</span>x</span>
<span id="cb116-34"><a href="#cb116-34" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> test_xy<span class="sc">$</span>y</span>
<span id="cb116-35"><a href="#cb116-35" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">mapply</span>(<span class="cf">function</span>(obj, degf) <span class="fu">predict</span>(obj, </span>
<span id="cb116-36"><a href="#cb116-36" aria-hidden="true" tabindex="-1"></a>                <span class="fu">data.frame</span>(<span class="fu">ns</span>(x, <span class="at">df =</span> degf))),</span>
<span id="cb116-37"><a href="#cb116-37" aria-hidden="true" tabindex="-1"></a>                fitted_models, df)</span>
<span id="cb116-38"><a href="#cb116-38" aria-hidden="true" tabindex="-1"></a>  cv_tmp[k, ] <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">as.list</span>(<span class="fu">data.frame</span>(pred)),</span>
<span id="cb116-39"><a href="#cb116-39" aria-hidden="true" tabindex="-1"></a>                <span class="cf">function</span>(y_hat) <span class="fu">mean</span>((y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb116-40"><a href="#cb116-40" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb116-41"><a href="#cb116-41" aria-hidden="true" tabindex="-1"></a>cv <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(cv_tmp)</span></code></pre></div>
<p></p>
<p>Then we can visualize the result in Figure <a href="#fig:f5-8">86</a> (the R script is shown below). Note that, in Figure <a href="#fig:f5-8">86</a>, we overlay the result of the <span class="math inline">\(10\)</span>-fold cross-validation (based on the <span class="math inline">\(100\)</span> training samples) with the prediction error on the testing dataset to get an idea about how closely the <span class="math inline">\(10\)</span>-fold cross-validation can approximate the testing error curve<label for="tufte-sn-128" class="margin-toggle sidenote-number">128</label><input type="checkbox" id="tufte-sn-128" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">128</span> Remember that, in practice, we will not have access to the testing data, but we want our model to succeed on the testing data, i.e., to obtain the lowest error on the testing error curve. Thus, using cross-validation to mimic this testing procedure based on our training data is a “rehearsal.”</span>.</p>
<p></p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;Hmisc&quot;)</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(Hmisc)</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(df, mse, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">gray</span>(<span class="fl">0.4</span>),</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Prediction error&quot;</span>, </span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;The degrees of freedom (logged) of the model&quot;</span>,</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">paste0</span>(n_folds,<span class="st">&quot;-fold Cross-Validation&quot;</span>),</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.8</span><span class="sc">*</span><span class="fu">min</span>(mse), <span class="fl">1.2</span><span class="sc">*</span><span class="fu">max</span>(mse)), <span class="at">log =</span> <span class="st">&quot;x&quot;</span>)</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(df, te, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;orange3&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>cv_sd <span class="ot">&lt;-</span> <span class="fu">apply</span>(cv_tmp, <span class="dv">2</span>, sd)<span class="sc">/</span><span class="fu">sqrt</span>(n_folds)</span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a><span class="fu">errbar</span>(df, cv, cv <span class="sc">+</span> cv_sd, cv <span class="sc">-</span> cv_sd, <span class="at">add =</span> <span class="cn">TRUE</span>,</span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;steelblue2&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, </span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a><span class="at">lwd =</span> <span class="fl">0.5</span>)</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(df, cv, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;steelblue2&quot;</span>)</span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(df, cv, <span class="at">col =</span> <span class="st">&quot;steelblue2&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>,</span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Training error&quot;</span>, <span class="st">&quot;Test error&quot;</span>,</span>
<span id="cb117-17"><a href="#cb117-17" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Cross-validation error&quot;</span>), </span>
<span id="cb117-18"><a href="#cb117-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">3</span>),</span>
<span id="cb117-19"><a href="#cb117-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">gray</span>(<span class="fl">0.4</span>), <span class="st">&quot;darkred&quot;</span>, <span class="st">&quot;steelblue2&quot;</span>), </span>
<span id="cb117-20"><a href="#cb117-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="fl">0.4</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-8"></span>
<img src="graphics/5_8.png" alt="Prediction errors of the models (from `df`$=0$ to `df`$=20$) on the training dataset without cross-validation, on the training dataset using $10$-fold cross-validation, and testing data of $100$ samples, respectively" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 86: Prediction errors of the models (from <code>df</code><span class="math inline">\(=0\)</span> to <code>df</code><span class="math inline">\(=20\)</span>) on the training dataset without cross-validation, on the training dataset using <span class="math inline">\(10\)</span>-fold cross-validation, and testing data of <span class="math inline">\(100\)</span> samples, respectively<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Overall, Figure <a href="#fig:f5-8">86</a> shows that the <span class="math inline">\(10\)</span>-fold cross-validation could generate fair evaluation of the models just like an independent and unseen testing dataset. Although its estimation of the error is smaller than the error estimation on the testing dataset, they both point towards the same range of model complexity that will neither overfit nor underfit the data.</p>
</div>
</div>
<div id="out-of-bag-error-in-random-forests" class="section level2 unnumbered">
<h2>Out-of-bag error in random forests</h2>
<p>The random forest model provides a concept named out-of-bag error that plays a similar role as the hold-out method. Let’s revisit how it works.</p>
<div id="rationale-and-formulation-8" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>Suppose that we have a training dataset of <span class="math inline">\(5\)</span> instances (IDs as <span class="math inline">\(1,2,3,4,5\)</span>). A random forest model with <span class="math inline">\(3\)</span> trees is built. The <span class="math inline">\(3\)</span> Bootstrapped datasets are shown in Table <a href="#tab:t5-2">16</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-2">Table 16: </span>Three trees and the bootstrapped datasets</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><em>Tree</em></th>
<th align="left">Bootstrap</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1,1,4,4,5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(2,3,3,4,4\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1,2,2,5,5\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Since Bootstrap <em>randomly</em> selects samples from the original dataset to form Bootstrapped datasets, some data points in the original dataset may not show up in the Bootstrapped datasets. These data points are called <strong>out-of-bag samples</strong> (<strong>OOB</strong>) samples. For instance, for the random forest model that corresponds to Table <a href="#tab:t5-2">16</a>, the OOB samples for each tree are shown in Table <a href="#tab:t5-3">17</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-3">Table 17: </span>Out-of-bag (OOB) samples</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><em>Tree</em></th>
<th align="left">OOB samples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2,3\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1,5\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(3,4\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The data points that are not used in training a tree could be used to test the tree. The errors on the OOB samples are called the <strong>out-of-bag errors</strong>. The OOB error can be calculated after a random forest model has been built, which seems to be computationally easier than cross-validation. An example to compute the OOB errors is shown in Table <a href="#tab:t5-OOB">18</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-OOB">Table 18: </span>Out-of-bag (OOB) errors</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Data ID</th>
<th align="left">True label</th>
<th align="left"><em>Tree</em> <span class="math inline">\(1\)</span></th>
<th align="left"><em>Tree</em> <span class="math inline">\(2\)</span></th>
<th align="left"><em>Tree</em> <span class="math inline">\(3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p>We can see that, as the data instance (<em>ID</em> = <span class="math inline">\(1\)</span>) is not used in training <em>Tree</em> <span class="math inline">\(2\)</span>, we can use <em>Tree</em> <span class="math inline">\(2\)</span> to predict on this data instance, and we see that it correctly predicts the class as <span class="math inline">\(C1\)</span>. Similarly, <em>Tree</em> <span class="math inline">\(1\)</span> is used to predict on data instance (<em>ID</em> = <span class="math inline">\(2\)</span>), and the prediction is wrong. Overall, the OOB error of the random forest model is <span class="math inline">\(1/6\)</span>.</p>
</div>
<div id="theory-and-method-5" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>The OOB error provides a computationally convenient approach to evaluate the random forest model without using a testing dataset or a cross-validation procedure. A technical concern is whether this idea can scale up. In other words, are there enough OOB samples to ensure that the OOB error is a fair and robust performance metric?</p>
<p>Recall that, for a random forest model with <span class="math inline">\(K\)</span> trees, each tree is built on a Bootstrapped dataset from the original training dataset <span class="math inline">\(D\)</span>. There are totally <span class="math inline">\(K\)</span> Bootstrapped datasets, denoted as <span class="math inline">\(B_{1,} B_{2}, \ldots, B_{K}\)</span>.</p>
<p>Usually, the size of each Bootstrapped dataset is the same size (denoted as <span class="math inline">\(N\)</span>) as the training dataset <span class="math inline">\(D\)</span>. Each data point in the Bootstrapped dataset is randomly and <em>independently</em> selected. Therefore, the probability of a data point from the training dataset <span class="math inline">\(D\)</span> missing from a Bootstrapped dataset is<label for="tufte-sn-129" class="margin-toggle sidenote-number">129</label><input type="checkbox" id="tufte-sn-129" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">129</span> Because there are <span class="math inline">\(N\)</span> independent trials of random selection, for a data point not to be selected, it has to be missed <span class="math inline">\(N\)</span> times. And the probability for “not to be selected” is <span class="math inline">\(\left(1-\frac{1}{N}\right)\)</span>.</span></p>
<p><span class="math display">\[\left(1-\frac{1}{N}\right)^{N}.\]</span></p>
<p>When <span class="math inline">\(N\)</span> is sufficiently large, we have</p>
<p><span class="math display">\[\lim _{N \rightarrow \infty}\left(1-\frac{1}{N}\right)^{N}=e^{-1} \approx 0.37.\]</span></p>
<p>Therefore, roughly <span class="math inline">\(37\%\)</span> of the data points from <span class="math inline">\(D\)</span> are not contained in a Bootstrapped dataset <span class="math inline">\(B_i\)</span>, and thus, not used for training the <em>tree</em> <span class="math inline">\(i\)</span>. These excluded data points are the OOB samples for the <em>tree</em> <span class="math inline">\(i\)</span>.</p>
<p>As there are <span class="math inline">\(37\%\)</span> of probability that a data point is not used for training a tree, we can infer that, on average, a data point is not used for training about <span class="math inline">\(37\%\)</span> of the trees<label for="tufte-sn-130" class="margin-toggle sidenote-number">130</label><input type="checkbox" id="tufte-sn-130" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">130</span> Note that the assumption is the Bootstrapped dataset has the same size as the original dataset, and the sampling is with replacement.</span>. In other words, for each data point, in theory <span class="math inline">\(37\%\)</span> of the trees are trained without this data point. This is a sizeable amount of data points, ensuring that the OOB error could be a stable and accurate evaluation of the model’s performance on <em>future unseen testing data</em>.</p>
</div>
<div id="r-lab-7" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We design a numeric study to compare the OOB error with the error obtained by a validation procedure and the error estimated on the training dataset. The three types of error rates are plotted in Figure <a href="#fig:f5-15">87</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-15"></span>
<img src="graphics/5_15.png" alt="Comparison of different types of error rates" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 87: Comparison of different types of error rates<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>First, we split the dataset into two halves: one for training and one for testing.</p>
<p></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(randomForest)</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb118-10"><a href="#cb118-10" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text =</span> <span class="fu">getURL</span>(url))</span>
<span id="cb118-11"><a href="#cb118-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-12"><a href="#cb118-12" aria-hidden="true" tabindex="-1"></a>target_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">==</span> <span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb118-13"><a href="#cb118-13" aria-hidden="true" tabindex="-1"></a>data[, target_indx] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, data[, target_indx]))</span>
<span id="cb118-14"><a href="#cb118-14" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>, <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb118-15"><a href="#cb118-15" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb118-16"><a href="#cb118-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-17"><a href="#cb118-17" aria-hidden="true" tabindex="-1"></a>para.v <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">150</span>, <span class="dv">200</span>)</span>
<span id="cb118-18"><a href="#cb118-18" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="cn">NULL</span></span></code></pre></div>
<p></p>
<p>Then, we build a set of random forest models by tuning the parameter <code>nodesize</code>, and obtain the OOB errors of the models.</p>
<p></p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB error</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (ipara <span class="cf">in</span> para.v) {</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">nodesize =</span> ipara, <span class="at">data =</span> data) </span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a><span class="co"># nodesize = inodesize</span></span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">c</span>(<span class="st">&quot;OOB_Error&quot;</span>, </span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a>                 ipara, <span class="fu">mean</span>(rf<span class="sc">$</span>err.rate[, <span class="st">&quot;OOB&quot;</span>])))</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p>We also use the random sampling method to evaluate the errors of the models.</p>
<p></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Validation error</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (ipara <span class="cf">in</span> para.v) {</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data), <span class="fu">floor</span>(<span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">nodesize =</span> ipara, </span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> data[train.ix, </span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true" tabindex="-1"></a>pred.test <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[<span class="sc">-</span>train.ix, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb120-9"><a href="#cb120-9" aria-hidden="true" tabindex="-1"></a>this.err <span class="ot">&lt;-</span> <span class="fu">length</span>(</span>
<span id="cb120-10"><a href="#cb120-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">which</span>(pred.test <span class="sc">!=</span> data[<span class="sc">-</span>train.ix, ]<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred.test)</span>
<span id="cb120-11"><a href="#cb120-11" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">c</span>(<span class="st">&quot;Validation_Error&quot;</span>, ipara, this.err))</span>
<span id="cb120-12"><a href="#cb120-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb120-13"><a href="#cb120-13" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p>Then, we obtain the training errors of the models.</p>
<p></p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training error</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (ipara <span class="cf">in</span> para.v) {</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">nodesize =</span> ipara, <span class="at">data =</span> data)  </span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a><span class="co"># nodesize = inodesize</span></span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a>this.err <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(pred <span class="sc">!=</span> data<span class="sc">$</span>DX_bl))<span class="sc">/</span><span class="fu">length</span>(pred)</span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">c</span>(<span class="st">&quot;Training_Error&quot;</span>, ipara, this.err))</span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-10"><a href="#cb121-10" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(results) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;type&quot;</span>, <span class="st">&quot;min_node_size&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb121-11"><a href="#cb121-11" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(results)</span>
<span id="cb121-12"><a href="#cb121-12" aria-hidden="true" tabindex="-1"></a>results<span class="sc">$</span>error <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(results<span class="sc">$</span>error))</span>
<span id="cb121-13"><a href="#cb121-13" aria-hidden="true" tabindex="-1"></a>results<span class="sc">$</span>min_node_size <span class="ot">&lt;-</span> <span class="fu">factor</span>(results<span class="sc">$</span>min_node_size,</span>
<span id="cb121-14"><a href="#cb121-14" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">unique</span>(results<span class="sc">$</span>min_node_size))</span>
<span id="cb121-15"><a href="#cb121-15" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> results,</span>
<span id="cb121-16"><a href="#cb121-16" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> min_node_size,</span>
<span id="cb121-17"><a href="#cb121-17" aria-hidden="true" tabindex="-1"></a>                            <span class="at">color =</span> type)) <span class="sc">+</span> </span>
<span id="cb121-18"><a href="#cb121-18" aria-hidden="true" tabindex="-1"></a>           <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f5-15">87</a> shows that the OOB error rates are reasonably aligned with the testing error rates, while the training error rates are deceptively smaller.</p>
<p>The following R code conducts another numeric experiment to see if the number of trees impacts the OOB errors. In particular, we compare <span class="math inline">\(50\)</span> trees with <span class="math inline">\(500\)</span> trees, with their OOB errors plotted in Figure <a href="#fig:f5-16">88</a>. On the other hand, we also observe that by increasing the number of trees, the OOB error decreases. This phenomenon is not universal (i.e., it is not always observed in all the datasets), but it does indicate the limitation of the OOB error: it is not as robust as the random sampling or cross-validation methods in preventing overfitting. But overall, the idea of OOB is inspiring.</p>
<p></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>para.v <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">150</span>, <span class="dv">200</span>)</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB error with 500 trees</span></span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (ipara <span class="cf">in</span> para.v) {</span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a>  rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">nodesize =</span> ipara, <span class="at">ntree =</span> <span class="dv">500</span>,</span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> data)  </span>
<span id="cb122-8"><a href="#cb122-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># nodesize = inodesize</span></span>
<span id="cb122-9"><a href="#cb122-9" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">c</span>(<span class="st">&quot;OOB_Error_500trees&quot;</span>, ipara,</span>
<span id="cb122-10"><a href="#cb122-10" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">mean</span>(rf<span class="sc">$</span>err.rate[,<span class="st">&quot;OOB&quot;</span>])))</span>
<span id="cb122-11"><a href="#cb122-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb122-12"><a href="#cb122-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-13"><a href="#cb122-13" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB error with 50 trees</span></span>
<span id="cb122-14"><a href="#cb122-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (ipara <span class="cf">in</span> para.v) {</span>
<span id="cb122-15"><a href="#cb122-15" aria-hidden="true" tabindex="-1"></a>  rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., <span class="at">nodesize =</span> ipara, <span class="at">ntree =</span> <span class="dv">50</span>,</span>
<span id="cb122-16"><a href="#cb122-16" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> data)  <span class="co"># nodesize = inodesize</span></span>
<span id="cb122-17"><a href="#cb122-17" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">c</span>(<span class="st">&quot;OOB_Error_50trees&quot;</span>, ipara,</span>
<span id="cb122-18"><a href="#cb122-18" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">mean</span>(rf<span class="sc">$</span>err.rate[,<span class="st">&quot;OOB&quot;</span>])))</span>
<span id="cb122-19"><a href="#cb122-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb122-20"><a href="#cb122-20" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(results) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;type&quot;</span>, <span class="st">&quot;min_node_size&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb122-21"><a href="#cb122-21" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(results)</span>
<span id="cb122-22"><a href="#cb122-22" aria-hidden="true" tabindex="-1"></a>results<span class="sc">$</span>error <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(results<span class="sc">$</span>error))</span>
<span id="cb122-23"><a href="#cb122-23" aria-hidden="true" tabindex="-1"></a>results<span class="sc">$</span>min_node_size <span class="ot">&lt;-</span> <span class="fu">factor</span>(results<span class="sc">$</span>min_node_size,</span>
<span id="cb122-24"><a href="#cb122-24" aria-hidden="true" tabindex="-1"></a>                                <span class="fu">unique</span>(results<span class="sc">$</span>min_node_size))</span>
<span id="cb122-25"><a href="#cb122-25" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> results,</span>
<span id="cb122-26"><a href="#cb122-26" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> min_node_size,</span>
<span id="cb122-27"><a href="#cb122-27" aria-hidden="true" tabindex="-1"></a>                            <span class="at">fill =</span> type)) <span class="sc">+</span> </span>
<span id="cb122-28"><a href="#cb122-28" aria-hidden="true" tabindex="-1"></a>           <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>,<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-16"></span>
<img src="graphics/5_16.png" alt="OOB error rates from random forests with a different number of trees" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 88: OOB error rates from random forests with a different number of trees<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
</div>
</div>
<div id="remarks-3" class="section level2 unnumbered">
<h2>Remarks</h2>
<!-- % ***More about cross-validation***: Usually, there is a relationship between the performance of the model on training dataset and its performance on testing dataset, as shown in Figure \@ref(fig:f5-11). Note that this relationship is theoretical, but has very high relevance with real applications. In our experiments, as shown in Figures \@ref(fig:f5-6) and \@ref(fig:f5-7), we have seen this relationship. This relationship predicts that, while the performance on the training data will decrease if we increase the model complexity, at a certain point, the gain on performance by increasing model complexity will stop. Beyond this point, the performance would be worse. Thus, a model that has a good performance on the training data and a reasonable complexity is likely to be among the best models that will perform well on the testing data (unseen).  -->
<div id="the-law-of-learning-errors" class="section level3 unnumbered">
<h3>The “law” of learning errors</h3>
<p>We have seen the <em>R-squared</em> could be manipulated to become larger, i.e., by adding into the model with more variables even if these variables are not predictive. This <em>bug</em> is not a special trait of the linear regression model only. The <em>R-squared</em> by its definition is computed based on the training data, and therefore, is essentially a <em>training error</em>. For any model that offers a flexible degree of complexity (e.g., examples are shown in Table <a href="#tab:t5-modelComplexity">19</a>), its <em>training error</em> could be decreased if we make the model more complex.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-modelComplexity">Table 19: </span>The complexity parameters of some models</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><strong>Model</strong></th>
<th align="left"><strong>Complexity parameter</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear regression</td>
<td align="left">Number of variables</td>
</tr>
<tr class="even">
<td align="left">Decision tree</td>
<td align="left">Depth of tree</td>
</tr>
<tr class="odd">
<td align="left">Random forest</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f5-traintest-tree"></span>
<p class="caption marginnote shownote">
Figure 89: A much more complex decision tree model than the one in Figure <a href="#fig:f3-tree-boundary">45</a>; (left) the tree model perfectly fits the <em>training data</em>; (right) the tree performs poorly on the <em>testing data</em>
</p>
<img src="graphics/5_traintest_tree.png" alt="A much more complex decision tree model than the one in Figure \@ref(fig:f3-tree-boundary); (left) the tree model perfectly fits the *training data*; (right) the tree performs poorly on the *testing data*" width="80%"  />
</div>
<p></p>
<!-- \caption[][-15mm]{The complexity parameters of some models}
 -->
<p>For example, let’s revisit the decision tree model shown in Figure <a href="#fig:f3-tree-boundary">45</a> in <strong>Chapter 3</strong>. A deeper tree segments the space into smaller rectangular regions, guided by the distribution of the <em>training data</em>, as shown in Figure <a href="#fig:f5-traintest-tree">89</a>. The model achieves <span class="math inline">\(100\%\)</span> accuracy—but this is an illusion, since the training data contains noise that could not be predicted. These rectangular regions, particularly those smaller ones, are susceptible to the noise. When we apply this deeper tree model on a <em>testing data</em> that is sampled from the same distribution of the <em>training data</em><label for="tufte-sn-131" class="margin-toggle sidenote-number">131</label><input type="checkbox" id="tufte-sn-131" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">131</span> The overall <em>morphology</em> of the two datasets looks alike; the differences, however, are due to the noise that is unpredictable.</span>, the model performs poorly.</p>
<p>It is generally true that the more complex a model gets, the lower the error on the training dataset becomes, as shown in Figure <a href="#fig:f5-law-errors">90</a> (left). This is the “law” of the <em>training error</em>, and training a model based on the training error could easily “spoil” the model. If there is a testing dataset, the error curve would look like <em>U-shaped</em>, as shown in Figure <a href="#fig:f5-law-errors">90</a> (middle), and the curve’s dip point helps us identify the best model complexity. While on the other hand, if there is no testing dataset, we could use cross-validation to obtain error estimates. The error curve obtained by cross-validation on the training data, as shown in
Figure <a href="#fig:f5-law-errors">90</a> (right), should provide a good approximation of the error curve of the testing data. The three figures in Figure <a href="#fig:f5-law-errors">90</a>, from left to right, illustrate a big picture of the <em>laws</em> of the errors and why some techniques such as the cross-validation have central importance in data analytics.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f5-law-errors"></span>
<img src="graphics/5_law_errors.png" alt="The law of learning errors" width="80%"  />
<p class="caption marginnote shownote">
Figure 90: The law of learning errors
</p>
</div>
<p></p>
<p>There are other approaches that play similar roles as the cross-validation, i.e., to approximate the error curve on unseen testing data. Examples include the <strong>Akaike information criterion</strong> (<strong>AIC</strong>), the <strong>Bayesian information criterion</strong> (<strong>BIC</strong>), and many other model selection criteria alike. Different from the cross-validation, they don’t resample the training data. Rather, they are analytic approaches that evaluate a model’s performance by offsetting the model’s training error with a complexity penalty, i.e., the more complex a model gets, the larger the penalty imposed. Skipping their mathematical details, Figure <a href="#fig:f5-AIC">91</a> illustrates the basic idea of these approaches.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f5-AIC"></span>
<img src="graphics/5_AIC.png" alt="The basic idea of the AIC and BIC criteria" width="80%"  />
<p class="caption marginnote shownote">
Figure 91: The basic idea of the AIC and BIC criteria
</p>
</div>
<p></p>
</div>
<div id="a-larger-view-of-model-selection-and-validation" class="section level3 unnumbered">
<h3>A larger view of <em>model selection and validation</em></h3>
<p>The practice of data analytics has evolved and developed an elaborate process to protect us from overfitting or underfitting a model. The 5-step process is illustrated in Figure <a href="#fig:f5-flowchart">92</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f5-flowchart"></span>
<p class="caption marginnote shownote">
Figure 92: A typical process of how data scientists work with clients to develop robust models
</p>
<img src="graphics/5_flowchart.png" alt="A typical process of how data scientists work with clients to develop robust models" width="80%"  />
</div>
<p></p>
<p>In the <span class="math inline">\(1^{st}\)</span> step, the client collects two datasets, one is the <em>training dataset</em> and another is the <em>testing dataset</em>.</p>
<p>In the <span class="math inline">\(2^{nd}\)</span> step, the client sends the <em>training dataset</em> to the data scientist to train the model. The client keeps the <em>testing dataset</em> for the client’s own use to test the final model submitted by the data scientist.</p>
<p>Now the data scientist should keep in mind that, no matter how the model is obtained<label for="tufte-sn-132" class="margin-toggle sidenote-number">132</label><input type="checkbox" id="tufte-sn-132" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">132</span> In a real application, you may try all you could think of to find your best model. Deep understanding of your models always help. Sometimes it is also luck, insight, and hard-working trial and error. What matters is your model is really good and can outperform your competitor’s. Data scientists survive in a harsh competitive environment.</span>, its goal is to predict well on the <em>unseen</em> <em>testing dataset</em>. How shall we do so, without access to the <em>testing dataset</em>?</p>
<!-- % ^[Three types of upset when we data scientists anxiously wait for results: for those who have the model as shown in the left panel of Figure \@ref(fig:f5-1), we know our model is under-performing, but it is better than random guess; for those who have the model as shown in the middle panel of Figure \@ref(fig:f5-1), we know we had been objective in training the model, the model should be fine, and we hope we had followed the right amount of balance and restrain to get the best model as we could; for those who have the model as shown in the right panel of Figure \@ref(fig:f5-1), if the twists of the curve around that few red squares still haven't raised red flags ... ]. -->
<!-- % After we build the model and deliver it to our client, the model will be evaluated on the testing dataset by the client. And our goal is to make sure that, although we don't have access to the testing dataset, the model we trained on the training dataset would succeed on the testing dataset as well  -->
<p>Just like in Bootstrap, we mimic the process.</p>
<p>In the <span class="math inline">\(3^{rd}\)</span> step, the data scientist mimics the testing procedure as the client would use. The data scientist splits the <em>training dataset</em> into two parts, one for model training and one for model testing<label for="tufte-sn-133" class="margin-toggle sidenote-number">133</label><input type="checkbox" id="tufte-sn-133" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">133</span> Generate a “training dataset” and a “testing dataset” from the <em>training dataset</em>. To avoid confusion, these two are often called <strong>internal</strong> training dataset and <strong>internal</strong> testing dataset, respectively. The training and testing datasets the client creates are often called <strong>external</strong> training dataset and <strong>external</strong> testing dataset, respectively.</span>.</p>
<p>In the <span class="math inline">\(4^{th}\)</span> step, the data scientist creates a model that should fit the <em>internal training dataset</em> well. Cross-validation is often used in this step.</p>
<p>In the <span class="math inline">\(5^{th}\)</span> step, the data scientist tests the model obtained in the <span class="math inline">\(4^{th}\)</span> step using the <em>internal testing data</em>. This is the final pass that will be conducted in house, before the final model is submitted to the client. Note that, the <span class="math inline">\(5^{th}\)</span> step could not be integrated into the model selection process conducted in the <span class="math inline">\(4^{th}\)</span> step—otherwise, the <em>internal testing data</em> is essentially used as an <em>internal training dataset</em><label for="tufte-sn-134" class="margin-toggle sidenote-number">134</label><input type="checkbox" id="tufte-sn-134" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">134</span> After all, the usage of the dataset dictates its name.</span>.</p>
<p>In the <span class="math inline">\(6^{th}\)</span> step, the data scientist submits the final model to the client. The model will be evaluated by the client on the <em>internal testing dataset</em>. The data scientist may or may not learn the evaluation result of the final model from the client.</p>
</div>
<div id="the-confusion-matrix" class="section level3 unnumbered">
<h3>The confusion matrix</h3>
<p>The <em>rare disease</em> example mentioned earlier in this chapter implies that the context matters. It also implies that <em>how we evaluate a model’s performance</em> matters as well.</p>
<p><em>Accuracy</em>, naturally, is a most important evaluation metric. As any <em>overall</em> evaluation metric, it averages things and blurs boundaries between categories, and for the same reason, it could be broken down into more <em>sub</em>categories. For example, a binary classification problem has two classes. We often care about specific accuracy on either class, i.e., if one class represents disease (positive) while another represents normal (negative), as a convention in medicine, we name the correct prediction on a positive case as <strong>true positive</strong> (<strong>TP</strong>) and name the correct prediction on a negative case as <strong>true negative</strong> (<strong>TN</strong>). Correspondingly, we define the <strong>false positive</strong> (<strong>FP</strong>) as incorrect prediction on a true negative case, and <strong>false negative</strong> (<strong>FN</strong>) as incorrect prediction on a true positive case. This is illustrated in Table <a href="#tab:t5-1">20</a>, the so-called <strong>confusion matrix</strong>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-1">Table 20: </span>The confusion matrix</span><!--</caption>--></p>
<table>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left"><strong>Reality</strong>: <em>Positive</em></td>
<td align="left"><strong>Reality</strong>: <em>Negative</em></td>
</tr>
<tr class="even">
<td align="left"><strong>Prediction</strong>: <em>Positive</em></td>
<td align="left">True positive (<strong>TP</strong>)</td>
<td align="left">False positive (<strong>FP</strong>)</td>
</tr>
<tr class="odd">
<td align="left"><strong>Prediction</strong>: <em>Negative</em></td>
<td align="left">False negative (<strong>FN</strong>)</td>
<td align="left">True negative (<strong>TN</strong>)</td>
</tr>
</tbody>
</table>
<p></p>
<p>Based on <em>TP</em>, the concept <strong>true positive rate</strong> (<strong>TPR</strong>) could also be defined, i.e., <em>TPR</em> = TP/(TP+FN). Similarly, we can also define the <strong>false positive rate</strong> (<strong>FPR</strong>) as FPR = FP/(FP+TN).</p>
</div>
<div id="the-roc-curve" class="section level3 unnumbered">
<h3>The ROC curve</h3>
<p>Building on the <em>confusion matrix</em>, the <strong>receiver operating characteristic curve</strong> (<strong>ROC curve</strong>) is an important evaluation metric for classification models.</p>
<p>Recall that, in a logistic regression model, before we make the final prediction, an intermediate result is obtained first</p>
<p><span class="math display">\[
p(\boldsymbol x)=\frac{1}{1+e^{-\left(\beta_{0}+\Sigma_{i=1}^{p} \beta_{i} x_{i}\right)}}.
\]</span></p>
<p>A <strong>cut-off value</strong><label for="tufte-sn-135" class="margin-toggle sidenote-number">135</label><input type="checkbox" id="tufte-sn-135" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">135</span> By default, <span class="math inline">\(0.5\)</span>.</span> is used to make the binary predictions, i.e., it classifies the cases whose <span class="math inline">\(p(\boldsymbol x)\)</span> are larger than the cut-off value as <em>positive</em>; otherwise, if <span class="math inline">\(p(\boldsymbol x)\)</span> is smaller than the cut-off value, <em>negative</em>. This means that, for each cut-off value, we can obtain a confusion matrix with different values of the TP, FP, FN, and TN. As there are many possible cut-off values, the <em>ROC curve</em> is a succinct way to synthesize all the scenarios of all possible cut-off values, i.e., it tries many cut-off values and plots the FPR (x-axis) against the TPR (y-axis). This is illustrated in Figure <a href="#fig:f5-12">93</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f5-12"></span>
<p class="caption marginnote shownote">
Figure 93: The logistic model produces an intermediate result <span class="math inline">\(p(\boldsymbol x)\)</span> for the cases of both classes: (left) shows the distributions of <span class="math inline">\(p(\boldsymbol x)\)</span> of both classes and a particular cut-off value; and (right) shows the ROC curve that synthesizes all the scenarios of all the cut-off values
</p>
<img src="graphics/5_12.png" alt="The logistic model produces an intermediate result $p(\boldsymbol x)$ for the cases of both classes: (left) shows the distributions of $p(\boldsymbol x)$ of both classes and a particular cut-off value; and (right) shows the ROC curve that synthesizes all the scenarios of all the cut-off values" width="80%"  />
</div>
<p></p>
<p>The ROC curve is more useful to evaluate a model’s <em>potential</em>, i.e., Figure <a href="#fig:f5-12">93</a> presents the performances of the logistic regression model for <em>all</em> cut-off values rather than <em>one</em> cut-off value. The <span class="math inline">\(45^{\circ}\)</span> line represents a model that is equivalent to <em>random guess</em>. In other words, the ROC curve of a model that lacks potential for prediction will be close to the <span class="math inline">\(45^{\circ}\)</span> line. A better model will show a ROC curve that is closer to the upper left corner point. Because of this, the <strong>area under the curve</strong> (<strong>AUC</strong>) is often used to summarize the ROC curve of a model. The higher the AUC, the better the model.</p>
<p><em>A Small Data Example.</em> Let’s study how a ROC curve could be created using an example. Consider a random forest model of <span class="math inline">\(100\)</span> trees and its prediction on <span class="math inline">\(9\)</span> data points. A random forest model uses the <em>majority voting</em> to aggregate the predictions of its trees to reach a final binary prediction. The <em>cut-off value</em> concerned here is the threshold of votes, i.e., here, we try three cut-off values, C=<span class="math inline">\(50\)</span> (default in <code>randomForest</code>), C=<span class="math inline">\(37\)</span>, and C=<span class="math inline">\(33\)</span>, as shown in Table <a href="#tab:t5-exampleROCrf">21</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-exampleROCrf">Table 21: </span>Prediction on <span class="math inline">\(9\)</span> data points via a random forest model of <span class="math inline">\(100\)</span> trees, with different cut-off values of the vote threshold, C=<span class="math inline">\(50\)</span> (default in <code>ra ndomForest</code>), C=<span class="math inline">\(37\)</span>, and C=<span class="math inline">\(33\)</span></span><!--</caption>--></p>
<table>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left">Vote</td>
<td align="left">True Label</td>
<td align="left">C=<span class="math inline">\(50\)</span></td>
<td align="left">C=<span class="math inline">\(37\)</span></td>
<td align="left">C=<span class="math inline">\(33\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(38\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(49\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(48\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(76\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(32\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(57\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(36\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(8\)</span></td>
<td align="left"><span class="math inline">\(36\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(9\)</span></td>
<td align="left"><span class="math inline">\(35\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Based on the definition of the confusion matrix in Table <a href="#tab:t5-1">20</a>, we calculate the metrics in Table <a href="#tab:t5-exampleROCrf2">22</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-exampleROCrf2">Table 22: </span>Metrics for predictions in Table <a href="#tab:t5-exampleROCrf">21</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">C=<span class="math inline">\(50\)</span></th>
<th align="left">C=<span class="math inline">\(37\)</span></th>
<th align="left">C=<span class="math inline">\(33\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Accuracy</td>
<td align="left"><span class="math inline">\(5/9\)</span></td>
<td align="left"><span class="math inline">\(6/9\)</span></td>
<td align="left"><span class="math inline">\(5/9\)</span></td>
</tr>
<tr class="even">
<td align="left">TP</td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
</tr>
<tr class="odd">
<td align="left">FP</td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
</tr>
<tr class="even">
<td align="left">FN</td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left">TN</td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left">FPR = FP/(FP+TN)</td>
<td align="left"><span class="math inline">\(1/(1+4)\)</span></td>
<td align="left"><span class="math inline">\(2/(2+3)\)</span></td>
<td align="left"><span class="math inline">\(4/(4+1)\)</span></td>
</tr>
<tr class="odd">
<td align="left">TPR = TP/(TP+FN)</td>
<td align="left"><span class="math inline">\(1/(1+3)\)</span></td>
<td align="left"><span class="math inline">\(3/(3+1)\)</span></td>
<td align="left"><span class="math inline">\(4/(4+0)\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>With three cut-off values, we map the three points in Figure <a href="#fig:f5-ROC">94</a> by plotting the <em>FPR</em> (x-axis) against the <em>TPR</em> (y-axis). There are a few R packages to generate a ROC curve for a classification model. Figure <a href="#fig:f5-ROC">94</a> illustrates the basic idea implemented in these packages to draw a ROC curve: sample a few cut-off values and map a few points in the figure, then draw a smooth curve that connects the point.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-ROC"></span>
<img src="graphics/5_ROC.png" alt="Illustration of how to draw a ROC curve using the data in Tables~\@ref(tab:t5-exampleROCrf) and~\@ref(tab:t5-exampleROCrf2)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 94: Illustration of how to draw a ROC curve using the data in Tables~<a href="#tab:t5-exampleROCrf">21</a> and~<a href="#tab:t5-exampleROCrf2">22</a><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>R Example.</em></p>
<p>We build a logistic regression model using the AD data as we have done in <strong>Chapter 3</strong>.</p>
<p></p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ROC and more performance metrics of logistic regression model</span></span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the AD dataset</span></span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb123-7"><a href="#cb123-7" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span>
<span id="cb123-8"><a href="#cb123-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb123-9"><a href="#cb123-9" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(AD)[<span class="dv">1</span>]</span>
<span id="cb123-10"><a href="#cb123-10" aria-hidden="true" tabindex="-1"></a>n.train <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fl">0.8</span> <span class="sc">*</span> n)</span>
<span id="cb123-11"><a href="#cb123-11" aria-hidden="true" tabindex="-1"></a>idx.train <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n.train)</span>
<span id="cb123-12"><a href="#cb123-12" aria-hidden="true" tabindex="-1"></a>AD.train <span class="ot">&lt;-</span> AD[idx.train,]</span>
<span id="cb123-13"><a href="#cb123-13" aria-hidden="true" tabindex="-1"></a>AD.test <span class="ot">&lt;-</span> AD[<span class="sc">-</span>idx.train,]</span>
<span id="cb123-14"><a href="#cb123-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-15"><a href="#cb123-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic selection of the model</span></span>
<span id="cb123-16"><a href="#cb123-16" aria-hidden="true" tabindex="-1"></a>logit.AD.full <span class="ot">&lt;-</span> <span class="fu">glm</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> AD.train[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>)], </span>
<span id="cb123-17"><a href="#cb123-17" aria-hidden="true" tabindex="-1"></a>                     <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb123-18"><a href="#cb123-18" aria-hidden="true" tabindex="-1"></a>logit.AD.final <span class="ot">&lt;-</span> <span class="fu">step</span>(logit.AD.full, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>, <span class="at">trace =</span> <span class="dv">0</span>)</span>
<span id="cb123-19"><a href="#cb123-19" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit.AD.final)</span></code></pre></div>
<p></p>
<p>Then we use the function, <code>confusionMatrix()</code> from the R package <code>caret</code> to obtain the confusion matrix</p>
<p></p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(e1071)</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(caret)</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prediction scores</span></span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(logit.AD.final, <span class="at">newdata=</span>AD.test,<span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data=</span><span class="fu">factor</span>(pred<span class="sc">&gt;</span><span class="fl">0.5</span>), <span class="fu">factor</span>(AD.test[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">1</span>))</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction FALSE TRUE</span></span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a><span class="do">##      FALSE    48    7</span></span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a><span class="do">##      TRUE      7   42</span></span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                Accuracy : 0.8654          </span></span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                  95% CI : (0.7845, 0.9244)</span></span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5288          </span></span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : 3.201e-13       </span></span>
<span id="cb125-12"><a href="#cb125-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb125-13"><a href="#cb125-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Kappa : 0.7299          </span></span>
<span id="cb125-14"><a href="#cb125-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 1               </span></span>
<span id="cb125-15"><a href="#cb125-15" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb125-16"><a href="#cb125-16" aria-hidden="true" tabindex="-1"></a><span class="do">##             Sensitivity : 0.8727          </span></span>
<span id="cb125-17"><a href="#cb125-17" aria-hidden="true" tabindex="-1"></a><span class="do">##             Specificity : 0.8571          </span></span>
<span id="cb125-18"><a href="#cb125-18" aria-hidden="true" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8727          </span></span>
<span id="cb125-19"><a href="#cb125-19" aria-hidden="true" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.8571          </span></span>
<span id="cb125-20"><a href="#cb125-20" aria-hidden="true" tabindex="-1"></a><span class="do">##              Prevalence : 0.5288          </span></span>
<span id="cb125-21"><a href="#cb125-21" aria-hidden="true" tabindex="-1"></a><span class="do">##          Detection Rate : 0.4615          </span></span>
<span id="cb125-22"><a href="#cb125-22" aria-hidden="true" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5288          </span></span>
<span id="cb125-23"><a href="#cb125-23" aria-hidden="true" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8649          </span></span>
<span id="cb125-24"><a href="#cb125-24" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb125-25"><a href="#cb125-25" aria-hidden="true" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : FALSE           </span></span>
<span id="cb125-26"><a href="#cb125-26" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
<p></p>
<p>The ROC curve could be drawn using the R Package <code>ROCR</code>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-13"></span>
<img src="graphics/5_13.png" alt="ROC curve of the logistic regression model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 95: ROC curve of the logistic regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the ROC curve using the testing data</span></span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute ROC and Precision-Recall curves</span></span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(<span class="st">&#39;ROCR&#39;</span>)</span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a>linear.roc.curve <span class="ot">&lt;-</span> <span class="fu">performance</span>(<span class="fu">prediction</span>(pred, AD.test[,<span class="dv">1</span>]),</span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a>                                <span class="at">measure=</span><span class="st">&#39;tpr&#39;</span>, <span class="at">x.measure=</span><span class="st">&#39;fpr&#39;</span> )</span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(linear.roc.curve,  <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;orange3&quot;</span>, </span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">&quot;Validation of the logistic model using testing data&quot;</span>)</span></code></pre></div>
<p></p>
<p>The ROC curve is shown in Figure <a href="#fig:f5-13">95</a>.</p>
</div>
</div>
<div id="exercises-3" class="section level2 unnumbered">
<h2>Exercises</h2>
<p><!-- begin{enumerate} --></p>
<ul>
<li> A random forest model is built on the training data with <span class="math inline">\(6\)</span> data points. The details of the trees and their bootstrapped datasets are shown in Table <a href="#tab:t5-hw-oob">23</a>.</li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-hw-oob">Table 23: </span>Bootstrapped datasets and the built trees</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Bootstrapped data</th>
<th align="left">Tree</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1,3,4,4,5,6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2,2,4,4,4,5\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1,2,2,5,6,6\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3,3,3,4,5,6\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>To calculate the out-of-bag (OOB) errors, which legitimate data points are to be used for each tree? You can mark them out in Table <a href="#tab:t5-hw-oob2">24</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t5-hw-oob2">Table 24: </span>Mark the elements where OOB errors could be collected</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Tree</th>
<th align="left">Bootstrapped data</th>
<th align="left"><span class="math inline">\(1(C1)\)</span></th>
<th align="left"><span class="math inline">\(2(C2)\)</span></th>
<th align="left"><span class="math inline">\(3(C2)\)</span></th>
<th align="left"><span class="math inline">\(4(C1)\)</span></th>
<th align="left"><span class="math inline">\(5(C2)\)</span></th>
<th align="left"><span class="math inline">\(6(C1)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1,3,4,4,5,6\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(2,2,4,4,4,5\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1,2,2,5,6,6\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(3,3,3,4,5,6\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li> Figure <a href="#fig:f5-hw-2ROC">96</a> shows the ROC curves of two classification models. Which model is better?</li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f5-hw-2ROC"></span>
<img src="graphics/5_2ROC.png" alt="The ROC curve of two models" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 96: The ROC curve of two models<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<ul>
<li><p> Follow up on the simulation experiment in Q9 in <strong>Chapter 2</strong> and the random forest model in Q5 in <strong>Chapter 4</strong>. Split the data into a training set and a testing test, then use <span class="math inline">\(10\)</span>-fold cross-validation to evaluate the performance of the random forest model with <span class="math inline">\(100\)</span> trees.</p></li>
<li><p> Follow up on Q3. Increase the sample size of the experiment to <span class="math inline">\(1000\)</span>, and comment on the result.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<div id="chapter-6.-diagnosis-residuals-heterogeneity" class="section level1 unnumbered">
<h1>Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</h1>
<div id="overview-4" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 6 is about <em>Diagnosis</em>. Diagnosis, in one sense, is to see if the assumptions of the model match the empirical characteristics of the data. For example, the t-test of linear regression model builds on the normality assumption of the errors. If this assumption is not met by the data, the result of the t-test is concerned. Departure from assumptions doesn’t always mean that the model is not useful<label for="tufte-sn-136" class="margin-toggle sidenote-number">136</label><input type="checkbox" id="tufte-sn-136" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">136</span> <em>“All models are wrong, some are useful.”—</em> George Box.</span>. The gap between the theoretical assumptions of the model and the empirical data characteristics, together with the model itself, should be taken as a whole when we evaluate the strength of the conclusion. This wholesome idea is what diagnosis is about. It also helps us to identify opportunities to improve the model. Models are representations/approximations of reality, so we have to be critical about them, yet being critical is different from being dismissive<label for="tufte-sn-137" class="margin-toggle sidenote-number">137</label><input type="checkbox" id="tufte-sn-137" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">137</span> A model that doesn’t fit the data also generates knowledge—revealed not by the failed model but by the fact that this model actually misfits. See Jaynes, E.T., <em>Probability Theory: the Logic of Science. </em> Cambridge Press, 2003.</span>. There are many diagnostic tools that we can use to strengthen our critical evaluation.</p>
<p>Many diagnostic tools focus on the <strong>residual analysis</strong>. Residuals provide a numerical evaluation of the difference between the model and the data. Recall that <span class="math inline">\(y\)</span> denotes the observed value of the outcome variable, <span class="math inline">\(f(\boldsymbol{x})\)</span> denotes the model, and <span class="math inline">\(\hat{y}\)</span> denotes the prediction (i.e., <span class="math inline">\(\hat{y} = f(\boldsymbol{x})\)</span> is the prediction made by the model on the data point <span class="math inline">\(\boldsymbol{x}\)</span>). The residual, denoted as <span class="math inline">\(\hat{\epsilon}\)</span>, is defined as <span class="math inline">\(\hat{\epsilon} = \hat{y} - y\)</span>. For any model that is trained on <span class="math inline">\(N\)</span> data points, we could obtain <span class="math inline">\(N\)</span> residuals, and draw the residuals as shown in Figure <a href="#fig:f6-3residuals">97</a>:</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-3residuals"></span>
<p class="caption marginnote shownote">
Figure 97: Suppose that three models are built on a dataset, and their residual plots are drawn: (left) decision tree; (middle) RF; (right) linear regression
</p>
<img src="graphics/6_residualplots.png" alt="Suppose that three models are built on a dataset, and their residual plots are drawn: (left) decision tree; (middle) RF; (right) linear regression" width="80%"  />
</div>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li> Figure <a href="#fig:f6-3residuals">97</a> (left). There is a linear relationship between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span>, which suggests an absurd fact: <span class="math inline">\(\hat{y}\)</span> could be used as a predictor to predict the <span class="math inline">\(\hat{\epsilon}\)</span>, the <em>error</em>. For instance, when <span class="math inline">\(\hat{y} = -1\)</span>, the error is between <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. If we adjust the prediction to be <span class="math inline">\(\hat{y} + 1\)</span>, wouldn’t that make the error to be between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>? A reduced error means a better prediction model.</li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-newmodel"></span>
<img src="graphics/6_newmodel.png" alt="A new model, inspired by the pattern seen in Figure \@ref(fig:f6-3residuals) (left)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 98: A new model, inspired by the pattern seen in Figure <a href="#fig:f6-3residuals">97</a> (left)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>To generalize this, let’s build another model <span class="math inline">\(g[f(\boldsymbol{x})]\)</span> that takes <span class="math inline">\(f(\boldsymbol{x})\)</span> as the predictor to predict <span class="math inline">\(\hat{\epsilon}\)</span>. Then, we can combine the two models, <span class="math inline">\(f(\boldsymbol{x})\)</span> and <span class="math inline">\(g[f(\boldsymbol{x})]\)</span>, and obtain an improved prediction <span class="math inline">\(\hat{y}\)</span> as <span class="math inline">\(f(\boldsymbol{x}) + g[f(\boldsymbol{x})]\)</span>. This is shown in Figure <a href="#fig:f6-newmodel">98</a>.</p>
<ul>
<li><p> Figure <a href="#fig:f6-3residuals">97</a> (middle). No correlation between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span> is observed. In other words, knowing <span class="math inline">\(\hat{y}\)</span> offers no help to predict <span class="math inline">\(\hat{\epsilon}\)</span>. This is what a good model would behave like.</p></li>
<li><p> Figure <a href="#fig:f6-3residuals">97</a> (right). There is a piece-wise linear relationship between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span>. If we segment the figure by a vertical line at zero, we could apply the same argument made in Figure <a href="#fig:f6-3residuals">97</a> (left) for each piece here: the model could be further improved following the same strategy outlined in Figure <a href="#fig:f6-newmodel">98</a>.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>As each data point contributes a residual, the <em>residual analysis</em> offers us opportunities to examine some collective phenomena to improve the overall quality of the model. It also helps us check local patterns where we may find areas of improvement of the model or particularities of the data that the model could not synthesize. The beauty of checking out the residuals is that there is always something that is beyond our experience and expectation.</p>
</div>
<div id="diagnosis-in-regression" class="section level2 unnumbered">
<h2>Diagnosis in regression</h2>
<div id="residual-analysis" class="section level3 unnumbered">
<h3>Residual analysis</h3>
<p>The R package <code>ggfortify</code> provides a nice bundle that includes the <strong>residual analysis, cook’s distance, leverage</strong>, and <strong>Q-Q plot</strong>.</p>
<p>Let’s use the final regression model we identified in <strong>Chapter 2</strong> for an example.</p>
<p></p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>AD<span class="sc">$</span>ID <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(AD)[<span class="dv">1</span>])</span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a full-scale model</span></span>
<span id="cb127-8"><a href="#cb127-8" aria-hidden="true" tabindex="-1"></a>AD_full <span class="ot">&lt;-</span> AD[,<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">17</span>)]</span>
<span id="cb127-9"><a href="#cb127-9" aria-hidden="true" tabindex="-1"></a>lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> ., <span class="at">data =</span> AD_full)</span>
<span id="cb127-10"><a href="#cb127-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD)</span>
<span id="cb127-11"><a href="#cb127-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic model selection</span></span>
<span id="cb127-12"><a href="#cb127-12" aria-hidden="true" tabindex="-1"></a>lm.AD.F <span class="ot">&lt;-</span> <span class="fu">step</span>(lm.AD, <span class="at">direction=</span><span class="st">&quot;backward&quot;</span>, <span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<p></p>
<p>Details of the model are shown below.</p>
<p></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="do">## MMSCORE ~ PTEDUCAT + FDG + AV45 + HippoNV + rs744373 + rs610932</span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="do">##     + rs3764650 + rs3865444</span></span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    </span></span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;                   1537.5 581.47                      </span></span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3764650  1     7.513 1545.0 581.99  2.4824  0.115750    </span></span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs744373   1    12.119 1549.6 583.53  4.0040  0.045924 *  </span></span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs610932   1    14.052 1551.6 584.17  4.6429  0.031652 *  </span></span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3865444  1    21.371 1558.9 586.61  7.0612  0.008125 ** </span></span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true" tabindex="-1"></a><span class="do">## - AV45       1    50.118 1587.6 596.05 16.5591 5.467e-05 ***</span></span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true" tabindex="-1"></a><span class="do">## - PTEDUCAT   1    82.478 1620.0 606.49 27.2507 2.610e-07 ***</span></span>
<span id="cb128-12"><a href="#cb128-12" aria-hidden="true" tabindex="-1"></a><span class="do">## - HippoNV    1   118.599 1656.1 617.89 39.1854 8.206e-10 ***</span></span>
<span id="cb128-13"><a href="#cb128-13" aria-hidden="true" tabindex="-1"></a><span class="do">## - FDG        1   143.852 1681.4 625.71 47.5288 1.614e-11 ***</span></span>
<span id="cb128-14"><a href="#cb128-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb128-15"><a href="#cb128-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span></code></pre></div>
<p></p>
<p>We use the <code>ggfortify</code> to produce <span class="math inline">\(6\)</span> diagnostic figures as shown in Figure <a href="#fig:f6-1">99</a>.</p>
<p></p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct diagnostics of the model</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;ggfortify&quot;)</span></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggfortify&quot;</span>)</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.AD.F, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-1"></span>
<p class="caption marginnote shownote">
Figure 99: Diagnostic figures of regression model on the AD dataset
</p>
<img src="graphics/6_1.png" alt="Diagnostic figures of regression model on the AD dataset" width="80%"  />
</div>
<p></p>
<p>The following is what we observe from Figure <a href="#fig:f6-1">99</a>.</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Figure <a href="#fig:f6-1">99</a> (upper left). This is the scatterplot of the residuals versus fitted values of the outcome variable. As we have discussed in Figure <a href="#fig:f6-3residuals">97</a>, this scatterplot is supposed to show purely random distributions of the dots. Here, we notice two abnormalities: (1) there is a relationship between the residuals and fitted values; and (2) there are unusual parallel lines<label for="tufte-sn-138" class="margin-toggle sidenote-number">138</label><input type="checkbox" id="tufte-sn-138" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">138</span> This is often observed if the outcome variable takes integer values.</span>. These abnormalities have a few implications: (1) the linear model <em>underfits</em> the data, so a nonlinear model is needed; (2) we have assumed that the data points are independent with each other, now this assumption needs to be checked; and (3) we have assumed <em>homoscedasticity</em><label for="tufte-sn-139" class="margin-toggle sidenote-number">139</label><input type="checkbox" id="tufte-sn-139" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">139</span> In <strong>Chapter 2</strong>, we assume that <span class="math inline">\(\epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right)\)</span>. It assumes the errors have the same variance, <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>, for all data points.</span> of the variance of the errors. This is another assumption that needs to be checked<label for="tufte-sn-140" class="margin-toggle sidenote-number">140</label><input type="checkbox" id="tufte-sn-140" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">140</span> To build nonlinear regression model or conditional variance regression model, see <strong>Chapter 9</strong>.</span>.</p></li>
<li><p> Figure <a href="#fig:f6-1">99</a> (upper right). The <strong>Q-Q plot</strong> checks the normality assumption of the errors. The <span class="math inline">\(45^{\circ}\)</span> line is a fixed <em>baseline</em>, while the dots correspond to the data points. If the normality assumption is met, the dots should align with the line. Here, we see mild departure of the data from the normality assumption. And some particular data points such as the data points <span class="math inline">\(282\)</span> and <span class="math inline">\(256\)</span> are labelled since they are outstanding<label for="tufte-sn-141" class="margin-toggle sidenote-number">141</label><input type="checkbox" id="tufte-sn-141" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">141</span> Are those points outliers? The Q-Q plot provides no conclusive evidence. It only suggests.</span>.</p></li>
<li><p> Figure <a href="#fig:f6-1">99</a> (middle left). This is a transformation of Figure <a href="#fig:f6-1">99</a> (upper left). Diagnostic tools are usually <em>opportunistic</em> approaches, i.e., what you see is what you get; if nothing particular is observed, it doesn’t mean there is no anomaly in the data. Changing perspectives is a common practice in model diagnosis.</p></li>
<li><p> Figure <a href="#fig:f6-1">99</a> (middle right). The <strong>Cook’s distance</strong> identifies influential data points that have <em>larger than average</em> influence on the parameter estimation of the model. For a data point <span class="math inline">\(\boldsymbol{x}_i\)</span>, its Cook’s distance <span class="math inline">\(D_{i}\)</span> is defined as the sum of all the changes in the regression model when <span class="math inline">\(\boldsymbol{x}_i\)</span> is removed from the training data. There is a closed-form formula<label for="tufte-sn-142" class="margin-toggle sidenote-number">142</label><input type="checkbox" id="tufte-sn-142" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">142</span> Cook, R.D., <em>Influential Observations in Linear Regression</em>, Journal of the American Statistical Association, Volume 74, Number 365, Pages 169-174, 1979.</span> to compute <span class="math inline">\({D_{i}, \text{ for } j=1,2,\dots,p}\)</span>, based on the <em>least squares estimator</em> of the regression parameters.</p></li>
<li><p> Figure <a href="#fig:f6-1">99</a> (lower left). The <strong>leverage</strong> of a data point, on the other hand, shows the influence of the data point in another way. The leverage of a data point is defined as <span class="math inline">\(\frac{\partial \hat{y}_{i}}{\partial y_{i}}\)</span>. This reflects how sensitively the prediction <span class="math inline">\(\hat{y}_{i}\)</span> is influenced by <span class="math inline">\(y_{i}\)</span>. What data point will have a larger leverage value? For those surrounded by many closeby data points, their leverages won’t be large: the impact of a data point’s removal in a dense neighborhood is limited, given many other similar data points nearby. It is the data points in sparsely occupied neighborhoods that have large leverages. These data points could either be outliers that severely deviate from the linear trend represented by the majority of the data points, or could be valuable data points that align with the linear trend but lack neighbor data points. Thus, a data point that is influential doesn’t necessarily imply it is an outlier, as shown in Figure <a href="#fig:f6-outlier-infl">100</a>. When a data point has a larger leverage value, in-depth examination of the data point is needed to determine which case it is.</p></li>
<li><p> Figure <a href="#fig:f6-1">99</a> (lower right). This is another form of showing the information that is presented in Figure <a href="#fig:f6-1">99</a> (middle right) and (lower left).</p></li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-outlier-infl"></span>
<img src="graphics/6_outlier_influ.png" alt="Outliers v.s. influential data points" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 100: Outliers v.s. influential data points<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- end{enumerate} --></p>
<p><em>A Simulation Experiment.</em> We simulate a dataset while all the assumptions of the linear regression model are met. The model is</p>
<p><span class="math display">\[ 
y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\varepsilon, \varepsilon \sim N(0,1).
\]</span></p>
<p>We simulate <span class="math inline">\(100\)</span> samples from this model.</p>
<p></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For comparison, let&#39;s simulate data </span></span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="co"># from a model that fits the assumptions</span></span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> beta1 <span class="sc">*</span> x1 <span class="sc">+</span> beta2 <span class="sc">*</span> x2</span>
<span id="cb130-8"><a href="#cb130-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, mu, <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p>We fit the data using linear regression model.</p>
<p></p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>lm.XY <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(y,x1,x2))</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.XY)</span></code></pre></div>
<p></p>
<p>The fitted model fairly reflects the underlying model.</p>
<p></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data.frame(y, x1, x2))</span></span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.6475 -0.6630 -0.1171  0.7986  2.5074 </span></span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)   0.0366     0.1089   0.336    0.738    </span></span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a><span class="do">## x1            0.9923     0.1124   8.825 4.60e-14 ***</span></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a><span class="do">## x2            0.9284     0.1159   8.011 2.55e-12 ***</span></span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.088 on 97 degrees of freedom</span></span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6225, Adjusted R-squared:  0.6147 </span></span>
<span id="cb132-19"><a href="#cb132-19" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 79.98 on 2 and 97 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.XY, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-2"></span>
<p class="caption marginnote shownote">
Figure 101: Diagnostic figures of regression model on a simulation dataset
</p>
<img src="graphics/6_2.png" alt="Diagnostic figures of regression model on a simulation dataset" width="80%"  />
</div>
<p></p>
<p>Then, we generate the diagnostic figures as shown in Figure <a href="#fig:f6-2">101</a>. Now Figure <a href="#fig:f6-2">101</a> provides a contrast of Figure <a href="#fig:f6-1">99</a>. For example, in Figure <a href="#fig:f6-2">101</a> (upper left), we don’t see a nonrandom statistical pattern. The relationship between the residual and fitted values seems to be null. From the <em>QQ-plot</em>, we see that the normality assumption is held well. On the other hand, from the <em>Cook’s distance</em> and the <em>leverage</em>, some data points are observed to be outstanding, which are labeled. As we simulated the data following the assumptions of the linear regression model, this experiment shows that it is normal to expect a few data points to show outstanding <em>Cook’s distance</em> and <em>leverage</em> values.</p>
<p></p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct diagnostics of the model</span></span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggfortify&quot;</span>)</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.XY, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
</div>
<div id="multicollinearity" class="section level3 unnumbered">
<h3>Multicollinearity</h3>
<p>Multicollinearity refers to the phenomenon that there is <em>a high correlation among the predictor variables</em>. This causes a serious problem for linear regression models. We can do a simple analysis. Consider a linear system shown below</p>
<p><span class="math display">\[ 
\begin{array}{c}{y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\cdots+\beta_{p} x_{p}+\varepsilon_y}, \\ {\varepsilon_y \sim N\left(0, \sigma_{\varepsilon_y}^{2}\right)}. \end{array}
\]</span></p>
<p>This looks like a regular linear regression model. However, here we further have</p>
<p><span class="math display">\[ 
\begin{array}{c}{x_{1}=2 x_{2}+\epsilon_x}, \\ {\epsilon_x \sim N\left(0, \sigma_{\varepsilon_x}^{2}\right).}\end{array}
\]</span></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-multilinear"></span>
<img src="graphics/6_multilinear.png" alt="The *data-generating mechanism* of a system that suffers from *multicollinearity*" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 102: The <em>data-generating mechanism</em> of a system that suffers from <em>multicollinearity</em><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>This <em>data-generating mechanism</em> is shown in Figure <a href="#fig:f6-multilinear">102</a>. It is a system that suffers from <em>multicollinearity</em>, i.e., if we apply a linear regression model on this system, the following models are <em>both</em> true models</p>
<p><span class="math display">\[ 
\begin{array}{c}{y=\beta_{0}+\left(2 \beta_{1}+\beta_{2}\right) x_{2}+\beta_{3} x_{3} \ldots+\beta_{p} x_{p}} \\ {y=\beta_{0}+\left(\beta_{1}+0.5 \beta_{2}\right) x_{1}+\beta_{3} x_{3}+\cdots+\beta_{p} x_{p}}\end{array}.
\]</span></p>
<p>The problem of multicollinearity results from an inherent ambiguity of the models that could be taken as faithful representation of the <em>data-generating mechanism</em>. If the <em>true</em> model is ambiguous, it is expected that an estimated model suffers from this problem as well.</p>
<p>There are some methods that we can use to diagnose <em>multicollinearity</em>. For instance, we may visualize the correlations among the predictor variables using the R package <code>corrplot</code>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-3"></span>
<img src="graphics/6_3.png" alt="Correlations of the predictors in the regression model of  `MMSCORE` " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 103: Correlations of the predictors in the regression model of <code>MMSCORE</code> <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the covariance matrix of the regression parameters</span></span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> <span class="fu">vcov</span>(lm.AD.F)</span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the correlation matrix of the estimated regression </span></span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a><span class="co"># parameters</span></span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;corrplot&quot;)</span></span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a><span class="fu">corrplot</span>(<span class="fu">cov2cor</span>(Sigma), <span class="at">method=</span><span class="st">&quot;ellipse&quot;</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f6-3">103</a> shows that there are significant correlations between the variables, <code>FDG</code>, <code>AV45</code>, and <code>HippoNV</code>, indicating a concern for multicollinearity. On the other hand, it seems that the correlations are moderate, and not all the variables are strongly correlated with each other.</p>
<p>It is of interest to see why the strong correlations among predictor variables cause problems in the <em>least squares</em> estimator of the regression coefficients. Recall that <span class="math inline">\(\widehat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y}\)</span>. If there are strong correlations among predictor variables, the matrix <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> is ill-conditioned, i.e., small changes on <span class="math inline">\(\boldsymbol{X}\)</span> result in large and unpredictable changes on the inverse matrix <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span>, which further causes instability of the parameter estimation in <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.<label for="tufte-sn-143" class="margin-toggle sidenote-number">143</label><input type="checkbox" id="tufte-sn-143" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">143</span> To overcome multicollinearity in linear regression, the <em>Principal Component Analysis</em> discussed in <strong>Chapter 8</strong> is useful.</span></p>
</div>
</div>
<div id="diagnosis-in-random-forests" class="section level2 unnumbered">
<h2>Diagnosis in random forests</h2>
<div id="residual-analysis-1" class="section level3 unnumbered">
<h3>Residual analysis</h3>
<p>We can use the <code>plotmo</code> package to perform residual analysis for a random forest model. For instance, we build a random forest model to predict the variable <code>AGE</code> in the AD dataset. We plot the residual versus the fitted values as shown in Figure <a href="#fig:f6-8">104</a> which shows there is a linear pattern between the fitted values and residuals. This indicates that this random forest model missed some linear relationship in the AD dataset.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-8"></span>
<img src="graphics/6_8.png" alt="Residuals versus fitted in the random forest model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 104: Residuals versus fitted in the random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(randomForest)</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(plotmo)</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD_hd.csv&quot;</span>)</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>target <span class="ot">&lt;-</span> data<span class="sc">$</span>AGE</span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> </span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>                   <span class="fu">c</span>(<span class="st">&quot;AGE&quot;</span>, <span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>, <span class="st">&quot;MMSCORE&quot;</span>,<span class="st">&quot;DX_bl&quot;</span>))</span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a>rf.mod <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(X, target)</span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plotres</span>(rf.mod, <span class="at">which =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>The random forest model doesn’t assume normality of its residuals. To make a comparison with the linear regression model, we draw the Q-Q plot of the random forest model in Figure <a href="#fig:f6-9">105</a>. It can be seen that the residuals deviate from the straight line.</p>
<p></p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plotres</span>(rf.mod, <span class="at">which =</span> <span class="dv">4</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-9"></span>
<img src="graphics/6_9.png" alt="The Q-Q plot of residuals of the random forest model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 105: The Q-Q plot of residuals of the random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>As the random forest model is an algorithmic modeling approach that imposes no analytic assumption, diagnosis could still be done but interpretations are not as strong as in a linear regression model. There is still value to do so, to find area of improvement of the model, e.g., as Figure <a href="#fig:f6-8">104</a> suggests the random forest model could be further improved to incorporate the linear pattern in the data.</p>
</div>
</div>
<div id="clustering" class="section level2 unnumbered">
<h2>Clustering</h2>
<div id="rationale-and-formulation-9" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><em>Clustering</em> takes the idea of <em>diagnosis</em> to a different level. If the <em>residual analysis</em> is like a tailor working out the perfect outfit for a client, clustering is … well, it is better to see Figure <a href="#fig:f6-twocluster-nd">106</a> first.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-twocluster-nd"></span>
<p class="caption marginnote shownote">
Figure 106: A tailor tries to (left) make an outfit (i.e., the normal curve) for a client (i.e., the data, represented as a histogram) vs. (right) then the tailor realizes the form of the outfit should be two normal curves
</p>
<img src="graphics/6_twocluster_nd.png" alt="A tailor tries to (left) make an outfit (i.e., the normal curve) for a client (i.e., the data, represented as a histogram) vs. (right) then the tailor realizes the form of the outfit should be two normal curves" width="80%"  />
</div>
<p></p>
<p>Figure <a href="#fig:f6-twocluster-nd">106</a> demonstrates one meaning of clustering: a dataset is heterogeneous and is probably collected from a few different populations (sometimes we call them <em>sub</em>populations). Understanding the clustering structure of a dataset not only benefits the statistical modeling, as shown in Figure <a href="#fig:f6-twocluster-nd">106</a> where we will use two normal distributions to model the data, but also reveals insights about the problem under study. For example, the dataset shown in Figure <a href="#fig:f6-twocluster-nd">106</a> was collected from a disease study of young children. It suggests that there are two disease mechanisms (we often call them two <em>phenotypes</em>). Phenotypes discovery is important for disease treatment, since patients with different disease mechanisms respond to treatments differently. A typical approach for phenotypes discovery is to collect an abundance of data from many patients. Then, we employ a range of algorithms to discover clusters of the data points. These clustering algorithms, differ from each other in their premises of what a cluster looks like, more or less bear the same conceptual framework as shown in Figure <a href="#fig:f6-twocluster-nd">106</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-twocluster-lr"></span>
<p class="caption marginnote shownote">
Figure 107: Another example of clustering: if the clustering structure is ignored, the fitted model (left) may show the opposite direction of the true model (right)
</p>
<img src="graphics/6_twocluster_lr.png" alt="Another example of clustering: if the clustering structure is ignored, the fitted model (left) may show the opposite direction of the true model (right) " width="80%"  />
</div>
<p></p>
<p>Clustering is a flexible concept that could be applied in other scenarios as well. Figure <a href="#fig:f6-twocluster-lr">107</a> demonstrates another meaning of clustering. It is less commonly perceived, but in practice it is not uncommon. The “moral of the story” shown in Figure <a href="#fig:f6-twocluster-lr">107</a> tells us that, when you have a dataset, you may want to conduct EDA and check the clustering structure first before imposing a model that may only fit the <em>data format</em> but not the <em>statistical structure</em><label for="tufte-sn-144" class="margin-toggle sidenote-number">144</label><input type="checkbox" id="tufte-sn-144" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">144</span> E.g., in Figure <a href="#fig:f6-twocluster-lr">107</a>: <em>data format</em>: we have predictors and outcome, so it seems natural to fit a linear regression model; <em>statistical structure</em>: however, it is a mix of two subpopulations that demand two models.</span>.</p>
</div>
<div id="theory-and-method-6" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>Given a dataset, how do we know there is a clustering structure? Consider the dataset shown in Table <a href="#tab:t6-example">25</a>. Are there <em>sub</em>populations as shown in Figure <a href="#fig:f6-twocluster-nd">106</a>?</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example">Table 25: </span>Example of a dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">$ $</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Value</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Cluster</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p>A visual check of the <span class="math inline">\(7\)</span> data points suggests there are probably two clusters. If each cluster can be modeled as a Gaussian distribution, this would be a two-component <strong>Gaussian Mixture Model</strong> (<strong>GMM</strong>)<label for="tufte-sn-145" class="margin-toggle sidenote-number">145</label><input type="checkbox" id="tufte-sn-145" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">145</span> A GMM consists of multiple Gaussian distributions. Figure <a href="#fig:f6-twocluster-nd">106</a> shows one example of two univariate Gaussian distributions mixed together. Generally, the parameters of a GMM are denoted as <span class="math inline">\(\boldsymbol{\Theta}\)</span>, which include the parameters of each Gaussian distribution: <span class="math inline">\(\mu_{i}\)</span> and <span class="math inline">\(\sigma_{i}\)</span> are the mean and variance of the <span class="math inline">\(i^{th}\)</span> Gaussian distribution, respectively, and <span class="math inline">\(\pi_{i}\)</span> is the proportion of the data points that were sampled from the <span class="math inline">\(i^{th}\)</span> Gaussian distribution. </span>.</p>
<p>In this particular dataset, clustering could be done by learning the parameters of the two-component (<strong>GMM</strong>), (i.e., to address the question marks in the last row of Table <a href="#tab:t6-example">25</a>). If we have known the parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span>, we could probabilistically infer which cluster each data point belongs to (i.e., to address the question marks in the second row of Table <a href="#tab:t6-example">25</a>). On the other hand, if we have known which cluster each data point belongs to, we can collect the data points of each cluster to estimate the parameters of the Gaussian distribution that characterizes each cluster. This “locked” relation between the two tasks is shown in Figure <a href="#fig:f6-cluster-cycle">108</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-cluster-cycle"></span>
<img src="graphics/6_cluster_cycle.png" alt="The locked relation between parameter estimation (M-step, i.e., last row of Table \@ref(tab:t6-example)) and data point inference (E-step, i.e., second row of Table \@ref(tab:t6-example)) in GMM" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 108: The locked relation between parameter estimation (M-step, i.e., last row of Table <a href="#tab:t6-example">25</a>) and data point inference (E-step, i.e., second row of Table <a href="#tab:t6-example">25</a>) in GMM<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The two interdependent tasks hold the key for each other. What is needed is <em>initialization</em>. As there are two blocks in Figure <a href="#fig:f6-cluster-cycle">108</a>, we have two locations to initialize the process of unlocking.</p>
<p><em>Initialization.</em> Let’s initialize the values in the second row of Table <a href="#tab:t6-example">25</a> for an example. We assign (i.e., <em>randomly</em>) labels on the data points as shown in Table <a href="#tab:t6-example-init">26</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-init">Table 26: </span>Initialization on the dataset example</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>M-step.</em> Then, we estimate <span class="math inline">\(\mu_{1}=1.75\)</span> and <span class="math inline">\(\sigma_{1}^{2}=2.83\)</span> based on the data points <span class="math inline">\(\{1.13, 4.76, 0.87, 1.03, 0.98\}\)</span>.<label for="tufte-sn-146" class="margin-toggle sidenote-number">146</label><input type="checkbox" id="tufte-sn-146" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">146</span> These <span class="math inline">\(5\)</span> data instances are initially assigned to <span class="math inline">\(C1\)</span>. Note that <span class="math inline">\(4.76\)</span> is different from the rest of the data points in the same cluster. This is an error introduced by the initialization. Later we will see that this error could be automatically fixed by the algorithm.</span></p>
<p>Similarly, we could estimate <span class="math inline">\(\mu_{2}=3.81\)</span> and <span class="math inline">\(\sigma_{2}^{2}=0.47\)</span> based on the data points <span class="math inline">\(\{3.32, 4.29\}\)</span>.</p>
<p>It is straightforward to estimate <span class="math inline">\(\pi_{1}=5/7 = 0.714\)</span> and <span class="math inline">\(\pi_{2}=2/7 = 0.286\)</span>.</p>
<p>Table <a href="#tab:t6-example-init">26</a> is updated.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-thetaupdated">Table 27: </span><span class="math inline">\(\boldsymbol{\Theta}\)</span> updated</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>E-step.</em> Since the labels of the data points were randomly initialized, they need to be updated given the latest estimation of <span class="math inline">\(\boldsymbol{\Theta}\)</span>. We continue to update the labels of the data points. To facilitate the presentation, we invent a binary indicator variable, denoted as <span class="math inline">\(z_{n m}\)</span>: <span class="math inline">\(z_{n m}=1\)</span> indicates that the data point <span class="math inline">\(x_{n}\)</span> was <em>assumed to be</em> sampled from the <span class="math inline">\(m^{th}\)</span> cluster; otherwise, <span class="math inline">\(z_{n m}=0\)</span>.</p>
<p>For example, if the first data point was sampled from the first cluster, the probability that <span class="math inline">\(x_1 = 1.13\)</span> is<label for="tufte-sn-147" class="margin-toggle sidenote-number">147</label><input type="checkbox" id="tufte-sn-147" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">147</span> In R, we could use the function <code>dnorm</code> to calculate it. For example, for <span class="math inline">\(p\left(x_{1} = 1.13| z_{11}=1 \right)\)</span>, we use <code>dnorm(1.13, mean = 1.75, sd = sqrt(2.83))</code> since <span class="math inline">\(\mu_{1}=1.75, \sigma_{1}^{2}=2.83\)</span>.</span></p>
<p><span class="math display">\[
    p\left(x_{1} = 1.13| z_{11}=1 \right)=0.22.
\]</span></p>
<p>And if the first data point was sampled from the second cluster, the probability that <span class="math inline">\(x_1 = 1.13\)</span> is</p>
<p><span class="math display">\[
    p\left(x_{1} = 1.13 | z_{12}=1 \right)=0.0003.
\]</span></p>
<p>Repeat it for all the other data points, we have:</p>
<p><!-- % \setlength{\belowdisplayskip}{0pt} % %%\setlength{\belowdisplayshortskip}{0pt} -->
<!-- % \setlength{\abovedisplayskip}{0pt} % %%\setlength{\abovedisplayshortskip}{0pt} --></p>
<p><span class="math display">\[
p\left(x_{2}=4.76 | z_{21}=1 \right)=0.05, p\left(x_{2}=4.76 | z_{22}=1 \right)=0.22;
\]</span></p>
<p><span class="math display">\[
p\left(x_{3}=0.87 | z_{31}=1 \right)=0.02, p\left(x_{3}=0.87 | z_{32}=1 \right)=0;
\]</span></p>
<p><span class="math display">\[
p\left(x_{4}=3.32 | z_{41}=1 \right)=0.15, p\left(x_{4}=3.32 | z_{42}=1 \right)=0.45;
\]</span></p>
<p><span class="math display">\[
p\left(x_{5}=4.29 | z_{51}=1 \right)=0.08, p\left(x_{5}=4.29 | z_{52}=1 \right)=0.45;
\]</span></p>
<p><span class="math display">\[
p\left(x_{6}=1.03 | z_{61}=1 \right)=0.22, p\left(x_{6}=1.03 | z_{62}=1 \right)=0.0001;
\]</span></p>
<p><span class="math display">\[
p\left(x_{7}=0.98 | z_{71}=1 \right)=0.21, p\left(x_{7}=0.98 | z_{72}=1 \right)=0.0001.
\]</span></p>
<!-- % \vspace{6pt} -->
<p>Note that we need to calculate “the probability of <em>which cluster</em> a data point was sampled from”<label for="tufte-sn-148" class="margin-toggle sidenote-number">148</label><input type="checkbox" id="tufte-sn-148" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">148</span> I.e., <span class="math inline">\(p\left(z_{11}=1 | x_1 = 1.13\right)\)</span>.</span>. This is different from the probabilities we have calculated as shown above, which concerns“if a data point was sampled from a cluster, then the probability of the <em>specific value</em> the data point took on”<label for="tufte-sn-149" class="margin-toggle sidenote-number">149</label><input type="checkbox" id="tufte-sn-149" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">149</span> I.e., <span class="math inline">\(p\left(x_{1} = 1.13| z_{11}=1 \right)\)</span>.</span>.</p>
<p>Thus, we further calculate the conditional probabilities of <span class="math inline">\(p\left(z_{i1} | x_i\right)\)</span></p>
<p><span class="math display">\[
p\left(z_{11}=1 | x_1 = 1.13 \right)=\frac{0.22 \times 0.714}{0.22 \times 0.714+0.0003 \times 0.286}=0.99; \text{ thus } x_1 \in C_1.
\]</span></p>
<p><span class="math display">\[
p\left(z_{21}=1 | x_2 = 4.76 \right)=\frac{0.05 \times 0.714}{0.05 \times 0.714+0.22 \times 0.286}=0.37; \text{ thus } x_2 \in C_2.
\]</span></p>
<p><span class="math display">\[
p\left(z_{31}=1 | x_3 = 0.87 \right)=\frac{0.02 \times 0.714}{0.02 \times 0.714+0.00 \times 0.286}=1; \text{ thus } x_3 \in C_1.
\]</span></p>
<p><span class="math display">\[
p\left(z_{41}=1 | x_4 = 3.32 \right)=\frac{0.15 \times 0.714}{0.15 \times 0.714+0.45 \times 0.286}=0.44; \text{ thus } x_4 \in C_2.
\]</span></p>
<p><span class="math display">\[
p\left(z_{51}=1 | x_5 = 4.29 \right)=\frac{0.08 \times 0.714}{0.08 \times 0.714+0.45 \times 0.286}=0.29; \text{ thus } x_5 \in C_2.
\]</span></p>
<p><span class="math display">\[
p\left(z_{61}=1 | x_6 = 1.03 \right)=\frac{0.22 \times 0.714}{0.22 \times 0.714+0.0001 \times 0.286}=0.99; \text{ thus } x_6 \in C_1.
\]</span></p>
<p><span class="math display">\[
p\left(z_{71}=1 | x_7 = 0.98 \right)=\frac{0.21 \times 0.714}{0.21 \times 0.714+0.0001 \times 0.286}=0.99; \text{ thus } x_7 \in C_1.
\]</span></p>
<!-- %\vspace{6pt} -->
<p>Table <a href="#tab:t6-example-thetaupdated">27</a> can be updated to Table <a href="#tab:t6-example-final">28</a>.</p>
<p>We can repeat this process and cycle through the two steps as shown in Figure <a href="#fig:f6-cluster-cycle">108</a>, until the process converges, i.e., <span class="math inline">\(\boldsymbol{\Theta}\)</span> remains the same (or its change is very small), or the labels of the data points remain the same. In this example, we actually only need one more iteration to reach convergence. This algorithm is a basic version of the so-called <strong>EM algorithm</strong>. Interested readers could find a complete derivation process in the <strong>Remarks</strong> section.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-final">Table 28: </span>Cluster labels updated</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
</div>
<div id="formal-definition-of-the-gmm" class="section level3 unnumbered">
<h3>Formal definition of the GMM</h3>
<p>As a <em>data modeling</em> approach, the GMM implies a <em>data-generating mechanism</em>, that is summarized in below.</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> [1.] Suppose that there are <span class="math inline">\(M\)</span> distributions mixed together.</p></li>
<li><p> [2.] In GMM, we assume that all the distributions are Gaussian distributions, i.e., the parameters of the <span class="math inline">\(m^{\text{th}}\)</span> distribution are <span class="math inline">\(\left\{\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m}\right\}\)</span>, and <span class="math inline">\(m=1,2, \ldots, M\)</span>.<label for="tufte-sn-150" class="margin-toggle sidenote-number">150</label><input type="checkbox" id="tufte-sn-150" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">150</span> <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span> is the mean vector; <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> is the covariance matrix.</span></p></li>
<li><p> [3.] For any data point <span class="math inline">\(\boldsymbol{x}\)</span>, without knowing its specific value, the prior probability that it comes from the <span class="math inline">\(m^{\text{th}}\)</span> distribution is denoted as <span class="math inline">\(\pi_m\)</span>.<label for="tufte-sn-151" class="margin-toggle sidenote-number">151</label><input type="checkbox" id="tufte-sn-151" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">151</span> In other words, this is the percentage of the data points in the whole mix that come from the <span class="math inline">\(m^{th}\)</span> distribution.</span> Note that <span class="math inline">\(\sum_{m=1}^{M} \pi_m=1\)</span>.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>The final distribution form of <span class="math inline">\(\boldsymbol{x}\)</span> is a mixed distribution with <span class="math inline">\(m\)</span> components</p>
<p><span class="math display">\[
\boldsymbol{x} \sim \pi_{1} N\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1}\right) + \pi_{2} N\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2}\right) + {\ldots} + \pi_{m} N\left(\boldsymbol{\mu}_{m}, \boldsymbol{\Sigma}_{m}\right).
\]</span></p>
<p>To learn the parameters from data, the EM algorithm is used. A basic walk-through of the EM algorithm has been given, i.e., see the example using Table <a href="#tab:t6-example">25</a>.</p>
</div>
<div id="r-lab-8" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We simulate a dataset with <span class="math inline">\(4\)</span> clusters as shown in Figure <a href="#fig:f6-4clusters">109</a>.</p>
<p></p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a clustering structure</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">10</span>,<span class="dv">2</span>), </span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>       <span class="fu">rnorm</span>(<span class="dv">200</span>,<span class="dv">20</span>,<span class="dv">1</span>), <span class="fu">rnorm</span>(<span class="dv">200</span>,<span class="dv">40</span>, <span class="dv">2</span>))</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(<span class="dv">800</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X,Y, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;gray25&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-4clusters"></span>
<img src="graphics/6_13.png" alt="A mixture of $4$ Gaussian distributions" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 109: A mixture of <span class="math inline">\(4\)</span> Gaussian distributions<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We use the R package <code>Mclust</code> to implement the GMM model using the EM algorithm.</p>
<p></p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use GMM to identify the clusters</span></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(mclust)</span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>XY.clust <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(<span class="fu">data.frame</span>(X,Y))</span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(XY.clust)</span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(XY.clust)</span></code></pre></div>
<p></p>
<p>We obtain the following result. Visualization of the identified clusters is shown in Figure <a href="#fig:f6-14">110</a>. Note that we didn’t specify the number of clusters in the analysis. <code>Mclust</code> used BIC and correctly identified the <span class="math inline">\(4\)</span> clusters. For each cluster, the data points are about <span class="math inline">\(200\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Gaussian finite mixture model fitted by EM algorithm </span></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Mclust VVI (diagonal, varying volume and shape) model with</span></span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 components:</span></span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a><span class="do">##  log.likelihood   n df       BIC       ICL</span></span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="do">##        -3666.07 800 19 -7459.147 -7459.539</span></span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Clustering table:</span></span>
<span id="cb140-12"><a href="#cb140-12" aria-hidden="true" tabindex="-1"></a><span class="do">##   1   2   3   4 </span></span>
<span id="cb140-13"><a href="#cb140-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 199 201 200 200</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-14"></span>
<img src="graphics/6_14.png" alt="Clustering results of the simulated data" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 110: Clustering results of the simulated data<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now let’s implement GMM on the AD data. Result is shown in Figure <a href="#fig:f6-15">111</a>.</p>
<p></p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;mclust&quot;)</span></span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(mclust)</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>AD.Mclust <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(AD[,<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">10</span>,<span class="dv">12</span>,<span class="dv">14</span>,<span class="dv">15</span>)])</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(AD.Mclust)</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>AD.Mclust<span class="sc">$</span>data <span class="ot">=</span> AD.Mclust<span class="sc">$</span>data[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)]</span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(AD.Mclust)</span></span>
<span id="cb141-7"><a href="#cb141-7" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb141-8"><a href="#cb141-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Gaussian finite mixture model fitted by EM algorithm </span></span>
<span id="cb141-9"><a href="#cb141-9" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb141-10"><a href="#cb141-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-11"><a href="#cb141-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Mclust EEI (diagonal, equal volume and shape) model </span></span>
<span id="cb141-12"><a href="#cb141-12" aria-hidden="true" tabindex="-1"></a><span class="do">## with 4 components:</span></span>
<span id="cb141-13"><a href="#cb141-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-14"><a href="#cb141-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  log.likelihood   n df       BIC       ICL</span></span>
<span id="cb141-15"><a href="#cb141-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       -3235.874 517 43 -6740.414 -6899.077</span></span>
<span id="cb141-16"><a href="#cb141-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-17"><a href="#cb141-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Clustering table:</span></span>
<span id="cb141-18"><a href="#cb141-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   1   2   3   4 </span></span>
<span id="cb141-19"><a href="#cb141-19" aria-hidden="true" tabindex="-1"></a><span class="do">##  43 253  92 129</span></span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-15"></span>
<p class="caption marginnote shownote">
Figure 111: Clustering results of the AD dataset
</p>
<img src="graphics/6_15.png" alt="Clustering results of the AD dataset" width="80%"  />
</div>
<p></p>
<p>Four clusters are identified as well. Figure <a href="#fig:f6-15">111</a> shows the boundaries between clusters are not as distinct as the boundaries in Figure <a href="#fig:f6-14">110</a>. In real applications, particularly for those applications of which we haven’t known enough, clustering is an exploration tool that could generate suggestive results but may not provide confirmatory conclusions.</p>
</div>
</div>
<div id="remarks-4" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="derivation-of-the-em-algorithm" class="section level3 unnumbered">
<h3>Derivation of the EM algorithm</h3>
<p>The aforementioned two-step iterative algorithm (i.e., as outlined in Figure <a href="#fig:f6-cluster-cycle">108</a>) illustrates how the <strong>EM Algorithm</strong> works. We have assumed that the two-step iterative algorithm would converge. Luckily, it had been proved that the EM Algorithm generally would converge<label for="tufte-sn-152" class="margin-toggle sidenote-number">152</label><input type="checkbox" id="tufte-sn-152" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">152</span> Wu, J., <em>On the Convergence Properties of the EM Algorithm</em>, The Annals of Statistics, Volume 11, Number 1, Pages 95-103, 1983.</span>.</p>
<!-- % Let's start with the Gaussian mixture model (GMM), that has been one of the most popular clustering model. GMM assumes that the data come from not just one distribution but a few. As shown in Figure \@ref(fig:f6-13) , the data is sampled from a mix of 4 distributions.  -->
<!-- % ```{r eval=FALSE,tidy=FALSE} -->
<!-- % # Simulate a clustering structure -->
<!-- % X <- c(rnorm(200, 0, 1), rnorm(200, 10,2), rnorm(200,20,1), rnorm(200,40, 2)) -->
<!-- % Y <- c(rnorm(800, 0, 1)) -->
<!-- % plot(X,Y, ylim = c(-5, 5), pch = 19, col = "gray25") -->
<!-- % ``` -->
<!-- % \begin{marginfigure} -->
<!-- %  \centering -->
<!-- %  \includegraphics{6_13.png} -->
<!-- %  \caption{A mixture of four Gaussian distributions} -->
<!-- %  \label{fig:6-13} -->
<!-- % \end{marginfigure} -->
<p>The task of the EM algorithm is to learn the unknown parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span> from a given dataset. The <span class="math inline">\(\boldsymbol{\Theta}\)</span> includes</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> [1.] The parameters of the <span class="math inline">\(M\)</span> Gaussian distributions: <span class="math inline">\(\left\{\boldsymbol{\mu}_{m}, \boldsymbol{\Sigma}_{m}, m=1,2, \ldots, M\right\}\)</span>.</p></li>
<li><p> [2.] The probability vector <span class="math inline">\(\boldsymbol{\pi}\)</span> that includes the elements <span class="math inline">\(\left\{\pi_{m}, m=1,2, \ldots, M\right\}\)</span>.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>Don’t forget the binary indicator variable for each data point, denoted as <span class="math inline">\(z_{n m}\)</span>: <span class="math inline">\(z_{n m}=1\)</span> indicates that the data point <span class="math inline">\(x_{n}\)</span> was sampled from the <span class="math inline">\(m^{th}\)</span> cluster<label for="tufte-sn-153" class="margin-toggle sidenote-number">153</label><input type="checkbox" id="tufte-sn-153" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">153</span> The reason that <span class="math inline">\(z_{n m}\)</span> is not included in <span class="math inline">\(\boldsymbol{\Theta}\)</span>, as it could be seen later, after the presentation of the EM algorithm, is that <span class="math inline">\(z_{n m}\)</span> provides a bridge to facilitate the learning of <span class="math inline">\(\boldsymbol{\Theta}\)</span>. They are not essential parameters of the model, although they are useful to facilitate the estimation of the parameters of the model. Entities like <span class="math inline">\(z_{n m}\)</span> are often called <strong>latent variables</strong> instead of <em>parameters</em>.</span>.</p>
<p><em>The Likelihood Function.</em> To learn these parameters from data, like in the logistic regression model, we derive a likelihood function to connect the data and parameters. For GMM, we cannot write <span class="math inline">\(p\left(\boldsymbol{x}_{n} | \boldsymbol{\Theta}\right)\)</span> directly. But it is possible to write <span class="math inline">\(p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right)\)</span> directly<label for="tufte-sn-154" class="margin-toggle sidenote-number">154</label><input type="checkbox" id="tufte-sn-154" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">154</span> That is what <span class="math inline">\(z_{n m}\)</span> is needed for.</span></p>
<p><span class="math display" id="eq:6-likelihood-xn">\[\begin{equation}
    p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) = \prod_{m=1}^{M}\left[p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right) p\left(z_{n m}=1\right)\right]^{z_{n m}}.
\tag{35}
\end{equation}\]</span></p>
<p>We apply <em>log</em> on Eq. <a href="#eq:6-likelihood-xn">(35)</a> and get the log-likelihood function in Eq <a href="#eq:6-loglike-xn">(36)</a><label for="tufte-sn-155" class="margin-toggle sidenote-number">155</label><input type="checkbox" id="tufte-sn-155" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">155</span> Note that, by definition, <span class="math inline">\(\pi_m = p\left(z_{n m}=1\right)\)</span>.</span></p>
<p><span class="math display" id="eq:6-loglike-xn">\[\begin{equation}
    \log p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) = \sum_{m=1}^{M}\left[z_{n m} \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right)+z_{n m} \log \pi_{m}\right]. 
\tag{36}
\end{equation}\]</span></p>
<p>It is known that<label for="tufte-sn-156" class="margin-toggle sidenote-number">156</label><input type="checkbox" id="tufte-sn-156" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">156</span> I.e., by the definition of multivariate normal distribution; interested readers may see the <strong>Appendix</strong> of this book for a brief review. Here, the constant term <span class="math inline">\((2 \pi)^{-p / 2}\)</span> in the density function of the multivariate normal distribution is ignored, so “<span class="math inline">\(\propto\)</span>” is used instead of “<span class="math inline">\(=\)</span>.”</span></p>
<p><span class="math display" id="eq:6-like-mnv-xn">\[\begin{equation}
    p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right) \propto \left|\boldsymbol{\Sigma}_{m}\right|^{-1 / 2} \exp \left\{-\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\right\}.
\tag{37}
\end{equation}\]</span></p>
<p>Plug Eq. <a href="#eq:6-like-mnv-xn">(37)</a> into Eq. <a href="#eq:6-loglike-xn">(36)</a>, we get</p>
<p><span class="math display" id="eq:6-loglike-xn2">\[\begin{equation}
\begin{gathered}
    \log p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) \propto \\
    \sum_{m=1}^{M}\left[z_{n m} \left( - \frac{1}{2}\log \left|\boldsymbol{\Sigma}_{m}\right|  -\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)+\right. z_{n m} \log \pi_{m} \right].
\end{gathered}
\tag{38}
\end{equation}\]</span></p>
<p>As there are <span class="math inline">\(N\)</span> data points, the complete log-likelihood function is defined as</p>
<p><span class="math display" id="eq:6-complete-loglike">\[\begin{equation}
    l(\boldsymbol{\Theta}) = \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\Theta}) = \log \prod_{n=1}^{N} p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right).
\tag{39}
\end{equation}\]</span></p>
<p>With Eq. <a href="#eq:6-loglike-xn2">(38)</a>, Eq. <a href="#eq:6-complete-loglike">(39)</a> can be rewritten as</p>
<p><span class="math display" id="eq:6-complete-loglike2">\[\begin{equation}
\begin{gathered}
    l(\boldsymbol{\Theta}) \propto \\
   \sum_{n=1}^{N} \sum_{m=1}^{M}\left[z_{n m} \left( - \frac{1}{2}\log \left|\boldsymbol{\Sigma}_{m}\right|  -\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)+\right. z_{n m} \log \pi_{m} \right].
\end{gathered}
\tag{40}
\end{equation}\]</span></p>
<p>Now we have an <em>explicit</em> form of <span class="math inline">\(l(\boldsymbol{\Theta})\)</span>, based on which we use an optimization algorithm to search for the best estimate of <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<p>Recall that <span class="math inline">\(z_{n m}\)</span> is unknown. Here comes the <em>initialization</em> again. Following the idea we have implemented in the data example shown in Table <a href="#tab:t6-example">25</a>, we propose the following strategy:</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Initialization. Either initialize <span class="math inline">\(\left\{z_{nm}, n=1,2, \ldots, N; m=1,2, \ldots, M\right\}\)</span> or <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p></li>
<li><p> E-step. We can estimate <span class="math inline">\(z_{n m}\)</span> if we have known <span class="math inline">\(\boldsymbol{\Theta}\)</span> (i.e., given <span class="math inline">\(\boldsymbol{\Theta}\)</span>), the best estimate of <span class="math inline">\(z_{n m}\)</span> is the expectation of <span class="math inline">\(z_{n m}\)</span> where the expectation is taken regarding the distribution <span class="math inline">\(p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)\)</span> (i.e., denoted as <span class="math inline">\(\left\langle Z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}\)</span>). By definition, we have</p></li>
</ul>
<p><span class="math display" id="eq:6-Eznm">\[\begin{equation}
\begin{gathered}
    \left\langle z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}=1\cdot p\left(z_{n m}=1 | \boldsymbol{x}_{n},{\boldsymbol{\Theta}}\right)+0 \cdot p\left(z_{n m}=0 | \boldsymbol{x}_{n}, {\boldsymbol{\Theta}}\right).
\end{gathered}
\tag{41}
\end{equation}\]</span></p>
<p>It is known that</p>
<p><span class="math display" id="eq:6-znm">\[\begin{equation}
    p\left(z_{n m}=1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)=\frac{p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right) \pi_{m}}{\sum_{k=1}^{M} p\left(\boldsymbol{x}_{n} | z_{n k}=1, \boldsymbol{\Theta}\right) \pi_{k}}.
\tag{42}
\end{equation}\]</span></p>
<p>Thus,</p>
<p><span class="math display" id="eq:6-Eznm2">\[\begin{equation}
\begin{gathered}
    \left\langle z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}= \frac{p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right) \pi_{m}}{\sum_{k=1}^{M}  p\left(\boldsymbol{x}_{n} | z_{n k}=1, \boldsymbol{\Theta}\right) \pi_{k}}.
\end{gathered}
\tag{43}
\end{equation}\]</span></p>
<ul>
<li> M-step. Then, we derive the expectation of <span class="math inline">\(l(\boldsymbol{\Theta})\)</span> regarding the distribution <span class="math inline">\(p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)\)</span></li>
</ul>
<p><span class="math display" id="eq:6-likehihood-eznm">\[\begin{equation}
\begin{gathered}
    \langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}=\sum_{n=1}^{N} \sum_{m=1}^{M}\left[\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)+\right. \\
    \left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \log \pi_{m} ].
\end{gathered}   
\tag{44}
\end{equation}\]</span></p>
<p>And we optimize Eq. <a href="#eq:6-likehihood-eznm">(44)</a> for <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<ul>
<li> Repeat the E-step and M-step. With the updated <span class="math inline">\(\boldsymbol{\Theta}\)</span>, we go back to the estimate of <span class="math inline">\(z_{n m}\)</span> using Eq. <a href="#eq:6-Eznm2">(43)</a>, and then, feed the new estimate of <span class="math inline">\(z_{n m}\)</span> into Eq. <a href="#eq:6-likehihood-eznm">(44)</a>, and solve for <span class="math inline">\(\boldsymbol{\Theta}\)</span> again. Repeat these iterations, until all the parameters in the iterations don’t change significantly<label for="tufte-sn-157" class="margin-toggle sidenote-number">157</label><input type="checkbox" id="tufte-sn-157" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">157</span> Usually, we define a tolerance, e.g., the difference between two consecutive estimates of <span class="math inline">\(\boldsymbol{\Theta}\)</span> is numerically bounded, such as <span class="math inline">\(10^{-4}\)</span>.</span>.</li>
</ul>
<p><!-- end{enumerate} --></p>
<p><em>More about the M-step.</em> To estimate the parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span>, in the M-step we use the First Derivative Test again and take derivatives of <span class="math inline">\(\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> (i.e., as shown in Eq. <a href="#eq:6-likehihood-eznm">(44)</a>) regarding <span class="math inline">\(\boldsymbol{\Theta}\)</span> and put the derivatives equal to zero.</p>
<p>For <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span>, we have</p>
<p><span class="math display" id="eq:6-diff-mu">\[\begin{equation}
    \frac{\partial\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}}{\partial \boldsymbol{\mu}_{m}}=\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\mu}_{m}}=\boldsymbol{0}.
\tag{45}
\end{equation}\]</span></p>
<p>Based on Eq. <a href="#eq:6-like-mnv-xn">(37)</a>, we can derive</p>
<p><span class="math display" id="eq:6-diff-mu2">\[\begin{equation}
    \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\mu}_{m}}=
    -\frac{1}{2} \frac{\partial\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)}{\partial \boldsymbol{\mu}_{m}}=\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}.
\tag{46}
\end{equation}\]</span></p>
<p>Putting the result of Eq. <a href="#eq:6-diff-mu2">(46)</a> into Eq. <a href="#eq:6-diff-mu">(45)</a>, we can estimate <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span> by solving Eq. <a href="#eq:6-diff-mu">(45)</a></p>
<p><span class="math display" id="eq:6-solution-mu">\[\begin{equation}
    \boldsymbol{\mu}_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \boldsymbol{x}_{n}}{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}.
\tag{47}
\end{equation}\]</span></p>
<p>Similarly, we take derivatives of <span class="math inline">\(\langle l({\boldsymbol{\Theta}})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> regarding <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> and put the derivatives equal to zero</p>
<p><span class="math display" id="eq:6-diff-sigma">\[\begin{equation}
    \frac{\partial\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}}{\partial \boldsymbol{\Sigma}_{m}}=\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\Sigma}_{m}}=\boldsymbol{O}.
\tag{48}
\end{equation}\]</span></p>
<p>Based on Eq. <a href="#eq:6-like-mnv-xn">(37)</a>, we can derive</p>
<p><span class="math display" id="eq:6-diff-sigma2">\[\begin{equation}
\begin{gathered}
    \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\Sigma}_{m}} =  \\
    \frac{1}{2} \frac{\partial\left\{\left|\boldsymbol{\Sigma}_{m}\right|^{-1 / 2}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\right\}}{\partial \boldsymbol{\Sigma}_{m}}=\frac{1}{2}\left[\boldsymbol{\Sigma}_{m}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right].
\end{gathered}
\tag{49}
\end{equation}\]</span></p>
<p>Plug Eq. <a href="#eq:6-diff-sigma2">(49)</a> into Eq. <a href="#eq:6-diff-sigma">(48)</a>, we have</p>
<p><span class="math display" id="eq:6-diff-sigma3">\[\begin{equation}
    \sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1 | \boldsymbol{X}, \boldsymbol{\Theta}\right)}\left[\boldsymbol{\Sigma}_{m}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right]=\boldsymbol{O}.
\tag{50}
\end{equation}\]</span></p>
<p>Solving Eq. <a href="#eq:6-diff-sigma3">(50)</a>, we estimate <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> as</p>
<p><span class="math display" id="eq:6-solution-sigma">\[\begin{equation}
    \boldsymbol{\Sigma}_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1| \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \left[\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right]}{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1| \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}.
\tag{51}
\end{equation}\]</span></p>
<p>Lastly, to estimate <span class="math inline">\(\pi_{m}\)</span>, recall that <span class="math inline">\(\pi_m\)</span> is the percentage of the data points in the whole mix that come from the <span class="math inline">\(m^{th}\)</span> distribution, and <span class="math inline">\(\pi_m = p\left(z_{n m}=1\right)\)</span>, we can estimate <span class="math inline">\(\pi_{m}\)</span> as</p>
<p><span class="math display" id="eq:6-solution-pi">\[\begin{equation}
    \pi_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}{N}.
\tag{52}
\end{equation}\]</span></p>
</div>
<div id="convergence-of-the-em-algorithm" class="section level3 unnumbered">
<h3>Convergence of the EM Algorithm</h3>
<p>Readers may have found that Eq. <a href="#eq:6-complete-loglike2">(40)</a> gives us the form of <span class="math inline">\(\log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\)</span>, that is what is denoted as <span class="math inline">\(l(\boldsymbol{\Theta})\)</span>. But, since <span class="math inline">\(\boldsymbol{Z}\)</span> is the latent variable and not part of the parameters, the objective function of the GMM model should be</p>
<p><span class="math display" id="eq:6-EMobj1">\[\begin{equation}
    \log p(\boldsymbol{X}| \boldsymbol{\Theta})=\log \int p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\Theta}) d \boldsymbol{Z}.
\tag{53}
\end{equation}\]</span></p>
<p>But this is not what has been done in the EM algorithm. Instead, the EM algorithm solves for Eq. <a href="#eq:6-likehihood-eznm">(44)</a>, that is essentially</p>
<p><span class="math display" id="eq:6-EMobj2">\[\begin{equation}
    \langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})} =
    \int \log p(\boldsymbol{X}, \boldsymbol{Z} ; \boldsymbol{\Theta}) p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta}) d \boldsymbol{Z}.
\tag{54}
\end{equation}\]</span></p>
<p>How does the solving of Eq. <a href="#eq:6-EMobj2">(54)</a> help the solving of Eq. <a href="#eq:6-EMobj1">(53)</a>?</p>
<p>The power of the EM algorithm draws on <strong>Jensen’s inequality</strong>. Let <span class="math inline">\(f\)</span> be a convex function defined on an interval <span class="math inline">\(I\)</span>. If <span class="math inline">\(x_{1}, x_{2}, \ldots x_{n} \in I \text { and } \gamma_{1}, \gamma_{2}, \ldots \gamma_{n} \geq0\)</span> with <span class="math inline">\(\sum_{i=1}^{n} \gamma_{i}=1\)</span>, then based on Jensen’s inequality, it is known that <span class="math inline">\(f\left(\sum_{i=1}^{n} \gamma_{i} x_{i}\right) \leq \sum_{i=1}^{n} \gamma_{i} f\left(x_{i}\right)\)</span>. Let’s apply this result to analyze the EM algorithm.</p>
<p>First, notice that</p>
<p><span class="math display">\[
\log p(\boldsymbol{X} | \boldsymbol \Theta)=\log \int p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}
\]</span></p>
<p><span class="math display">\[
=\log \int Q(\boldsymbol{Z}) \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
\]</span></p>
<p>Here, <span class="math inline">\(Q(\boldsymbol{Z})\)</span> is any distribution of <span class="math inline">\(\boldsymbol{Z}\)</span>. In the EM algorithm</p>
<p><span class="math display">\[Q(\boldsymbol{Z})=p(\boldsymbol{Z} | \boldsymbol{X}, \Theta).\]</span></p>
<p>Using Jensen’s inequality here, we have</p>
<p><span class="math display">\[
\log \int Q(\boldsymbol{Z}) \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}
\]</span></p>
<p><span class="math display">\[
\geq \int Q(\boldsymbol{Z}) \log \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
\]</span></p>
<p>Since</p>
<p><span class="math display">\[
\int Q(\boldsymbol{Z}) \log \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
\]</span></p>
<p><span class="math display">\[
=\int Q(\boldsymbol{Z}) \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}-\int Q(\boldsymbol{Z}) Q(\boldsymbol{Z}) d \boldsymbol{Z},
\]</span></p>
<p>and <span class="math inline">\(\int Q(\boldsymbol{Z}) Q(\boldsymbol{Z}) d \boldsymbol{Z}\)</span> is quadratic and thus non-negative,</p>
<p>our final result is</p>
<p><span class="math display" id="eq:6-JI">\[\begin{equation}
    \log p(\boldsymbol{X} | \boldsymbol \Theta) \geq \int Q(\boldsymbol{Z}) \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}.
\tag{55}
\end{equation}\]</span></p>
<p>When we set <span class="math inline">\(Q(\boldsymbol{Z}) = p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, Eq. <a href="#eq:6-JI">(55)</a> is rewritten as</p>
<p><span class="math display" id="eq:6-JI2">\[\begin{equation}
    \log p(\boldsymbol{X} | \boldsymbol \Theta) \geq \langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}.
\tag{56}
\end{equation}\]</span></p>
<p>Eq. <a href="#eq:6-JI2">(56)</a> reveals that <span class="math inline">\(\langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> is the <strong>lower bound</strong> of <span class="math inline">\(\log p(\boldsymbol{X} | \boldsymbol \Theta)\)</span>. Thus, maximization of <span class="math inline">\(\langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> can only increase the value of <span class="math inline">\(\log p(\boldsymbol{X} | \boldsymbol \Theta)\)</span>. This is why solving Eq. <a href="#eq:6-EMobj2">(54)</a> helps the solving of Eq. <a href="#eq:6-EMobj1">(53)</a>. This is the foundation of the effectiveness of the EM algorithm. The EM algorithm is often used to solve for problems that involve latent variables. Note that, <span class="math inline">\(Q(\boldsymbol{Z})\)</span> could be any distribution rather than <span class="math inline">\(p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, and Eq. <a href="#eq:6-JI">(55)</a> still holds. In applications where we could not explicitly derive <span class="math inline">\(p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, a surrogate distribution is used for <span class="math inline">\(Q(\boldsymbol{Z})\)</span>. This variant of the EM algorithm is called the <em>variational inference</em><label for="tufte-sn-158" class="margin-toggle sidenote-number">158</label><input type="checkbox" id="tufte-sn-158" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">158</span> A good starting point to know more about variational inference within a context of GMM, see: David, B., Kucukelbir, A. and McAuliffe, J., <em>Variational Inference: A Review for Statisticians</em>, Journal of the American Statistical Association, Volume 112, Number 518, Pages 859-877, 2017.</span>.</p>
</div>
<div id="clustering-by-random-forest" class="section level3 unnumbered">
<h3>Clustering by random forest</h3>
<p>Many clustering algorithms have been developed. The random forest model can be used for clustering as well. This is a byproduct utility of a random forest model. One advantage of using random forest for clustering is that it can cluster data points with mixed types of variables. To conduct clustering in random forests is to extract the distance information between data points that have been learned by the random forest model. There are multiple ways to do so. For example, one approach<label for="tufte-sn-159" class="margin-toggle sidenote-number">159</label><input type="checkbox" id="tufte-sn-159" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">159</span> Shi, T. and Horvath, S., <em>Unsupervised learning with random forest predictors.</em> Journal of Computational and Graphical Statistics, Volume 15, Issue 1, Pages 118-138, 2006.</span> that has been implemented in the R package <code>randomForests</code> is to generate a synthetic dataset with the same size as the original dataset, e.g., randomly generate the measurements of each variable using its empirical marginal distribution. The original dataset is taken as one class, while the synthetic dataset is taken as another class. Since the random forest model is used to classify the two classes, it will stress on the difference between the two datasets, which is, the variable dependency that is embedded in the original dataset but lost in the synthetic dataset because of the way the synthetic dataset is generated. Hence, each tree will be enriched with splitting variables that are dependent on other variables. After the random forest model is built, a distance between any pair of two data points can be calculated based on the frequency of this pair of data points existing in the same nodes of the random forest model. With this distance information, distance-based clustering algorithms such as the <em>hierarchical clustering</em> or <em>K-means clustering</em><label for="tufte-sn-160" class="margin-toggle sidenote-number">160</label><input type="checkbox" id="tufte-sn-160" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">160</span> E.g., both could be implemented using the R package <code>cluster</code>.</span> algorithms can be applied to detect the clusters.</p>
<p>In the following example, we generate a dataset with two clusters. The clusters produced from the random forest model are shown in Figure <a href="#fig:f6-16">112</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-16"></span>
<img src="graphics/6_16.png" alt="Clusters produced by the random forest model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 112: Clusters produced by the random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We then use the following R code to apply a random forest model on this dataset to find the clusters. It can be seen that the clusters are reasonably recovered by the random forest model.</p>
<p></p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>(<span class="at">all =</span> <span class="cn">TRUE</span>))</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb142-5"><a href="#cb142-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb142-6"><a href="#cb142-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb142-7"><a href="#cb142-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb142-8"><a href="#cb142-8" aria-hidden="true" tabindex="-1"></a>ndata <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb142-9"><a href="#cb142-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-10"><a href="#cb142-10" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb142-11"><a href="#cb142-11" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>), sigma)</span>
<span id="cb142-12"><a href="#cb142-12" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">2</span>), sigma)</span>
<span id="cb142-13"><a href="#cb142-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data1, data2)</span>
<span id="cb142-14"><a href="#cb142-14" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(data)</span>
<span id="cb142-15"><a href="#cb142-15" aria-hidden="true" tabindex="-1"></a>prox <span class="ot">&lt;-</span> rf<span class="sc">$</span>proximity</span>
<span id="cb142-16"><a href="#cb142-16" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">pam</span>(prox, <span class="dv">2</span>)</span>
<span id="cb142-17"><a href="#cb142-17" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(data)</span>
<span id="cb142-18"><a href="#cb142-18" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>cluster <span class="ot">&lt;-</span> <span class="fu">as.character</span>(clusters<span class="sc">$</span>clustering)</span>
<span id="cb142-19"><a href="#cb142-19" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> V1, <span class="at">y =</span> V2, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb142-20"><a href="#cb142-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Data points&#39;</span>)</span></code></pre></div>
<p></p>
</div>
<div id="clustering-based-prediction-models" class="section level3 unnumbered">
<h3>Clustering-based prediction models</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-clusterwisepred"></span>
<img src="graphics/6_clusterwisepred.png" alt="Clustering-based prediction models" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 113: Clustering-based prediction models<!--</p>-->
<!--</div>--></span>
</p>
<p>
<!-- %[width=0.6\textwidth] --></p>
<p>As we have mentioned, clustering is a flexible concept. And it could be used in a combination of methods. Figure <a href="#fig:f6-clusterwisepred">113</a> illustrates the basic idea of <em>clustering-based prediction models</em>. It applies a clustering algorithm first on the data and then builds a model for each cluster. As a data analytics strategy, we could combine different clustering algorithms and prediction models that are appropriate for an application context. There are also integrated algorithms that have articulated this strategy on the formulation level. For example, the <em>Treed Regression</em> method<label for="tufte-sn-161" class="margin-toggle sidenote-number">161</label><input type="checkbox" id="tufte-sn-161" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">161</span> Alexander, W. and Grimshaw, S., <em>Treed regression.</em> Journal of Computational and Graphical Statistics, Volume 5, Issue 2, Pages 156-175, 1996.</span>
is one example that proposed to build a tree to stratify the dataset first, and then, create regression models on the leaf nodes—here, each leaf node is a cluster. Similarly, the <em>logistic model trees</em><label for="tufte-sn-162" class="margin-toggle sidenote-number">162</label><input type="checkbox" id="tufte-sn-162" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">162</span> Landwehr, N., Hall, M. and Frank, E. <em>Logistic model trees.</em> Machine Learning, Volume 59, Issue 1, Pages 161–205, 2004.</span> also use a tree model to cluster data points into different leaf nodes and build different logistic regression model for each leaf node. Motivated by this line of thought, more models have been developed with different combination of tree models and prediction models (or other types of statistical models) on the leaf nodes<label for="tufte-sn-163" class="margin-toggle sidenote-number">163</label><input type="checkbox" id="tufte-sn-163" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">163</span> Gramacy, R. and Lee, H. <em>Bayesian treed Gaussian process models with an application to computer modeling.</em> Journal of American Statistical Association, Volume 103, Issue 483, Pages 1119-1130, 2008.</span>.<label for="tufte-sn-164" class="margin-toggle sidenote-number">164</label><input type="checkbox" id="tufte-sn-164" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">164</span> Liu, H., Chen, X., Lafferty, J. and Wasserman, L. <em>Graph-valued regression.</em> In the Proceeding of Advances in Neural Information Processing Systems 23 (NIPS), 2010.</span></p>
</div>
</div>
<div id="exercises-4" class="section level2 unnumbered">
<h2>Exercises</h2>
<p><!-- begin{enumerate} --></p>
<ul>
<li> In what follows is a summary of the clustering result on a dataset by using the R package <code>mclust</code>.</li>
</ul>
<p><!-- end{enumerate} --></p>
<p></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ---------------------------------------------------- </span></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Gaussian finite mixture model fitted by EM algorithm </span></span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a><span class="do">## ---------------------------------------------------- </span></span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Mclust VVV (ellipsoidal, varying volume, shape, </span></span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a><span class="do">## and orientation) model with 3</span></span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a><span class="do">## components: </span></span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb143-9"><a href="#cb143-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  log-likelihood   n df       BIC       ICL</span></span>
<span id="cb143-10"><a href="#cb143-10" aria-hidden="true" tabindex="-1"></a><span class="do">##       -2303.496 145 29 -4751.316 -4770.169</span></span>
<span id="cb143-11"><a href="#cb143-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb143-12"><a href="#cb143-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Clustering table:</span></span>
<span id="cb143-13"><a href="#cb143-13" aria-hidden="true" tabindex="-1"></a><span class="do">##  1  2  3 </span></span>
<span id="cb143-14"><a href="#cb143-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 81 36 28 </span></span>
<span id="cb143-15"><a href="#cb143-15" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb143-16"><a href="#cb143-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Mixing probabilities:</span></span>
<span id="cb143-17"><a href="#cb143-17" aria-hidden="true" tabindex="-1"></a><span class="do">##         1         2         3 </span></span>
<span id="cb143-18"><a href="#cb143-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.5368974 0.2650129 0.1980897 </span></span>
<span id="cb143-19"><a href="#cb143-19" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb143-20"><a href="#cb143-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Means:</span></span>
<span id="cb143-21"><a href="#cb143-21" aria-hidden="true" tabindex="-1"></a><span class="do">##              [,1]     [,2]       [,3]</span></span>
<span id="cb143-22"><a href="#cb143-22" aria-hidden="true" tabindex="-1"></a><span class="do">## glucose  90.96239 104.5335  229.42136</span></span>
<span id="cb143-23"><a href="#cb143-23" aria-hidden="true" tabindex="-1"></a><span class="do">## insulin 357.79083 494.8259 1098.25990</span></span>
<span id="cb143-24"><a href="#cb143-24" aria-hidden="true" tabindex="-1"></a><span class="do">## sspg    163.74858 309.5583   81.60001</span></span>
<span id="cb143-25"><a href="#cb143-25" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb143-26"><a href="#cb143-26" aria-hidden="true" tabindex="-1"></a><span class="do">## Variances:</span></span>
<span id="cb143-27"><a href="#cb143-27" aria-hidden="true" tabindex="-1"></a><span class="do">## [,,1]</span></span>
<span id="cb143-28"><a href="#cb143-28" aria-hidden="true" tabindex="-1"></a><span class="do">##          glucose    insulin       sspg</span></span>
<span id="cb143-29"><a href="#cb143-29" aria-hidden="true" tabindex="-1"></a><span class="do">## glucose 57.18044   75.83206   14.73199</span></span>
<span id="cb143-30"><a href="#cb143-30" aria-hidden="true" tabindex="-1"></a><span class="do">## insulin 75.83206 2101.76553  322.82294</span></span>
<span id="cb143-31"><a href="#cb143-31" aria-hidden="true" tabindex="-1"></a><span class="do">## sspg    14.73199  322.82294 2416.99074</span></span>
<span id="cb143-32"><a href="#cb143-32" aria-hidden="true" tabindex="-1"></a><span class="do">## [,,2]</span></span>
<span id="cb143-33"><a href="#cb143-33" aria-hidden="true" tabindex="-1"></a><span class="do">##           glucose   insulin       sspg</span></span>
<span id="cb143-34"><a href="#cb143-34" aria-hidden="true" tabindex="-1"></a><span class="do">## glucose  185.0290  1282.340  -509.7313</span></span>
<span id="cb143-35"><a href="#cb143-35" aria-hidden="true" tabindex="-1"></a><span class="do">## insulin 1282.3398 14039.283 -2559.0251</span></span>
<span id="cb143-36"><a href="#cb143-36" aria-hidden="true" tabindex="-1"></a><span class="do">## sspg    -509.7313 -2559.025 23835.7278</span></span>
<span id="cb143-37"><a href="#cb143-37" aria-hidden="true" tabindex="-1"></a><span class="do">## [,,3]</span></span>
<span id="cb143-38"><a href="#cb143-38" aria-hidden="true" tabindex="-1"></a><span class="do">##           glucose   insulin       sspg</span></span>
<span id="cb143-39"><a href="#cb143-39" aria-hidden="true" tabindex="-1"></a><span class="do">## glucose  5529.250  20389.09  -2486.208</span></span>
<span id="cb143-40"><a href="#cb143-40" aria-hidden="true" tabindex="-1"></a><span class="do">## insulin 20389.088  83132.48 -10393.004</span></span>
<span id="cb143-41"><a href="#cb143-41" aria-hidden="true" tabindex="-1"></a><span class="do">## sspg    -2486.208 -10393.00   2217.533</span></span></code></pre></div>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<ul>
<li><p> (a) How many samples in total are in this dataset? How many variables? (b) How many clusters are found? What are the sizes of the clusters? (c) What is the fitted GMM model? Please write its mathematical form.</p></li>
<li><p> Consider the dataset in Table <a href="#tab:t6-hw-q2">29</a> that has <span class="math inline">\(9\)</span> data points. Let’s use it to estimate a GMM model with <span class="math inline">\(3\)</span> clusters. The initial values are shown in Table <a href="#tab:t6-hw-q2">29</a></p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-hw-q2">Table 29: </span>Initial values for a GMM model with <span class="math inline">\(3\)</span> clusters</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(1.53\)</span></th>
<th align="left"><span class="math inline">\(0.57\)</span></th>
<th align="left"><span class="math inline">\(2.56\)</span></th>
<th align="left"><span class="math inline">\(1.22\)</span></th>
<th align="left"><span class="math inline">\(4.13\)</span></th>
<th align="left"><span class="math inline">\(6.03\)</span></th>
<th align="left"><span class="math inline">\(0.98\)</span></th>
<th align="left"><span class="math inline">\(5.21\)</span></th>
<th align="left"><span class="math inline">\(-0.37\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C3\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C3\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<ol style="list-style-type: lower-alpha">
<li>Write the Gaussian mixture model (GMM) that you want to estimate. (b) Estimate the parameters of your GMM model. (c) Update the labels with your estimated parameters. (d) Estimate the parameters again.</li>
</ol>
<ul>
<li><p> Follow up on the dataset in Q3. Use the R pipeline for clustering on this data. Compare the result from R and the result from your manual calculation.</p></li>
<li><p> Consider the dataset in Table <a href="#tab:t6-hw-q4">30</a> that has <span class="math inline">\(10\)</span> data points. Let’s use it to estimate a GMM model with <span class="math inline">\(3\)</span> clusters. The initial values are shown in Table <a href="#tab:t6-hw-q4">30</a></p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-hw-q4">Table 30: </span>Initial values for a GMM model with <span class="math inline">\(3\)</span> clusters</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(2.22\)</span></th>
<th align="left"><span class="math inline">\(6.33\)</span></th>
<th align="left"><span class="math inline">\(3.15\)</span></th>
<th align="left"><span class="math inline">\(-0.89\)</span></th>
<th align="left"><span class="math inline">\(3.21\)</span></th>
<th align="left"><span class="math inline">\(1.10\)</span></th>
<th align="left"><span class="math inline">\(1.58\)</span></th>
<th align="left"><span class="math inline">\(0.03\)</span></th>
<th align="left"><span class="math inline">\(8.05\)</span></th>
<th align="left"><span class="math inline">\(0.26\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C3\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C3\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<ol style="list-style-type: lower-alpha">
<li>Write the Gaussian mixture model (GMM) that you want to estimate. (b) Estimate the parameters of your GMM model. (c) Update the labels with your estimated parameters. (d) Estimate the parameters again.</li>
</ol>
<ul>
<li> Design a simulation experiment to test the effectiveness of the R package <code>mclust</code>. For instance, simulate a three-cluster structure in your dataset by this GMM model
<span class="math display">\[
\boldsymbol{x} \sim \pi_{1} N\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1}\right) + \pi_{2} N\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2}\right) + \pi_{3} N\left(\boldsymbol{\mu}_{3}, \boldsymbol{\Sigma}_{3}\right),
\]</span>
where <span class="math inline">\(\pi_{1} = 0.5\)</span>, <span class="math inline">\(\pi_{2} = 0.25\)</span>, and <span class="math inline">\(\pi_{3} = 0.25\)</span>, and</li>
</ul>
<p><span class="math display">\[
\boldsymbol{\mu}_{1} = \left[ \begin{array}{c}{5} \\ {3} \\ {3}\end{array}\right],  \text {     } \boldsymbol{\mu}_{2} = \left[ \begin{array}{c}{10} \\ {5} \\ {1}\end{array}\right], \text {     } \boldsymbol{\mu}_{3} = \left[ \begin{array}{c}{-5} \\ {10} \\ {-2}\end{array}\right];
\]</span></p>
<p><span class="math display">\[
\boldsymbol{\Sigma}_{1}=\left[\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right], \text {     } \boldsymbol{\Sigma}_{2}=\left[\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right], \text {     } 
\boldsymbol{\Sigma}_{3}=\left[\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right].
\]</span>
Then, use the <code>mclust</code> package on this dataset and see if the true clustering structure could be recovered.</p>
<ul>
<li> Follow up on the simulation experiment in Q6. Let’s consider a GMM model with larger variance
<span class="math display">\[
\boldsymbol{x} \sim \pi_{1} N\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1}\right) + \pi_{2} N\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2}\right) + \pi_{3} N\left(\boldsymbol{\mu}_{3}, \boldsymbol{\Sigma}_{3}\right),
\]</span>
where <span class="math inline">\(\pi_{1} = 0.5\)</span>, <span class="math inline">\(\pi_{2} = 0.25\)</span>, and <span class="math inline">\(\pi_{3} = 0.25\)</span>, and</li>
</ul>
<p><span class="math display">\[
\boldsymbol{\mu}_{1} = \left[ \begin{array}{c}{5} \\ {3} \\ {3}\end{array}\right],  \text {     } \boldsymbol{\mu}_{2} = \left[ \begin{array}{c}{10} \\ {5} \\ {1}\end{array}\right], \text {     } \boldsymbol{\mu}_{3} = \left[ \begin{array}{c}{-5} \\ {10} \\ {-2}\end{array}\right];
\]</span></p>
<p><span class="math display">\[
\boldsymbol{\Sigma}_{1}=\left[\begin{array}{ccc}3 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{array}\right], \text {     } \boldsymbol{\Sigma}_{2}=\left[\begin{array}{ccc}3 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{array}\right], \text {     } 
\boldsymbol{\Sigma}_{3}=\left[\begin{array}{ccc}3 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{array}\right].
\]</span>
Then, use the R package <code>mclust</code> on this dataset and see if the true clustering structure could be recovered.</p>
<ul>
<li><p> Design a simulation experiment to test the effectiveness of the diagnostic tools in the <code>ggfortify</code> R package. For instance, use the same simulation procedure that has been used in Q9 of <strong>Chapter 2</strong> to design a linear regression model with two variables, simulate <span class="math inline">\(100\)</span> samples from this model, fit the model, and draw the diagnostic figures.</p></li>
<li><p> Follow up on the simulation experiment in Q8. Add a few outliers into your dataset and see if the diagnostic tools in the <code>ggfortify</code> R package can detect them.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<div id="chapter-7.-learning-ii-svm-ensemble-learning" class="section level1 unnumbered">
<h1>Chapter 7. Learning (II): SVM &amp; Ensemble Learning</h1>
<div id="overview-5" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 7 revisits <em>learning</em> from a perspective that is different from <strong>Chapter 5</strong>. In <strong>Chapter 5</strong> we have introduced the concept of overfitting and the use of cross-validation as a safeguard mechanism to help us build models that don’t overfit the data. It focused on fair evaluation of the performances of a <em>specific model</em>. <strong>Chapter 7</strong>, taking on a process-oriented view of the issue of overfitting, focuses on performances of a <em>learning algorithm</em><label for="tufte-sn-165" class="margin-toggle sidenote-number">165</label><input type="checkbox" id="tufte-sn-165" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">165</span> Algorithms are computational procedures that learn models from data. They are processes.</span>. This chapter introduces two methods that aim to build a safeguard mechanism into the learning algorithms themselves. The two methods are the <strong>Support Vector Machine</strong> (<strong>SVM</strong>) and <strong>Ensemble Learning</strong><label for="tufte-sn-166" class="margin-toggle sidenote-number">166</label><input type="checkbox" id="tufte-sn-166" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">166</span> The random forest model is a typical example of ensemble learning</span>. While all models could overfit a dataset, these methods aim to reduce risk of overfitting based on their unique modeling principles.</p>
<p>In short, <strong>Chapter 5</strong> introduced evaluative methods that concern <em>if a model has learned from the data</em>. It is about quality assessment. <strong>Chapter 7</strong> introduces learning methods that concern <em>how to learn better from the data</em>. It is about quality improvement.</p>
</div>
<div id="support-vector-machine" class="section level2 unnumbered">
<h2>Support vector machine</h2>
<div id="rationale-and-formulation-10" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>A learning algorithm has an <em>objective function</em> and sometimes a set of <em>constraints</em>. The objective function corresponds to a quality of the learned model that could help it succeed on the unseen testing data. Eqs. <a href="#eq:2-multiLR-LS-matrix">(16)</a>, <a href="#eq:3-likelihood">(28)</a>, and <a href="#eq:6-complete-loglike2">(40)</a>, are examples of objective functions. They are developed based on the <em>likelihood principle</em>. Besides the likelihood principle, researchers have been studying what else quality a model should have and what objective function we should optimize to enhance this quality of the model. The constraints, on the other hand, guard the bottom line: the learned model needs to at least perform well on the training data so it is possible to perform well on future unseen data<label for="tufte-sn-167" class="margin-toggle sidenote-number">167</label><input type="checkbox" id="tufte-sn-167" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">167</span> The testing data, while unseen, is assumed to be statistically the same as the training data. This is a basic assumption in machine learning.</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-models"></span>
<img src="graphics/7_models.png" alt="Which model (i.e., here, which line) should we use as our classification model to separate the two classes of data points?" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 114: Which model (i.e., here, which line) should we use as our classification model to separate the two classes of data points?<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="#fig:f7-models">114</a> shows an example of a binary classification problem. The constraints here are obvious: the models should correctly classify the data points. And the <span class="math inline">\(3\)</span> models all perform well, while we hesitate to say that the <span class="math inline">\(3\)</span> models are equally good. Common sense tells us that Model <span class="math inline">\(3\)</span> is the least favorable. Unlike the other two, Model <span class="math inline">\(3\)</span> is close to a few data points. This makes Model <span class="math inline">\(3\)</span> bear a risk of misclassification on future unseen data: the locations of the existing data points provide a suggestion about where future unseen data may locate; but this is a suggestion, not a hard boundary.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-margins"></span>
<img src="graphics/7_margins.png" alt="The model that has a larger margin is better---the basic idea of SVM" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 115: The model that has a larger margin is better—the basic idea of SVM<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>In other words, the line of Model <span class="math inline">\(3\)</span> is too close to the data points and therefore lacks a safe <strong>margin</strong>. The concept of margin is shown in Figure <a href="#fig:f7-margins">115</a>. To reduce risk, we should have the margin as large as possible. The other two models have larger margins, and Model <span class="math inline">\(2\)</span> is the best because it has the largest margin.</p>
<p>In summary, while all the models shown in Figure <a href="#fig:f7-models">114</a> meet the <em>constraints</em> (i.e., perform well on the training data points), this is just the bottom line for a model to be good, and they are ranked differently based on an <em>objective function</em> that maximizes the margin of the model. This is the <strong>maximum margin</strong> principle invented in SVM.</p>
</div>
<div id="theory-and-method-7" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p><em>Derivation of the SVM formulation.</em></p>
<p>Consider a binary classification problem as shown in Figure <a href="#fig:f7-margins">115</a>. At this moment, we consider situations that all data points could be correctly classified by a line, which is clearly the case in Figure <a href="#fig:f7-models">114</a>. This is called <strong>the linearly separable case</strong>. Denote the data points as <span class="math inline">\(\left\{\left(x_{n}, y_{n}\right), n=1,2, \dots, N\right\}\)</span>. Here, the outcome variable <span class="math inline">\(y\)</span> is denoted as <span class="math inline">\(y_n \in \{1,-1\}\)</span>, i.e., <span class="math inline">\(y=1\)</span> denotes the circle points; <span class="math inline">\(y=-1\)</span> denotes the square points.</p>
<p>The mathematical model to represent a line is <span class="math inline">\(\boldsymbol{w}^{T} \boldsymbol{x}+b = 0\)</span>. Based on this form, we can segment the space into <span class="math inline">\(5\)</span> regions, as shown in Figure <a href="#fig:f7-5regions">116</a>. And by looking at the value of <span class="math inline">\(\boldsymbol{w}^{T} \boldsymbol{x}+b\)</span>, we know which region the data point <span class="math inline">\(\boldsymbol{x}\)</span> falls into. In other words, Figure <a href="#fig:f7-5regions">116</a> tells us a <em>classification rule</em></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-5regions"></span>
<img src="graphics/7_model_values.png" alt="The $5$ regions" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 116: The <span class="math inline">\(5\)</span> regions<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display" id="eq:7-DM-SVM">\[\begin{equation}
  \begin{aligned}
  \text { If } \boldsymbol{w}^{T} \boldsymbol{x}+b&gt;0, \text { then } y=1;  \\
  \text { Otherwise, } y=-1.
  \end{aligned}
\tag{57}
\end{equation}\]</span></p>
<p>Note that</p>
<p><span class="math display" id="eq:7-5regions">\[\begin{equation}
\begin{gathered}
    \text{For data points on the margin: } \left|\boldsymbol{w}^{T} \boldsymbol{x}+b\right|=1; \\
    \text {For data points beyond the margin: } \left|\boldsymbol{w}^{T} \boldsymbol{x}+b\right|&gt;1.
\end{gathered}
\tag{58}
\end{equation}\]</span></p>
<!-- % \begin{equation} -->
<!-- % \begin{aligned} -->
<!-- %  \text {Data points on the margin:} & \left|\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right|=1; \\ -->
<!-- %  \text {Data points beyond the margin:} & \left|\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right|>1. -->
<!-- %  (\#eq:7-SVMcons) -->
<!-- % \end{aligned} -->
<!-- % \end{equation} -->
<p>These two equations in Eq. <a href="#eq:7-5regions">(58)</a> provide the <em>constraints</em> for the SVM formulation, i.e., the bottom line for a model to be a good model. The two equations can be succinctly rewritten as one</p>
<p><span class="math display">\[
y\left(\boldsymbol{w}^{T} \boldsymbol{x}+b\right) \geq 1.
\]</span></p>
<p>Thus, a draft version of the SVM formulation is</p>
<p><span class="math display" id="eq:7-SVM-draft">\[\begin{equation}
\begin{gathered}
    \text{*Objective function*: Maximize Margin}, \\
    \text { *Subject to*: } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right) \geq 1 \text { for } n=1,2, \ldots, N.
\end{gathered}
\tag{59}
\end{equation}\]</span></p>
<p>The <em>objective function</em> is to maximize the <em>margin</em> of the model. Note that a model is characterized by its parameters <span class="math inline">\(\boldsymbol{w}\)</span> and <span class="math inline">\(b\)</span>. And the goal of Eq. <a href="#eq:7-SVM-draft">(59)</a> is to find the model—and therefore, the parameters—that maximizes the margin. In order to carry out this idea, we need the margin to be a concrete mathematical entity that <em>could be</em> characterized by the parameters <span class="math inline">\(\boldsymbol{w}\)</span> and <span class="math inline">\(b\)</span><label for="tufte-sn-168" class="margin-toggle sidenote-number">168</label><input type="checkbox" id="tufte-sn-168" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">168</span> Not all good ideas could be readily materialized in concrete mathematical forms. There is no guaranteed mathematical reality and if there is one it is always hard-earned.</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-margin-w"></span>
<img src="graphics/7_margin_w.png" alt="Illustration of the margin as a function of $\boldsymbol{w}$" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 117: Illustration of the margin as a function of <span class="math inline">\(\boldsymbol{w}\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We refer readers to the <strong>Remarks</strong> section to see details of how the margin is derived as a function of <span class="math inline">\(\boldsymbol{w}\)</span>. Figure <a href="#fig:f7-margin-w">117</a> shows the result: the margin of the model is <span class="math inline">\(\frac{2}{\|\boldsymbol{w}\|}\)</span>. Here, <span class="math inline">\(\|\boldsymbol{w}\|^{2} = \boldsymbol{w}^{T} \boldsymbol{w}\)</span>. And note that to <em>maximize the margin of a model</em> is equivalent to <em>minimize <span class="math inline">\(\|\boldsymbol{w}\|\)</span></em>. This gives us the objective function of the SVM model<label for="tufte-sn-169" class="margin-toggle sidenote-number">169</label><input type="checkbox" id="tufte-sn-169" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">169</span> Note that here we use <span class="math inline">\(\|\boldsymbol{w}\|^{2}\)</span> instead of <span class="math inline">\(\|\boldsymbol{w}\|\)</span>. This formulation is easier to solve.</span></p>
<p><span class="math display" id="eq:7-SVMobj">\[\begin{equation}
  \text {Maximize Margin} = \min _{\boldsymbol{w}} \frac{1}{2}\|\boldsymbol{w}\|^{2}.
\tag{60}
\end{equation}\]</span></p>
<p>Thus, the final SVM formulation is</p>
<p><span class="math display" id="eq:7-SVM">\[\begin{equation}
\begin{gathered}
    \min _{\boldsymbol{w}} \frac{1}{2}\|\boldsymbol{w}\|^{2}, \\
    \text { Subject to: } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right) \geq 1 \text { for } n=1,2, \ldots, N.
\end{gathered}
\tag{61}
\end{equation}\]</span></p>
<p><em>Optimization solution.</em></p>
<p>Eq. <a href="#eq:7-SVM">(61)</a> is called the <strong>primal formulation</strong> of SVM. To solve it, it is often converted into its dual form, the <strong>dual formulation</strong> of SVM. This could be done by the method of <strong>Lagrange multiplier</strong> that introduces a dummy variable, <span class="math inline">\(\alpha_{n}\)</span>, for each constraint, i.e., <span class="math inline">\(y_{n}\left(\boldsymbol{w}^{T}\boldsymbol{x}_{n}+b\right)\geq 1\)</span>, such that we could move the constraints into the objective function. By definition, <span class="math inline">\(\alpha_{n} \geq 0\)</span>.</p>
<p><span class="math display">\[ 
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\frac{1}{2}\|\boldsymbol{w}\|^{2}-\sum_{n=1}^{N} \alpha_{n}\left[y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1\right].
\]</span></p>
<p>This could be rewritten as</p>
<p><span class="math display" id="eq:7-SVM-lag">\[\begin{equation}
    L(\boldsymbol{w}, b, \boldsymbol{\alpha}) = \underbrace{\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}}_{(1)} - \underbrace{\sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{w}^{T} \boldsymbol{x}_{n}}_{(2)}-\underbrace{b \sum_{n=1}^{N} \alpha_{n} y_{n}}_{(3)}+\underbrace{\sum_{n=1}^{N} \alpha_{n}}_{(4)}.
\tag{62}
\end{equation}\]</span></p>
<p>Then we use the First Derivative Test again: differentiating <span class="math inline">\(L(\boldsymbol{w}, b, \boldsymbol{\alpha})\)</span> with respect to <span class="math inline">\(\boldsymbol{w} \text { and } b\)</span>, and setting them to <span class="math inline">\(0\)</span> yields the following solutions</p>
<p><span class="math display" id="eq:7-SVM-w">\[\begin{equation}
    \boldsymbol{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{x}_{n};
\tag{63}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:7-SVM-alpha">\[\begin{equation}
    \sum_{n=1}^{N} \alpha_{n} y_{n}=0.
\tag{64}
\end{equation}\]</span></p>
<p>Using the conclusion in Eq. <a href="#eq:7-SVM-w">(63)</a>, part (1) of Eq. <a href="#eq:7-SVM-lag">(62)</a> could be rewritten as</p>
<p><span class="math display">\[ 
\frac{1}{2} \boldsymbol{w}^{T} \boldsymbol{w}=\frac{1}{2} \boldsymbol{w}^{T} \sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{x}_{n}=\frac{1}{2} \sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{w}^{T} \boldsymbol{x}_{n}.
\]</span></p>
<p>It has the same form as part (2) of Eq. <a href="#eq:7-SVM-lag">(62)</a>. The two could be merged together into <span class="math inline">\(-\frac{1}{2} \sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{w}^{T} \boldsymbol{x}_{n}\)</span>. Note that<label for="tufte-sn-170" class="margin-toggle sidenote-number">170</label><input type="checkbox" id="tufte-sn-170" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">170</span> I.e., use the conclusion in Eq. <a href="#eq:7-SVM-w">(63)</a> again.</span></p>
<p><span class="math display">\[ 
\frac{1}{2} \sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{w}^{T} \boldsymbol{x}_{n}=\frac{1}{2} \sum_{n=1}^{N} \alpha_{n} y_{n}\left(\sum_{n=1}^{N} \alpha_{n} y_{n} \boldsymbol{x}_{n}\right)^{T} \boldsymbol{x}_{n}=\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} \boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}.
\]</span></p>
<p>Part (3) of Eq. <a href="#eq:7-SVM-lag">(62)</a>, according to the conclusion in Eq. <a href="#eq:7-SVM-alpha">(64)</a>, is <span class="math inline">\(0\)</span>.</p>
<p>Based on these results, we can rewrite <span class="math inline">\(L(\boldsymbol{w}, b, \boldsymbol{\alpha})\)</span> as</p>
<p><span class="math display">\[ 
L(\boldsymbol{w}, b, \boldsymbol{\alpha})=\sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} \boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}.
\]</span></p>
<p>This is the objective function of the dual formulation of Eq. <a href="#eq:7-SVM">(61)</a>. The decision variables are the Lagrange multipliers, the <span class="math inline">\(\boldsymbol{\alpha}\)</span>. By definition the Lagrange multipliers should be non-negative, and we have the constraint of the Lagrange multipliers described in Eq. <a href="#eq:7-SVM-alpha">(64)</a>. All together, the <strong>dual formulation</strong> of the SVM model is</p>
<p><span class="math display" id="eq:7-SVM-dual">\[\begin{equation}
\begin{gathered}
    \max _{\boldsymbol{\alpha}} \sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} \boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}, \\
    \text { Subject to: } \alpha_{n} \geq 0 \text { for } n=1,2, \dots, N \text {, and } \sum_{n=1}^{N} \alpha_{n} y_{n}=0.
\end{gathered}
\tag{65}
\end{equation}\]</span></p>
<p>This is a <em>quadratic programming</em> problem that can be solved using many existing well established algorithms.</p>
<p><em>Support vectors.</em></p>
<p>The data points that lay on the margins, as shown in Figure <a href="#fig:f7-sv">118</a>, are called <strong>support vectors</strong>. These geometrically unique data points are also found to be numerically interesting: in the solution of the dual formulation of SVM as shown in Eq. <a href="#eq:7-SVM-dual">(65)</a>, the <span class="math inline">\(\alpha_{n}\)</span>s that correspond to the support vectors are those that are nonzero. In other words, the data points that are not support vectors will have their <span class="math inline">\(\alpha_{n}\)</span>s to be zero in the solution of Eq. <a href="#eq:7-SVM-dual">(65)</a>.<label for="tufte-sn-171" class="margin-toggle sidenote-number">171</label><input type="checkbox" id="tufte-sn-171" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">171</span> Note that each data point contributes a constraint in the primal formulation of SVM, and therefore, corresponds to a <span class="math inline">\(\alpha_{n}\)</span> in the dual formulation.</span></p>
<p>If we revisit Eq. <a href="#eq:7-SVM-w">(63)</a>, we can see that only the nonzero <span class="math inline">\(\alpha_n\)</span> contribute to the estimation of <span class="math inline">\(\boldsymbol{w}\)</span>. Indeed, Figure <a href="#fig:f7-sv">118</a> shows that support vectors are sufficient to geometrically define the margins. And if we know the margins, the decision boundary is determined, i.e., as the central line in the middle of the two margins.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-sv"></span>
<img src="graphics/7_sv.png" alt="Support vectors are the data points that lay on the margins. In other words, the support vectors define the margins." width="250px"  />
<!--
<p class="caption marginnote">-->Figure 118: Support vectors are the data points that lay on the margins. In other words, the support vectors define the margins.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The support vectors hold crucial implications for the learned model. Theoretical evidences showed that the number of support vectors is a metric that can indicate the “healthiness” of the model, i.e., the smaller the total number of support vectors, the better the model. It also reveals that the main statistical information of a given dataset the SVM model uses is the support vectors. The number of support vectors is usually much smaller than the number of data points <span class="math inline">\(N\)</span>. Some works have been inspired to accelerate the SVM model training by discarding the data points that are probably not support vectors<label for="tufte-sn-172" class="margin-toggle sidenote-number">172</label><input type="checkbox" id="tufte-sn-172" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">172</span> If we can screen the data points before we solve Eq. <a href="#eq:7-SVM-dual">(65)</a> by discarding some data points that are not support vectors, the size of the optimization problem in Eq. <a href="#eq:7-SVM-dual">(65)</a> could be reduced.</span>. To understand why the nonzero <span class="math inline">\(\alpha_n\)</span> correspond to the support vectors, interested readers can find the derivation in the <strong>Remarks</strong> section.</p>
<p><em>Summary.</em> After solving Eq. <a href="#eq:7-SVM-dual">(65)</a>, we obtain the solutions of <span class="math inline">\(\boldsymbol{\alpha}\)</span>. With that, we estimate the parameter <span class="math inline">\(\boldsymbol{w}\)</span> based on Eq. <a href="#eq:7-SVM-w">(63)</a>. To estimate the parameter <span class="math inline">\(b\)</span>, we use any <em>support vector</em>, i.e., say, <span class="math inline">\((\boldsymbol{x}_{n}, y_n)\)</span>, and estimate <span class="math inline">\(b\)</span> by</p>
<p><span class="math display">\[\begin{equation*}
    \text{If } y_n = 1, b=1-\boldsymbol{w}^{T} \boldsymbol{x}_{n};
\end{equation*}\]</span></p>
<p><span class="math display" id="eq:7-SVM-b">\[\begin{equation}
    \text{If } y_n = -1, b=-1-\boldsymbol{w}^{T} \boldsymbol{x}_{n}.\tag{66}
\end{equation}\]</span></p>
<p><em>Extension to nonseparable cases.</em></p>
<p>We have assumed that the two classes are separable. Since this is impossible in some applications, we revise the SVM formulation—specifically, to revise the constraints of the SVM formulation—by allowing some data points to be within the margins or even on the wrong side of the decision boundary.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-slackvar"></span>
<img src="graphics/7_slackvar.png" alt="Behaviors of the slack variables" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 119: Behaviors of the slack variables<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Note that the original constraint structure in Eq. <a href="#eq:7-SVM">(61)</a> is derived based on the linearly separable case shown in Figure <a href="#fig:f7-models">114</a>. For the nonseparable case, Figure <a href="#fig:f7-slackvar">119</a> shows three scenarios: the <em>Type A</em> data points fall within the margins but still on the right side of their class, the <em>Type B</em> data points fall on the wrong side of their class, and the <em>Type C</em> data points fall on the right side of their class and also beyond or on the margin.</p>
<p>The <em>Type A</em> data points and the <em>Type B</em> data points are both <em>compromised</em>, and we introduce a <strong>slack variable</strong> to describe the <em>degree</em> of compromise for both types of data points.</p>
<p>For instance, consider the circle points that belong to the class (<span class="math inline">\(y_n=1\)</span>), we have<label for="tufte-sn-173" class="margin-toggle sidenote-number">173</label><input type="checkbox" id="tufte-sn-173" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">173</span> Readers may revisit Figure <a href="#fig:f7-5regions">116</a> to understand Eq. <a href="#eq:7-SVMcons-a">(67)</a>.</span></p>
<p><span class="math display" id="eq:7-SVMcons-a">\[\begin{equation}
  \begin{gathered}
  \text {Data points (Type A): } \boldsymbol{w}^{T} \boldsymbol{x}_{n}+b \in (0,1); \\
  \text {Data points (Type B): }
  \boldsymbol{w}^{T} \boldsymbol{x}_{n}+b &lt; 0.
  \end{gathered}
\tag{67}
\end{equation}\]</span></p>
<p>Then we define a slack variable <span class="math inline">\(\xi_{n}\)</span> for any data point <span class="math inline">\(n\)</span> of Types A or B</p>
<p><span class="math display">\[
    \text {The slack variable $\xi_{n}$}: \xi_{n} = 1 - \left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right).
\]</span></p>
<p>And we define <span class="math inline">\(\xi_{n}\)</span> for any data point of Type C to be <span class="math inline">\(0\)</span> since there is no compromise.</p>
<p>All together, as shown in Figure <a href="#fig:f7-slackvar">119</a>, we have</p>
<p><span class="math display" id="eq:7-SVMcons">\[\begin{equation}
  \begin{gathered}
  \text {Data points (Type A): } \xi_{n} \in (0,1]; \\
  \text {Data points (Type B): } \xi_{n} &gt; 1; \\
  \text {Data points (Type C): } \xi_{n}=0.
  \end{gathered}
\tag{68}
\end{equation}\]</span></p>
<p>Similarly, for the square points that belong to the class (<span class="math inline">\(y= -1\)</span>), we define a slack variable <span class="math inline">\(\xi_{n}\)</span> for each data point <span class="math inline">\(n\)</span></p>
<p><span class="math display">\[
    \text {The slack variable $\xi_{n}$}: \xi_{n} = 1 + \left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right).
\]</span></p>
<p>The same result in Eq. <a href="#eq:7-SVMcons">(68)</a> could be derived.</p>
<p>As the slack variable <span class="math inline">\(\xi_{n}\)</span> describes the <em>degree</em> of compromise for the data point <span class="math inline">\(\boldsymbol{x}_{n}\)</span>, an optimal SVM model should also minimize the total amount of compromise. Based on this additional learning principle, we revise the objective function in Eq. <a href="#eq:7-SVM">(61)</a> and get</p>
<p><span class="math display" id="eq:7-SVMobj2">\[\begin{equation}
  \underbrace{\min _{\boldsymbol{w}} \frac{1}{2}\|\boldsymbol{w}\|^{2}}_{\text{*Maximize Margin*}} + \underbrace{C \sum_{n=1}^{N} \xi_{n}.}_{\text{*Minimize Slacks*}}
\tag{69}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(C\)</span> is a user-specified parameter to control the balance between the two objectives: <em>maximum margin</em> and <em>minimum sum of slacks</em>.</p>
<p>Then we revise the constraints<label for="tufte-sn-174" class="margin-toggle sidenote-number">174</label><input type="checkbox" id="tufte-sn-174" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">174</span> I.e., use the results in Figure <a href="#fig:f7-5regions">116</a> and Figure <a href="#fig:f7-slackvar">119</a>.</span> to be</p>
<p><span class="math display" id="eq:7-SVMcons2">\[ 
y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right) \geq 1-\xi_{n} \text {, for } n=1,2, \dots, N.
\tag{70}
\]</span></p>
<p>Putting the revised objective function and constraints together, the formulation of the SVM model for nonseparable case becomes</p>
<p><span class="math display" id="eq:7-SVM2">\[\begin{equation}
    \begin{gathered}
    \min _{\boldsymbol{w}} \frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{n=1}^{N} \xi_{n}, \\
    \text { Subject to: } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right) \geq 1-\xi_{n}, \\
    \xi_{n} \geq 0, \text { for } n=1,2, \ldots, N.
    \end{gathered}
\tag{71}  
\end{equation}\]</span></p>
<p>A dual form that is similar to Eq. <a href="#eq:7-SVM-dual">(65)</a> could be derived, which is skipped here<label for="tufte-sn-175" class="margin-toggle sidenote-number">175</label><input type="checkbox" id="tufte-sn-175" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">175</span> Interested readers could read this book for a comprehensive and deep understanding of SVM: Scholkopf, B. and Smola, A.J., <em>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</em> MIT Press, 2001.</span>.</p>
<p><em>Extension to nonlinear SVM.</em></p>
<p>Sometimes, the decision boundary could not be characterized as linear models, i.e., see Figure <a href="#fig:f7-7">120</a> (a).</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f7-7"></span>
<p class="caption marginnote shownote">
Figure 120: (a) A nonseparable dataset; (b) with the right transformation, (a) becomes linearly separable
</p>
<img src="graphics/7_7.png" alt="(a) A nonseparable dataset; (b) with the right transformation, (a) becomes linearly separable" width="80%"  />
</div>
<p></p>
<p>A common strategy to create a nonlinear model is to conduct <em>transformation</em> of the original variables. For Figure <a href="#fig:f7-7">120</a> (a), we conduct a transformation from the original two-dimensional coordinate system <span class="math inline">\(\boldsymbol{x}\)</span> to a new coordinate system <span class="math inline">\(\boldsymbol{z}\)</span> that is three-dimensional</p>
<p><span class="math display" id="eq:7-SVM-xtoz">\[\begin{equation}
z_{1}=x_{1}^{2},
z_{2}=\sqrt{2} x_{1} x_{2},
z_{3}=x_{2}^{2}.
\tag{72}
\end{equation}\]</span></p>
<p>In the new coordinate system, as shown in Figure <a href="#fig:f7-7">120</a> (b), the data points of the two classes become linearly separable.</p>
<!-- % ^[This is an approach we often use in linear regression models as well to capture nonlinearity in the data. It is needed to create *explicit* transformation that asks us to write up how the *transformed features* $\boldsymbol{z}$ could be represented by the original features $\boldsymbol{x}$.] -->
<p>The transformation employed in Eq. <a href="#eq:7-SVM-xtoz">(72)</a> is <em>explicit</em>, which may not be suitable for applications where we don’t know what is a good transformation<label for="tufte-sn-176" class="margin-toggle sidenote-number">176</label><input type="checkbox" id="tufte-sn-176" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">176</span> Try a ten-dimensional <span class="math inline">\(\boldsymbol{x}\)</span> and see how troublesome it is to define an explicit transformation to enable linear separability of the classes.</span>. Thus, transformation that could be automatically identified by the learning algorithm is needed, even if the transformation is <em>implicit</em>. A remarkable thing about SVM is that its formulation allows automatic transformation.</p>
<p>Let’s revisit the dual formulation of SVM for the linearly separable case, as shown in Eq. <a href="#eq:7-SVM-dual">(65)</a>. Assume that the transformation has been performed and now we build the SVM model based on the transformed features, <span class="math inline">\(\boldsymbol{z}\)</span>. The dual formulation of SVM on the transformed variables is</p>
<p><span class="math display" id="eq:7-SVM2-dual">\[\begin{equation}
\begin{gathered}
    \max _{\boldsymbol{\alpha}} \sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} \boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}, \\
    \text { Subject to: } \alpha_{n} \geq 0 \text { for } n=1,2, \dots, N, \\
    \sum_{n=1}^{N} \alpha_{n} y_{n}=0.
\end{gathered}
\tag{73}
\end{equation}\]</span></p>
<p>It can be seen that, the dual formulation of SVM doesn’t directly concern <span class="math inline">\(\boldsymbol{z}_{n}\)</span>. Rather, only the inner product of <span class="math inline">\(\boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}\)</span> is needed. As <span class="math inline">\(\boldsymbol{z}\)</span> is essentially a function of <span class="math inline">\(\boldsymbol{x}\)</span>, i.e., denote it as <span class="math inline">\(\boldsymbol{z}=\phi(\boldsymbol{x})\)</span>, <span class="math inline">\(\boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}\)</span> is essentially a function of <span class="math inline">\(\boldsymbol{x}_{n} \text { and } \boldsymbol{x}_{m}\)</span>. We can write it up as <span class="math inline">\(\boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}=K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)\)</span>. This is called the <strong>kernel function</strong>.</p>
<p>A kernel function is a function that entails a transformation <span class="math inline">\(\boldsymbol{z}=\phi(\boldsymbol{x})\)</span> such that <span class="math inline">\(K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)\)</span> is an inner product: <span class="math inline">\(K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=\phi(\boldsymbol{x}_{n})^{T} \phi(\boldsymbol{x}_{m})\)</span>. In other words, we now do not seek explicit form of <span class="math inline">\(\phi(\boldsymbol{x}_{n})\)</span>; rather, we seek kernel functions that entail such transformations<label for="tufte-sn-177" class="margin-toggle sidenote-number">177</label><input type="checkbox" id="tufte-sn-177" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">177</span> If a kernel function is proven to entail a transformation function <span class="math inline">\(\phi(\boldsymbol{x})\)</span>—even it is only proven <em>in theory</em> and never really made explicit in practice—it is as good as explicit transformation, because only the inner product of <span class="math inline">\(\boldsymbol{z}_{n}^{T} \boldsymbol{z}_{m}\)</span> is needed in Eq. <a href="#eq:7-SVM2-dual">(73)</a>.</span>.</p>
<p>Many kernel functions have been developed. For example, the <strong>Gaussian radial basis kernel function</strong> is a popular choice</p>
<p><span class="math display">\[ 
K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=e^{-\gamma\left\|\boldsymbol{x}_{n}-\boldsymbol{x}_{m}\right\|^{2}},
\]</span></p>
<p>where the transformation <span class="math inline">\(\boldsymbol{z}=\phi(\boldsymbol{x})\)</span> is implicit and is proved to be infinitely long<label for="tufte-sn-178" class="margin-toggle sidenote-number">178</label><input type="checkbox" id="tufte-sn-178" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">178</span> Which means it is very flexible and can represent any smooth function.</span>.</p>
<p>The polynomial kernel function is defined as</p>
<p><span class="math display">\[
K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=\left(\boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}+1\right)^{q}.
\]</span></p>
<p>The linear kernel function<label for="tufte-sn-179" class="margin-toggle sidenote-number">179</label><input type="checkbox" id="tufte-sn-179" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">179</span> For linear kernel function, the transformation is trivial, i.e., <span class="math inline">\(\phi(\boldsymbol{x}) = \boldsymbol{x}\)</span>.</span> is defined as</p>
<p><span class="math display">\[ 
K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=\boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}.
\]</span></p>
<p>With a given kernel function, the dual formulation of SVM is</p>
<p><span class="math display" id="eq:7-SVM-dual2">\[\begin{equation}
\begin{gathered}
    \max _{\boldsymbol{\alpha}} \sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} \alpha_{n} \alpha_{m} y_{n} y_{m} K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right), \\
    \text { Subject to: } \alpha_{n} \geq 0 \text { for } n=1,2, \dots, N, \\
    \sum_{n=1}^{N} \alpha_{n} y_{n}=0.
\end{gathered}
\tag{74}
\end{equation}\]</span></p>
<p>After solving Eq. <a href="#eq:7-SVM-dual2">(74)</a>, <em>in theory</em> we could obtain the estimation of the parameter <span class="math inline">\(\boldsymbol{w}\)</span> based on Eq. <a href="#eq:7-SVM-w2">(75)</a></p>
<p><span class="math display" id="eq:7-SVM-w2">\[\begin{equation}
    \boldsymbol{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \phi(\boldsymbol{x_{n}}).
\tag{75}
\end{equation}\]</span></p>
<p>However, for kernel functions that we don’t know the explicit transformation function <span class="math inline">\(\phi(\boldsymbol{x})\)</span>, it is no longer possible to write the parameter <span class="math inline">\(\boldsymbol{w}\)</span> in the same way as in linear SVM models. This won’t prevent us from using the learned SVM model for prediction. For a data point, denoted as <span class="math inline">\(\boldsymbol{x}_{*}\)</span>, we can use the learned SVM model to predict on it<label for="tufte-sn-180" class="margin-toggle sidenote-number">180</label><input type="checkbox" id="tufte-sn-180" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">180</span> I.e., combine Eq. <a href="#eq:7-SVM-w2">(75)</a> and Eq. <a href="#eq:7-DM-SVM">(57)</a> we could derive Eq. <a href="#eq:7-DM-SVM-kernel">(76)</a>.</span></p>
<p><span class="math display" id="eq:7-DM-SVM-kernel">\[\begin{equation}
\begin{gathered}
    \text { If } \sum_{n=1}^{N} \alpha_{n} y_{n} K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{*}\right)+b&gt;0, \text { then } y_{*}=1; \\
\text { Otherwise, } y_{*}=-1.
\end{gathered}
\tag{76}
\end{equation}\]</span></p>
<p>Again, the specific form of <span class="math inline">\(\phi(\boldsymbol{x})\)</span> is not needed since only the kernel function is used.</p>
<p><em>A small-data example.</em></p>
<p>Consider a dataset with <span class="math inline">\(4\)</span> data points</p>
<p><span class="math display">\[ 
\begin{array}{l}{\boldsymbol{x}_{1}=(-1,-1)^{T}, y_{1}=-1}; \\ {\boldsymbol{x}_{2}=(-1,+1)^{T}, y_{2}=+1}; \\ {\boldsymbol{x}_{3}=(+1,-1)^{T}, y_{3}=+1} ;\\ {\boldsymbol{x}_{4}=(+1,+1)^{T}, y_{4}=-1.}\end{array}
\]</span></p>
<p>The dataset is visualized in Figure <a href="#fig:f7-8">121</a>. The R code to draw Figure <a href="#fig:f7-8">121</a> is shown below.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-8"></span>
<img src="graphics/7_8.png" alt="A linearly inseparable dataset" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 121: A linearly inseparable dataset<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For the toy problem</span></span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">4</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>linear.train <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x,y)</span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of data points of two classes</span></span>
<span id="cb144-7"><a href="#cb144-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>( <span class="st">&#39;ggplot2&#39;</span> )</span>
<span id="cb144-8"><a href="#cb144-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">qplot</span>( <span class="at">data=</span>linear.train, X1, X2, </span>
<span id="cb144-9"><a href="#cb144-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">colour=</span><span class="fu">factor</span>(y),<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>), </span>
<span id="cb144-10"><a href="#cb144-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>))</span>
<span id="cb144-11"><a href="#cb144-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Scatterplot of data points of two classes&quot;</span>)</span>
<span id="cb144-12"><a href="#cb144-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p></p>
<p>It is a <em>nonlinear</em> case. We use a nonlinear kernel function to build the SVM model.</p>
<p>Consider the polynomial kernel function with <code>df=2</code></p>
<p><span class="math display" id="eq:7-polykernel2">\[\begin{equation}
K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right)=\left(\boldsymbol{x}_{n}^{T} \boldsymbol{x}_{m}+1\right)^{2}, 
\tag{77}
\end{equation}\]</span></p>
<p>which corresponds to the transformation</p>
<p><span class="math display" id="eq:7-polykernel2-tran">\[\begin{equation}
\phi\left(\boldsymbol{x}_{n}\right)=\left[1, \sqrt{2} x_{n, 1}, \sqrt{2} x_{n, 2}, \sqrt{2} x_{n, 1} x_{n, 2}, x_{n, 1}^{2}, x_{n, 2}^{2}\right]^{T}.
\tag{78}
\end{equation}\]</span></p>
<p>Based on Eq. <a href="#eq:7-SVM-dual">(65)</a>, a specific formulation of the SVM model of this dataset is</p>
<p><span class="math display" id="eq:7-SVM-4points">\[\begin{equation}
    \begin{gathered}
    \max _{\boldsymbol{\alpha}} \sum_{n=1}^{4} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{4} \sum_{m=1}^{4} \alpha_{n} \alpha_{m} y_{n} y_{m} K\left(\boldsymbol{x}_{n}, \boldsymbol{x}_{m}\right), \\
    \text { Subject to: } \alpha_{n} \geq 0 \text { for } n=1,2, \dots, 4, \\
    \text { and } \sum_{n=1}^{4} \alpha_{n} y_{n}=0.
    \end{gathered}
\tag{79}
\end{equation}\]</span></p>
<p>We calculate the kernel matrix as<label for="tufte-sn-181" class="margin-toggle sidenote-number">181</label><input type="checkbox" id="tufte-sn-181" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">181</span> E.g., using Eq. <a href="#eq:7-polykernel2">(77)</a>, <span class="math inline">\(K\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right) = \left(\boldsymbol{x}_{1}^{T} \boldsymbol{x}_{2}+1\right)^{2} = 3^2 = 9\)</span>. Readers can try other instances.</span></p>
<p><span class="math display">\[
\boldsymbol{K}=\left[\begin{array}{cccc}{9} &amp; {1} &amp; {1} &amp; {1} \\ {1} &amp; {9} &amp; {1} &amp; {1} \\ {1} &amp; {1} &amp; {9} &amp; {1} \\ {1} &amp; {1} &amp; {1} &amp; {9}\end{array}\right].
\]</span></p>
<p>We solve the quadratic programming problem<label for="tufte-sn-182" class="margin-toggle sidenote-number">182</label><input type="checkbox" id="tufte-sn-182" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">182</span> I.e., use the R package <code>quadprog</code>.</span> in Eq. <a href="#eq:7-SVM-4points">(79)</a> and get</p>
<p><span class="math display" id="eq:7-alpha">\[\begin{equation}
\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=0.125.
\tag{80}
\end{equation}\]</span></p>
<p>In this particular case, since we can write up the transformation explicitly<label for="tufte-sn-183" class="margin-toggle sidenote-number">183</label><input type="checkbox" id="tufte-sn-183" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">183</span> I.e., as shown in Eq. <a href="#eq:7-polykernel2-tran">(78)</a></span>, we can write up <span class="math inline">\(\boldsymbol{w}\)</span> explicitly as well<label for="tufte-sn-184" class="margin-toggle sidenote-number">184</label><input type="checkbox" id="tufte-sn-184" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">184</span> It should be written as <span class="math inline">\(\widehat{\boldsymbol{w}}\)</span>, since it is an estimator of <span class="math inline">\(\boldsymbol{w}\)</span>. Here for simplicity we skip this.</span></p>
<p><span class="math display">\[
\boldsymbol{w}=\sum_{n=1}^{4} \alpha_{n} y_{n} \phi\left(\boldsymbol{x}_{n}\right)=[0,0,0,1 / \sqrt{2}, 0,0]^{T}.
\]</span></p>
<p>For any given data point <span class="math inline">\(\boldsymbol{x}_{*}\)</span>, the explicit decision function is</p>
<p><span class="math display">\[
f\left(\boldsymbol{x}_{*}\right)=\boldsymbol{w}^{T} \phi\left(\boldsymbol{x}_{*}\right)=x_{*, 1} x_{*, 2}.
\]</span></p>
<p>This is the decision boundary for a typical <strong>XOR</strong> problem<label for="tufte-sn-185" class="margin-toggle sidenote-number">185</label><input type="checkbox" id="tufte-sn-185" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">185</span> Also known as <em>exclusive or</em> or <em>exclusive disjunction</em>, the XOR problem is a logical operation that outputs <em>true</em> only when inputs differ (e.g., one is <em>true</em>, the other is <em>false</em>).</span>.</p>
<p>We then use R to build an SVM model on this dataset<label for="tufte-sn-186" class="margin-toggle sidenote-number">186</label><input type="checkbox" id="tufte-sn-186" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">186</span> We use the R package <code>kernlab</code>—more details are shown in the section <strong>R Lab</strong>.</span>. The R code is shown in below.</p>
<p></p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a nonlinear SVM model </span></span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a><span class="co"># polynomial kernel function with `df=2`</span></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">poly</span>(x, <span class="at">degree =</span> <span class="dv">2</span>, <span class="at">raw =</span> <span class="cn">TRUE</span>))</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>coefs <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="fu">sqrt</span>(<span class="dv">2</span>),<span class="dv">1</span>,<span class="fu">sqrt</span>(<span class="dv">2</span>),<span class="fu">sqrt</span>(<span class="dv">2</span>),<span class="dv">1</span>)</span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> x <span class="sc">*</span> <span class="fu">t</span>(<span class="fu">matrix</span>(<span class="fu">rep</span>(coefs,<span class="dv">4</span>),<span class="at">nrow=</span><span class="dv">6</span>,<span class="at">ncol=</span><span class="dv">4</span>))</span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>linear.train <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x,y)</span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>( <span class="st">&#39;kernlab&#39;</span> )</span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>linear.svm <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(y <span class="sc">~</span> ., <span class="at">data=</span>linear.train, </span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">type=</span><span class="st">&#39;C-svc&#39;</span>, <span class="at">kernel=</span><span class="st">&#39;vanilladot&#39;</span>, <span class="at">C=</span><span class="dv">10</span>, <span class="at">scale=</span><span class="fu">c</span>())</span></code></pre></div>
<p></p>
<p>The function <code>alpha()</code> returns the values of <span class="math inline">\(\alpha_{n} \text { for } n=1,2, \dots, 4\)</span>. Our results as shown in Eq. <a href="#eq:7-alpha">(80)</a> are consistent with the results obtained by using R.<label for="tufte-sn-187" class="margin-toggle sidenote-number">187</label><input type="checkbox" id="tufte-sn-187" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">187</span> If your answer is different, check if the <code>alpha()</code> function in the <code>kernlab</code>() package scales the vector <span class="math inline">\(\alpha\)</span>, i.e., to make the sum as <span class="math inline">\(1\)</span>.</span></p>
<p></p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">alpha</span>(linear.svm) <span class="co">#scaled alpha vector</span></span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [[1]]</span></span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.125 0.125 0.125 0.125</span></span></code></pre></div>
<p></p>
<div style="page-break-after: always;"></div>
</div>
<div id="r-lab-9" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 7-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get data into R and make appropriate preprocessing.</p>
<p></p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb147-7"><a href="#cb147-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-8"><a href="#cb147-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb147-9"><a href="#cb147-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb147-10"><a href="#cb147-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data[,<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>]</span>
<span id="cb147-11"><a href="#cb147-11" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl</span>
<span id="cb147-12"><a href="#cb147-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-13"><a href="#cb147-13" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c&quot;</span>, Y) </span>
<span id="cb147-14"><a href="#cb147-14" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(Y) </span>
<span id="cb147-15"><a href="#cb147-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-16"><a href="#cb147-16" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,Y)</span>
<span id="cb147-17"><a href="#cb147-17" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">16</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb147-18"><a href="#cb147-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-19"><a href="#cb147-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data (half the original data size)</span></span>
<span id="cb147-20"><a href="#cb147-20" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)<span class="sc">/</span><span class="dv">2</span>) )</span>
<span id="cb147-21"><a href="#cb147-21" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb147-22"><a href="#cb147-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data (half the original data size)</span></span>
<span id="cb147-23"><a href="#cb147-23" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> puts together a list of candidate models.</p>
<p></p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a><span class="co"># SVM: often to compare models with different kernels, </span></span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a><span class="co"># different values of C, different set of variables</span></span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use different set of variables</span></span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(DX_bl <span class="sc">~</span> .)</span>
<span id="cb148-8"><a href="#cb148-8" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(DX_bl <span class="sc">~</span> AGE <span class="sc">+</span> PTEDUCAT <span class="sc">+</span> FDG </span>
<span id="cb148-9"><a href="#cb148-9" aria-hidden="true" tabindex="-1"></a>                     <span class="sc">+</span> AV45 <span class="sc">+</span> HippoNV <span class="sc">+</span> rs3865444)</span>
<span id="cb148-10"><a href="#cb148-10" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(DX_bl <span class="sc">~</span> AGE <span class="sc">+</span> PTEDUCAT)</span>
<span id="cb148-11"><a href="#cb148-11" aria-hidden="true" tabindex="-1"></a>model4 <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(DX_bl <span class="sc">~</span> FDG <span class="sc">+</span> AV45 <span class="sc">+</span> HippoNV)</span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses <span class="math inline">\(10\)</span>-fold cross-validation to evaluate the performance of the candidate models. Below we show how it works for one model. For other models, the same script could be used with a slight modification.</p>
<p></p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Use 10-fold cross-validation to evaluate the models</span></span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">=</span> <span class="dv">10</span> </span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a><span class="co"># number of fold </span></span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(data.train)[<span class="dv">1</span>] </span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N))  </span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the first model</span></span>
<span id="cb149-9"><a href="#cb149-9" aria-hidden="true" tabindex="-1"></a>cv_err <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb149-10"><a href="#cb149-10" aria-hidden="true" tabindex="-1"></a><span class="co"># cv_err makes records of the prediction error for each fold</span></span>
<span id="cb149-11"><a href="#cb149-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb149-12"><a href="#cb149-12" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb149-13"><a href="#cb149-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># In each iteration, use one fold of data as the testing data</span></span>
<span id="cb149-14"><a href="#cb149-14" aria-hidden="true" tabindex="-1"></a>  data.test.cv <span class="ot">&lt;-</span> data.train[test_i, ] </span>
<span id="cb149-15"><a href="#cb149-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The remaining 9 folds&#39; data form our training data</span></span>
<span id="cb149-16"><a href="#cb149-16" aria-hidden="true" tabindex="-1"></a>  data.train.cv <span class="ot">&lt;-</span> data.train[<span class="sc">-</span>test_i, ]   </span>
<span id="cb149-17"><a href="#cb149-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">require</span>( <span class="st">&#39;kernlab&#39;</span> )</span>
<span id="cb149-18"><a href="#cb149-18" aria-hidden="true" tabindex="-1"></a>  linear.svm <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(model1, <span class="at">data=</span>data.train.cv, </span>
<span id="cb149-19"><a href="#cb149-19" aria-hidden="true" tabindex="-1"></a>                     <span class="at">type=</span><span class="st">&#39;C-svc&#39;</span>, <span class="at">kernel=</span><span class="st">&#39;vanilladot&#39;</span>, <span class="at">C=</span><span class="dv">10</span>) </span>
<span id="cb149-20"><a href="#cb149-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the linear SVM model with the training data</span></span>
<span id="cb149-21"><a href="#cb149-21" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear.svm, data.test.cv)  </span>
<span id="cb149-22"><a href="#cb149-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb149-23"><a href="#cb149-23" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test.cv<span class="sc">$</span>DX_bl  </span>
<span id="cb149-24"><a href="#cb149-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get the the error rate</span></span>
<span id="cb149-25"><a href="#cb149-25" aria-hidden="true" tabindex="-1"></a>  cv_err[k] <span class="ot">&lt;-</span><span class="fu">length</span>(<span class="fu">which</span>(y_hat <span class="sc">!=</span> true_y))<span class="sc">/</span><span class="fu">length</span>(y_hat) </span>
<span id="cb149-26"><a href="#cb149-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb149-27"><a href="#cb149-27" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_err)</span>
<span id="cb149-28"><a href="#cb149-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-29"><a href="#cb149-29" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the second model ...</span></span>
<span id="cb149-30"><a href="#cb149-30" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the third model ...</span></span>
<span id="cb149-31"><a href="#cb149-31" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span></code></pre></div>
<p></p>
<p>Results are shown below.</p>
<p></p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1781538</span></span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1278462</span></span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.4069231</span></span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1316923</span></span></code></pre></div>
<p></p>
<p>The second model is the best.</p>
<p><strong>Step 5</strong> uses the training data to fit a final model, through the <code>ksvm()</code> function in the package <code>kernlab</code>.</p>
<p></p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, </span></span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a><span class="co"># use ksvm() function to build your final model</span></span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a>linear.svm <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(model2, <span class="at">data=</span>data.train,</span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">type=</span><span class="st">&#39;C-svc&#39;</span>, <span class="at">kernel=</span><span class="st">&#39;vanilladot&#39;</span>, <span class="at">C=</span><span class="dv">10</span>) </span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the fitted final model for prediction on the testing data.</p>
<p></p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Predict using your SVM model</span></span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear.svm, data.test) </span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-BS-ROC"></span>
<img src="graphics/7_BS_ROC.png" alt="The ROC curve of the final SVM model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 122: The ROC curve of the final SVM model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 7</strong> evaluates the performance of the model.</p>
<p></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7 -&gt; Evaluate the prediction performance of the SVM model</span></span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (1) The confusion matrix</span></span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-5"><a href="#cb153-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret) </span>
<span id="cb153-6"><a href="#cb153-6" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(y_hat, data.test<span class="sc">$</span>DX_bl)</span>
<span id="cb153-7"><a href="#cb153-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-8"><a href="#cb153-8" aria-hidden="true" tabindex="-1"></a><span class="co"># (2) ROC curve </span></span>
<span id="cb153-9"><a href="#cb153-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC) </span>
<span id="cb153-10"><a href="#cb153-10" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear.svm, data.test, <span class="at">type =</span> <span class="st">&#39;decision&#39;</span>) </span>
<span id="cb153-11"><a href="#cb153-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">roc</span>(data.test<span class="sc">$</span>DX_bl, y_hat),</span>
<span id="cb153-12"><a href="#cb153-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">main=</span><span class="st">&quot;ROC Curve&quot;</span>)</span></code></pre></div>
<p></p>
<p>Results are shown below. And the ROC curve is shown in Figure <a href="#fig:f7-BS-ROC">122</a>.</p>
<p></p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction  c0  c1</span></span>
<span id="cb154-5"><a href="#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="do">##         c0 131  27</span></span>
<span id="cb154-6"><a href="#cb154-6" aria-hidden="true" tabindex="-1"></a><span class="do">##         c1  11  90</span></span>
<span id="cb154-7"><a href="#cb154-7" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-8"><a href="#cb154-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                Accuracy : 0.8533         </span></span>
<span id="cb154-9"><a href="#cb154-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                  95% CI : (0.8042, 0.894)</span></span>
<span id="cb154-10"><a href="#cb154-10" aria-hidden="true" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5483         </span></span>
<span id="cb154-11"><a href="#cb154-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : &lt; 2e-16        </span></span>
<span id="cb154-12"><a href="#cb154-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-13"><a href="#cb154-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Kappa : 0.7002         </span></span>
<span id="cb154-14"><a href="#cb154-14" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-15"><a href="#cb154-15" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.01496        </span></span>
<span id="cb154-16"><a href="#cb154-16" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-17"><a href="#cb154-17" aria-hidden="true" tabindex="-1"></a><span class="do">##             Sensitivity : 0.9225         </span></span>
<span id="cb154-18"><a href="#cb154-18" aria-hidden="true" tabindex="-1"></a><span class="do">##             Specificity : 0.7692         </span></span>
<span id="cb154-19"><a href="#cb154-19" aria-hidden="true" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8291         </span></span>
<span id="cb154-20"><a href="#cb154-20" aria-hidden="true" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.8911         </span></span>
<span id="cb154-21"><a href="#cb154-21" aria-hidden="true" tabindex="-1"></a><span class="do">##             Prevalence : 0.5483         </span></span>
<span id="cb154-22"><a href="#cb154-22" aria-hidden="true" tabindex="-1"></a><span class="do">##          Detection Rate : 0.5058         </span></span>
<span id="cb154-23"><a href="#cb154-23" aria-hidden="true" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.6100         </span></span>
<span id="cb154-24"><a href="#cb154-24" aria-hidden="true" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8459         </span></span>
<span id="cb154-25"><a href="#cb154-25" aria-hidden="true" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb154-26"><a href="#cb154-26" aria-hidden="true" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : c0</span></span></code></pre></div>
<p></p>
<p><em>Beyond the 7-Step R Pipeline.</em></p>
<p>In the 7-step pipeline, we create a list of candidate models by different selections of predictors. There are other parameters, such as the kernel function, the value of <span class="math inline">\(C\)</span>, that should be concerned in model selection. The R package <code>caret</code> can automate the process of cross-validation and facilitate the optimization of multiple parameters simultaneously. Below is an example</p>
<p></p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Train and Tune the SVM</span></span>
<span id="cb155-7"><a href="#cb155-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(AD)[<span class="dv">1</span>]</span>
<span id="cb155-8"><a href="#cb155-8" aria-hidden="true" tabindex="-1"></a>n.train <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fl">0.8</span> <span class="sc">*</span> n)</span>
<span id="cb155-9"><a href="#cb155-9" aria-hidden="true" tabindex="-1"></a>idx.train <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n.train)</span>
<span id="cb155-10"><a href="#cb155-10" aria-hidden="true" tabindex="-1"></a>AD[<span class="fu">which</span>(AD[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">0</span>),<span class="dv">1</span>] <span class="ot">=</span> <span class="fu">rep</span>(<span class="st">&quot;Normal&quot;</span>,<span class="fu">length</span>(<span class="fu">which</span>(AD[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">0</span>)))</span>
<span id="cb155-11"><a href="#cb155-11" aria-hidden="true" tabindex="-1"></a>AD[<span class="fu">which</span>(AD[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">1</span>),<span class="dv">1</span>] <span class="ot">=</span> <span class="fu">rep</span>(<span class="st">&quot;Diseased&quot;</span>,<span class="fu">length</span>(<span class="fu">which</span>(AD[,<span class="dv">1</span>]<span class="sc">==</span><span class="dv">1</span>)))</span>
<span id="cb155-12"><a href="#cb155-12" aria-hidden="true" tabindex="-1"></a>AD.train <span class="ot">&lt;-</span> AD[idx.train,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>)]</span>
<span id="cb155-13"><a href="#cb155-13" aria-hidden="true" tabindex="-1"></a>AD.test <span class="ot">&lt;-</span> AD[<span class="sc">-</span>idx.train,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>)]</span>
<span id="cb155-14"><a href="#cb155-14" aria-hidden="true" tabindex="-1"></a>trainX <span class="ot">&lt;-</span> AD.train[,<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">16</span>)]</span>
<span id="cb155-15"><a href="#cb155-15" aria-hidden="true" tabindex="-1"></a>trainy<span class="ot">=</span> AD.train[,<span class="dv">1</span>]</span>
<span id="cb155-16"><a href="#cb155-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-17"><a href="#cb155-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup for cross-validation:</span></span>
<span id="cb155-18"><a href="#cb155-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 10-fold cross validation</span></span>
<span id="cb155-19"><a href="#cb155-19" aria-hidden="true" tabindex="-1"></a><span class="co"># do 5 repetitions of cv</span></span>
<span id="cb155-20"><a href="#cb155-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Use AUC to pick the best model</span></span>
<span id="cb155-21"><a href="#cb155-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-22"><a href="#cb155-22" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb155-23"><a href="#cb155-23" aria-hidden="true" tabindex="-1"></a>                     <span class="at">repeats=</span><span class="dv">1</span>,</span>
<span id="cb155-24"><a href="#cb155-24" aria-hidden="true" tabindex="-1"></a>                     <span class="at">summaryFunction=</span>twoClassSummary,</span>
<span id="cb155-25"><a href="#cb155-25" aria-hidden="true" tabindex="-1"></a>                     <span class="at">classProbs=</span><span class="cn">TRUE</span>)</span>
<span id="cb155-26"><a href="#cb155-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-27"><a href="#cb155-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the expand.grid to specify the search space   </span></span>
<span id="cb155-28"><a href="#cb155-28" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">sigma =</span> <span class="fu">c</span>(<span class="fl">0.002</span>, <span class="fl">0.005</span>, <span class="fl">0.01</span>, <span class="fl">0.012</span>, <span class="fl">0.015</span>),</span>
<span id="cb155-29"><a href="#cb155-29" aria-hidden="true" tabindex="-1"></a><span class="at">C =</span> <span class="fu">c</span>(<span class="fl">0.3</span>,<span class="fl">0.4</span>,<span class="fl">0.5</span>,<span class="fl">0.6</span>)</span>
<span id="cb155-30"><a href="#cb155-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb155-31"><a href="#cb155-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-32"><a href="#cb155-32" aria-hidden="true" tabindex="-1"></a><span class="co"># method: Radial kernel </span></span>
<span id="cb155-33"><a href="#cb155-33" aria-hidden="true" tabindex="-1"></a><span class="co"># tuneLength: 9 values of the cost function</span></span>
<span id="cb155-34"><a href="#cb155-34" aria-hidden="true" tabindex="-1"></a><span class="co"># preProc: Center and scale data</span></span>
<span id="cb155-35"><a href="#cb155-35" aria-hidden="true" tabindex="-1"></a>svm.tune <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">x =</span> trainX, <span class="at">y =</span> trainy, </span>
<span id="cb155-36"><a href="#cb155-36" aria-hidden="true" tabindex="-1"></a>                  <span class="at">method =</span> <span class="st">&quot;svmRadial&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">9</span>,</span>
<span id="cb155-37"><a href="#cb155-37" aria-hidden="true" tabindex="-1"></a>                  <span class="at">preProc =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>), <span class="at">metric=</span><span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb155-38"><a href="#cb155-38" aria-hidden="true" tabindex="-1"></a>                  <span class="at">tuneGrid =</span> grid,</span>
<span id="cb155-39"><a href="#cb155-39" aria-hidden="true" tabindex="-1"></a>                  <span class="at">trControl=</span>ctrl)</span>
<span id="cb155-40"><a href="#cb155-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-41"><a href="#cb155-41" aria-hidden="true" tabindex="-1"></a>svm.tune</span></code></pre></div>
<p></p>
<p>Then we can obtain the following results</p>
<p></p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Support Vector Machines with Radial Basis Function Kernel </span></span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 413 samples</span></span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  15 predictor</span></span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a><span class="do">##   2 classes: &#39;Diseased&#39;, &#39;Normal&#39; </span></span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Pre-processing: centered (15), scaled (15) </span></span>
<span id="cb156-8"><a href="#cb156-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold, repeated 1 times) </span></span>
<span id="cb156-9"><a href="#cb156-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Summary of sample sizes: 371, 372, 372, 371, 372, 372, ... </span></span>
<span id="cb156-10"><a href="#cb156-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb156-11"><a href="#cb156-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb156-12"><a href="#cb156-12" aria-hidden="true" tabindex="-1"></a><span class="do">##   sigma  C    ROC        Sens       Spec     </span></span>
<span id="cb156-13"><a href="#cb156-13" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.002  0.3  0.8929523  0.9121053  0.5932900</span></span>
<span id="cb156-14"><a href="#cb156-14" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.002  0.4  0.8927130  0.8757895  0.6619048</span></span>
<span id="cb156-15"><a href="#cb156-15" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.002  0.5  0.8956402  0.8452632  0.7627706</span></span>
<span id="cb156-16"><a href="#cb156-16" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.002  0.6  0.8953759  0.8192105  0.7991342</span></span>
<span id="cb156-17"><a href="#cb156-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.005  0.3  0.8965129  0.8036842  0.8036797</span></span>
<span id="cb156-18"><a href="#cb156-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.005  0.4  0.8996565  0.7989474  0.8357143</span></span>
<span id="cb156-19"><a href="#cb156-19" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.005  0.5  0.9020830  0.7936842  0.8448052</span></span>
<span id="cb156-20"><a href="#cb156-20" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.005  0.6  0.9032422  0.7836842  0.8450216</span></span>
<span id="cb156-21"><a href="#cb156-21" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.010  0.3  0.9030514  0.7889474  0.8541126</span></span>
<span id="cb156-22"><a href="#cb156-22" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.010  0.4  0.9058248  0.7886842  0.8495671</span></span>
<span id="cb156-23"><a href="#cb156-23" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.010  0.5  0.9060999  0.8044737  0.8541126</span></span>
<span id="cb156-24"><a href="#cb156-24" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.010  0.6  0.9077848  0.8094737  0.8450216</span></span>
<span id="cb156-25"><a href="#cb156-25" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.012  0.3  0.9032308  0.7781579  0.8538961</span></span>
<span id="cb156-26"><a href="#cb156-26" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.012  0.4  0.9049043  0.7989474  0.8538961</span></span>
<span id="cb156-27"><a href="#cb156-27" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.012  0.5  0.9063505  0.8094737  0.8495671</span></span>
<span id="cb156-28"><a href="#cb156-28" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.012  0.6  0.9104511  0.8042105  0.8586580</span></span>
<span id="cb156-29"><a href="#cb156-29" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.015  0.3  0.9060412  0.7886842  0.8493506</span></span>
<span id="cb156-30"><a href="#cb156-30" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.015  0.4  0.9068165  0.8094737  0.8495671</span></span>
<span id="cb156-31"><a href="#cb156-31" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.015  0.5  0.9109051  0.8042105  0.8541126</span></span>
<span id="cb156-32"><a href="#cb156-32" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.015  0.6  0.9118615  0.8042105  0.8632035</span></span>
<span id="cb156-33"><a href="#cb156-33" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb156-34"><a href="#cb156-34" aria-hidden="true" tabindex="-1"></a><span class="do">## ROC was used to select the optimal model using  the largest </span></span>
<span id="cb156-35"><a href="#cb156-35" aria-hidden="true" tabindex="-1"></a><span class="do">## value. The final values used for the model were </span></span>
<span id="cb156-36"><a href="#cb156-36" aria-hidden="true" tabindex="-1"></a><span class="do">## sigma = 0.015 and C = 0.6.</span></span></code></pre></div>
<p></p>
</div>
</div>
<div id="ensemble-learning" class="section level2 unnumbered">
<h2>Ensemble learning</h2>
<div id="rationale-and-formulation-11" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><strong>Ensemble learning</strong> is another example of how we design better learning algorithms. The random forest model is a particular case of <strong>ensemble models</strong>. An ensemble model consists of <span class="math inline">\(K\)</span> <em>base models</em>, denoted as, <span class="math inline">\(h_{1}, h_{2}, \ldots, h_{K}\)</span>. The algorithms to create ensemble models differ from each other in terms of the types of the base models, the way to create diversity in the base models, etc.</p>
<p>We have known the random forest model uses Bootstrap to create many datasets and builds a set of decision tree models. Some other ensemble learning methods, such as the <strong>AdaBoost</strong> model, also use decision tree as the base model. The two differ in the way to build a <em>diverse</em> set of base models. The framework of AdaBoost is illustrated in Figure <a href="#fig:f7-AdaBoost">123</a>. AdaBoost employs a sequential process to build its base models: it uses the original dataset (when the weights for the data points are equal) to build a decision tree; then it uses the decision tree to predict on the dataset, obtains the errors, and updates the weights of the data points<label for="tufte-sn-188" class="margin-toggle sidenote-number">188</label><input type="checkbox" id="tufte-sn-188" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">188</span> I.e., those data points that are wrongly classified will gain higher weights.</span>; then it builds another decision tree on the same dataset with the new weights, obtains the errors, and updates the weights of the data points again. The sequential process continues, until a given number of decision trees are built. This sequential process is designed for adaptability: later models focus more on the <em>hard</em> data points that present challenges for previous base models to achieve good prediction performance. Interested readers may find a formal presentation of the AdaBoost algorithm in the <strong>Remarks</strong> section.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f7-AdaBoost"></span>
<img src="graphics/adaboost.png" alt="A general framework of AdaBoost" width="80%"  />
<p class="caption marginnote shownote">
Figure 123: A general framework of AdaBoost
</p>
</div>
<p></p>
<p>The ensemble learning is flexible, given that any model could be a base model. And there are a variety of ways to resample or perturb a dataset to create a diverse set of base models. Like SVM, the ensemble learning is another approach to have a built-in mechanism to reduce the risk of overfitting. Here, we provide a discussion of this built-in mechanism using the framework proposed by Dietterich, where three perspectives (statistical, computational, and representational) were used to explain why ensemble methods could lead to robust performance. Each perspective is described in details below.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-23"></span>
<img src="graphics/7_EL_stat.png" alt="Ensemble learning approximates the true model with a combination of good models (statistical perspective)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 124: Ensemble learning approximates the true model with a combination of good models (statistical perspective)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Statistical perspective.</em> The statistical reason is illustrated in Figure <a href="#fig:f7-23">124</a>. <span class="math inline">\(\mathcal{H}\)</span> is the model space where a learning algorithm searches for the best model guided by the training data. A model corresponds to a <em>point</em> in Figure <a href="#fig:f7-23">124</a>, e.g., the point labelled as <span class="math inline">\(f\)</span> is the true model. When the data is limited and the best models are multiple, the problem is a statistical one and we need to make an optimal decision despite the uncertainty. This is illustrated by the inner circle in Figure <a href="#fig:f7-23">124</a>. By building an ensemble of multiple base models, e.g., the <span class="math inline">\(h_{1}, h_{2}, \text { and } h_{3}\)</span> in Figure <a href="#fig:f7-23">124</a>, the average of the models is a good approximation to the true model <span class="math inline">\(f\)</span>. This combined solution, comparing with other models that only identify one best model, has less variance, and therefore, could be more robust.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-24"></span>
<img src="graphics/7_EL_comp.png" alt=" Ensemble learning provides a robust coverage of the true model (computational perspective)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 125:  Ensemble learning provides a robust coverage of the true model (computational perspective)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Computational perspective.</em> A computational perspective is shown in Figure <a href="#fig:f7-24">125</a>. This perspective concerns the way we build base models. Often greedy approaches such as the recursive splitting procedure are used to solve optimization problems in training machine learning models. This is optimal only in a <em>local</em> sense<label for="tufte-sn-189" class="margin-toggle sidenote-number">189</label><input type="checkbox" id="tufte-sn-189" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">189</span> E.g., to grow a decision tree, at each node, the node is split according to the maximum information gain <em>at this particular node</em>. To grow a decision tree model, a sequence of splits is needed. Optimization of all the splits <em>simultaneously</em> leads to a <em>global</em> optimal solution, but it is a <em>NP-hard</em> problem that is not solved yet. Optimization of each split is more practical, only we know that the local optimal solution may result in suboptimal situations for further splitting of descendant nodes.</span>. As a remedy to this problem, the ensemble learning initializes the learning algorithm (that is greedy and heuristic) from multiple locations in <span class="math inline">\(\mathcal{H}\)</span>, i.e., as shown in Figure <a href="#fig:f7-24">125</a>, three models are identified by the same algorithm that starts from different initial points. Exploring multiple trajectories help us find a robust coverage of the true model <span class="math inline">\(f\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-25"></span>
<img src="graphics/7_EL_rep.png" alt=" Ensemble learning approximates the true model with a combination of good models (representational perspective)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 126:  Ensemble learning approximates the true model with a combination of good models (representational perspective)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Representational perspective.</em> Due to the size of the dataset or the limitations of a model, sometimes the model space <span class="math inline">\(\mathcal{H}\)</span> does not cover the true model, i.e., in Figure <a href="#fig:f7-25">126</a> the true model is outside the region of <span class="math inline">\(\mathcal{H}\)</span>. This is not uncommon in real-world problems, for example, linear models cannot learn nonlinear patterns, or decision trees have difficulty in learning linear patterns. Using multiple base models may provide an approximation of the true model that is outside <span class="math inline">\(\mathcal{H}\)</span>, as shown in Figure <a href="#fig:f7-25">126</a>.</p>
</div>
<div id="analysis-of-the-decision-tree-random-forests-and-adaboost" class="section level3 unnumbered">
<h3>Analysis of the decision tree, random forests, and AdaBoost</h3>
<p>The three models are analyzed using the three perspectives. Results are shown in Table <a href="#tab:t8-threemodels">31</a>. In-depth discussions are provided in the following.</p>
<p><em>Single decision tree.</em> A single decision tree lacks the capability to overcome overfitting in terms of each of the three perspectives. From the statistical perspective, a decision tree algorithm constructs each node using the maximum information gain <em>at that particular node only</em>; thus, random errors in data may mislead subsequent splits. On the other hand, when the training dataset is limited, many models may perform equally well, since there are not enough data to distinguish these models. This results in a large <em>inner circle</em> as shown in Figure <a href="#fig:f7-23">124</a>. With the true model <span class="math inline">\(f\)</span> hidden in a large area in <span class="math inline">\(\mathcal{H}\)</span>, and the sensitivity of the learning algorithm to random noises in data (an issue from the computational perspective), the learning algorithm may end up with a model far away from the true model <span class="math inline">\(f\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-threemodels">Table 31: </span>Analysis of the decision tree (DT), random forests (RF), and AdaBoost using the three perspectives</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><em>Perspectives</em></th>
<th align="left">DT</th>
<th align="left">RF</th>
<th align="left">AdaBoost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Statistical</td>
<td align="left">No</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Computational</td>
<td align="left">No</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Representational</td>
<td align="left">No</td>
<td align="left">No</td>
<td align="left">Yes</td>
</tr>
</tbody>
</table>
<p></p>
<p>From the representational perspective, there are also limitations of the decision tree model; i.e., in <strong>Chapter 2</strong> we have shown that the decision tree model has difficulty in modeling linear patterns in the data.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f7-RF-analysis"></span>
<img src="graphics/7_EL_rf.png" alt="Analysis of the random forest in terms of the statistical (left), computational (middle), and representational (right) perspectives" width="80%"  />
<p class="caption marginnote shownote">
Figure 127: Analysis of the random forest in terms of the statistical (left), computational (middle), and representational (right) perspectives
</p>
</div>
<p></p>
<p><em>Random forests.</em> From the statistical perspective, the random forest model is a good ensemble learning model. As shown in Figure <a href="#fig:f7-RF-analysis">127</a> (left), the way the random forest model grows the base models is to construct the <em>circle</em> of dotted line. Models located in this circle of dotted line have reasonably good accuracy. These models may not be the best models with great accuracy, they do provide a good coverage/approximation of the true model.</p>
<p>Note that, if we could directly build a model that is close to <span class="math inline">\(f\)</span>, or build many best models that are located in the circle of dotted line, that would be ideal. However, both tasks are challenging. Comparing with these ideal goals, the random forest model is more pragmatic. It cleverly uses <em>simple</em><label for="tufte-sn-190" class="margin-toggle sidenote-number">190</label><input type="checkbox" id="tufte-sn-190" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">190</span> As we have seen, <em>Simple</em> is a complex word.</span> techniques of <em>randomness</em>, i.e., the Bootstrap and the random selection of variables, that are robust, effective, and easy to implement. It grows a set of models that are not the best, but good models. Most importantly, these good models complement each other<label for="tufte-sn-191" class="margin-toggle sidenote-number">191</label><input type="checkbox" id="tufte-sn-191" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">191</span> In practice, the challenge to grow a set of <em>best</em> models is that it usually ends up with these <em>best models</em> more or less being the same.</span>.</p>
<p>Random forest model can also address the computational issue. As shown in Figure <a href="#fig:f7-RF-analysis">127</a> (middle), while the circle of solid line (i.e., that represents the space of best models) is computationally difficult to reach, averaging multiple models could provide a good approximation.</p>
<p>It seems that the random forest models do not actively solve the representational issue. If the true model <span class="math inline">\(f\)</span> lies outside <span class="math inline">\(\mathcal{H}\)</span>, as shown in Figure <a href="#fig:f7-RF-analysis">127</a> (right), averaging multiple models won’t necessarily approximate the true model.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-29"></span>
<img src="graphics/7_EL_adaBoost.png" alt="Analysis of the AdaBoost in terms of the representational perspective" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 128: Analysis of the AdaBoost in terms of the representational perspective<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>AdaBoost.</em> Similar to random forest, AdaBoost solves the computational issue by generating many base models. The difference is that, AdaBoost actively solves the representational issue, i.e., it tries to do better on the <em>hard</em> data points where the previous base models fail to predict correctly. For each base model in AdaBoost, the training dataset is not resampled by Bootstrap, but weighted based on the error rates from previous base models, i.e., data points that are difficult to be correctly predicted by the previous models are given more weights in the new training dataset for the subsequent base model. Figure <a href="#fig:f7-29">128</a> shows this sequential learning process helps AdaBoost identify more models around the true model, and put more weight to the models that are closer to the true model.</p>
<p>But AdaBoost is not as good as random forest in terms of addressing the statistical issue. As AdaBoost aggressively solves the representational issue and allows its base models to be impacted by some <em>hard</em> data points<label for="tufte-sn-192" class="margin-toggle sidenote-number">192</label><input type="checkbox" id="tufte-sn-192" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">192</span> This is a common root cause for a model to overfit the training data, if the model tries <em>too hard</em> on a particular training data.</span>, it is more likely to overfit, and may be less stable than the random forest models that place more emphasis on addressing the statistical issue.</p>
</div>
<div id="r-lab-10" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We use the AD dataset to study decision tree (<code>rpart</code> package), random forests (<code>randomForest</code> package), and AdaBoost (<code>gbm</code> package).</p>
<p>First, we evaluate the overall performance of the three models. Results are shown in Figure <a href="#fig:f7-30">129</a>, produced by the following R code.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-30"></span>
<img src="graphics/7_30.png" alt="Boxplots of the classification error rates for single decision tree, random forest, and AdaBoost" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 129: Boxplots of the classification error rates for single decision tree, random forest, and AdaBoost<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_gray</span>(<span class="at">base_size =</span> <span class="dv">15</span>))</span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb157-5"><a href="#cb157-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb157-6"><a href="#cb157-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb157-7"><a href="#cb157-7" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb157-8"><a href="#cb157-8" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb157-9"><a href="#cb157-9" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb157-10"><a href="#cb157-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-11"><a href="#cb157-11" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>,</span>
<span id="cb157-12"><a href="#cb157-12" aria-hidden="true" tabindex="-1"></a>                                       <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb157-13"><a href="#cb157-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb157-14"><a href="#cb157-14" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>DX_bl <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>DX_bl)</span>
<span id="cb157-15"><a href="#cb157-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-16"><a href="#cb157-16" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb157-17"><a href="#cb157-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-18"><a href="#cb157-18" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb157-19"><a href="#cb157-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (K <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>)) {</span>
<span id="cb157-20"><a href="#cb157-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-21"><a href="#cb157-21" aria-hidden="true" tabindex="-1"></a>testing.indices <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb157-22"><a href="#cb157-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb157-23"><a href="#cb157-23" aria-hidden="true" tabindex="-1"></a>testing.indices <span class="ot">&lt;-</span> <span class="fu">rbind</span>(testing.indices, <span class="fu">sample</span>(<span class="fu">nrow</span>(data),</span>
<span id="cb157-24"><a href="#cb157-24" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">floor</span>((<span class="dv">1</span> <span class="sc">-</span> K) <span class="sc">*</span> <span class="fu">nrow</span>(data))))</span>
<span id="cb157-25"><a href="#cb157-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb157-26"><a href="#cb157-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-27"><a href="#cb157-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb157-28"><a href="#cb157-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-29"><a href="#cb157-29" aria-hidden="true" tabindex="-1"></a>  testing.ix <span class="ot">&lt;-</span> testing.indices[i, ]</span>
<span id="cb157-30"><a href="#cb157-30" aria-hidden="true" tabindex="-1"></a>  target.testing <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl[testing.ix]</span>
<span id="cb157-31"><a href="#cb157-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb157-32"><a href="#cb157-32" aria-hidden="true" tabindex="-1"></a>  tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ])</span>
<span id="cb157-33"><a href="#cb157-33" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree, data[testing.ix, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb157-34"><a href="#cb157-34" aria-hidden="true" tabindex="-1"></a>  error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb157-35"><a href="#cb157-35" aria-hidden="true" tabindex="-1"></a>                  target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb157-36"><a href="#cb157-36" aria-hidden="true" tabindex="-1"></a>  err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;tree&quot;</span>, K, error))</span>
<span id="cb157-37"><a href="#cb157-37" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb157-38"><a href="#cb157-38" aria-hidden="true" tabindex="-1"></a>  rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ])</span>
<span id="cb157-39"><a href="#cb157-39" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[testing.ix, ])</span>
<span id="cb157-40"><a href="#cb157-40" aria-hidden="true" tabindex="-1"></a>  error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span> </span>
<span id="cb157-41"><a href="#cb157-41" aria-hidden="true" tabindex="-1"></a>                  target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb157-42"><a href="#cb157-42" aria-hidden="true" tabindex="-1"></a>  err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;RF&quot;</span>, K, error))</span>
<span id="cb157-43"><a href="#cb157-43" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb157-44"><a href="#cb157-44" aria-hidden="true" tabindex="-1"></a>  data1 <span class="ot">&lt;-</span> data</span>
<span id="cb157-45"><a href="#cb157-45" aria-hidden="true" tabindex="-1"></a>  data1<span class="sc">$</span>DX_bl <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(data1<span class="sc">$</span>DX_bl))</span>
<span id="cb157-46"><a href="#cb157-46" aria-hidden="true" tabindex="-1"></a>  boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data1[<span class="sc">-</span>testing.ix, ],</span>
<span id="cb157-47"><a href="#cb157-47" aria-hidden="true" tabindex="-1"></a>               <span class="at">dist =</span> <span class="st">&quot;adaboost&quot;</span>,<span class="at">interaction.depth =</span> <span class="dv">6</span>,</span>
<span id="cb157-48"><a href="#cb157-48" aria-hidden="true" tabindex="-1"></a>               <span class="at">n.tree =</span> <span class="dv">2000</span>)  <span class="co">#cv.folds = 5, </span></span>
<span id="cb157-49"><a href="#cb157-49" aria-hidden="true" tabindex="-1"></a>  <span class="co"># best.iter &lt;- gbm.perf(boost,method=&#39;cv&#39;)</span></span>
<span id="cb157-50"><a href="#cb157-50" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost, data1[testing.ix, ], <span class="at">n.tree =</span> <span class="dv">2000</span>,</span>
<span id="cb157-51"><a href="#cb157-51" aria-hidden="true" tabindex="-1"></a>                  <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)  <span class="co"># best.iter n.tree = 400, </span></span>
<span id="cb157-52"><a href="#cb157-52" aria-hidden="true" tabindex="-1"></a>  pred[pred <span class="sc">&gt;</span> <span class="fl">0.5</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb157-53"><a href="#cb157-53" aria-hidden="true" tabindex="-1"></a>  pred[pred <span class="sc">&lt;=</span> <span class="fl">0.5</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb157-54"><a href="#cb157-54" aria-hidden="true" tabindex="-1"></a>  error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb157-55"><a href="#cb157-55" aria-hidden="true" tabindex="-1"></a>                        target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb157-56"><a href="#cb157-56" aria-hidden="true" tabindex="-1"></a>  err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;AdaBoost&quot;</span>, K, error))</span>
<span id="cb157-57"><a href="#cb157-57" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb157-58"><a href="#cb157-58" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb157-59"><a href="#cb157-59" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb157-60"><a href="#cb157-60" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;training_percent&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb157-61"><a href="#cb157-61" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">training_percent =</span></span>
<span id="cb157-62"><a href="#cb157-62" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(training_percent)), <span class="at">error =</span></span>
<span id="cb157-63"><a href="#cb157-63" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb157-64"><a href="#cb157-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-65"><a href="#cb157-65" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> err.mat <span class="sc">%&gt;%</span></span>
<span id="cb157-66"><a href="#cb157-66" aria-hidden="true" tabindex="-1"></a>       <span class="fu">mutate</span>(<span class="at">training_percent =</span> <span class="fu">as.factor</span>(training_percent)), </span>
<span id="cb157-67"><a href="#cb157-67" aria-hidden="true" tabindex="-1"></a>         <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> training_percent,</span>
<span id="cb157-68"><a href="#cb157-68" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> method)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f7-30">129</a> shows that the decision tree is less accurate than the other two ensemble methods. The random forest has lower error rates than AdaBoost in general. As the training data size increases, the gap between random forest and AdaBoost decreases. This may indicate that when the training data size is small, the random forest is more stable due to its advantage of addressing the statistical issue. Overall, all models become better as the percentage of the training data increases.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-32"></span>
<img src="graphics/7_32.png" alt=" Boxplots of the classification error rates for AdaBoost with a different number of trees" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 130:  Boxplots of the classification error rates for AdaBoost with a different number of trees<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We adjust the number of trees in AdaBoost and show the results in Figure <a href="#fig:f7-32">130</a>. It can be seen that the error rates first go down as the number of trees increases to <span class="math inline">\(400\)</span>. Then the error rates increase, and decrease again. The unstable relationship between the error rates with the number of trees of AdaBoost indicates that AdaBoost is impacted by some particularity of the dataset and seems less robust than random forest.</p>
<p></p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb158-4"><a href="#cb158-4" aria-hidden="true" tabindex="-1"></a>  data1 <span class="ot">&lt;-</span> data</span>
<span id="cb158-5"><a href="#cb158-5" aria-hidden="true" tabindex="-1"></a>  data1<span class="sc">$</span>DX_bl <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(data1<span class="sc">$</span>DX_bl))</span>
<span id="cb158-6"><a href="#cb158-6" aria-hidden="true" tabindex="-1"></a>  ntree.v <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">200</span>, <span class="dv">300</span>, <span class="dv">400</span>, <span class="dv">500</span>, <span class="dv">600</span>, <span class="dv">800</span>, <span class="dv">1000</span>, <span class="dv">1200</span>,</span>
<span id="cb158-7"><a href="#cb158-7" aria-hidden="true" tabindex="-1"></a>               <span class="dv">1400</span>, <span class="dv">1600</span>, <span class="dv">1800</span>, <span class="dv">2000</span>)</span>
<span id="cb158-8"><a href="#cb158-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> ntree.v) {</span>
<span id="cb158-9"><a href="#cb158-9" aria-hidden="true" tabindex="-1"></a>    boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data1[<span class="sc">-</span>testing.ix, ],</span>
<span id="cb158-10"><a href="#cb158-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">dist =</span> <span class="st">&quot;adaboost&quot;</span>, <span class="at">interaction.depth =</span> <span class="dv">6</span>,</span>
<span id="cb158-11"><a href="#cb158-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">n.tree =</span> j)</span>
<span id="cb158-12"><a href="#cb158-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># best.iter &lt;- gbm.perf(boost,method=&#39;cv&#39;)</span></span>
<span id="cb158-13"><a href="#cb158-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost, data1[testing.ix, ], <span class="at">n.tree =</span> j,</span>
<span id="cb158-14"><a href="#cb158-14" aria-hidden="true" tabindex="-1"></a>                    <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb158-15"><a href="#cb158-15" aria-hidden="true" tabindex="-1"></a>    pred[pred <span class="sc">&gt;</span> <span class="fl">0.5</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb158-16"><a href="#cb158-16" aria-hidden="true" tabindex="-1"></a>    pred[pred <span class="sc">&lt;=</span> <span class="fl">0.5</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb158-17"><a href="#cb158-17" aria-hidden="true" tabindex="-1"></a>    error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb158-18"><a href="#cb158-18" aria-hidden="true" tabindex="-1"></a>                    target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb158-19"><a href="#cb158-19" aria-hidden="true" tabindex="-1"></a>    err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;AdaBoost&quot;</span>, j, error))</span>
<span id="cb158-20"><a href="#cb158-20" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb158-21"><a href="#cb158-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb158-22"><a href="#cb158-22" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb158-23"><a href="#cb158-23" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;num_trees&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb158-24"><a href="#cb158-24" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span></span>
<span id="cb158-25"><a href="#cb158-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">num_trees =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(num_trees)), </span>
<span id="cb158-26"><a href="#cb158-26" aria-hidden="true" tabindex="-1"></a>         <span class="at">error =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb158-27"><a href="#cb158-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb158-28"><a href="#cb158-28" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> err.mat <span class="sc">%&gt;%</span> </span>
<span id="cb158-29"><a href="#cb158-29" aria-hidden="true" tabindex="-1"></a>          <span class="fu">mutate</span>(<span class="at">num_trees =</span> <span class="fu">as.factor</span>(num_trees)), </span>
<span id="cb158-30"><a href="#cb158-30" aria-hidden="true" tabindex="-1"></a>          <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> num_trees, <span class="at">color =</span> method)) <span class="sc">+</span></span>
<span id="cb158-31"><a href="#cb158-31" aria-hidden="true" tabindex="-1"></a>              <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>We repeat the experiment on random forest and show the result in Figure <a href="#fig:f7-33">131</a>. Similar to AdaBoost, when the number of trees is small, the random forest has higher error rates. Then, the error rates decrease as more trees are added. And the error rates become stable when more trees are added. The random forest handles the statistical issue better than the AdaBoost.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-33"></span>
<img src="graphics/7_33.png" alt=" Boxplots of the classification error rates for random forests with a different number of trees" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 131:  Boxplots of the classification error rates for random forests with a different number of trees<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a>testing.ix <span class="ot">&lt;-</span> testing.indices[i, ]</span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a>target.testing <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl[testing.ix]</span>
<span id="cb159-6"><a href="#cb159-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb159-7"><a href="#cb159-7" aria-hidden="true" tabindex="-1"></a>ntree.v <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">400</span>, <span class="dv">600</span>, <span class="dv">800</span>, <span class="dv">1000</span>)</span>
<span id="cb159-8"><a href="#cb159-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> ntree.v) {</span>
<span id="cb159-9"><a href="#cb159-9" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ], <span class="at">ntree =</span> j)</span>
<span id="cb159-10"><a href="#cb159-10" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[testing.ix, ])</span>
<span id="cb159-11"><a href="#cb159-11" aria-hidden="true" tabindex="-1"></a>error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb159-12"><a href="#cb159-12" aria-hidden="true" tabindex="-1"></a>                        target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb159-13"><a href="#cb159-13" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;RF&quot;</span>, j, error))</span>
<span id="cb159-14"><a href="#cb159-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb159-15"><a href="#cb159-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb159-16"><a href="#cb159-16" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb159-17"><a href="#cb159-17" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;num_trees&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb159-18"><a href="#cb159-18" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">num_trees =</span></span>
<span id="cb159-19"><a href="#cb159-19" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(num_trees)), </span>
<span id="cb159-20"><a href="#cb159-20" aria-hidden="true" tabindex="-1"></a><span class="at">error =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb159-21"><a href="#cb159-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb159-22"><a href="#cb159-22" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> </span>
<span id="cb159-23"><a href="#cb159-23" aria-hidden="true" tabindex="-1"></a>            err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">num_trees =</span> <span class="fu">as.factor</span>(num_trees)), </span>
<span id="cb159-24"><a href="#cb159-24" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> num_trees, <span class="at">color =</span> method)) <span class="sc">+</span> </span>
<span id="cb159-25"><a href="#cb159-25" aria-hidden="true" tabindex="-1"></a>            <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>Building on the result shown in Figure <a href="#fig:f7-33">131</a>, we pursue a further study of the behavior of random forest. Recall that, in random forest, there are two approaches to increase diversity, one is to Bootstrap samples for each tree, while another is to conduct random feature selection for splitting each node.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-36"></span>
<img src="graphics/7_36.png" alt=" Boxplots of the classification error rates for random forest with a different sample sizes" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 132:  Boxplots of the classification error rates for random forest with a different sample sizes<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>First, we investigate the effectiveness of the use of Bootstrap. We change the sampling strategy from <em>sampling with replacement</em> to <em>sampling without replacement</em> and change the sampling size<label for="tufte-sn-193" class="margin-toggle sidenote-number">193</label><input type="checkbox" id="tufte-sn-193" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">193</span> The sampling size is the sample size of the Bootstrapped dataset.</span> from <span class="math inline">\(10\%\)</span> to <span class="math inline">\(100\%\)</span>. The number of features tested at each node is kept at the default value, i.e., <span class="math inline">\(\sqrt{p}\)</span>, where <span class="math inline">\(p\)</span> is the number of features. Figure <a href="#fig:f7-36">132</a> shows that the increased sample size has an impact on the error rates.</p>
<p></p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a>  testing.ix <span class="ot">&lt;-</span> testing.indices[i, ]</span>
<span id="cb160-5"><a href="#cb160-5" aria-hidden="true" tabindex="-1"></a>  target.testing <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl[testing.ix]</span>
<span id="cb160-6"><a href="#cb160-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb160-7"><a href="#cb160-7" aria-hidden="true" tabindex="-1"></a>  sample.size.v <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb160-8"><a href="#cb160-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> sample.size.v) {</span>
<span id="cb160-9"><a href="#cb160-9" aria-hidden="true" tabindex="-1"></a>    sample.size <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fu">nrow</span>(data[<span class="sc">-</span>testing.ix, ]) <span class="sc">*</span> j)</span>
<span id="cb160-10"><a href="#cb160-10" aria-hidden="true" tabindex="-1"></a>    rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ],</span>
<span id="cb160-11"><a href="#cb160-11" aria-hidden="true" tabindex="-1"></a>                       <span class="at">sampsize =</span> sample.size, </span>
<span id="cb160-12"><a href="#cb160-12" aria-hidden="true" tabindex="-1"></a>                       <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb160-13"><a href="#cb160-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[testing.ix, ])</span>
<span id="cb160-14"><a href="#cb160-14" aria-hidden="true" tabindex="-1"></a>    error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb160-15"><a href="#cb160-15" aria-hidden="true" tabindex="-1"></a>                target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb160-16"><a href="#cb160-16" aria-hidden="true" tabindex="-1"></a>    err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;RF&quot;</span>, j, error))</span>
<span id="cb160-17"><a href="#cb160-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb160-18"><a href="#cb160-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb160-19"><a href="#cb160-19" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb160-20"><a href="#cb160-20" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;sample_size&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb160-21"><a href="#cb160-21" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">sample_size =</span></span>
<span id="cb160-22"><a href="#cb160-22" aria-hidden="true" tabindex="-1"></a>                <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(sample_size)), </span>
<span id="cb160-23"><a href="#cb160-23" aria-hidden="true" tabindex="-1"></a>                <span class="at">error =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb160-24"><a href="#cb160-24" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> err.mat <span class="sc">%&gt;%</span> </span>
<span id="cb160-25"><a href="#cb160-25" aria-hidden="true" tabindex="-1"></a>              <span class="fu">mutate</span>(<span class="at">sample_size =</span> <span class="fu">as.factor</span>(sample_size)), </span>
<span id="cb160-26"><a href="#cb160-26" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> sample_size,<span class="at">color =</span> method)) <span class="sc">+</span> </span>
<span id="cb160-27"><a href="#cb160-27" aria-hidden="true" tabindex="-1"></a>           <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-35"></span>
<img src="graphics/7_35.png" alt=" Boxplots of the classification error rates for random forest with a different number of features" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 133:  Boxplots of the classification error rates for random forest with a different number of features<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We then investigate the effectiveness of using random selection of features for node splitting. We fix the sampling size to be the same size as the original dataset, and change the number of features to be selected. Results are shown in Figure <a href="#fig:f7-35">133</a>. When the number of features reaches <span class="math inline">\(11\)</span>, the error rate starts to increase. This is probably because of the loss of the diversity of the trees, i.e., the more features to be used, the less randomness is introduced into the trees.</p>
<p></p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a>  testing.ix <span class="ot">&lt;-</span> testing.indices[i, ]</span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a>  target.testing <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl[testing.ix]</span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a>  num.fea.v <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>(<span class="fu">ncol</span>(data) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> num.fea.v) {</span>
<span id="cb161-9"><a href="#cb161-9" aria-hidden="true" tabindex="-1"></a>    sample.size <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data[<span class="sc">-</span>testing.ix, ])</span>
<span id="cb161-10"><a href="#cb161-10" aria-hidden="true" tabindex="-1"></a>    rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ],</span>
<span id="cb161-11"><a href="#cb161-11" aria-hidden="true" tabindex="-1"></a>                       <span class="at">mtry =</span> j, <span class="at">sampsize =</span> sample.size, </span>
<span id="cb161-12"><a href="#cb161-12" aria-hidden="true" tabindex="-1"></a>                       <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb161-13"><a href="#cb161-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[testing.ix, ])</span>
<span id="cb161-14"><a href="#cb161-14" aria-hidden="true" tabindex="-1"></a>    error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb161-15"><a href="#cb161-15" aria-hidden="true" tabindex="-1"></a>                  target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb161-16"><a href="#cb161-16" aria-hidden="true" tabindex="-1"></a>    err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;RF&quot;</span>, j, error))</span>
<span id="cb161-17"><a href="#cb161-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb161-18"><a href="#cb161-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb161-19"><a href="#cb161-19" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb161-20"><a href="#cb161-20" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;num_fea&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb161-21"><a href="#cb161-21" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(num_fea</span>
<span id="cb161-22"><a href="#cb161-22" aria-hidden="true" tabindex="-1"></a>                    <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(num_fea)),</span>
<span id="cb161-23"><a href="#cb161-23" aria-hidden="true" tabindex="-1"></a>                    <span class="at">error =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb161-24"><a href="#cb161-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-25"><a href="#cb161-25" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span></span>
<span id="cb161-26"><a href="#cb161-26" aria-hidden="true" tabindex="-1"></a>             err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">num_fea =</span> <span class="fu">as.factor</span>(num_fea)), </span>
<span id="cb161-27"><a href="#cb161-27" aria-hidden="true" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> num_fea, <span class="at">color =</span> method)) <span class="sc">+</span></span>
<span id="cb161-28"><a href="#cb161-28" aria-hidden="true" tabindex="-1"></a>           <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
</div>
</div>
<div id="remarks-5" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="is-svm-a-more-complex-model" class="section level3 unnumbered">
<h3>Is SVM a more complex model?</h3>
<p>In the preface of his seminar book<label for="tufte-sn-194" class="margin-toggle sidenote-number">194</label><input type="checkbox" id="tufte-sn-194" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">194</span> Vapnik, V., <em>The Nature of Statistical Learning Theory</em>, Springer, 2000.</span>, Vladimir Vapnik wrote that “<em>…during the last few years at different computer science conferences, I heard reiteration of the following claim: ‘Complex theories do not work, simple algorithms do’…this is not true…Nothing is more practical than a good theory…</em>.” He created the concept of <em>VC dimension</em> to specifically characterize his concept of the complexity of a model.</p>
<p>A model is often <em>perceived</em> to be complex. The SVM model looks more complex than the linear regression model. It asks us to characterize the margin using model parameters, write the optimization formulation, learn the trick of kernel function, and understand the support vectors and the slack variables for the nonseparable case. But, don’t forget that the reason for a model to look simple is probably only because this model may presuppose stronger conditions, too strong that we forget they are assumptions.</p>
<p>It is fair to say that a model is more complex if it provides more capacity to represent the statistical phenomena in the training data. In other words, a more complex model is more flexible to respond to subtle patterns in the data by adjusting itself. In this sense, SVM with kernel functions is a complex model since it can model nonlinearity in the data. But on the other hand, comparing the SVM model with other linear models as shown in Figure <a href="#fig:f7-20">134</a>, it is hard to tell that the SVM model is simpler, but it is clear that it is more stubborn; because of its pursuit of maximum margin, it ends up with one model only. If you are looking for an example of an idea that is radical and conservative, flexible and disciplined, this is it.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f7-20"></span>
<p class="caption marginnote shownote">
Figure 134: (Left) some other linear models; (b) the SVM model
</p>
<img src="graphics/7_20.png" alt="(Left) some other linear models; (b) the SVM model" width="80%"  />
</div>
<p></p>
</div>
<div id="is-svm-a-neural-network-model" class="section level3 unnumbered">
<h3>Is SVM a neural network model?</h3>
<p>Another interesting fact about SVM is that, when it was developed, it was named “support vector network”<label for="tufte-sn-195" class="margin-toggle sidenote-number">195</label><input type="checkbox" id="tufte-sn-195" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">195</span> Cortes, C. and Vapnik, V., <em>Support-vector networks,</em> Machine Learning, Volume 20, Issue 3, Pages 273–297, 1995.</span>. In other words, it has a connection with the artificial neural network that will be discussed in <strong>Chapter 10</strong>. This is revealed in Figure <a href="#fig:f7-21">135</a>. Readers who know neural network models are encouraged to write up the mathematical model of the SVM model following the neural network format as shown in Figure <a href="#fig:f7-21">135</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f7-21"></span>
<p class="caption marginnote shownote">
Figure 135: SVM as a neural network model
</p>
<img src="graphics/7_21.png" alt="SVM as a neural network model " width="80%"  />
</div>
<p></p>
</div>
<div id="derivation-of-the-margin" class="section level3 unnumbered">
<h3>Derivation of the margin</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-margin-proj"></span>
<img src="graphics/7_margin_proj.png" alt="Illustration of how to derive the margin" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 136: Illustration of how to derive the margin<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Consider any two points on the two margins, e.g., the <span class="math inline">\(\boldsymbol{x}_A\)</span> and <span class="math inline">\(\boldsymbol{x}_B\)</span> in Figure <a href="#fig:f7-margin-proj">136</a>. The <em>margin width</em> is equal to the projection of the vector <span class="math inline">\(\overrightarrow{A B} = \boldsymbol{x}_B - \boldsymbol{x}_A\)</span> on the direction <span class="math inline">\(\boldsymbol{w}\)</span>, which is</p>
<p><span class="math display" id="eq:7-marginpre">\[\begin{equation} 
    \text{margin } = \frac{ (\boldsymbol{x}_B - \boldsymbol{x}_A) \cdot \vec{\boldsymbol{w}}}{\|\boldsymbol{w}\|}.
\tag{81}
\end{equation}\]</span></p>
<p>It is known that</p>
<p><span class="math display">\[ \boldsymbol{w}^{T} \boldsymbol{x}_B + b =1, \]</span></p>
<p>and</p>
<p><span class="math display">\[ \boldsymbol{w}^{T} \boldsymbol{x}_A + b = -1. \]</span></p>
<p>Thus, Eq. <a href="#eq:7-marginpre">(81)</a> is rewritten as</p>
<p><span class="math display" id="eq:7-margin">\[\begin{equation} 
    \text{margin } = \frac{2}{\|\boldsymbol{w}\|}.
\tag{82}
\end{equation}\]</span></p>
</div>
<div id="why-the-nonzero-alpha_n-are-the-support-vectors" class="section level3 unnumbered">
<h3>Why the nonzero <span class="math inline">\(\alpha_n\)</span> are the support vectors</h3>
<p>Theoretically, to understand why the nonzero <span class="math inline">\(\alpha_n\)</span> are the support vectors, we can use the <em>Karush–Kuhn–Tucker (KKT) conditions</em><label for="tufte-sn-196" class="margin-toggle sidenote-number">196</label><input type="checkbox" id="tufte-sn-196" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">196</span> Bertsekas, D., <em>Nonlinear Programming: 3rd Edition</em>, Athena Scientific, 2016.</span>. Based on the <em>complementary slackness</em> as one of the KKT conditions, the following equations must hold</p>
<p><span class="math display">\[ 
\alpha_{n}\left[y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1\right]=0 \text {, for } n=1,2, \dots, N.
\]</span></p>
<p>Thus, for any data point <span class="math inline">\(\boldsymbol{x}_n\)</span>, it is either</p>
<p><span class="math display">\[ 
\alpha_{n} = 0 \text {, and } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1 \neq 0;
\]</span></p>
<p>or</p>
<p><span class="math display">\[ 
\alpha_{n} \neq 0 \text {, and } y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1 = 0.
\]</span></p>
<p>Revisiting Eq. <a href="#eq:7-5regions">(58)</a> or Figure <a href="#fig:f7-5regions">116</a>, we know that only the support vectors have <span class="math inline">\(\alpha_{n} \neq 0\)</span> and <span class="math inline">\(y_{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{n}+b\right)-1 = 0\)</span>.</p>
<!-- % A model is determined by $\boldsymbol{w}$ and $b$. In order for the idea of the *maximum margin* to work, the *margin* of the model has to be a function of the model parameters, $\boldsymbol{w}$ and $b$. This is not necessary always granted in reality. First, let's assume that the *margin* could be characterized by the model parameters. Then, let's concern a simpler problem first. Look at Figure~\@ref(fig:f7-3), and derive the perpendicular distance^[Denoted as $\|A N\|$] between the point $A$ with the line^[The line's mathematical model is $\boldsymbol{w}^{T} \boldsymbol{x}+b=0$]. -->
<!-- % To facilitate the derivation, let's further identify two points, $N$ and $B$, on the line.  -->
<!-- % It is known that -->
<!-- % \begin{equation} -->
<!-- %   \|A N\|=\|A B\| \cos \theta.   -->
<!-- %   (\#eq:7-marginAN) -->
<!-- % \end{equation} -->
<!-- % And by definition, we know that: -->
<!-- % \begin{equation}  -->
<!-- %     \cos \theta = \frac{\overrightarrow{A B} \cdot \vec{\boldsymbol{w}}}{\|A B\|\|\boldsymbol{w}\|}, -->
<!-- %     (\#eq:7-costheta) -->
<!-- % \end{equation} -->
<!-- % \noindent where $\overrightarrow{A B}$ is defined as^[$\boldsymbol{x}_{a}$ and  -->
<!-- % $\boldsymbol{x}_{b}$ are the coordinates of the two data points $A$ and $B$, respectively.]: -->
<!-- % \begin{equation} -->
<!-- %     \overrightarrow{A B} = \boldsymbol{x}_{a}-\boldsymbol{x}_{b}. -->
<!-- %     (\#eq:7-arrowAB) -->
<!-- % \end{equation} -->
<!-- % We plug Eq.~\@ref(eq:7-costheta) in Eq.~\@ref(eq:7-marginAN) and get: -->
<!-- % \begin{equation} -->
<!-- %     \|A N\| = \frac{\overrightarrow{A B} \cdot \vec{\boldsymbol{w}}}{\|\boldsymbol{w}\|}. -->
<!-- %     (\#eq:7-marginAN2) -->
<!-- % \end{equation} -->
<!-- % We plug Eq.~\@ref(eq:7-arrowAB) in Eq.~\@ref(eq:7-marginAN2) and get: -->
<!-- % \[  -->
<!-- % \|A N\| = \frac{\overrightarrow{A B} \cdot \vec{\boldsymbol{w}}}{\|\boldsymbol{w}\|}=\frac{\boldsymbol{w}^{T}\left(\boldsymbol{x}_{a}-\boldsymbol{x}_{b}\right)}{\|\boldsymbol{w}\|}. -->
<!-- % \] -->
<!-- % Recall that the data point $B$ is a data point on the line $\boldsymbol{w}^{T} \boldsymbol{x}+b=0$,  -->
<!-- % \begin{equation} -->
<!-- %     \boldsymbol{w}^{T} \boldsymbol{x}_{b} = -b. -->
<!-- % \end{equation} -->
<!-- % We can finally derive the mathematical expression of $\|A N\|$ in terms of $\boldsymbol{w}$ and $b$: -->
<!-- % \begin{equation} -->
<!-- %     \|A N\|=\frac{\boldsymbol{w}^{T}\left(\boldsymbol{x}_{a}-\boldsymbol{x}_{\boldsymbol{b}}\right)}{\|\boldsymbol{w}\|}=\frac{\boldsymbol{w}^{T} \boldsymbol{x}_{\boldsymbol{a}}+b}{\|\boldsymbol{w}\|}. -->
<!-- %    (\#eq:7-ANfinal) -->
<!-- % \end{equation} -->
<!-- % This is a major step towards the development of the *maximum margin* for SVM. To see that, let's apply this conclusion Eq.~\@ref(eq:7-ANfinal) on Figure \@ref(fig:f7-2) to obtain Figure \@ref(fig:f7-4). -->
</div>
<div id="adaboost-algorithm" class="section level3 unnumbered">
<h3>AdaBoost algorithm</h3>
<p>The specifics of the AdaBoost algorithm shown in Figure <a href="#fig:f7-AdaBoost">123</a> are described below.</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> Input: <span class="math inline">\(N\)</span> data points, <span class="math inline">\(\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{N}, y_{N}\right)\)</span>.</p></li>
<li><p> Initialization: Initialize equal weights for all data points <span class="math display">\[ \boldsymbol{w}_{0}=\left(\frac{1}{N}, \ldots, \frac{1}{N}\right).\]</span></p></li>
<li><p> At iteration <span class="math inline">\(t\)</span>:</p></li>
</ul>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> Step 1: Build model <span class="math inline">\(h_t\)</span> on the dataset with weights <span class="math inline">\(\boldsymbol{w}_{t-1}\)</span>.</p></li>
<li><p> Step 2: Calculate errors using <span class="math inline">\(h_t\)</span> <span class="math display">\[ \epsilon_{t}=\sum_{n=1}^{N} w_{t, n}\left\{h_{t}\left(x_{n}\right) \neq y_{n}\right\}.\]</span></p></li>
<li><p> Step 3: Update weights of the data points <span class="math display">\[
  \boldsymbol{w}_{t+1, i}=\frac{w_{t, i}}{Z_{t}} \times \left\{\begin{array}{c}{e^{-\alpha_{t}} \text { if } h_{t}\left(x_{n}\right)=y_{n}} \\ {e^{\alpha_{t}} \text { if } h_{t}\left(x_{n}\right) \neq y_{n}}.\end{array} \right.\]</span> Here, <span class="math display">\[Z_{t} \text { is a normalization factor so that } \sum_{n=1}^{N} w_{t+1, n}=1,\]</span> and <span class="math display">\[ \alpha_{t}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{t}}{\epsilon_{t}}\right).\]</span></p></li>
</ul>
<p><!-- end{itemize} --></p>
<ul>
<li><p> Iterations: Repeat Step 1 to Step 3 for <span class="math inline">\(T\)</span> times, to get <span class="math inline">\(h_1\)</span>, <span class="math inline">\(h_2\)</span>, <span class="math inline">\(h_3\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(h_T\)</span>.</p></li>
<li><p> Output: <span class="math display">\[ H(x)=\operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} h_{t}(x)\right).\]</span></p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>When all the base models are trained, the aggregation of these models in predicting on a data instance <span class="math inline">\(\boldsymbol{x}\)</span> is a weighted sum of base models</p>
<p><span class="math display">\[
h(\boldsymbol{x})=\sum_{i} \gamma_{i} h_{i}(\boldsymbol{x}),
\]</span></p>
<p>where the weight <span class="math inline">\(\gamma_{i}\)</span> is proportional to the accuracy of <span class="math inline">\(h_{i}(x)\)</span> on the training dataset.</p>
</div>
</div>
<div id="exercises-5" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-hw-sv"></span>
<img src="graphics/7_hw_sv.png" alt="How many support vectors are needed?" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 137: How many support vectors are needed?<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> To build a linear SVM on the data shown in Figure <a href="#fig:f7-hw-sv">137</a>, how many support vectors are needed (use visual inspection)?</p></li>
<li><p> Let’s consider the dataset in Table <a href="#tab:t7-hw-svm">32</a>. Please (a) draw scatterplots and identify the support vectors if you’d like to build a linear SVM classifier; (b) manually derive the alpha values (i.e., the <span class="math inline">\(\alpha_i\)</span>) for the support vectors and the offset parameter <span class="math inline">\(b\)</span>; (c) derive the weight vector (i.e., the <span class="math inline">\(\hat{\boldsymbol{w}}\)</span>) of the SVM model; and (d) predict on the new dataset and fill in the column of <span class="math inline">\(y\)</span> in Table <a href="#tab:t7-hw-svm-test">33</a>.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t7-hw-svm">Table 32: </span>Dataset for building a SVM model in Q2</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(8\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(-2.5\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(-0.3\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(2.5\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(8\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t7-hw-svm-test">Table 33: </span>Test data points for the SVM model in Q2</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(9\)</span></td>
<td align="left"><span class="math inline">\(5.4\)</span></td>
<td align="left"><span class="math inline">\(1.2\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(10\)</span></td>
<td align="left"><span class="math inline">\(1.5\)</span></td>
<td align="left"><span class="math inline">\(-2\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(11\)</span></td>
<td align="left"><span class="math inline">\(-3.4\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-2\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(12\)</span></td>
<td align="left"><span class="math inline">\(-2.2\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(-4\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Follow up on the dataset used in Q2. Use the R pipeline for SVM on this data. Compare the alpha values (i.e., the <span class="math inline">\(\alpha_i\)</span>), the offset parameter <span class="math inline">\(b\)</span>, and the weight vector (i.e., the <span class="math inline">\(\hat{\boldsymbol{w}}\)</span>) from R and the result by your manual calculation in Q2.</p></li>
<li><p> Modify the R pipeline for Bootstrap and incorporate the <code>glm</code> package to write your own version of ensemble learning that ensembles a set of logistic regression models. Test it using the same data that has been used in the R lab for logistic regression models.</p></li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-hw-2class"></span>
<img src="graphics/7_hw_2class.png" alt="A dataset with two classes" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 138: A dataset with two classes<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<ul>
<li><p> Use the dataset <code>PimaIndiansDiabetes2</code> in the <code>mlbench</code> R package, run the R SVM pipeline on it, and summarize your findings.</p></li>
<li><p> Use R to generate a dataset with two classes as shown in Figure <a href="#fig:f7-hw-2class">138</a>. Then, run SVM model with a properly selected kernel function on this dataset.</p></li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-svm-visual"></span>
<img src="graphics/7_svm_visual.png" alt="Visualization of the decision boundary of an SVM model with Gaussian kernel" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 139: Visualization of the decision boundary of an SVM model with Gaussian kernel<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<ul>
<li> Follow up on the dataset generated in Q6. Try visualizing the decision boundaries by different kernel functions such as linear, Laplace, Gaussian, and polynomial kernel functions. Below is one example using Gaussian kernel with its bandiwidth parameter <span class="math inline">\(\gamma = 0.2\)</span>.<label for="tufte-sn-197" class="margin-toggle sidenote-number">197</label><input type="checkbox" id="tufte-sn-197" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">197</span> In the following R code, the bandiwidth parameter is specified as <code>sigma=0.2</code>.</span> Result is shown in Figure <a href="#fig:f7-svm-visual">139</a>. The blackened points are support vectors, and the contour reflects the characteristics of the decision boundary.</li>
</ul>
<p>The R code for generating Figure <a href="#fig:f7-svm-visual">139</a> is shown below.</p>
<p>Please follow this example and visualize linear, Laplace, Gaussian, and polynomial kernel functions with different parameter values.</p>
<p><!-- end{enumerate} --></p>
<p></p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>( <span class="st">&#39;kernlab&#39;</span> )</span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>rbf.svm <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(y <span class="sc">~</span> ., <span class="at">data=</span>data, <span class="at">type=</span><span class="st">&#39;C-svc&#39;</span>, <span class="at">kernel=</span><span class="st">&#39;rbfdot&#39;</span>,</span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">kpar=</span><span class="fu">list</span>(<span class="at">sigma=</span><span class="fl">0.2</span>), <span class="at">C=</span><span class="dv">100</span>, <span class="at">scale=</span><span class="fu">c</span>())</span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rbf.svm, <span class="at">data=</span>data)</span></code></pre></div>
<p></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<div id="chapter-8.-scalability-lasso-pca" class="section level1 unnumbered">
<h1>Chapter 8. Scalability: LASSO &amp; PCA</h1>
<div id="overview-6" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 8 is about <em>Scalability</em>. <strong>LASSO</strong> and <strong>PCA</strong> will be introduced. LASSO stands for the <strong>least absolute shrinkage and selection operator</strong>, which is a representative method for <em>feature selection</em>. PCA stands for the <strong>principal component analysis</strong>, which is a representative method for <em>dimension reduction</em>. Both methods can reduce the dimensionality of a dataset but follow different styles. LASSO, as a feature selection method, focuses on deletion of irrelevant or redundant features. PCA, as a dimension reduction method, combines the features into a smaller number of aggregated components (a.k.a., the new features). A remarkable difference between the two approaches is that, while both create a dataset with a smaller dimensionality, in PCA the original features are used to derive the new features<label for="tufte-sn-198" class="margin-toggle sidenote-number">198</label><input type="checkbox" id="tufte-sn-198" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">198</span> As a result, no feature is discarded.</span>.</p>
</div>
<div id="lasso" class="section level2 unnumbered">
<h2>LASSO</h2>
<p></p>
<div id="rationale-and-formulation-12" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-intro1"></span>
<img src="graphics/8_intro1.png" alt="A line is not a model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 140: A line is not a model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Two points determine a line, as shown in Figure <a href="#fig:f8-intro1">140</a>. It shares the same geometric form as a linear regression model, but it is a deterministic geometric pattern and has nothing to do with <em>error</em>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-intro"></span>
<img src="graphics/8_intro.png" alt="Revisit the linear regression model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 141: Revisit the linear regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>With one more data point, magic happens: as shown in Figure <a href="#fig:f8-intro">141</a>, now we can estimate the <em>residuals</em> and study the systematic patterns of <em>error</em>. The line in Figure <a href="#fig:f8-intro">141</a> becomes a statistical model.</p>
<p>The two lines in Figures <a href="#fig:f8-intro1">140</a> and <a href="#fig:f8-intro">141</a>, one is a deterministic pattern, while another is a statistical model, are like <em>homonym</em>. The different meanings share the same form of their signifier (e.g., like the word <em>bass</em> that means a certain sound that is low and deep, or a type of fish).</p>
<p>The <em>error</em> is a defining component of a statistical model. It models the <em>noise</em> in the data. In an application context, understanding the noise and knowing how much proportion the noise contributes to the total variation of the dataset is important knowledge. And, to derive the <em>p-values</em> of the regression coefficients, we need the noise so that we can compare the strength of the estimated coefficients with the noise to evaluate if the estimated coefficients are significantly different from random manifestation (i.e., if we cannot model the noise, then we have no basis to define what is random manifestation.).</p>
<p>To model a linear regression model, we need enough data points to estimate the error. For the simple example when there is only one predictor <span class="math inline">\(x\)</span>, as shown in Figures <a href="#fig:f8-intro1">140</a> and <a href="#fig:f8-intro">141</a>, we would need at least <span class="math inline">\(3\)</span> data points to estimate the error<label for="tufte-sn-199" class="margin-toggle sidenote-number">199</label><input type="checkbox" id="tufte-sn-199" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">199</span> While this is obvious from Figures <a href="#fig:f8-intro1">140</a> and <a href="#fig:f8-intro">141</a>, we can also obtain the conclusion by derivation. I.e., given two data points, <span class="math inline">\((x_1, y_1)\)</span> and <span class="math inline">\((x_2, y_2)\)</span> we could write two equations, <span class="math inline">\(y_1 = \beta_{0}+\beta_{1} x_1\)</span> and <span class="math inline">\(y_2 = \beta_{0}+\beta_{1} x_2\)</span>.</span>. This is just enough to solve for the <span class="math inline">\(2\)</span> regression coefficients. Consider a problem with <span class="math inline">\(10\)</span> variables, what is the minimum number of data points needed to enable the estimation of error<label for="tufte-sn-200" class="margin-toggle sidenote-number">200</label><input type="checkbox" id="tufte-sn-200" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">200</span> The answer is <span class="math inline">\(12\)</span>. Suppose that we only have <span class="math inline">\(11\)</span> data points. The regression <em>line</em> is defined by <span class="math inline">\(11\)</span> regression coefficients. For each data point, we can write up an equation. Thus, <span class="math inline">\(11\)</span> data points are just enough to estimate the <span class="math inline">\(11\)</span> regression coefficients, leaving no room for estimating errors.</span>?</p>
<p>From the examples aforementioned, we could deduce that the number of data points, i.e., denoted as <span class="math inline">\(N\)</span>, needs to be larger than the number of variables, i.e., denoted as <span class="math inline">\(p\)</span>. This is barely a minimum requirement of linear regression, as we haven’t asked how many data points are needed to ensure high-quality estimation of the parameters. In classic settings in statistics, <span class="math inline">\(N\)</span> is assumed to be much larger than <span class="math inline">\(p\)</span> in order to prove asymptotics—a common approach to prove a statistical model is valid. Practically, linear regression model finds difficulty in applications where the ratio <span class="math inline">\(N/p\)</span> is small. In recent years, there are applications where the number of data points is even smaller than the number of variables, i.e., commonly referred to as <span class="math inline">\(N &lt; p\)</span> problems.</p>
<p>When increasing <span class="math inline">\(N\)</span> is not always a feasible option, reducing <span class="math inline">\(p\)</span> is a necessity. Some variables may be irrelevant or simply noise. Even if all variables are statistically informative, when considered as a whole, some of them may be redundant, and some are weaker than others. In those scenarios, there is room for us to wriggle with the problematic dataset and improve on the ratio <span class="math inline">\(N/p\)</span> by reducing <span class="math inline">\(p\)</span>.</p>
<p>LASSO was invented in 1996 to sparsify the linear regression model and allow the regression model to select significant predictors automatically<label for="tufte-sn-201" class="margin-toggle sidenote-number">201</label><input type="checkbox" id="tufte-sn-201" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">201</span> Tibshirani, R. <em>Regression shrinkage and selection via the Lasso,</em> Journal of the Royal Statistical Society (Series B), Volume 58, Issue 1, Pages 267-288, 1996.</span>.</p>
<p>Remember that, to estimate <span class="math inline">\(\boldsymbol{\beta}\)</span>, the least squares estimation of linear regression is</p>
<p><span class="math display" id="eq:8-LS">\[\begin{equation}
        \boldsymbol{\hat \beta} = \arg\min_{\boldsymbol \beta} {  (\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta}), }   
\tag{83}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{N \times 1}\)</span> is the measurement vector of the outcome variable, <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{N \times p}\)</span> is the data matrix of the <span class="math inline">\(N\)</span> measurement vectors of the <span class="math inline">\(p\)</span> predictors, <span class="math inline">\(\boldsymbol \beta \in \mathbb{R}^{p \times 1}\)</span> is the regression coefficient vector<label for="tufte-sn-202" class="margin-toggle sidenote-number">202</label><input type="checkbox" id="tufte-sn-202" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">202</span> Here, we assume that the data is normalized/standardized and no intercept coefficient <span class="math inline">\(\beta_0\)</span> is needed. Normalization means <span class="math inline">\(\sum_{n=1}^N x_{nj}/N=0\)</span>, <span class="math inline">\(\sum_{n=1}^N x_{ij}^2/N=1\)</span> for <span class="math inline">\(j=1,2,\dots,p\)</span> and <span class="math inline">\(\sum_{n=1}^N y_n/N=0\)</span>. Normalization is a common practice, and some R packages automatically normalize the data as a default preprocessing step before the application of a model.</span>.</p>
<p>The formulation of LASSO is</p>
<p><span class="math display" id="eq:8-LASSO">\[\begin{equation}
        \boldsymbol{\hat \beta} = \arg\min_{\boldsymbol \beta} \left \{   \underbrace{(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})}_{\text{Least squares}} + \underbrace{\lambda \lVert \boldsymbol{\beta}\rVert_{1}}_{L_1 \text{ norm penalty}} \right \}
\tag{84}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\lVert \boldsymbol{\beta} \rVert_1 = \sum_{i=1}^p \lvert \beta_i \rvert\)</span>. The parameter, <span class="math inline">\(\lambda\)</span>, is called the <strong>penalty parameter</strong> that is specified by user of LASSO. The larger the parameter <span class="math inline">\(\lambda\)</span>, the more zeros in <span class="math inline">\(\boldsymbol{\hat \beta}\)</span>.</p>
<p>It could be seen that LASSO embodies two components in its formulation. The <span class="math inline">\(1\)</span>^{st} term is the least squares loss function inherited from linear regression that is used to measure the goodness-of-fit of the model. The <span class="math inline">\(2\)</span> term is the sum of absolute values of elements in <span class="math inline">\(\boldsymbol{\beta}\)</span> that is called the <span class="math inline">\(L_1\)</span> norm penatly. It measures the <em>model complexity</em>, i.e., smaller <span class="math inline">\(\lVert \boldsymbol{\beta} \rVert_1\)</span> tends to create more zeros in <span class="math inline">\(\boldsymbol{\beta}\)</span>, leading to a simpler model. In practice, by tuning the parameter <span class="math inline">\(\lambda\)</span>, we hope to find the best model with an optimal balance between model fit and model complexity.</p>
<!-- $\left\lVert \boldsymbol{\beta} \right\rVert_1 $ -->
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f8-1"></span>
<p class="caption marginnote shownote">
Figure 142: Path solution trajectory of the coefficients; each curve corresponds to a regression coefficient
</p>
<img src="graphics/8_1.png" alt="Path solution trajectory of the coefficients; each curve corresponds to a regression coefficient" width="80%"  />
</div>
<p></p>
<p>As shown in Figure <a href="#fig:f8-1">142</a>, LASSO can generate a <strong>path solution trajectory</strong> that visualizes the solutions of <span class="math inline">\(\boldsymbol{\beta}\)</span> for a continuum of values of <span class="math inline">\(\lambda\)</span>. Model selection criteria such as the Akaike Information Criteria (AIC) or cross-validation can be used to identify the best <span class="math inline">\(\lambda\)</span> that would help us find the final model, i.e., as the vertical line shown in Figure <a href="#fig:f8-1">142</a>. When many variables are deleted from the model, the dimensionality of the model is reduced, and <span class="math inline">\(N/p\)</span> is increased.</p>
<!-- %\begin{equation} -->
<!-- %    \begin{gathered} -->
<!-- %    y = \beta_{0}+\beta_{1} x + \epsilon, \\ -->
<!-- %    \epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right). -->
<!-- %    \end{gathered} -->
<!-- %(\#eq:8-simLR) -->
<!-- %\end{equation} -->
</div>
<div id="the-shooting-algorithm" class="section level3 unnumbered">
<h3>The shooting algorithm</h3>
<p>We introduce the <strong>shooting algorithm</strong> to solve for the optimization problem shown in Eq. <a href="#eq:8-LASSO">(84)</a>. Let’s consider a simple example when there is only one predictor <span class="math inline">\(x\)</span>. The objective function in Eq. <a href="#eq:8-LASSO">(84)</a> could be rewritten as</p>
<p><span class="math display" id="eq:8-simLR-LASSO">\[\begin{equation}
    l\left(\beta\right)=(\boldsymbol{y} - \boldsymbol{X}\beta)^{T}(\boldsymbol{y} - \boldsymbol{X}\beta) + \lambda \lvert \beta \rvert.
\tag{85}
\end{equation}\]</span></p>
<p>To solve Eq. <a href="#eq:8-simLR-LASSO">(85)</a>, we take the differential of <span class="math inline">\(l\left(\beta\right)\)</span> and put it equal to zero</p>
<p><span class="math display" id="eq:8-simLASSO-grad">\[\begin{equation}
    \frac {\partial l(\beta)}{\partial \beta}=0.
\tag{86}
\end{equation}\]</span></p>
<p>A complication of this differential operation is that the <span class="math inline">\(L_1\)</span>-norm term, <span class="math inline">\(\lvert \beta \rvert\)</span>, has no gradient when <span class="math inline">\(\beta=0\)</span>. There are three scenarios:</p>
<p><!-- begin{itemize} --></p>
<ul>
<li><p> If <span class="math inline">\(\beta&gt;0\)</span>, then <span class="math inline">\(\frac {\partial L(\beta)}{\partial \beta}=2\beta-2\boldsymbol{X}^T\boldsymbol{y}+\lambda\)</span>. Based on Eq. <a href="#eq:8-simLASSO-grad">(86)</a>, we can estimate <span class="math inline">\(\beta\)</span> as <span class="math inline">\(\hat \beta =\boldsymbol{X}^T\boldsymbol{y}-\lambda/2\)</span>. Note that this estimate of <span class="math inline">\(\beta\)</span> may turn out to be negative. If that happens, it would be a contradiction since we have assumed <span class="math inline">\(\beta&gt;0\)</span> to derive the result. This contradiction points to the only possibility that <span class="math inline">\(\beta=0\)</span>.</p></li>
<li><p> If <span class="math inline">\(\beta&lt;0\)</span>, then <span class="math inline">\(\frac {\partial L(\beta)}{\partial \beta}=2\beta-2\boldsymbol{X}^T\boldsymbol{y}-\lambda\)</span>. Based on Eq. <a href="#eq:8-simLASSO-grad">(86)</a>, we have <span class="math inline">\(\beta = \boldsymbol{X}^T\boldsymbol{y}+\lambda/2\)</span>. Note that this estimate of <span class="math inline">\(\beta\)</span> may turn out to be positive. If that happens, it would be a contradiction since we have assumed <span class="math inline">\(\beta&lt;0\)</span> to derive the result. This contradiction points to the only possibility that <span class="math inline">\(\beta=0\)</span>.</p></li>
<li><p> If <span class="math inline">\(\beta=0\)</span>, then we have had the solution and no longer need to derive the gradient.</p></li>
</ul>
<p><!-- end{itemize} --></p>
<p>In summary, the solution of <span class="math inline">\(\beta\)</span> is</p>
<p><span class="math display" id="eq:8-simLASSO-sol">\[\begin{equation}
    \hat \beta = \begin{cases}
    \boldsymbol{X}^T\boldsymbol{y}-\lambda/2, &amp;if \, \boldsymbol{X}^T\boldsymbol{y}-\lambda/2&gt;0 \\
    \boldsymbol{X}^T\boldsymbol{y}+\lambda/2, &amp;if \, \boldsymbol{X}^T\boldsymbol{y}+\lambda/2&lt;0 \\
    0, &amp; if \, \lambda/2 \geq \lvert\boldsymbol{X}^T\boldsymbol{y}\rvert.
    \end{cases}\tag{87}
\end{equation}\]</span></p>
<p>Now let’s consider the general case as shown in Eq. <a href="#eq:8-LASSO">(84)</a>. Figure <a href="#fig:f8-lasso-iter">143</a> illustrates the basic idea: to apply the conclusion (with a slight variation) we have obtained in Eq. <a href="#eq:8-simLASSO-sol">(87)</a> to solve Eq. <a href="#eq:8-LASSO">(84)</a>. Each iteration solves for one regression coefficient, assuming that all the other coefficients are fixed (i.e., to their latest values).</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-lasso-iter"></span>
<img src="graphics/8_lasso_iter.png" alt="The shooting algorithm iterates through the coefficients" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 143: The shooting algorithm iterates through the coefficients<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>In each iteration, we solve a similar problem with the one-predictor special problem shown in Eq. <a href="#eq:8-simLR-LASSO">(85)</a>. For instance, denote <span class="math inline">\(\beta_j^{(t)}\)</span> as the estimate of <span class="math inline">\(\beta_j\)</span> in the <span class="math inline">\(t^{th}\)</span> iteration. If we fix the other regression coefficients as their latest estimates, we can rewrite Eq. <a href="#eq:8-LASSO">(84)</a> as a function of <span class="math inline">\(\beta_j^{(t)}\)</span> only</p>
<p><span class="math display" id="eq:8-LR-LASSOsim">\[\begin{equation}
    l(\beta_j^{(t)}) = \left (\boldsymbol{y}^{(t)}_j - \boldsymbol{X}_{(:,j)}\beta_j^{(t)} \right )^{T} \left (\boldsymbol{y}^{(t)}_j - \boldsymbol{X}_{(:,j)}\beta_j^{(t)}\right ) +
    \lambda \lvert \beta_j^{(t)} \rvert,
\tag{88}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{X}_{(:,j)}\)</span> is the <span class="math inline">\(j^{th}\)</span> column of the matrix <span class="math inline">\(\boldsymbol{X}\)</span>, and</p>
<p><span class="math display" id="eq:8-yt">\[\begin{equation}
    \boldsymbol{y}^{(t)}_j = \boldsymbol{y-} \sum\nolimits_{k\neq j}\boldsymbol{X}_{(:,k)}\hat\beta^{(t)}_{k}.
\tag{89}
\end{equation}\]</span></p>
<p>Eq. <a href="#eq:8-LR-LASSOsim">(88)</a> has the same structure as Eq. <a href="#eq:8-simLR-LASSO">(85)</a>. We can readily apply the conclusion in Eq. <a href="#eq:8-simLASSO-sol">(87)</a> here and obtain</p>
<p><span class="math display" id="eq:8-LASSO-sol">\[\begin{equation}
    \hat\beta_j^{(t)}=\begin{cases}
    q^{(t)}_j - \lambda / 2, &amp; if \, q^{(t)}_j - \lambda/2 &gt;0 \\
    q^{(t)}_j + \lambda / 2, &amp; if \, q^{(t)}_j + \lambda/2 &lt;0 \\
    0, &amp; if \, \lambda/2 \geq \lvert q^{(t)}_j \rvert, \end{cases}
\tag{90}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display" id="eq:8-qj">\[\begin{equation}
    q^{(t)}_j=\boldsymbol{X}_{(:, j)}^T \boldsymbol{y}^{(t)}_j.
\tag{91}
\end{equation}\]</span></p>
</div>
<div id="a-small-data-example" class="section level3 unnumbered">
<h3>A small data example</h3>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-1">Table 34: </span>A dataset example for LASSO</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-0.707\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(-0.77\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.707\)</span></td>
<td align="left"><span class="math inline">\(0.15\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(0.707\)</span></td>
<td align="left"><span class="math inline">\(-0.707\)</span></td>
<td align="left"><span class="math inline">\(0.62\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Consider a dataset example as shown in Table <a href="#tab:t8-1">34</a>.<label for="tufte-sn-203" class="margin-toggle sidenote-number">203</label><input type="checkbox" id="tufte-sn-203" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">203</span> To generate this dataset, we sampled the values of <span class="math inline">\(y\)</span> using the model
<span class="math display">\[
y =0.8x_1+\varepsilon, \text{ where } \varepsilon \sim N(0,0.5).
\]</span>
Only <span class="math inline">\(x_1\)</span> is important.</span></p>
<p>In matrix form, the dataset is rewritten as</p>
<p><span class="math display">\[
\boldsymbol{y}=\left[ \begin{array}{c}{-0.77} \\ {0.15} \\ {0.62}\end{array}\right], \text {     }  \boldsymbol{X}=\left[ \begin{array}{ccccc} {-0.707} &amp; {0} \\ {0} &amp; {0.707} \\ {0.707} &amp; {-0.707}\end{array}\right].
\]</span></p>
<p>Now let’s implement the <em>Shooting algorithm</em> on this data. The objective function of LASSO on this case is</p>
<p><span class="math display">\[
\sum\nolimits_{n=1}\nolimits^3 [y_n-(\beta_1x_{n,1}+\beta_2x_{n,2})]^2+\lambda(\lvert \beta_1 \rvert+\lvert \beta_2 \rvert).
\]</span></p>
<p>Suppose that <span class="math inline">\(\lambda=1\)</span>, and we initiate the regression coefficients as <span class="math inline">\(\hat \beta_1^{(0)}=0\)</span> and <span class="math inline">\(\hat \beta_2^{(0)}=1\)</span>.</p>
<p>To update <span class="math inline">\(\hat \beta_1^{(1)}\)</span>, based on Eq. <a href="#eq:8-LASSO-sol">(90)</a>, we first calculate <span class="math inline">\(\boldsymbol{y}^{(1)}_1\)</span> using Eq. <a href="#eq:8-yt">(89)</a></p>
<p><span class="math display">\[
\boldsymbol{y}^{(1)}_1 = \boldsymbol{y-}\boldsymbol{X}_{(:,2)}\hat\beta_2^{(0)} = \begin{bmatrix}
-0.7700 \\
-0.557 \\
1.3270 \\
\end{bmatrix}
.\]</span></p>
<p>Then we calculate <span class="math inline">\(q^{(1)}_1\)</span> using Eq. <a href="#eq:8-qj">(91)</a></p>
<p><span class="math display">\[
q^{(1)}_1=\boldsymbol{X}_{(:,1)}^T\boldsymbol{y}^{(1)}_1 = 1.7654.
\]</span>
As
<span class="math display">\[
q^{(1)}_1 - \lambda/2 &gt;0,
\]</span>
based on Eq. <a href="#eq:8-LASSO-sol">(90)</a> we know that</p>
<p><span class="math display">\[\hat\beta_1^{(1)} = q^{(1)}_1 - \lambda/2 = 1.2654.
\]</span></p>
<p>Then we update <span class="math inline">\(\hat \beta_2^{(1)}\)</span>. We can obtain that
<span class="math display">\[
\boldsymbol{y}^{(1)}_2 = \boldsymbol{y-}\boldsymbol{X}_{(:,1)}\hat\beta_1^{(1)} = \begin{bmatrix}
0.1876 \\
0.1500 \\
-0.2746\\
\end{bmatrix}
.\]</span>
And we can get
<span class="math display">\[
q^{(1)}_2=\boldsymbol{X}_{(:,2)}^T\boldsymbol{y}^{(1)}_2 = 0.3002.
\]</span>
As
<span class="math display">\[\lambda / 2 \geq \lvert q^{(1)}_2\rvert ,
\]</span>
we know that
<span class="math display">\[\hat \beta_2^{(1)} = 0.\]</span></p>
<p>Thus, with only one iteration, the <em>Shooting algorithm</em> identified the irrelevant variable.</p>
</div>
<div id="r-lab-11" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 7-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get dataset into R and organize the dataset in required format.</p>
<p></p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a><span class="do">#### Read data from a CSV file</span></span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a><span class="do">#### Example: Alzheimer&#39;s Disease</span></span>
<span id="cb163-4"><a href="#cb163-4" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb163-5"><a href="#cb163-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb163-6"><a href="#cb163-6" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb163-7"><a href="#cb163-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD_hd.csv&quot;</span>)</span>
<span id="cb163-8"><a href="#cb163-8" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb163-9"><a href="#cb163-9" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and Y </span></span>
<span id="cb164-3"><a href="#cb164-3" aria-hidden="true" tabindex="-1"></a><span class="co"># vector (outcome variable)</span></span>
<span id="cb164-4"><a href="#cb164-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)]</span>
<span id="cb164-5"><a href="#cb164-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span>
<span id="cb164-6"><a href="#cb164-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-7"><a href="#cb164-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb164-8"><a href="#cb164-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Y,X)</span>
<span id="cb164-9"><a href="#cb164-9" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">1</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span>
<span id="cb164-10"><a href="#cb164-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-11"><a href="#cb164-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data</span></span>
<span id="cb164-12"><a href="#cb164-12" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)) <span class="sc">*</span> <span class="dv">4</span> <span class="sc">/</span> <span class="dv">5</span> )</span>
<span id="cb164-13"><a href="#cb164-13" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb164-14"><a href="#cb164-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data</span></span>
<span id="cb164-15"><a href="#cb164-15" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span>
<span id="cb164-16"><a href="#cb164-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-17"><a href="#cb164-17" aria-hidden="true" tabindex="-1"></a><span class="co"># as.matrix is used here, because the package </span></span>
<span id="cb164-18"><a href="#cb164-18" aria-hidden="true" tabindex="-1"></a><span class="co"># glmnet requires this data format.</span></span>
<span id="cb164-19"><a href="#cb164-19" aria-hidden="true" tabindex="-1"></a>trainX <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.train[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb164-20"><a href="#cb164-20" aria-hidden="true" tabindex="-1"></a>testX <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb164-21"><a href="#cb164-21" aria-hidden="true" tabindex="-1"></a>trainY <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.train[,<span class="dv">1</span>])</span>
<span id="cb164-22"><a href="#cb164-22" aria-hidden="true" tabindex="-1"></a>testY <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.test[,<span class="dv">1</span>])</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f8-BC-path"></span>
<p class="caption marginnote shownote">
Figure 144: Path trajectory of the fitted regression parameters. The figure should be read from right to left (i.e., <span class="math inline">\(\lambda\)</span> from small to large). Variables that become zero later are stronger (i.e., since a larger <span class="math inline">\(\lambda\)</span> is needed to make them become <span class="math inline">\(0\)</span>). The variables that quickly become zero are weak or insignificant variables.
</p>
<img src="graphics/8_BC_path.png" alt="Path trajectory of the fitted regression parameters. The figure should be read from right to left (i.e., $\lambda$ from small to large). Variables that become zero later are stronger (i.e., since a larger $\lambda$ is needed to make them become $0$). The variables that quickly become zero are weak or insignificant variables. " width="80%"  />
</div>
<p></p>
<p><strong>Step 3</strong> uses the R package <code>glmnet</code><label for="tufte-sn-204" class="margin-toggle sidenote-number">204</label><input type="checkbox" id="tufte-sn-204" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">204</span> Check out the argument <code>glmnet</code> to learn more.</span> to build a LASSO model.</p>
<p></p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; Use glmnet to conduct LASSO</span></span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;glmnet&quot;)</span></span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(glmnet)</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">glmnet</span>(trainX,trainY, <span class="at">family=</span><span class="fu">c</span>(<span class="st">&quot;gaussian&quot;</span>))</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(fit<span class="sc">$</span>beta) </span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The fitted sparse regression parameters under </span></span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a><span class="co"># different lambda values are stored in fit$beta.</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> draws the path trajectory of the LASSO models (i.e., as the one shown in Figure <a href="#fig:f8-1">142</a>). The result is shown in Figure <a href="#fig:f8-BC-path">144</a>. It displays the information stored in <code>fit$beta</code>. Each curve shows how the estimated regression coefficient of a variable changes according to the value of <span class="math inline">\(\lambda\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; visualization of the path trajectory of </span></span>
<span id="cb166-2"><a href="#cb166-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the fitted sparse regression parameters</span></span>
<span id="cb166-3"><a href="#cb166-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit,<span class="at">label =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-BC-cv"></span>
<img src="graphics/8_BC_cv.png" alt="Cross-validation result. It is hoped (because it is not always the case in practice) that it is *U-shaped*, like the one shown here, so that we can spot the optimal value of $\lambda$, i.e., the one that corresponds to the lowest dip point. " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 145: Cross-validation result. It is hoped (because it is not always the case in practice) that it is <em>U-shaped</em>, like the one shown here, so that we can spot the optimal value of <span class="math inline">\(\lambda\)</span>, i.e., the one that corresponds to the lowest dip point. <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 5</strong> uses cross-validation to identify the best <span class="math inline">\(\lambda\)</span> value for the LASSO model. The result is shown in Figure <a href="#fig:f8-BC-cv">145</a>.</p>
<p></p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; Use cross-validation to decide which lambda to use</span></span>
<span id="cb167-2"><a href="#cb167-2" aria-hidden="true" tabindex="-1"></a>cv.fit <span class="ot">=</span> <span class="fu">cv.glmnet</span>(trainX,trainY)</span>
<span id="cb167-3"><a href="#cb167-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.fit) </span>
<span id="cb167-4"><a href="#cb167-4" aria-hidden="true" tabindex="-1"></a><span class="co"># look for the u-shape, and identify the lowest </span></span>
<span id="cb167-5"><a href="#cb167-5" aria-hidden="true" tabindex="-1"></a><span class="co"># point that corresponds to the best model</span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> views the best model and evaluates its predictions.</p>
<p></p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; To view the best model and the </span></span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a><span class="co"># corresponding coefficients</span></span>
<span id="cb168-3"><a href="#cb168-3" aria-hidden="true" tabindex="-1"></a>cv.fit<span class="sc">$</span>lambda.min </span>
<span id="cb168-4"><a href="#cb168-4" aria-hidden="true" tabindex="-1"></a><span class="co"># cv.fit$lambda.min is the best lambda value that results </span></span>
<span id="cb168-5"><a href="#cb168-5" aria-hidden="true" tabindex="-1"></a><span class="co"># in the best model with smallest mean squared error (MSE)</span></span>
<span id="cb168-6"><a href="#cb168-6" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(cv.fit, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>) </span>
<span id="cb168-7"><a href="#cb168-7" aria-hidden="true" tabindex="-1"></a><span class="co"># This extracts the fitted regression parameters of </span></span>
<span id="cb168-8"><a href="#cb168-8" aria-hidden="true" tabindex="-1"></a><span class="co"># the linear regression model using the given lambda value.</span></span>
<span id="cb168-9"><a href="#cb168-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-10"><a href="#cb168-10" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv.fit, <span class="at">newx =</span> testX, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>) </span>
<span id="cb168-11"><a href="#cb168-11" aria-hidden="true" tabindex="-1"></a><span class="co"># This is to predict using the best model</span></span>
<span id="cb168-12"><a href="#cb168-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(y_hat, data.test<span class="sc">$</span>MMSCORE)</span>
<span id="cb168-13"><a href="#cb168-13" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((y_hat <span class="sc">-</span> data.test<span class="sc">$</span>MMSCORE)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb168-14"><a href="#cb168-14" aria-hidden="true" tabindex="-1"></a><span class="co"># The mean squared error (mse)</span></span>
<span id="cb168-15"><a href="#cb168-15" aria-hidden="true" tabindex="-1"></a>mse</span></code></pre></div>
<p></p>
<p>Results are shown below.</p>
<p></p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.2969686 # cor(y_hat, data.test$MMSCORE)</span></span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 2.453638  # mse</span></span></code></pre></div>
<p></p>
<p><strong>Step 7</strong> re-fits the regression model using the variables selected by LASSO. As LASSO put <span class="math inline">\(L_1\)</span> norm on the regression parameters, it not only penalizes the regression coefficients of the irrelevant variables to be zero, but also penalizes the regression coefficients of the selected variable. Thus, the estimated regression coefficients of a LASSO model tend to be smaller than they are (i.e., this is called <em>bias</em> in machine learning terminology).</p>
<p></p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7 -&gt; Re-fit the regression model with selected variables </span></span>
<span id="cb170-2"><a href="#cb170-2" aria-hidden="true" tabindex="-1"></a><span class="co"># by LASSO</span></span>
<span id="cb170-3"><a href="#cb170-3" aria-hidden="true" tabindex="-1"></a>var_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">coef</span>(cv.fit, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>) <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb170-4"><a href="#cb170-4" aria-hidden="true" tabindex="-1"></a>lm.AD.reduced <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> ., <span class="at">data =</span> </span>
<span id="cb170-5"><a href="#cb170-5" aria-hidden="true" tabindex="-1"></a>                      data.train[,var_idx,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb170-6"><a href="#cb170-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD.reduced)</span></code></pre></div>
<p></p>
</div>
</div>
<div id="principal-component-analysis" class="section level2 unnumbered">
<h2>Principal component analysis</h2>
<p></p>
<div id="rationale-and-formulation-13" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<!-- % *Our power comes from the perception of our power*^[A line quoted from HBO's Chernobyl]. -->
<p>A dataset has many variables, but its inherent dimensionality may be smaller than it appears to be. For example, as shown in Figure <a href="#fig:f8-PCAintro">146</a>, the <span class="math inline">\(10\)</span> variables of the dataset, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(x_{10}\)</span>, are manifestations of three underlying independent variables, <span class="math inline">\(z_1\)</span>, <span class="math inline">\(z_2\)</span>, and <span class="math inline">\(z_3\)</span>. In other words, a dataset of <span class="math inline">\(10\)</span> variables is not necessarily a system of <span class="math inline">\(10\)</span> <em>degrees of freedom</em>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-PCAintro"></span>
<img src="graphics/8_PCAintro.png" alt="PCA---to uncover the Master of Puppets ($z_1$, $z_2$, and $z_3$) " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 146: PCA—to uncover the Master of Puppets (<span class="math inline">\(z_1\)</span>, <span class="math inline">\(z_2\)</span>, and <span class="math inline">\(z_3\)</span>) <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The question is how to uncover the “Master of Puppets,” i.e., <span class="math inline">\(z_1\)</span>, <span class="math inline">\(z_2\)</span>, and <span class="math inline">\(z_3\)</span>, based on data of the observed variables, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(x_{10}\)</span>.</p>
<p>Let’s look at the scattered data points in Figure <a href="#fig:f8-11">147</a>. If we think of the data points as <em>stars</em>, and this is the universe after the <em>Big Bang</em>, we can identify two potential forces here: a force that stretches the data points towards one direction (i.e., labeled as <em>the <span class="math inline">\(1^{st}\)</span> PC)<label for="tufte-sn-205" class="margin-toggle sidenote-number">205</label><input type="checkbox" id="tufte-sn-205" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">205</span> PC stands for the <em>principal component</em>.</span></em>; and another force (i.e., labeled as <em>the <span class="math inline">\(2^{nd}\)</span> PC</em>) that drags the data points towards another direction. The forces are independent, so in mathematical terms they follow <strong>orthogonal</strong> directions. And it might be possible that the <span class="math inline">\(2^{nd}\)</span> PC only represents noise. If that is the case, calling it a force may not be the best way. Sometimes we say each PC represents a <em>variation source</em>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f8-11"></span>
<p class="caption marginnote shownote">
Figure 147: Illustration of the principal components in a dataset with 2 variables; the main variation source is represented by th e 1<sup>st</sup> dimension
</p>
<img src="graphics/8_11.png" alt="Illustration of the principal components in a dataset with 2 variables; the main variation source is represented by th e 1^st^  dimension" width="80%"  />
</div>
<p></p>
<p>This interpretation of Figure <a href="#fig:f8-11">147</a> may seem natural. If so, it is only because it makes a tacit assumption that seems too <em>natural</em> to draw our attention: the forces are <em>represented</em> as <em>lines</em><label for="tufte-sn-206" class="margin-toggle sidenote-number">206</label><input type="checkbox" id="tufte-sn-206" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">206</span> Why is a force a line? It could be a wave, a spiral, or anything other than a line. But the challenge is to write up the mathematical form of an idea—like the example of maximum margin in <strong>Chapter 7</strong>.</span>, their mathematical forms are <em>linear models</em> that are defined by the existing variables, i.e., the two lines in Figure <a href="#fig:f8-11">147</a> could be defined by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. The PCA seeks linear combinations of the original variables to pinpoint the directions towards which the underlying forces push the data points. These directions are called <em>principal components</em> (PCs). In other words, the PCA assumes that the relationship between the underlying PCs and the observed variables is linear. And because they are linear, it takes <em>orthogonality</em> to separate different forces.</p>
</div>
<div id="theory-and-method-8" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>The lines in Figure <a href="#fig:f8-11">147</a> take the form as <span class="math inline">\(w_1x_1 + w_2x_2\)</span>,<label for="tufte-sn-207" class="margin-toggle sidenote-number">207</label><input type="checkbox" id="tufte-sn-207" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">207</span> For simplicity, from now on, we assume that all the variables in the dataset are normalized, i.e., for any variable <span class="math inline">\(x_i\)</span>, its mean is <span class="math inline">\(0\)</span> and its variance is <span class="math inline">\(1\)</span>.</span> where <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are free parameters. To estimate <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> for the lines, we need to write an <em>optimization</em> formulation with an objective function and a constraints structure that carries out the idea outlined in Figure <a href="#fig:f8-11">147</a>: to identify the two lines.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f8-PCA-line"></span>
<p class="caption marginnote shownote">
Figure 148: Any line <span class="math inline">\(z = w_1x_1 + w_2x_2\)</span> leads to a new one-dimensional space defined by <span class="math inline">\(z\)</span>
</p>
<img src="graphics/8_PCA_line.png" alt="Any line $z = w_1x_1 + w_2x_2$ leads to a new one-dimensional space defined by $z$" width="80%"  />
</div>
<p></p>
<p>As shown in Figure <a href="#fig:f8-PCA-line">148</a>, any line <span class="math inline">\(z = w_1x_1 + w_2x_2\)</span> leads to a new one-dimensional space defined by <span class="math inline">\(z\)</span>. Data points find their projections on this new space, i.e., the white dots on the line. The variance of the white dots provides a quantitative evaluation of the strength of the force that stretched the data points. The PCA seeks the lines that have the largest variances, which are the strongest forces stretching the data and scattering the data points along the PCs. Specifically, as there would be one line that represents the strongest force (a.k.a., as the <span class="math inline">\(1^{st}\)</span> PC), the second line is called the <span class="math inline">\(2^{nd}\)</span> PC, and so on.</p>
<p>To generalize the idea of Figure <a href="#fig:f8-PCA-line">148</a>, let’s focus on the identification of the <span class="math inline">\(1^{st}\)</span> PC first. Suppose there are <span class="math inline">\(p\)</span> variables, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(x_{p}\)</span>. The <em>line</em> for the <span class="math inline">\(1^{st}\)</span> PC is <span class="math inline">\(\boldsymbol{w}_{(1)}^T\boldsymbol{x}\)</span>. <span class="math inline">\(\boldsymbol{w}_{(1)}\in \mathbb{R}^{p \times 1}\)</span> is the weight vector of the <span class="math inline">\(1^{st}\)</span> PC<label for="tufte-sn-208" class="margin-toggle sidenote-number">208</label><input type="checkbox" id="tufte-sn-208" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">208</span> It is also called the <strong>loading</strong> of the PC.</span>. The projections of <span class="math inline">\(N\)</span> data points on the line of the <span class="math inline">\(1^{st}\)</span> PC, i.e., the coordinates of the <em>white dots</em>, are</p>
<p><span class="math display" id="eq:8-PCA-z">\[\begin{equation}
    z_{1n} = \boldsymbol{w}_{(1)}^T\boldsymbol{x}_n, \text{ for } n=1, 2, \ldots, N,
\tag{92}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{x}_n \in \mathbb{R}^{1 \times p}\)</span> is the <span class="math inline">\(n^{th}\)</span> data point.</p>
<p>As we mentioned, the <span class="math inline">\(1^{st}\)</span> PC is the line that has the largest variance of <span class="math inline">\(z_1\)</span>. Suppose that the data have been standardized, we have</p>
<p><span class="math display">\[\begin{equation}
    var(z_1) = var\left(\boldsymbol{w}_{(1)}^T\boldsymbol{x}\right)=\frac{1}{N}\sum_{n=1}^N\left[\boldsymbol{w}_{(1)}^T\boldsymbol{x}_{n}\right]^2.
\end{equation}\]</span></p>
<p>This leads to the following formulation to learn the parameter <span class="math inline">\(\boldsymbol{w}_{(1)}\)</span></p>
<p><span class="math display" id="eq:8-PC1">\[\begin{equation}
    \boldsymbol{w}_{(1)} = \arg\max_{\boldsymbol{w}_{(1)}^T\boldsymbol{w}_{(1)}=1} \left \{ \sum\nolimits_{n=1}\nolimits^{N}\left [ \boldsymbol{w}_{(1)}^T\boldsymbol{x}_{n} \right ]^2\right \},
\tag{93}
\end{equation}\]</span></p>
<p>where the constraint <span class="math inline">\(\boldsymbol{w}_{(1)}^T\boldsymbol{w}_{(1)}=1\)</span> is to normalize the scale of <span class="math inline">\(\boldsymbol{w}\)</span>.<label for="tufte-sn-209" class="margin-toggle sidenote-number">209</label><input type="checkbox" id="tufte-sn-209" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">209</span> Without which the optimization problem in Eq. <a href="#eq:8-PC1">(93)</a> is unbounded. This also indicates that the absolute magnitudes of <span class="math inline">\(\boldsymbol{w}_{(1)}\)</span> are often misleading. The relative magnitudes are more useful.</span></p>
<p>A more succinct form of Eq. <a href="#eq:8-PC1">(93)</a> is</p>
<p><span class="math display" id="eq:8-PC1-X">\[\begin{equation}
    \boldsymbol{w}_{(1)} = \arg\max_{\boldsymbol{w}_{(1)}^T\boldsymbol{w}_{(1)}=1}\left \{ \boldsymbol{w}_{(1)}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{w}_{(1)} \right\},
\tag{94}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{X}\in \mathbb{R}^{N \times p}\)</span> is the data matrix that concatenates the <span class="math inline">\(N\)</span> samples into a matrix, i.e., each sample forms a row in <span class="math inline">\(\boldsymbol{X}\)</span>. Eq. <a href="#eq:8-PC1-X">(94)</a> is also known as the <strong>eigenvalue decomposition</strong> problem of the matrix <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span>.<label for="tufte-sn-210" class="margin-toggle sidenote-number">210</label><input type="checkbox" id="tufte-sn-210" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">210</span> <span class="math inline">\(\frac{\boldsymbol{X}^T\boldsymbol{X}}{N-1}\)</span> is called the <strong>sample covariance matrix</strong> , usually denoted as <span class="math inline">\(\boldsymbol{S}\)</span>. <span class="math inline">\(\boldsymbol{S}\)</span> could be used in Eq. <a href="#eq:8-PC1-X">(94)</a> to replace <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span>.</span> In this context, <span class="math inline">\(\boldsymbol{w}_{(1)}\)</span> is called the <span class="math inline">\(1^{st}\)</span> <strong>eigenvector</strong>.</p>
<p>To identify the <span class="math inline">\(2^{nd}\)</span> PC, we again find a way to <em>iterate</em>. The idea is simple: as the <span class="math inline">\(1^{st}\)</span> PC represents a variance source, and the data <span class="math inline">\(\boldsymbol{X}\)</span> contains an aggregation of multiple variance sources, why not remove the first variance source from <span class="math inline">\(\boldsymbol{X}\)</span> and then create a new dataset that contains the remaining variance sources? Then, the procedure for finding <span class="math inline">\(\boldsymbol{w}_{(1)}\)</span> could be used for finding <span class="math inline">\(\boldsymbol{w}_{(2)}\)</span>—with <span class="math inline">\(\boldsymbol{w}_{(1)}\)</span> removed, <span class="math inline">\(\boldsymbol{w}_{(2)}\)</span> is the largest variance source.</p>
<p>This process could be generalized as:</p>
<p><!-- begin{itemize} --></p>
<ul>
<li> [<em>Create <span class="math inline">\(\boldsymbol{X}_{(k)}\)</span></em>] In order to find the <span class="math inline">\(k^{th}\)</span> PC, we could create a dataset by removing the variation sources from the previous <span class="math inline">\(k-1\)</span> PCs</li>
</ul>
<p><span class="math display" id="eq:8-PCA-removePC">\[\begin{equation}
        \boldsymbol{X}_{(k)}=\boldsymbol{X}-\sum_{s=1}^{k-1}\boldsymbol{w}_{(s)}\boldsymbol{w}_{(s)}^T.
\tag{95}
    \end{equation}\]</span></p>
<ul>
<li> [<em>Solve for <span class="math inline">\(\boldsymbol{w}_{(k)}\)</span></em>] Then, we solve</li>
</ul>
<p><span class="math display" id="eq:8-PCA-wk">\[\begin{equation}
        \boldsymbol{w}_{(k)}=\arg\max_{\boldsymbol{w}_{(k)}^T\boldsymbol{w}_{(k)}=1}\left \{ \boldsymbol{w}_{(k)}^T\boldsymbol{X}_{(k)}^T\boldsymbol{X}_{(k)}\boldsymbol{w}_{(k)} \right \}.
\tag{96}
    \end{equation}\]</span>
We then compute <span class="math inline">\(\lambda_{(k)} = \boldsymbol{w}_{(k)}^T\boldsymbol{X}_{(k)}^T\boldsymbol{X}_{(k)}\boldsymbol{w}_{(k)}.\)</span> <span class="math inline">\(\lambda_{(k)}\)</span> is called the <strong>eigenvalue</strong> of the <span class="math inline">\(k^{th}\)</span> PC.</p>
<p><!-- end{itemize} --></p>
<p>So we create <span class="math inline">\(\boldsymbol{X}_{(k)}\)</span> and solve Eq. <a href="#eq:8-PCA-wk">(96)</a> in multiple iterations. Many R packages have packed all the iterations into one batch. Usually, we only need to calculate <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> or <span class="math inline">\(\boldsymbol{S}\)</span> and use it as input of these packages, then obtain all the eigenvalues and eigenvectors.</p>
<p>This iterative algorithm would yield in total <span class="math inline">\(p\)</span> PCs for a dataset with <span class="math inline">\(p\)</span> variables. But usually, not all the PCs are significant. If we apply PCA on the dataset generated by the data-generating mechanism as shown in Figure <a href="#fig:f8-PCAintro">146</a>, only the first 3 PCs should be significant, and the other 7 PCs, although they <em>computationally exist</em>, statistically do not exist, as they are manifestations of noise.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-12"></span>
<img src="graphics/8_12.png" alt="The scree plot shows that only the first 2 PCs are significant" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 149: The scree plot shows that only the first 2 PCs are significant<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>In practice, we need to decide how many PCs are needed for a dataset. The <strong>scree plot</strong> as shown in Figure <a href="#fig:f8-12">149</a> is a common tool: it draws the eigenvalues of the PCs, <span class="math inline">\(\lambda_{(1)}, \lambda_{(2)}, \ldots, \lambda_{(p)}\)</span>. Then we look for the change point if it exists. We discard the PCs after the change point as they may be statistically insignificant.</p>
</div>
<div id="a-small-data-example-1" class="section level3 unnumbered">
<h3>A small data example</h3>
The dataset is shown in Table <a href="#tab:t8-2">35</a>. It has <span class="math inline">\(3\)</span> variables and <span class="math inline">\(5\)</span> data points.
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-2">Table 35: </span>A dataset example for PCA</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-10\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-4\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(8\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(14\)</span></td>
<td align="left"><span class="math inline">\(-4\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>First, we normalize (or, standardize) the variables<label for="tufte-sn-211" class="margin-toggle sidenote-number">211</label><input type="checkbox" id="tufte-sn-211" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">211</span> Recall that we assumed that all the variables are normalized when we derived the PCA algorithm</span>. I.e., for <span class="math inline">\(x_1\)</span>, we compute its mean and standard derivation first, which are <span class="math inline">\(2\)</span> and <span class="math inline">\(9.48\)</span>,<label for="tufte-sn-212" class="margin-toggle sidenote-number">212</label><input type="checkbox" id="tufte-sn-212" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">212</span> In this example, numbers are rounded to <span class="math inline">\(2\)</span> decimal places.</span> respectively. Then, we distract each measurement of <span class="math inline">\(x_1\)</span> from its mean and further divide it by its standard derivation. For example, for the first measurement of <span class="math inline">\(x_1\)</span>, <span class="math inline">\(-10\)</span>, it is converted as</p>
<p><span class="math display">\[
\frac{-10 - 2}{9.48}=-1.26.
\]</span></p>
<p>The second measurement, <span class="math inline">\(-4\)</span>, is converted as</p>
<p><span class="math display">\[
\frac{-4 - 2}{9.48}=-0.63.
\]</span></p>
<p>And so on.</p>
<p>Similarly, for <span class="math inline">\(x_2\)</span>, we compute its mean and standard derivation, which are <span class="math inline">\(1\)</span> and <span class="math inline">\(3.61\)</span>, respectively. The standardized dataset is shown in Table <a href="#tab:t8-standardx">36</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-standardx">Table 36: </span>Standardized dataset of Table <a href="#tab:t8-2">35</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-1.26\)</span></td>
<td align="left"><span class="math inline">\(1.39\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-0.63\)</span></td>
<td align="left"><span class="math inline">\(0.28\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.63\)</span></td>
<td align="left"><span class="math inline">\(-0.28\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1.26\)</span></td>
<td align="left"><span class="math inline">\(-1.39\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>We calculate <span class="math inline">\(\boldsymbol{S}\)</span> as</p>
<p><span class="math display">\[ \boldsymbol{S}=\boldsymbol{X}^T \boldsymbol{X} / 4 = \begin{bmatrix}
1 &amp; -0.96 \\
-0.96 &amp; 1 \\
\end{bmatrix}
.\]</span></p>
<p>Solving this eigenvalue decomposition problem<label for="tufte-sn-213" class="margin-toggle sidenote-number">213</label><input type="checkbox" id="tufte-sn-213" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">213</span> E.g., using <code>eigen()</code> in R.</span>, for the <span class="math inline">\(1^{st}\)</span> PC, we have</p>
<p><span class="math display">\[
\lambda_1=1.96 \, \text{ and } \, \boldsymbol{w}_{(1)}=\left[ -0.71, \, 0.71\right].
\]</span></p>
<p>Continuing to the <span class="math inline">\(2^{nd}\)</span> PC, we have</p>
<p><span class="math display">\[
\lambda_2=0.04 \, \text{ and } \, \boldsymbol{w}_{(2)}=\left[  -0.71, \, -0.71\right].
\]</span></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-PCA-example1"></span>
<img src="graphics/8_PCA_example1.png" alt="Gray dots are data points (standardized); the black line is the $1^{st}$ PC" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 150: Gray dots are data points (standardized); the black line is the <span class="math inline">\(1^{st}\)</span> PC<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We can calculate the cumulative contributions of the <span class="math inline">\(2\)</span> PCs</p>
<p><span class="math display">\[
\text{For the } 1^{st} \text{ PC: } 1.96/(1.96+0.04) = 0.98.
\]</span></p>
<p><span class="math display">\[
\text{For the } 2^{nd} \text{ PC: } 0.04/(1.96+0.04) = 0.02.
\]</span></p>
<p>The <span class="math inline">\(2^{nd}\)</span> PC is statistically insignificant.</p>
<p>We visualize the <span class="math inline">\(1^{st}\)</span> PC in Figure <a href="#fig:f8-PCA-example1">150</a> (compare it with Figure <a href="#fig:f8-11">147</a>). The R code to generate Figure <a href="#fig:f8-PCA-example1">150</a> is shown below.</p>
<p></p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="sc">-</span><span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">14</span>)</span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">4</span>)</span>
<span id="cb171-3"><a href="#cb171-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x1,x2)</span>
<span id="cb171-4"><a href="#cb171-4" aria-hidden="true" tabindex="-1"></a>x.scale <span class="ot">&lt;-</span> <span class="fu">scale</span>(x) <span class="co">#standardize the data</span></span>
<span id="cb171-5"><a href="#cb171-5" aria-hidden="true" tabindex="-1"></a>eigen.x <span class="ot">&lt;-</span> <span class="fu">eigen</span>(<span class="fu">cor</span>(x))</span>
<span id="cb171-6"><a href="#cb171-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x.scale, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb171-7"><a href="#cb171-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>,eigen.x<span class="sc">$</span>vectors[<span class="dv">2</span>,<span class="dv">1</span>]<span class="sc">/</span>eigen.x<span class="sc">$</span>vectors[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb171-8"><a href="#cb171-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p></p>
<p>The coordinates of the <em>white dots</em> (a.k.a., the projections of the data points on the PCs, as shown in Figure <a href="#fig:f8-PCA-line">148</a>) can be obtained by using Eq. <a href="#eq:8-PCA-z">(92)</a>. Results are shown in Table <a href="#tab:t8-example1-PC">37</a>. This is an example of <em>data transformation</em>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-example1-PC">Table 37: </span>The coordinates of the <em>white dots</em>, i.e., a.k.a., the projections of the data points on the PCs</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(z_1\)</span></th>
<th align="left"><span class="math inline">\(z_2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1.88\)</span></td>
<td align="left"><span class="math inline">\(-0.09\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.64\)</span></td>
<td align="left"><span class="math inline">\(0.25\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-0.64\)</span></td>
<td align="left"><span class="math inline">\(-0.25\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-1.88\)</span></td>
<td align="left"><span class="math inline">\(0.09\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Data transformation is often a data preprocessing step before the use of other methods. For example, in clustering, sometimes we could not discover any clustering structure on the dataset of original variables, but we may discover clusters on the transformed dataset. In a regression model, as we have mentioned the issue of multicollinearity<label for="tufte-sn-214" class="margin-toggle sidenote-number">214</label><input type="checkbox" id="tufte-sn-214" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">214</span> I.e., in <strong>Chapter 6</strong> and <strong>Chapter 2</strong>.</span>, the <strong>Principal Component Regression</strong> (<strong>PCR</strong>) method uses the PCA first to convert the original <span class="math inline">\(x\)</span> variables into the <span class="math inline">\(z\)</span> variables and then applies the linear regression model on the transformed variables. This is because the <span class="math inline">\(z\)</span> variables are PCs and they are orthogonal with each other, without issue of multicollinearity.</p>
</div>
<div id="r-lab-12" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get dataset into R and organize the dataset in the required format.<label for="tufte-sn-215" class="margin-toggle sidenote-number">215</label><input type="checkbox" id="tufte-sn-215" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">215</span> It is not necessary to split the dataset into training and testing datasets before the use of PCA, <em>if</em> the purpose of the analysis is <em>exploratory data analysis</em>. But if the purpose of using PCA is for <em>dimension reduction</em> or <em>feature extraction</em>, which is an intermediate step before building a prediction model, then we should split the dataset into training and testing datasets, and apply PCA only on the training dataset to learn the loadings of the significant PCs. The R lab shows an example of this process.</span></p>
<p></p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R</span></span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a><span class="do">#### Read data from a CSV file</span></span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a><span class="do">#### Example: Alzheimer&#39;s Disease</span></span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a><span class="co"># RCurl is the R package to read csv file using a link</span></span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb172-8"><a href="#cb172-8" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD_hd.csv&quot;</span>)</span>
<span id="cb172-9"><a href="#cb172-9" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb172-10"><a href="#cb172-10" aria-hidden="true" tabindex="-1"></a><span class="co"># str(AD)</span></span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create your X matrix (predictors) and Y vector </span></span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (outcome variable)</span></span>
<span id="cb173-4"><a href="#cb173-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> AD[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>)]</span>
<span id="cb173-5"><a href="#cb173-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span>
<span id="cb173-6"><a href="#cb173-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-7"><a href="#cb173-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we integrate everything into a data frame</span></span>
<span id="cb173-8"><a href="#cb173-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Y,X)</span>
<span id="cb173-9"><a href="#cb173-9" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(data)[<span class="dv">1</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span>
<span id="cb173-10"><a href="#cb173-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-11"><a href="#cb173-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data </span></span>
<span id="cb173-12"><a href="#cb173-12" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data)) <span class="sc">*</span> <span class="dv">4</span> <span class="sc">/</span> <span class="dv">5</span> )</span>
<span id="cb173-13"><a href="#cb173-13" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb173-14"><a href="#cb173-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data </span></span>
<span id="cb173-15"><a href="#cb173-15" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span>
<span id="cb173-16"><a href="#cb173-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-17"><a href="#cb173-17" aria-hidden="true" tabindex="-1"></a>trainX <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.train[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb173-18"><a href="#cb173-18" aria-hidden="true" tabindex="-1"></a>testX <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb173-19"><a href="#cb173-19" aria-hidden="true" tabindex="-1"></a>trainY <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.train[,<span class="dv">1</span>])</span>
<span id="cb173-20"><a href="#cb173-20" aria-hidden="true" tabindex="-1"></a>testY <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data.test[,<span class="dv">1</span>])</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> implements the PCA analysis using the <code>FactoMineR</code> package.</p>
<p></p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; Implement principal component analysis</span></span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;factoextra&quot;)</span></span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(FactoMineR)</span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct the PCA analysis</span></span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a>pca.AD <span class="ot">&lt;-</span> <span class="fu">PCA</span>(trainX,  <span class="at">graph =</span> <span class="cn">FALSE</span>,<span class="at">ncp=</span><span class="dv">10</span>) </span>
<span id="cb174-6"><a href="#cb174-6" aria-hidden="true" tabindex="-1"></a><span class="co"># names(pca.AD) will give you the list of variable names in the</span></span>
<span id="cb174-7"><a href="#cb174-7" aria-hidden="true" tabindex="-1"></a><span class="co"># object pca.AD created by PCA(). For instance, pca.AD$eig records</span></span>
<span id="cb174-8"><a href="#cb174-8" aria-hidden="true" tabindex="-1"></a><span class="co"># the eigenvalues of all the PCs, also the transformed value into </span></span>
<span id="cb174-9"><a href="#cb174-9" aria-hidden="true" tabindex="-1"></a><span class="co"># cumulative percentage of variance. pca.AD$var stores the </span></span>
<span id="cb174-10"><a href="#cb174-10" aria-hidden="true" tabindex="-1"></a><span class="co"># loadings of the variables in each of the PCs.</span></span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f8-BC-scree"></span>
<p class="caption marginnote shownote">
Figure 151: Scree plot of the PCA analysis on the AD dataset
</p>
<img src="graphics/8_BC_scree.png" alt="Scree plot of the PCA analysis on the AD dataset" width="80%"  />
</div>
<p></p>
<p><strong>Step 4</strong> ranks the PCs based on their eigenvalues and identifies the significant ones.</p>
<p></p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Examine the contributions of the PCs in explaining </span></span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the variation in data.</span></span>
<span id="cb175-3"><a href="#cb175-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(factoextra ) </span>
<span id="cb175-4"><a href="#cb175-4" aria-hidden="true" tabindex="-1"></a><span class="co"># to use the following functions such as get_pca_var() </span></span>
<span id="cb175-5"><a href="#cb175-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and fviz_contrib()</span></span>
<span id="cb175-6"><a href="#cb175-6" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_screeplot</span>(pca.AD, <span class="at">addlabels =</span> <span class="cn">TRUE</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>))</span></code></pre></div>
<p></p>
<p>The result is shown in Figure <a href="#fig:f8-BC-scree">151</a>. The <span class="math inline">\(1^{st}\)</span> PC explains away <span class="math inline">\(17.4\%\)</span> of the total variation, and the <span class="math inline">\(2^{nd}\)</span> PC explains away <span class="math inline">\(13\%\)</span> of the total variation. There is a change point at the <span class="math inline">\(3^{rd}\)</span> PC, indicating that the following PCs may be insignificant.</p>
<p><strong>Step 5</strong> looks into the details of the learned PCA model, e.g., the <em>loadings</em> of the PCs. It leads to Figures <a href="#fig:f8-14">152</a> and <a href="#fig:f8-15">153</a> which visualize the contributions of the variables to the <span class="math inline">\(1^{st}\)</span> and <span class="math inline">\(2^{nd}\)</span> PC, respectively.</p>
<p></p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; Examine the loadings of the PCs.</span></span>
<span id="cb176-2"><a href="#cb176-2" aria-hidden="true" tabindex="-1"></a>var <span class="ot">&lt;-</span> <span class="fu">get_pca_var</span>(pca.AD) <span class="co"># to get the loadings of the PCs</span></span>
<span id="cb176-3"><a href="#cb176-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(var<span class="sc">$</span>contrib) <span class="co"># to show the first 10 PCs</span></span>
<span id="cb176-4"><a href="#cb176-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-5"><a href="#cb176-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the contributions of top variables to </span></span>
<span id="cb176-6"><a href="#cb176-6" aria-hidden="true" tabindex="-1"></a><span class="co"># PC1 using a bar plot</span></span>
<span id="cb176-7"><a href="#cb176-7" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_contrib</span>(pca.AD, <span class="at">choice =</span> <span class="st">&quot;var&quot;</span>, <span class="at">axes =</span> <span class="dv">1</span>, <span class="at">top =</span> <span class="dv">20</span>)</span>
<span id="cb176-8"><a href="#cb176-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the contributions of top variables to PC2 using </span></span>
<span id="cb176-9"><a href="#cb176-9" aria-hidden="true" tabindex="-1"></a><span class="co"># a bar plot</span></span>
<span id="cb176-10"><a href="#cb176-10" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_contrib</span>(pca.AD, <span class="at">choice =</span> <span class="st">&quot;var&quot;</span>, <span class="at">axes =</span> <span class="dv">2</span>, <span class="at">top =</span> <span class="dv">20</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-14"></span>
<img src="graphics/8_BC_varimp1.png" alt="Loading of the $1^{st}$ PC, i.e., coefficients are ranked in terms of their absolute magnitude and only the top 20 are shown" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 152: Loading of the <span class="math inline">\(1^{st}\)</span> PC, i.e., coefficients are ranked in terms of their absolute magnitude and only the top 20 are shown<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><strong>Step 6</strong> implements linear regression model using the transformed data.</p>
<p></p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; use the transformed data fit a line regression model</span></span>
<span id="cb177-2"><a href="#cb177-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-3"><a href="#cb177-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Data pre-processing</span></span>
<span id="cb177-4"><a href="#cb177-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Transformation of the X matrix of the training data</span></span>
<span id="cb177-5"><a href="#cb177-5" aria-hidden="true" tabindex="-1"></a>trainX <span class="ot">&lt;-</span> pca.AD<span class="sc">$</span>ind<span class="sc">$</span>coord </span>
<span id="cb177-6"><a href="#cb177-6" aria-hidden="true" tabindex="-1"></a>trainX <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(trainX)</span>
<span id="cb177-7"><a href="#cb177-7" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(trainX) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;PC1&quot;</span>,<span class="st">&quot;PC2&quot;</span>,<span class="st">&quot;PC3&quot;</span>,<span class="st">&quot;PC4&quot;</span>,<span class="st">&quot;PC5&quot;</span>,<span class="st">&quot;PC6&quot;</span>,<span class="st">&quot;PC7&quot;</span>,</span>
<span id="cb177-8"><a href="#cb177-8" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&quot;PC8&quot;</span>,<span class="st">&quot;PC9&quot;</span>,<span class="st">&quot;PC10&quot;</span>)</span>
<span id="cb177-9"><a href="#cb177-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Transformation of the X matrix of the testing data</span></span>
<span id="cb177-10"><a href="#cb177-10" aria-hidden="true" tabindex="-1"></a>testX <span class="ot">&lt;-</span> <span class="fu">predict</span>(pca.AD , <span class="at">newdata =</span> testX) </span>
<span id="cb177-11"><a href="#cb177-11" aria-hidden="true" tabindex="-1"></a>testX <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(testX<span class="sc">$</span>coord)</span>
<span id="cb177-12"><a href="#cb177-12" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(testX) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;PC1&quot;</span>,<span class="st">&quot;PC2&quot;</span>,<span class="st">&quot;PC3&quot;</span>,<span class="st">&quot;PC4&quot;</span>,<span class="st">&quot;PC5&quot;</span>,<span class="st">&quot;PC6&quot;</span>,</span>
<span id="cb177-13"><a href="#cb177-13" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;PC7&quot;</span>,<span class="st">&quot;PC8&quot;</span>,<span class="st">&quot;PC9&quot;</span>,<span class="st">&quot;PC10&quot;</span>)</span>
<span id="cb177-14"><a href="#cb177-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-15"><a href="#cb177-15" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(trainY,trainX)</span>
<span id="cb177-16"><a href="#cb177-16" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(tempData)[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;MMSCORE&quot;</span>)</span>
<span id="cb177-17"><a href="#cb177-17" aria-hidden="true" tabindex="-1"></a>lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> ., <span class="at">data =</span> tempData)</span>
<span id="cb177-18"><a href="#cb177-18" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD)</span>
<span id="cb177-19"><a href="#cb177-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-20"><a href="#cb177-20" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm.AD, testX)</span>
<span id="cb177-21"><a href="#cb177-21" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(y_hat, testY)</span>
<span id="cb177-22"><a href="#cb177-22" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((y_hat <span class="sc">-</span> testY)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># The mean squared error (mse)</span></span>
<span id="cb177-23"><a href="#cb177-23" aria-hidden="true" tabindex="-1"></a>mse</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-15"></span>
<img src="graphics/8_BC_varimp2.png" alt="Loading of the $2^{nd}$ PC, i.e., coefficients are ranked in terms of their absolute magnitude and only the top 20 are shown" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 153: Loading of the <span class="math inline">\(2^{nd}\)</span> PC, i.e., coefficients are ranked in terms of their absolute magnitude and only the top 20 are shown<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = AGE ~ ., data = tempData)</span></span>
<span id="cb178-4"><a href="#cb178-4" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb178-5"><a href="#cb178-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb178-6"><a href="#cb178-6" aria-hidden="true" tabindex="-1"></a><span class="do">##      Min       1Q   Median       3Q      Max</span></span>
<span id="cb178-7"><a href="#cb178-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -17.3377  -2.5627   0.0518   2.6820  11.1772</span></span>
<span id="cb178-8"><a href="#cb178-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb178-9"><a href="#cb178-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb178-10"><a href="#cb178-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb178-11"><a href="#cb178-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 73.68767    0.59939 122.938  &lt; 2e-16 ***</span></span>
<span id="cb178-12"><a href="#cb178-12" aria-hidden="true" tabindex="-1"></a><span class="do">## PC1          0.04011    0.08275   0.485 0.629580</span></span>
<span id="cb178-13"><a href="#cb178-13" aria-hidden="true" tabindex="-1"></a><span class="do">## PC2         -0.31556    0.09490  -3.325 0.001488 **</span></span>
<span id="cb178-14"><a href="#cb178-14" aria-hidden="true" tabindex="-1"></a><span class="do">## PC3          0.50022    0.13510   3.702 0.000456 ***</span></span>
<span id="cb178-15"><a href="#cb178-15" aria-hidden="true" tabindex="-1"></a><span class="do">## PC4          0.14812    0.17462   0.848 0.399578</span></span>
<span id="cb178-16"><a href="#cb178-16" aria-hidden="true" tabindex="-1"></a><span class="do">## PC5          0.47954    0.19404   2.471 0.016219 *</span></span>
<span id="cb178-17"><a href="#cb178-17" aria-hidden="true" tabindex="-1"></a><span class="do">## PC6         -0.29760    0.20134  -1.478 0.144444</span></span>
<span id="cb178-18"><a href="#cb178-18" aria-hidden="true" tabindex="-1"></a><span class="do">## PC7          0.10160    0.21388   0.475 0.636440</span></span>
<span id="cb178-19"><a href="#cb178-19" aria-hidden="true" tabindex="-1"></a><span class="do">## PC8         -0.25015    0.22527  -1.110 0.271100</span></span>
<span id="cb178-20"><a href="#cb178-20" aria-hidden="true" tabindex="-1"></a><span class="do">## PC9         -0.02837    0.22932  -0.124 0.901949</span></span>
<span id="cb178-21"><a href="#cb178-21" aria-hidden="true" tabindex="-1"></a><span class="do">## PC10         0.16326    0.23282   0.701 0.485794</span></span>
<span id="cb178-22"><a href="#cb178-22" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb178-23"><a href="#cb178-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb178-24"><a href="#cb178-24" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb178-25"><a href="#cb178-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 5.121 on 62 degrees of freedom</span></span>
<span id="cb178-26"><a href="#cb178-26" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.3672, Adjusted R-squared:  0.2651</span></span>
<span id="cb178-27"><a href="#cb178-27" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 3.598 on 10 and 62 DF,  p-value: 0.0008235</span></span></code></pre></div>
<p></p>
<p>It is not uncommon to see that the <span class="math inline">\(1^{st}\)</span> PC is insignificant in a prediction model. The <span class="math inline">\(1^{st}\)</span> PC is the largest <em>force</em> or <em>variation source</em> in <span class="math inline">\(\boldsymbol{X}\)</span> by definition, but not necessarily the one that correlates with any outcome variable <span class="math inline">\(y\)</span> with the strongest correlation.</p>
<p>On the other hand, the <em>R-squared</em> of this model is <span class="math inline">\(0.3672\)</span>, and the <em>p-value</em> is <span class="math inline">\(0.0008235\)</span>. Overall, the data transformation by PCA yielded an effective linear regression model.</p>
<p><em>Beyond the 6-Step R Pipeline.</em> PCA is a popular tool for <em>EDA</em>. For example, we can visualize the distribution of the data points in the new space spanned by a few selected PCs<label for="tufte-sn-216" class="margin-toggle sidenote-number">216</label><input type="checkbox" id="tufte-sn-216" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">216</span> It may reveal some structures of the dataset. For example, for a classification problem, it is hoped that in the space spanned by the selected PCs the data points from different classes would cluster around different centers.</span>. We use the following R script to draw a visualization figure.</p>
<p></p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Projection of data points in the new space defined by </span></span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the first two PCs</span></span>
<span id="cb179-3"><a href="#cb179-3" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_pca_ind</span>(pca.AD, <span class="at">label=</span><span class="st">&quot;none&quot;</span>, </span>
<span id="cb179-4"><a href="#cb179-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">habillage=</span><span class="fu">as.factor</span>(AD[train.ix,]<span class="sc">$</span>DX_bl),</span>
<span id="cb179-5"><a href="#cb179-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">addEllipses=</span><span class="cn">TRUE</span>, <span class="at">ellipse.level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<p></p>
<p>The result is shown in Figure <a href="#fig:f8-17">154</a>. Two clusters are identified, which overlap significantly. One group is the <em>LMCI</em> (i.e., mild cognitive impairment) and the other one is <em>NC</em> (i.e., normal aging). The result is consistent with the fact that the clinical difference between the two groups is not as significant as <em>NC</em> versus <em>Diseased</em>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f8-17"></span>
<p class="caption marginnote shownote">
Figure 154: Scatterplot of the subjects in the space defined by the <span class="math inline">\(1^{st}\)</span> and <span class="math inline">\(2^{nd}\)</span> PCs
</p>
<img src="graphics/8_17.png" alt="Scatterplot of the subjects in the space defined by the $1^{st}$ and $2^{nd}$ PCs" width="80%"  />
</div>
<p></p>
</div>
</div>
<div id="remarks-6" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="why-lasso-uses-the-l1-norm" class="section level3 unnumbered">
<h3>Why LASSO uses the L<sub>1</sub> norm</h3>
<p>LASSO is often compared with another model, the <strong>Ridge regression</strong> that was developed about <span class="math inline">\(30\)</span> years before LASSO<label for="tufte-sn-217" class="margin-toggle sidenote-number">217</label><input type="checkbox" id="tufte-sn-217" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">217</span> Hoerl, A.E. and Kennard, R.W. <em>Ridge regression: biased estimation for nonorthogonal problems</em>, Technometrics, Volume 12, Issue 1, Pages 55-67, 1970.</span>.</p>
<p>The formulation of Ridge regression is</p>
<p><span class="math display" id="eq:8-RIDGE">\[\begin{equation}
        \boldsymbol{\hat \beta} = \arg\min_{\boldsymbol \beta} \left \{   \underbrace{(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X} \boldsymbol{\beta})}_{\text{Least squares}} + \underbrace{\lambda \lVert \boldsymbol{\beta}\rVert^2_2}_{L_2 \text{ norm penalty}} \right \}
\tag{97}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\lVert \boldsymbol \beta \rVert^2_2=\sum_{i=1}^p \lvert\beta_i\rvert^2\)</span>.</p>
<p>Ridge regression seems to bear the same spirit of LASSO—they both penalize the magnitudes of the regression parameters. However, it has been noticed that in the Ridge regression model the estimated regression parameters are less likely to be <span class="math inline">\(0\)</span>. Even with a very large <span class="math inline">\(\lambda\)</span>, many elements in <span class="math inline">\(\boldsymbol{\beta}\)</span> may be close to zero (i.e., with a tiny numerical magnitude), but not zero<label for="tufte-sn-218" class="margin-toggle sidenote-number">218</label><input type="checkbox" id="tufte-sn-218" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">218</span> If they are not zero, these variables can still generate impact on the estimation of other regression parameters.</span>. This may not be entirely a surprise, as the Ridge regression is often used as a <em>stabilization</em> strategy to handle the <em>multicollinearity</em> issue or other issues that result in numerical instability of parameter estimation in linear regression, while LASSO is mainly used as a <em>variable selection</em> strategy.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f8-10"></span>
<p class="caption marginnote shownote">
Figure 155: Why LASSO (left) generates sparse estimates, while Ridge regression (right) does not
</p>
<img src="graphics/8_10.png" alt="Why LASSO (left) generates sparse estimates, while Ridge regression (right) does not" width="80%"  />
</div>
<p></p>
<p>To reveal why the <span class="math inline">\(L_1\)</span> norm in LASSO regression differs from the <span class="math inline">\(L_2\)</span> norm used in the Ridge regression, we adopt an explanation<label for="tufte-sn-219" class="margin-toggle sidenote-number">219</label><input type="checkbox" id="tufte-sn-219" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">219</span> Hastie, T., Tibshirani, R. and Friedman, J. <em>The Elements of Statistical Learning</em>, <span class="math inline">\(2^{nd}\)</span> edition. Springer, 2009.</span> as shown in Figure <a href="#fig:f8-10">155</a>. There are <span class="math inline">\(2\)</span> predictors, thus, two regression coefficients <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>. The contour plot corresponds to the least squares loss function which is shared by both the LASSO and the Ridge regression models. And the least squares estimator, <span class="math inline">\(\boldsymbol{\hat\beta}\)</span>, is in the center of the contour plots. The shadowed rhombus in Figure <a href="#fig:f8-10">155</a> (left) corresponds to the <span class="math inline">\(L_1\)</span> norm, and the shadowed circle in Figure <a href="#fig:f8-10">155</a> (right) corresponds to the <span class="math inline">\(L_2\)</span> norm. For either model, the optimal solution happens at the <em>contact point</em> of the two shapes.</p>
<p>Figure <a href="#fig:f8-10">155</a> shows that the contact point of the elliptic contour plot with the shadowed rhombus is likely to be one of the <em>sharp</em> corner points. A feature of these corner points is that some variables are zero, e.g., in Figure <a href="#fig:f8-10">155</a> (left), the point of contact implies that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
<p>As a comparison, in Ridge regression, the shadowed circle has no such sharp corner points. Given the infinite number of potential contact points of the elliptic contour plot with the shadowed circle, it is expected that the Ridge regression will not result in sparse solutions with exact zeros in the estimated regression coefficients.</p>
<p>Following this idea<label for="tufte-sn-220" class="margin-toggle sidenote-number">220</label><input type="checkbox" id="tufte-sn-220" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">220</span> I.e., to create sharp contact points between the elliptical contour with the shape representing the norm.</span>, the <span class="math inline">\(L_1\)</span> norm is extended to the <span class="math inline">\(L_q\)</span> norm, where <span class="math inline">\(q \leq 1\)</span>. For any <span class="math inline">\(q\leq 1\)</span>, we could generate sharp corner points to enable sparse solutions. The advantage of using <span class="math inline">\(q&lt;1\)</span> is to reduce bias in the model<label for="tufte-sn-221" class="margin-toggle sidenote-number">221</label><input type="checkbox" id="tufte-sn-221" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">221</span> I.e., the <span class="math inline">\(L_1\)</span> norm not only penalizes the regression coefficients of the irrelevant variables to be zero, it also penalizes the regression coefficients of the relevant variables. This is a <em>bias</em> in the model.</span>. The cost of using <span class="math inline">\(q&lt;1\)</span> is that it will result in <em>non-convex</em> penalty terms, creating a more challenging optimization problem than LASSO. Considerable amounts of efforts have been devoted to two main directions: development of new norms, and development of new algorithms (i.e., which are usually iterative procedures with closed-form solution in each iteration, like the Shooting algorithm). Interested readers can read more of these works<label for="tufte-sn-222" class="margin-toggle sidenote-number">222</label><input type="checkbox" id="tufte-sn-222" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">222</span> A good place to start with: <a href="https://github.com/jiayuzhou/SLEP">https://github.com/jiayuzhou/SLEP</a> and its manual (in PDF).</span>.</p>
<!-- % The shooting algorithm ^[Fu, WJ. Penalized regressions: the bridge versus the lasso. *Journal of Computational and Graphical Statistics* , 1998.]  has been widely used in many extension models of LASSO in the statistics community. The shooting algorithm is easy to use and has nice interpretation of each iteration. But it could be slow in very high-dimensional situations. Also, with more complex penalty terms such as those $L_21$-norm regularization or group regularization terms, the shooting algorithm may not work anymore. In machine learning community where the computational efficiency is of particular interest, many scalable algorithms such as the projection operator based methods have been developed. Interested readers can read more of these works^[[https://github.com/jiayuzhou/SLEP](https://github.com/jiayuzhou/SLEP)] in this direction that provided closed form iterative updating rules by projection operator on a variety of regularization terms. -->
</div>
<div id="the-myth-of-pca" class="section level3 unnumbered">
<h3>The myth of PCA</h3>
<p>While PCA has been widely used, it is often criticized as a <em>black box</em> model that lacks <em>interpretability</em>. It depends on the circumstances where the PCA is used. Sometimes, it is not easy to connect the identified principal components with physical entities. The applications of PCA in many areas have formed a convention, or a myth—some statisticians may say—such that formulistic rubrics have been invented to convert their data into patterns, then further convert these patterns into formulated sentences such as, “the variables that have larger magnitudes in the first <span class="math inline">\(3\)</span> PCs correspond to the brain regions in the hippocampus area, indicating that these brain regions manifest significant functional connectivity to deliver the verbal function,” or “we have identified <span class="math inline">\(5\)</span> significant PCs, and the genes that show dominant magnitudes in the loading of the <span class="math inline">\(1^{st}\)</span> PC are all related to T-cell production and immune functions … each of the PC indicates a biological pathway that consists of these constitutional genes working together to produce specific types of proteins.” Then hear what had been said by financial analysts: “using PCA on <span class="math inline">\(100\)</span> stocks<label for="tufte-sn-223" class="margin-toggle sidenote-number">223</label><input type="checkbox" id="tufte-sn-223" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">223</span> Each stock is a variable.</span>, we found that the <span class="math inline">\(1^{st}\)</span> PC consists of <span class="math inline">\(10\)</span> stocks as their weights in the loading are significantly larger than the other stocks. This may indicate that there is strong correlation between these <span class="math inline">\(10\)</span> stocks … consider this fact when you come up with your investment strategy…”</p>
<p>Having said that, sometimes there is magic in PCA.</p>
<p>Consider another small data example that is shown in Table <a href="#tab:t8-PCAnet">38</a>. It has <span class="math inline">\(3\)</span> variables and <span class="math inline">\(8\)</span> data points.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-PCAnet">Table 38: </span>A dataset example for PCA</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-3\)</span></td>
<td align="left"><span class="math inline">\(-2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>First, we normalize the variables, i.e., for <span class="math inline">\(x_1\)</span>, we compute its mean and standard derivation first, which are <span class="math inline">\(2.375\)</span> and <span class="math inline">\(3.159\)</span>, respectively. Then, we distract each measurement of <span class="math inline">\(x_1\)</span> from its mean and further divide it by its standard derivation. For example, for the first measurement of <span class="math inline">\(x_1\)</span>, <span class="math inline">\(-1\)</span>, it is converted as</p>
<p><span class="math display">\[
\frac{-1-2.375}{3.159}=-1.07.
\]</span></p>
<p>The second measurement, <span class="math inline">\(3\)</span>, is converted as</p>
<p><span class="math display">\[
\frac{3-2.375}{3.159}=0.20.
\]</span></p>
<p>And so on.</p>
<p>Similarly, for <span class="math inline">\(x_2\)</span>, we compute its mean and standard derivation, which are <span class="math inline">\(3\)</span> and <span class="math inline">\(2.88\)</span>, respectively. For <span class="math inline">\(x_3\)</span>, we compute its mean and standard derivation, which are <span class="math inline">\(0.88\)</span> and <span class="math inline">\(0.35\)</span>, respectively …. Then the standardized dataset is shown in Table <a href="#tab:t8-standardx2">39</a>.<label for="tufte-sn-224" class="margin-toggle sidenote-number">224</label><input type="checkbox" id="tufte-sn-224" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">224</span> In this example, numbers are rounded to <span class="math inline">\(2\)</span> decimal places.</span></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-standardx2">Table 39: </span>Standardized dataset of Table <a href="#tab:t8-PCAnet">38</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-1.07\)</span></td>
<td align="left"><span class="math inline">\(-1.04\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.2\)</span></td>
<td align="left"><span class="math inline">\(0.00\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(0.2\)</span></td>
<td align="left"><span class="math inline">\(0.69\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-1.70\)</span></td>
<td align="left"><span class="math inline">\(-1.73\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(0.20\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.83\)</span></td>
<td align="left"><span class="math inline">\(1.04\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1.46\)</span></td>
<td align="left"><span class="math inline">\(1.04\)</span></td>
<td align="left"><span class="math inline">\(0.35\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-0.11\)</span></td>
<td align="left"><span class="math inline">\(-0.35\)</span></td>
<td align="left"><span class="math inline">\(-2.48\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>We calculate the sample covariance matrix<label for="tufte-sn-225" class="margin-toggle sidenote-number">225</label><input type="checkbox" id="tufte-sn-225" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">225</span> From <span class="math inline">\(\boldsymbol{S}\)</span> we see that the correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> is quite large, while the correlation between them with <span class="math inline">\(x_3\)</span> is very small. Figure <a href="#fig:f8-PCAnet">156</a> visualizes this relationship of the three variables.</span> <span class="math inline">\(\boldsymbol{S}\)</span> as</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-PCAnet"></span>
<img src="graphics/8_PCAnet.png" alt="Visualization of the relationship between the three variables" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 156: Visualization of the relationship between the three variables<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display">\[ \boldsymbol{S}=\frac{\boldsymbol{X}^T \boldsymbol{X}}{N-1} = \begin{bmatrix}
1 &amp; 0.96 &amp; 0.05 \\
0.96 &amp; 1 &amp; 0.14 \\
0.05 &amp; 0.14 &amp; 1\\
\end{bmatrix}.
\]</span></p>
<p>By solving the eigenvalue decomposition problem of the matrix <span class="math inline">\(\boldsymbol{S}\)</span>, we obtain the PCs and their loadings.</p>
<p>For the <span class="math inline">\(1^{st}\)</span> PC, we get</p>
<p><span class="math display">\[
\lambda_1=1.98 \, \text{ and } \, \boldsymbol{w}_{(1)}=\left[ -0.69, \, -0.70, \, -0.14\right].
\]</span></p>
<p>For the <span class="math inline">\(2^{nd}\)</span> PC, we get</p>
<p><span class="math display">\[
\lambda_2=0.98 \, \text{ and } \, \boldsymbol{w}_{(2)}=\left[  0.14, \,0.05, \, -0.99\right].
\]</span></p>
<p>For the <span class="math inline">\(3^{rd}\)</span> PC, we get</p>
<p><span class="math display">\[
\lambda_3=0.04 \, \text{ and } \, \boldsymbol{w}_{(3)}=\left[  0.70, \, -0.71, \, 0.07\right].
\]</span></p>
<p>We can calculate the cumulative contributions of the three PCs</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-PCA3net-scree"></span>
<img src="graphics/8_PCA3net_scree.png" alt="Scree plot of the PCA analysis on data in Table \@ref(tab:t8-standardx2)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 157: Scree plot of the PCA analysis on data in Table <a href="#tab:t8-standardx2">39</a><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math display">\[
\text{For the } 1^{st} \text{ PC: } 1.98/(1.98+0.98+0.04) = 0.66.
\]</span></p>
<p><span class="math display">\[
\text{For the } 2^{nd} \text{ PC: } 0.98/(1.98+0.98+0.04) = 0.33.
\]</span></p>
<p><span class="math display">\[
\text{For the } 3^{rd} \text{ PC: } 0.04/(1.98+0.98+0.04) = 0.01.
\]</span></p>
<p>The <span class="math inline">\(3^{rd}\)</span> PC is statistically insignificant. The scree plot is shown in Figure <a href="#fig:f8-PCA3net-scree">157</a>.</p>
<p>We look into the details of the learned PCA model, e.g., the <em>loadings</em> of the PCs. It leads to Figure <a href="#fig:f8-PCAnet-pc">158</a>.</p>
<p></p>
<div class="figure"><span id="fig:f8-PCAnet-pc"></span>
<p class="caption marginnote shownote">
Figure 158: Loadings of the <span class="math inline">\(1^{st}\)</span> PC (left), <span class="math inline">\(2^{nd}\)</span> PC (middle), and <span class="math inline">\(3^{rd}\)</span> PC (right)
</p>
<img src="graphics/8_PCAnet_pc1.png" alt="Loadings of the $1^{st}$ PC (left), $2^{nd}$ PC (middle), and $3^{rd}$ PC (right)" width="30%"  /><img src="graphics/8_PCAnet_pc2.png" alt="Loadings of the $1^{st}$ PC (left), $2^{nd}$ PC (middle), and $3^{rd}$ PC (right)" width="30%"  /><img src="graphics/8_PCAnet_pc3.png" alt="Loadings of the $1^{st}$ PC (left), $2^{nd}$ PC (middle), and $3^{rd}$ PC (right)" width="30%"  />
</div>
<p></p>
<p>Figure <a href="#fig:f8-PCAnet-pc">158</a> shows that the <span class="math inline">\(1^{st}\)</span> PC is mainly defined by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, the <span class="math inline">\(2^{nd}\)</span> PC is mainly defined by <span class="math inline">\(x_3\)</span>, and the <span class="math inline">\(3^{rd}\)</span> PC, despite its small proportion of importance, mainly consists of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> as well<label for="tufte-sn-226" class="margin-toggle sidenote-number">226</label><input type="checkbox" id="tufte-sn-226" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">226</span> Readers may compare Figure <a href="#fig:f8-PCAnet-pc">158</a> with Figure <a href="#fig:f8-PCAnet">156</a>—Is this a coincidence?</span>.</p>
<p>The R code for generating Figure <a href="#fig:f8-PCAnet-pc">158</a> is shown below.</p>
<p></p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA example</span></span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">2</span>)</span>
<span id="cb180-3"><a href="#cb180-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="sc">-</span><span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">2</span>)</span>
<span id="cb180-4"><a href="#cb180-4" aria-hidden="true" tabindex="-1"></a>x3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb180-5"><a href="#cb180-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x1,x2,x3)</span>
<span id="cb180-6"><a href="#cb180-6" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(FactoMineR)</span>
<span id="cb180-7"><a href="#cb180-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(factoextra)</span>
<span id="cb180-8"><a href="#cb180-8" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">PCA</span>(X)</span>
<span id="cb180-9"><a href="#cb180-9" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>eig</span>
<span id="cb180-10"><a href="#cb180-10" aria-hidden="true" tabindex="-1"></a>t<span class="sc">$</span>var<span class="sc">$</span>coord</span>
<span id="cb180-11"><a href="#cb180-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-12"><a href="#cb180-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the screeplot</span></span>
<span id="cb180-13"><a href="#cb180-13" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_screeplot</span>(t, <span class="at">addlabels =</span> <span class="cn">TRUE</span>)</span>
<span id="cb180-14"><a href="#cb180-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-15"><a href="#cb180-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the variable loadings plot</span></span>
<span id="cb180-16"><a href="#cb180-16" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_contrib</span>(t, <span class="at">choice =</span> <span class="st">&quot;var&quot;</span>, <span class="at">axes =</span> <span class="dv">1</span>, <span class="at">top =</span> <span class="dv">20</span>,</span>
<span id="cb180-17"><a href="#cb180-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">sort.val =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb180-18"><a href="#cb180-18" aria-hidden="true" tabindex="-1"></a>            <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">20</span>))</span>
<span id="cb180-19"><a href="#cb180-19" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_contrib</span>(t, <span class="at">choice =</span> <span class="st">&quot;var&quot;</span>, <span class="at">axes =</span> <span class="dv">2</span>, <span class="at">top =</span> <span class="dv">20</span>,</span>
<span id="cb180-20"><a href="#cb180-20" aria-hidden="true" tabindex="-1"></a>             <span class="at">sort.val =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb180-21"><a href="#cb180-21" aria-hidden="true" tabindex="-1"></a>            <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">20</span>))</span>
<span id="cb180-22"><a href="#cb180-22" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_contrib</span>(t, <span class="at">choice =</span> <span class="st">&quot;var&quot;</span>, <span class="at">axes =</span> <span class="dv">3</span>, <span class="at">top =</span> <span class="dv">20</span>,</span>
<span id="cb180-23"><a href="#cb180-23" aria-hidden="true" tabindex="-1"></a>             <span class="at">sort.val =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb180-24"><a href="#cb180-24" aria-hidden="true" tabindex="-1"></a>            <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">20</span>))</span></code></pre></div>
<p></p>
<p>Now, suppose that there is an outcome variable <span class="math inline">\(y\)</span>. Data in Table <a href="#tab:t8-PCAnet">38</a> is augmented with a new column, as shown in Table <a href="#tab:t8-PCAnet2">40</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-PCAnet2">Table 40: </span>Table <a href="#tab:t8-PCAnet">38</a> is augmented with an outcome variable</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1.33\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.70\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.99\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-3\)</span></td>
<td align="left"><span class="math inline">\(-2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-1.78\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0.07\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(4.62\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3.87\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0.58\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The goal is to build a linear regression model to predict <span class="math inline">\(y\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a linear regression model</span></span>
<span id="cb181-2"><a href="#cb181-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">2</span>)</span>
<span id="cb181-3"><a href="#cb181-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="sc">-</span><span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">2</span>)</span>
<span id="cb181-4"><a href="#cb181-4" aria-hidden="true" tabindex="-1"></a>x3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb181-5"><a href="#cb181-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x1,x2,x3)</span>
<span id="cb181-6"><a href="#cb181-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.33</span>,<span class="fl">0.7</span>,<span class="fl">2.99</span>,<span class="sc">-</span><span class="fl">1.78</span>,<span class="fl">0.07</span>,<span class="fl">4.62</span>,<span class="fl">3.87</span>,<span class="fl">0.58</span>)</span>
<span id="cb181-7"><a href="#cb181-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(y,X))</span>
<span id="cb181-8"><a href="#cb181-8" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>., <span class="at">data =</span> data)</span>
<span id="cb181-9"><a href="#cb181-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data)</span></span>
<span id="cb182-3"><a href="#cb182-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb182-4"><a href="#cb182-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb182-5"><a href="#cb182-5" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) -0.64698    1.60218  -0.404    0.707</span></span>
<span id="cb182-6"><a href="#cb182-6" aria-hidden="true" tabindex="-1"></a><span class="do">## x1          -0.03686    0.67900  -0.054    0.959</span></span>
<span id="cb182-7"><a href="#cb182-7" aria-hidden="true" tabindex="-1"></a><span class="do">## x2           0.65035    0.75186   0.865    0.436</span></span>
<span id="cb182-8"><a href="#cb182-8" aria-hidden="true" tabindex="-1"></a><span class="do">## x3           0.37826    1.75338   0.216    0.840</span></span>
<span id="cb182-9"><a href="#cb182-9" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb182-10"><a href="#cb182-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.546 on 4 degrees of freedom</span></span>
<span id="cb182-11"><a href="#cb182-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6999, Adjusted R-squared:  0.4749</span></span>
<span id="cb182-12"><a href="#cb182-12" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  3.11 on 3 and 4 DF,  p-value: 0.1508</span></span></code></pre></div>
<p>
The <em>R-squared</em> is <span class="math inline">\(0.6999\)</span>, but the three variables are not significant. This is unusual. Recall that <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are highly correlated—there is an issue of <em>multicollinearity</em> in this dataset.</p>
<p>Try a linear regression model with <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_3\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a linear regression model</span></span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data)</span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb184-2"><a href="#cb184-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data)</span></span>
<span id="cb184-3"><a href="#cb184-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb184-4"><a href="#cb184-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb184-5"><a href="#cb184-5" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  -0.4764     1.5494  -0.307   0.7709</span></span>
<span id="cb184-6"><a href="#cb184-6" aria-hidden="true" tabindex="-1"></a><span class="do">## x1            0.5282     0.1805   2.927   0.0328 *</span></span>
<span id="cb184-7"><a href="#cb184-7" aria-hidden="true" tabindex="-1"></a><span class="do">## x3            0.8793     1.6127   0.545   0.6090</span></span>
<span id="cb184-8"><a href="#cb184-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb184-9"><a href="#cb184-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.507 on 5 degrees of freedom</span></span>
<span id="cb184-10"><a href="#cb184-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6438, Adjusted R-squared:  0.5013</span></span>
<span id="cb184-11"><a href="#cb184-11" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 4.519 on 2 and 5 DF,  p-value: 0.07572</span></span></code></pre></div>
<p>
Now <span class="math inline">\(x_1\)</span> is significant. Without fitting another model, we know that <span class="math inline">\(x_2\)</span> has to be significant as well, i.e., as shown in Figure <a href="#fig:f8-PCAnet">156</a>, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are two highly correlated variables that are just like one variable. But because of the multicollinearity, when both are included in the model, neither turns out to be significant.</p>
<p>To overcome the multicollinearity, we have mentioned that the Principal Component Regression (PCR) method is a good approach. First, we calculate the transformed data (i.e., the projections of the data points on the PCs, as shown in Figure <a href="#fig:f8-PCA-line">148</a>) using Eq. <a href="#eq:8-PCA-z">(92)</a>. Results are shown in Table <a href="#tab:t8-PCAnet-PC">41</a>. An important characteristic of the new variables, <span class="math inline">\(\text{PC}_1\)</span>, <span class="math inline">\(\text{PC}_2\)</span>, and <span class="math inline">\(\text{PC}_3\)</span>, is that they are orthogonal to each other, and thus, their correlations are <span class="math inline">\(0\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-PCAnet-PC">Table 41: </span>The coordinates of the <em>white dots</em>, i.e., aka, the projections of the data points on the PCs</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(\text{PC}_1\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_2\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1.43\)</span></td>
<td align="left"><span class="math inline">\(-0.55\)</span></td>
<td align="left"><span class="math inline">\(0.01\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-0.18\)</span></td>
<td align="left"><span class="math inline">\(-0.32\)</span></td>
<td align="left"><span class="math inline">\(0.16\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-0.67\)</span></td>
<td align="left"><span class="math inline">\(-0.29\)</span></td>
<td align="left"><span class="math inline">\(-0.33\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2.36\)</span></td>
<td align="left"><span class="math inline">\(-0.68\)</span></td>
<td align="left"><span class="math inline">\(0.06\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-0.43\)</span></td>
<td align="left"><span class="math inline">\(-0.30\)</span></td>
<td align="left"><span class="math inline">\(-0.08\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.18\)</span></td>
<td align="left"><span class="math inline">\(-0.13\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-1.80\)</span></td>
<td align="left"><span class="math inline">\(-0.09\)</span></td>
<td align="left"><span class="math inline">\(0.31\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.66\)</span></td>
<td align="left"><span class="math inline">\(2.41\)</span></td>
<td align="left"><span class="math inline">\(-0.01\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Then we can build a linear regression model of <span class="math inline">\(y\)</span> using the three new predictors, <span class="math inline">\(\text{PC}_1\)</span>, <span class="math inline">\(\text{PC}_2\)</span>, and <span class="math inline">\(\text{PC}_3\)</span>. The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ PC1 + PC2 + PC3, data = data)</span></span>
<span id="cb185-3"><a href="#cb185-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb185-4"><a href="#cb185-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb185-5"><a href="#cb185-5" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  1.54750    0.54668   2.831   0.0473 *</span></span>
<span id="cb185-6"><a href="#cb185-6" aria-hidden="true" tabindex="-1"></a><span class="do">## PC1         -1.25447    0.41571  -3.018   0.0393 *</span></span>
<span id="cb185-7"><a href="#cb185-7" aria-hidden="true" tabindex="-1"></a><span class="do">## PC2         -0.06022    0.58848  -0.102   0.9234</span></span>
<span id="cb185-8"><a href="#cb185-8" aria-hidden="true" tabindex="-1"></a><span class="do">## PC3         -1.39950    3.02508  -0.463   0.6677</span></span>
<span id="cb185-9"><a href="#cb185-9" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb185-10"><a href="#cb185-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.546 on 4 degrees of freedom</span></span>
<span id="cb185-11"><a href="#cb185-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6999, Adjusted R-squared:  0.4749</span></span>
<span id="cb185-12"><a href="#cb185-12" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:  3.11 on 3 and 4 DF,  p-value: 0.1508</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-PCAnetY"></span>
<img src="graphics/8_PCAnetY.png" alt="Visualization of the relationship between all the variables" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 159: Visualization of the relationship between all the variables<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><span class="math inline">\(\text{PC}_1\)</span> is significant. Since <span class="math inline">\(\text{PC}_1\)</span> is mainly defined by <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, this is consistent with all the analysis done so far, and a structure of the relationships between the variables is revealed in Figure <a href="#fig:f8-PCAnetY">159</a>.</p>
</div>
</div>
<div id="exercises-6" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f8-hw-solutionpath"></span>
<img src="graphics/8_hw_solutionpath.png" alt="The path trajectory of a LASSO model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 160: The path trajectory of a LASSO model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Figure <a href="#fig:f8-hw-solutionpath">160</a> shows the path trajectory generated by applying <code>glmnet()</code> on a dataset with <span class="math inline">\(10\)</span> predictors. Which two variables are the top two significant variables (note the index of the variables is shown in the right end of the figure)?</p></li>
<li><p> Consider the dataset shown in Table <a href="#tab:t8-hw-lasso">42</a>. Set <span class="math inline">\(\lambda = 1\)</span> and initial values for <span class="math inline">\(\beta_1 = 0\)</span>, and <span class="math inline">\(\beta_2 = 1\)</span>. Implement the Shooting algorithm by manual operation. Do one iteration. Report <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-hw-lasso">Table 42: </span>Dataset for Q2</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(-0.15\)</span></td>
<td align="left"><span class="math inline">\(-0.48\)</span></td>
<td align="left"><span class="math inline">\(0.46\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-0.72\)</span></td>
<td align="left"><span class="math inline">\(-0.54\)</span></td>
<td align="left"><span class="math inline">\(-0.37\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1.36\)</span></td>
<td align="left"><span class="math inline">\(-0.91\)</span></td>
<td align="left"><span class="math inline">\(-0.27\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(0.61\)</span></td>
<td align="left"><span class="math inline">\(1.59\)</span></td>
<td align="left"><span class="math inline">\(1.35\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-1.11\)</span></td>
<td align="left"><span class="math inline">\(0.34\)</span></td>
<td align="left"><span class="math inline">\(-0.11\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Follow up on the dataset in Q2. Use the R pipeline for LASSO on this data. Compare the result from R and the result by your manual calculation.</p></li>
<li><p> Conduct a principal component analysis for the dataset shown in Table <a href="#tab:t8-hw-pca">43</a>. Show details of the process.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-hw-pca">Table 43: </span>Dataset for Q4</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1.8\)</span></td>
<td align="left"><span class="math inline">\(2.08\)</span></td>
<td align="left"><span class="math inline">\(-0.28\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3.6\)</span></td>
<td align="left"><span class="math inline">\(-0.78\)</span></td>
<td align="left"><span class="math inline">\(0.79\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.2\)</span></td>
<td align="left"><span class="math inline">\(-0.08\)</span></td>
<td align="left"><span class="math inline">\(-0.52\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4.3\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left"><span class="math inline">\(-0.47\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.1\)</span></td>
<td align="left"><span class="math inline">\(0.71\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3.6\)</span></td>
<td align="left"><span class="math inline">\(1.29\)</span></td>
<td align="left"><span class="math inline">\(0.67\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.2\)</span></td>
<td align="left"><span class="math inline">\(0.57\)</span></td>
<td align="left"><span class="math inline">\(0.15\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4.0\)</span></td>
<td align="left"><span class="math inline">\(1.12\)</span></td>
<td align="left"><span class="math inline">\(1.18\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<ol style="list-style-type: lower-alpha">
<li>Standardize the dataset (i.e., by making the means of the variables to be zero, and the standard derivations of the variables to be <span class="math inline">\(1\)</span>). (b) Calculate the sample covariance matrix (i.e., <span class="math inline">\(\boldsymbol{S} =(\boldsymbol{X}^T\boldsymbol{X})/(N-1))\)</span>. (c) Conduct eigenvalue decomposition on the sample covariance matrix, obtain the four eigenvectors and their eigenvalues. (d) Report the percentage of variances that could be explained by the four PCs, respectively. Draw the screeplot. How many PCs are sufficient to represent the dataset? In other words, which PCs are significant? (e) Interpret the PCs you have selected, i.e., which variables define which PCs? (f) Convert the original data into the space spanned by the four PCs, by filling in Table <a href="#tab:t8-hw-pca2">44</a>.</li>
</ol>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-hw-pca2">Table 44: </span>Dataset for Q4</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(\text{PC}_1\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_2\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_3\)</span></th>
<th align="left"><span class="math inline">\(\text{PC}_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Follow up on the dataset in Q2 from Chapter 7. (a) Conduct the PCA analysis on the three predictors to identify the three principal components and their contributions on explaining the variance in data; and (b) use the R pipeline for PCA to do the PCA analysis and compare with your manual calculation.</p></li>
<li><p> Suppose that we have an outcome variable that could be augmented into the dataset in Q4, as shown in Table <a href="#tab:t8-hw-pca-lr">45</a>. Apply the shooting algorithm for LASSO on this dataset to identify important variables. Use the following initial values for the parameters, <span class="math inline">\(\lambda=1, \beta_1=0, \beta_2=1, \beta_3=1, \beta_4=1\)</span>, and just do one iteration of the shooting algorithm. Show details of manual calculation.</p></li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-hw-pca-lr">Table 45: </span>Dataset for Q6</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1.8\)</span></td>
<td align="left"><span class="math inline">\(2.08\)</span></td>
<td align="left"><span class="math inline">\(-0.28\)</span></td>
<td align="left"><span class="math inline">\(1.2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3.6\)</span></td>
<td align="left"><span class="math inline">\(-0.78\)</span></td>
<td align="left"><span class="math inline">\(0.79\)</span></td>
<td align="left"><span class="math inline">\(2.1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.2\)</span></td>
<td align="left"><span class="math inline">\(-0.08\)</span></td>
<td align="left"><span class="math inline">\(-0.52\)</span></td>
<td align="left"><span class="math inline">\(0.8\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4.3\)</span></td>
<td align="left"><span class="math inline">\(0.38\)</span></td>
<td align="left"><span class="math inline">\(-0.47\)</span></td>
<td align="left"><span class="math inline">\(1.5\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.1\)</span></td>
<td align="left"><span class="math inline">\(0.71\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.8\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(3.6\)</span></td>
<td align="left"><span class="math inline">\(1.29\)</span></td>
<td align="left"><span class="math inline">\(0.67\)</span></td>
<td align="left"><span class="math inline">\(1.6\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(2.2\)</span></td>
<td align="left"><span class="math inline">\(0.57\)</span></td>
<td align="left"><span class="math inline">\(0.15\)</span></td>
<td align="left"><span class="math inline">\(1.2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(4.0\)</span></td>
<td align="left"><span class="math inline">\(1.12\)</span></td>
<td align="left"><span class="math inline">\(1.18\)</span></td>
<td align="left"><span class="math inline">\(1.6\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> After extraction of the four PCs from Q4, use <code>lm()</code> in R to build a linear regression model with the outcome variable (as shown in Table <a href="#tab:t8-hw-pca-lr">45</a>) and the four PCs as the predictors. (a) Report the summary of your linear regression model with the four PCs; and (b) which PCs significantly affect the outcome variable?</p></li>
<li><p> Revisit Q1 in <strong>Chapter 3</strong>. Derive the shooting algorithm for weighted least squares regression with <span class="math inline">\(L_1\)</span> norm penalty.</p></li>
<li><p> Design a simulated experiment to evaluate the effectiveness of the <code>glmet()</code> in the R package <code>glmnet</code>. (a) For instance, you can simulate <span class="math inline">\(20\)</span> samples from a linear regression model with <span class="math inline">\(10\)</span> variables, where only <span class="math inline">\(2\)</span> out of the <span class="math inline">\(10\)</span> variables are truly significant, e.g., the true model is
<span class="math display">\[
  y = \beta_{1}x_1 +\beta_{2}x_2 + \epsilon,
  \]</span>
where <span class="math inline">\(\beta_{1} = 1\)</span>, <span class="math inline">\(\beta_{2} = 1\)</span>, and
<span class="math display">\[
  \epsilon \sim N\left(0, 1\right).
  \]</span>
You can simulate <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> using the standard normal distribution <span class="math inline">\(N\left(0, 1\right)\)</span>. For the other <span class="math inline">\(8\)</span> variables, <span class="math inline">\(x_3\)</span> to <span class="math inline">\(x_{10}\)</span>, you can simulate each from <span class="math inline">\(N\left(0, 1\right)\)</span>. In data analysis, we will use all <span class="math inline">\(10\)</span> variables as predictors, since we won’t know the true model. (b) Run <code>lm()</code> on the simulated data and comment on the results. (c) Run <code>glmnet()</code> on the simulated data, and check the path trajectory plot to see if the true significant variables could be detected. (d) Use the cross-validation process integrated into the <code>glmnet</code> package to see if the true significant variables could be detected. (e) Use <code>rpart()</code> to build a decision tree and extract the variable importance score to see if the true significant variables could be detected. (f) Use <code>randomforest()</code> to build a random forest model and extract the variable importance score, to see if the true significant variables could be detected.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<div id="chapter-9.-pragmatism-experience-experimental" class="section level1 unnumbered">
<h1>Chapter 9. Pragmatism: Experience &amp; Experimental</h1>
<div id="overview-7" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 9 is about <em>pragmatism</em>. Pragmatism is an interesting word because, sometimes, being pragmatic means the opposite of “<em>-ism</em>.” Read what was said about pragmatism: “<em>Consider the practical effects of the objects of your conception. Then, your conception of those effects is the whole of your conception of the object".<label for="tufte-sn-227" class="margin-toggle sidenote-number">227</label><input type="checkbox" id="tufte-sn-227" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">227</span> Peirce, C. S., <em>How to Make Our Ideas Clear</em>, Popular Science Monthly, v. 12, 286–302, 1878.</span></em> In a sense, this resonates with”<em>why by their fruits you shall know them</em>."<label for="tufte-sn-228" class="margin-toggle sidenote-number">228</label><input type="checkbox" id="tufte-sn-228" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">228</span> Matthew 7:20.</span> To analyze a dataset, we shall be aware of a tendency that we often apply concepts before we see the dataset. EDA has provided a practical way to help us see our preconceptions. In this chapter, two more methods are presented, including the <strong>kernel regression model</strong> that generalizes the idea of linear regression, and the <strong>conditional variance regression model</strong> that creates a convolution by using regression twice.</p>
</div>
<div id="kernel-regression-model" class="section level2 unnumbered">
<h2>Kernel regression model</h2>
<div id="rationale-and-formulation-14" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>Simple models, similar to the linear regression model, are like <em>parents who tell white lies</em>. A model is simple, not only in the sense that it looks simple, but also because it builds on assumptions that simplify reality. Among all the simple models, the linear regression model is particularly good at disguising its simplicity—it seems so natural that we often forget that its simplicity is its assumption. Simple in its cosmology, not necessary in its terminology—that is what the phrase <em>simple model</em> means<label for="tufte-sn-229" class="margin-toggle sidenote-number">229</label><input type="checkbox" id="tufte-sn-229" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">229</span> In this sense, a model, regardless of how sophisticated its mathematical representation is, is a simple model if its assumptions simplify reality to such an extent that demands our leap of faith.</span>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-1"></span>
<p class="caption marginnote shownote">
Figure 161: (Left) A single outlier (a local pattern) could impact the regression model as a whole; (right) a <em>localized</em> regression model (i.e., the curvature adapts to the locality instead of following a prescribed global form such as a straight line)
</p>
<img src="graphics/9_1_left.png" alt="(Left) A single outlier (a local pattern) could impact the regression model as a whole; (right) a *localized* regression model (i.e., the curvature adapts to the locality instead of following a prescribed global form such as a straight line) " width="49%" height="49%"  /><img src="graphics/9_1_right.png" alt="(Left) A single outlier (a local pattern) could impact the regression model as a whole; (right) a *localized* regression model (i.e., the curvature adapts to the locality instead of following a prescribed global form such as a straight line) " width="49%" height="49%"  />
</div>
<p></p>
<p>“<em>Simple, but not simpler</em>” said Albert Einstein.</p>
<p>One such assumption the linear regression model has made, obviously, is <em>linearity</em>. It would be OK in practice, as Figure <a href="#fig:f2-2"><strong>??</strong></a> in <strong>Chapter 2</strong> assures us: it is not perfect but it is a good approximation.</p>
<p>Now let’s look at Figure <a href="#fig:f9-1">161</a> (left). The <em>true model</em>, represented by the black line, is truly a line. But the fitted model, the orange line, deviates from the black line. In other words, even if the linearity assumption is correct, the consequence is not what we hope for.</p>
<p>The troublemaker appears to be the outlier located on the upper right corner of the figure. As discernible data scientists, we should be aware that the <em>model</em> is general, while the <em>dataset</em> at hand is particular. It is OK to say the outlier is the troublemaker, but note that this outlier is accidental. The real troublemaker is what <em>enables</em> the possibility of outlier to be a troublemaker. The real troublemaker lies deeper.</p>
<p>A common theme of the methods in this book is to establish certainty in a world of uncertainty. The linearity assumption is an assumption, since real data rarely give you a perfect straight line. The way it deals with uncertainty is to use the least squares principle for model estimation. It aims to look for a line that could pierce through <em>all</em> the data points. This makes each data point have a <em>global</em> impact: mentally move any data point in Figure <a href="#fig:f9-1">161</a> (left) up and down and imagine how the fitted orange line would move up and down accordingly. In other words, as a data point in any location could change the line dramatically, the linear regression model, together with its least squares estimation method, has imposed an even stronger assumption than merely linearity: it assumes that knowledge learned from one location would be universally useful to all other locations. This implicit assumption<label for="tufte-sn-230" class="margin-toggle sidenote-number">230</label><input type="checkbox" id="tufte-sn-230" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">230</span> I.e., models that have made this assumption are often termed as <em>global models</em>.</span> could be irrational in some applications, where the data points collected in one location may only tell information about that local area, not easily generalizable to the whole space. Thus, when the global models fail, we need <em>local models</em><label for="tufte-sn-231" class="margin-toggle sidenote-number">231</label><input type="checkbox" id="tufte-sn-231" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">231</span> A <em>local model</em> more relies on the data points in a neighborhood to build up the part of the curve that comes through that particular neighborhood.</span> to fit the data, as shown in Figure <a href="#fig:f9-1">161</a> (right).</p>
</div>
<div id="theory-and-method-9" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>Suppose there are <span class="math inline">\(N\)</span> data points, denoted as, <span class="math inline">\(\left(x_{n}, y_{n}\right)\)</span> for <span class="math inline">\(n=1,2, \dots, N\)</span>. To predict on a point <span class="math inline">\(x^*\)</span>, a <em>local model</em> assumes the following structure</p>
<p><span class="math display" id="eq:9-kr">\[\begin{equation}
    y^* = \sum_{n=1}^{N} y_n w(x_n, x^*).
\tag{98}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(w(x_n,x^*)\)</span> is the <strong>weight function</strong> that characterizes the <em>similarity</em> between <span class="math inline">\(x^*\)</span> and the training data points, <span class="math inline">\(x_n\)</span>, for <span class="math inline">\(n=1,2,\dots,N\)</span>. The idea is to predict on a data point based on the data points that are nearby. Methods differ from each other in terms of how they define <span class="math inline">\(w(x_n,x^*)\)</span>.</p>
<p>Roughly speaking, there are two main methods. One is the <strong>K-nearest neighbor</strong> (<strong>KNN</strong>) smoother, and another is the <strong>kernel</strong> smoother.</p>
<p><em>The KNN smoother.</em> The KNN smoother defines <span class="math inline">\(w(x_n,x^*)\)</span> as</p>
<p><span class="math display">\[w\left(x_{n}, x^{*}\right)=\left\{\begin{array}{l}{\frac{1}{k}, \text { if } x_{n} \text { is one of the } k \text { nearest neighbors of } x^{*}}; \\ {0, \text { if } x_{n} \text { is NOT among the } k \text{ nearest neighbors of } x^{*}}.\end{array}\right.\]</span></p>
<p>Here, to define the <em>nearest neighbors</em> of a data point, a <em>distance function</em> is needed. Examples include the <em>Euclidean</em><label for="tufte-sn-232" class="margin-toggle sidenote-number">232</label><input type="checkbox" id="tufte-sn-232" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">232</span> E.g., <span class="math inline">\(d\left(\boldsymbol{x}_n, \boldsymbol{x}_m\right) = \sqrt {\sum _{i=1}^{p} \left( x_{ni}-x_{mi}\right)^2 }\)</span>.</span>, <em>Mahalanobis</em>, and <em>Cosine</em> distance functions. What distance function to use depends on the characteristics of the data. Model selection methods such as the cross-validation can be used to select the best distance function for a dataset.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-knn"></span>
<p class="caption marginnote shownote">
Figure 162: Three KNN smoother models (<span class="math inline">\(k=1\)</span>, <span class="math inline">\(k=2\)</span>, and <span class="math inline">\(k=6\)</span>)
</p>
<img src="graphics/9_knn_illu.png" alt="Three KNN smoother models ($k=1$, $k=2$, and $k=6$)" width="80%"  />
</div>
<p></p>
<p>Consider a data example as shown in Table <a href="#tab:t9-knn">46</a>. A visualization of the data points is shown in Figure <a href="#fig:f9-knn">162</a>, i.e., the gray data points.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-knn">Table 46: </span>Example of a dataset with <span class="math inline">\(6\)</span> data points</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(6\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(9\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(10\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Let’s build <span class="math inline">\(3\)</span> KNN smoother models (e.g., <span class="math inline">\(k=1\)</span>, <span class="math inline">\(k=2\)</span>, and <span class="math inline">\(k=6\)</span>) and use the Euclidean distance function to identify the <em>nearest neighbors</em> of a data point. Results are presented in Tables <a href="#tab:t9-knnK1">47</a>, <a href="#tab:t9-knnK2">48</a>, and <a href="#tab:t9-knnK6">49</a>, respectively. Note that, in this dataset, as there are in total <span class="math inline">\(6\)</span> data points, the KNN model with <span class="math inline">\(k=6\)</span> is the same as the trivial model that uses the average of <span class="math inline">\(y\)</span> as predictions for all data points.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-knnK1">Table 47: </span>Predictions by a KNN smoother model with <span class="math inline">\(k=1\)</span> on some locations of <span class="math inline">\(x^*\)</span></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x^*\)</span></th>
<th align="left">KNN</th>
<th align="left"><span class="math inline">\(y^*\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0.4\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span></td>
<td align="left"><span class="math inline">\(y_1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1.6\)</span></td>
<td align="left"><span class="math inline">\(x_3\)</span></td>
<td align="left"><span class="math inline">\(y_3\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3.2\)</span></td>
<td align="left"><span class="math inline">\(x_4\)</span></td>
<td align="left"><span class="math inline">\(y_4\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4.8\)</span></td>
<td align="left"><span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(y_6\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-knnK2">Table 48: </span>Predictions by a KNN smoother model with <span class="math inline">\(k=2\)</span> on some locations of <span class="math inline">\(x^*\)</span></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x^*\)</span></th>
<th align="left">KNN</th>
<th align="left"><span class="math inline">\(y^*\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0.4\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span></td>
<td align="left"><span class="math inline">\((y_1 + y_2)/2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1.6\)</span></td>
<td align="left"><span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span></td>
<td align="left"><span class="math inline">\((y_2 + y_3)/2\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3.2\)</span></td>
<td align="left"><span class="math inline">\(x_4\)</span>, <span class="math inline">\(x_5\)</span></td>
<td align="left"><span class="math inline">\((y_4 + y_5)/2\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4.8\)</span></td>
<td align="left"><span class="math inline">\(x_5\)</span>, <span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\((y_5 + y_6)/2\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-knnK6">Table 49: </span>Predictions by a KNN smoother model with <span class="math inline">\(k=6\)</span> on some locations of <span class="math inline">\(x^*\)</span></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x^*\)</span></th>
<th align="left">KNN</th>
<th align="left"><span class="math inline">\(y^*\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0.4\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(\sum_{n=1}^{6} y_n/6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1.6\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(\sum_{n=1}^{6} y_n/6\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3.2\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(\sum_{n=1}^{6} y_n/6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4.8\)</span></td>
<td align="left"><span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_6\)</span></td>
<td align="left"><span class="math inline">\(\sum_{n=1}^{6} y_n/6\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The <span class="math inline">\(3\)</span> KNN smoother models are also shown in Figure <a href="#fig:f9-knn">162</a>.</p>
<p>A distinct feature of the <strong>KNN smoother</strong> is the <em>discrete</em> manner to define the similarity between data points, which is, for any data point <span class="math inline">\(x^*\)</span>, the data point <span class="math inline">\(x_n\)</span> is either a neighbor or not. The KNN smoother only uses the <span class="math inline">\(k\)</span> nearest neighbors of <span class="math inline">\(x^*\)</span> to predict <span class="math inline">\(y^*\)</span>. This discrete manner of the KNN smoother results in the serrated curves shown in Figure <a href="#fig:f9-knn">162</a>. This is obviously artificial, pointing out a systematic <em>bias</em> imposed by the KNN smoother model.</p>
<p><em>The kernel smoother.</em> To remove this bias, the <strong>kernel smoother</strong> creates <em>continuity</em> in the similarity between data points. A kernel smoother defines <span class="math inline">\(w(x_n,x^* )\)</span> in the following manner</p>
<p><span class="math display">\[w\left(x_{n}, x^{*}\right)=\frac{K\left(x_{n}, x^{*}\right)}{\sum_{n=1}^{N} K\left(x_{n}, x^{*}\right)}.\]</span></p>
<p>Here, <span class="math inline">\(K\left(x_{n}, x^{*}\right)\)</span> is a <em>kernel function</em> as we have discussed in <strong>Chapter 7</strong>. There have been many kernel functions developed, for example, as shown in Table <a href="#tab:t9-1">50</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-1">Table 50: </span>Some kernel functions used in machine learning</span><!--</caption>--></p>
<table>
<colgroup>
<col width="9%" />
<col width="83%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Kernel Function</strong></th>
<th align="left"><strong>Mathematical Form</strong></th>
<th align="left"><strong>Parameters</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Line</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \boldsymbol{x}_i^T\boldsymbol{x}_j\)</span></td>
<td align="left"><em>null</em></td>
</tr>
<tr class="even">
<td align="left">Polynomial</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j)= \left(\boldsymbol{x}_i^T\boldsymbol{x}_j + 1\right)^q\)</span></td>
<td align="left"><span class="math inline">\(q\)</span></td>
</tr>
<tr class="odd">
<td align="left">Gaussian radial basis</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = e^{-\gamma\Vert \boldsymbol{x}_i - \boldsymbol{x}_j\Vert^2}\)</span></td>
<td align="left"><span class="math inline">\(\gamma \geq 0\)</span></td>
</tr>
<tr class="even">
<td align="left">Laplace radial basis</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = e^{-\gamma\Vert \boldsymbol{x}_i - \boldsymbol{x}_j\Vert}\)</span></td>
<td align="left"><span class="math inline">\(\gamma \geq 0\)</span></td>
</tr>
<tr class="odd">
<td align="left">Hyperbolic tangent</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = tanh(\boldsymbol{x}_i^T\boldsymbol{x}_j+b)\)</span></td>
<td align="left">b</td>
</tr>
<tr class="even">
<td align="left">Sigmoid</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = tanh(a\boldsymbol{x}_i^T\boldsymbol{x}_j+b)\)</span></td>
<td align="left">a,b</td>
</tr>
<tr class="odd">
<td align="left">Bessel function</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \frac{bessel_{v+1}^n(\sigma\Vert \boldsymbol{x}_i - \boldsymbol{x}_j \Vert)}{\left(\Vert \boldsymbol{x}_i -\boldsymbol{x}_j \Vert\right)^{-n(v+1)}}\)</span></td>
<td align="left"><span class="math inline">\(\sigma, n,v\)</span></td>
</tr>
<tr class="even">
<td align="left">ANOVA radial basis</td>
<td align="left"><span class="math inline">\(K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \left( \sum_{k=1}^n e^{-\sigma\left(x_i^k - x_j^k\right)}\right)^d\)</span></td>
<td align="left"><span class="math inline">\(\sigma, d\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Many kernel functions are smooth functions. To understand a kernel function, using R to draw it is a good approach. For example, the following R code draws a few instances of the <em>Gaussian radial basis</em> kernel function and shows them in Figure <a href="#fig:f9-gauss">163</a>. The curve illustrates how the similarity <em>smoothly</em> decreases when the distance between the two data points increases. And the <em>bandwidth</em> parameter <span class="math inline">\(\gamma\)</span> controls the rate of decrease, i.e., the smaller the <span class="math inline">\(\gamma\)</span>, the less sensitive the kernel function to the <em>Euclidean</em> distance of the data points (measured by <span class="math inline">\(\Vert \boldsymbol{x}_i - \boldsymbol{x}_j\Vert^2\)</span>).</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-gauss"></span>
<img src="graphics/9_gauss.png" alt="Three instances of the *Gaussian radial basis* kernel function ($\gamma=0.2$, $\gamma=0.5$, and $\gamma=1$)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 163: Three instances of the <em>Gaussian radial basis</em> kernel function (<span class="math inline">\(\gamma=0.2\)</span>, <span class="math inline">\(\gamma=0.5\)</span>, and <span class="math inline">\(\gamma=1\)</span>)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use R to visualize a kernel function</span></span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(latex2exp) <span class="co"># enable the use of latex in R graphics</span></span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a><span class="co"># write a function for the kernel function</span></span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a>gauss <span class="ot">&lt;-</span> <span class="cf">function</span>(x,gamma) <span class="fu">exp</span>(<span class="sc">-</span> gamma <span class="sc">*</span> x<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb186-5"><a href="#cb186-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">3</span>, <span class="at">to =</span> <span class="dv">3</span>, <span class="at">by =</span> <span class="fl">0.001</span>) </span>
<span id="cb186-6"><a href="#cb186-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">gauss</span>(x,<span class="fl">0.2</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">xlab =</span> <span class="fu">TeX</span>(<span class="st">&#39;$x_i  - x_j$&#39;</span>),</span>
<span id="cb186-7"><a href="#cb186-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Gaussian radial basis kernel&quot;</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb186-8"><a href="#cb186-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">gauss</span>(x,<span class="fl">0.5</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb186-9"><a href="#cb186-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">gauss</span>(x,<span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb186-10"><a href="#cb186-10" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>, </span>
<span id="cb186-11"><a href="#cb186-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="fu">TeX</span>(<span class="st">&#39;$</span><span class="sc">\\</span><span class="st">gamma = 0.2$&#39;</span>), <span class="fu">TeX</span>(<span class="st">&#39;$</span><span class="sc">\\</span><span class="st">gamma = 0.5$&#39;</span>),</span>
<span id="cb186-12"><a href="#cb186-12" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">TeX</span>(<span class="st">&#39;$</span><span class="sc">\\</span><span class="st">gamma = 1$&#39;</span>)), </span>
<span id="cb186-13"><a href="#cb186-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,</span>
<span id="cb186-14"><a href="#cb186-14" aria-hidden="true" tabindex="-1"></a>                                <span class="st">&quot;darkorange&quot;</span>,<span class="st">&quot;forestgreen&quot;</span>))</span></code></pre></div>
<p></p>
</div>
<div id="r-lab-13" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get the dataset into R and organize it in required format.</p>
<p></p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb187-2"><a href="#cb187-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-3"><a href="#cb187-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb187-4"><a href="#cb187-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb187-5"><a href="#cb187-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/KR.csv&quot;</span>)</span>
<span id="cb187-6"><a href="#cb187-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb187-7"><a href="#cb187-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(data)</span></span>
<span id="cb187-8"><a href="#cb187-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-9"><a href="#cb187-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb187-10"><a href="#cb187-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb187-11"><a href="#cb187-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data<span class="sc">$</span>x</span>
<span id="cb187-12"><a href="#cb187-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>y</span>
<span id="cb187-13"><a href="#cb187-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-14"><a href="#cb187-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data </span></span>
<span id="cb187-15"><a href="#cb187-15" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data) <span class="sc">*</span> <span class="dv">4</span><span class="sc">/</span><span class="dv">5</span>) )</span>
<span id="cb187-16"><a href="#cb187-16" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb187-17"><a href="#cb187-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data </span></span>
<span id="cb187-18"><a href="#cb187-18" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates a list of models. For a kernel regression model, important decisions are made on the kernel function and its parameter(s). For example, here, we create two models with two kernel functions and their parameters:</p>
<p></p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb188-2"><a href="#cb188-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-3"><a href="#cb188-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model1: ksmooth(x,y, kernel = &quot;normal&quot;, bandwidth=10)</span></span>
<span id="cb188-4"><a href="#cb188-4" aria-hidden="true" tabindex="-1"></a><span class="co"># model2: ksmooth(x,y, kernel = &quot;box&quot;, bandwidth=5)</span></span>
<span id="cb188-5"><a href="#cb188-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model3: ...</span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses cross-validation to evaluate the candidate models to identify the best model.</p>
<p></p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Use 5-fold cross-validation to evaluate the models</span></span>
<span id="cb189-2"><a href="#cb189-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-3"><a href="#cb189-3" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">=</span> <span class="dv">10</span> <span class="co"># number of fold </span></span>
<span id="cb189-4"><a href="#cb189-4" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(data.train)[<span class="dv">1</span>] </span>
<span id="cb189-5"><a href="#cb189-5" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb189-6"><a href="#cb189-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-7"><a href="#cb189-7" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate model1</span></span>
<span id="cb189-8"><a href="#cb189-8" aria-hidden="true" tabindex="-1"></a>cv_mse <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb189-9"><a href="#cb189-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb189-10"><a href="#cb189-10" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb189-11"><a href="#cb189-11" aria-hidden="true" tabindex="-1"></a>  data.train.cv <span class="ot">&lt;-</span> data.train[<span class="sc">-</span>test_i, ] </span>
<span id="cb189-12"><a href="#cb189-12" aria-hidden="true" tabindex="-1"></a>  data.test.cv <span class="ot">&lt;-</span> data.train[test_i, ]  </span>
<span id="cb189-13"><a href="#cb189-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">require</span>( <span class="st">&#39;kernlab&#39;</span> )</span>
<span id="cb189-14"><a href="#cb189-14" aria-hidden="true" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">ksmooth</span>(data.train.cv<span class="sc">$</span>x, data.train.cv<span class="sc">$</span>y, </span>
<span id="cb189-15"><a href="#cb189-15" aria-hidden="true" tabindex="-1"></a>                    <span class="at">kernel =</span> <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth =</span> <span class="dv">10</span>,</span>
<span id="cb189-16"><a href="#cb189-16" aria-hidden="true" tabindex="-1"></a>                    <span class="at">x.points=</span>data.test.cv[,<span class="dv">1</span>]) </span>
<span id="cb189-17"><a href="#cb189-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (1) Fit the kernel regression model with Gaussian kernel</span></span>
<span id="cb189-18"><a href="#cb189-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (argument: kernel = &quot;normal&quot;) and bandwidth = 0.5; (2) There is</span></span>
<span id="cb189-19"><a href="#cb189-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># no predict() for ksmooth. Use the argument</span></span>
<span id="cb189-20"><a href="#cb189-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># &quot;x.points=data.test.cv&quot; instead. </span></span>
<span id="cb189-21"><a href="#cb189-21" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> model1<span class="sc">$</span>y  </span>
<span id="cb189-22"><a href="#cb189-22" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test.cv<span class="sc">$</span>y  </span>
<span id="cb189-23"><a href="#cb189-23" aria-hidden="true" tabindex="-1"></a>  cv_mse[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb189-24"><a href="#cb189-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb189-25"><a href="#cb189-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-26"><a href="#cb189-26" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_mse)</span>
<span id="cb189-27"><a href="#cb189-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-28"><a href="#cb189-28" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate model2 using the same script above</span></span>
<span id="cb189-29"><a href="#cb189-29" aria-hidden="true" tabindex="-1"></a><span class="co"># ... </span></span></code></pre></div>
<p></p>
<p>The result is shown below</p>
<p></p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.2605955  # Model1</span></span>
<span id="cb190-2"><a href="#cb190-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.2662046  # Model2</span></span></code></pre></div>
<p>
<strong>Step 5</strong> builds the final model.</p>
<p></p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, use ksmooth() function to </span></span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a><span class="co"># build your final model</span></span>
<span id="cb191-3"><a href="#cb191-3" aria-hidden="true" tabindex="-1"></a>kr.final <span class="ot">&lt;-</span> <span class="fu">ksmooth</span>(data.train<span class="sc">$</span>x, data.train<span class="sc">$</span>y, <span class="at">kernel =</span> <span class="st">&quot;normal&quot;</span>,</span>
<span id="cb191-4"><a href="#cb191-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">bandwidth =</span> <span class="dv">10</span>, <span class="at">x.points=</span>data.test[,<span class="dv">1</span>]) <span class="co"># </span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the final model for prediction.</p>
<p></p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Evaluate the prediction performance of your model</span></span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> kr.final<span class="sc">$</span>y  </span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a>true_y <span class="ot">&lt;-</span> data.test<span class="sc">$</span>y   </span>
<span id="cb192-4"><a href="#cb192-4" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb192-5"><a href="#cb192-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(mse)</span></code></pre></div>
<p></p>
<p>This pipeline could be easily extended to KNN smoother model, i.e., using the <code>knn.reg</code> in the <code>FNN</code> package.</p>
<p><em>Simulation Experiment.</em> We have created a R script in <strong>Chapter 5</strong> to simulate data from nonlinear regression models. Here, we use the same R script as shown below.</p>
<p></p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate one batch of data</span></span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a><span class="co"># coefficients of the true model</span></span>
<span id="cb193-4"><a href="#cb193-4" aria-hidden="true" tabindex="-1"></a>coef <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.68</span>,<span class="fl">0.82</span>,<span class="sc">-</span><span class="fl">0.417</span>,<span class="fl">0.32</span>,<span class="sc">-</span><span class="fl">0.68</span>) </span>
<span id="cb193-5"><a href="#cb193-5" aria-hidden="true" tabindex="-1"></a>v_noise <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb193-6"><a href="#cb193-6" aria-hidden="true" tabindex="-1"></a>n_df <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb193-7"><a href="#cb193-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n_df</span>
<span id="cb193-8"><a href="#cb193-8" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_train, coef, v_noise)</span></code></pre></div>
<p></p>
<p>The simulated data are shown in Figure <a href="#fig:f9-3">164</a> (i.e., the gray data points).</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-3"></span>
<p class="caption marginnote shownote">
Figure 164: Kernel regression models with different choices on the <em>bandwidth</em> parameter (<span class="math inline">\(\gamma\)</span>) of the Gaussian radial basis kernel function
</p>
<img src="graphics/9_3.png" alt="Kernel regression models with different choices on the *bandwidth* parameter ($\gamma$) of the Gaussian radial basis kernel function" width="80%"  />
</div>
<p></p>
<p>The following R code overlays the <em>true</em> model, i.e., as the black curve, in Figure <a href="#fig:f9-3">164</a>.</p>
<p></p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the true model</span></span>
<span id="cb194-2"><a href="#cb194-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb194-3"><a href="#cb194-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, X <span class="sc">%*%</span> coef, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p></p>
<p>Let’s use the kernel regression model to fit the data. We use the <em>Gaussian radial basis</em> kernel function, with three different choices of the <em>bandwidth</em> parameter (<span class="math inline">\(\gamma\)</span>), i.e., (<span class="math inline">\(\gamma = 2\)</span>, <span class="math inline">\(\gamma = 5\)</span>, <span class="math inline">\(\gamma = 15\)</span>). Then we overlay the three fitted kernel regression models in Figure <a href="#fig:f9-3">164</a> using the following R code.</p>
<p></p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">ksmooth</span>(x,y, <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth=</span><span class="dv">2</span>),<span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb195-3"><a href="#cb195-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">ksmooth</span>(x,y, <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth=</span><span class="dv">5</span>),<span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb195-4"><a href="#cb195-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;dodgerblue4&quot;</span>)</span>
<span id="cb195-5"><a href="#cb195-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">ksmooth</span>(x,y, <span class="st">&quot;normal&quot;</span>, <span class="at">bandwidth=</span><span class="dv">15</span>),<span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb195-6"><a href="#cb195-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb195-7"><a href="#cb195-7" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>,</span>
<span id="cb195-8"><a href="#cb195-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>, <span class="st">&quot;Kernel Reg (bw = 2)&quot;</span>,</span>
<span id="cb195-9"><a href="#cb195-9" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Kernel Reg (bw = 5)&quot;</span>, <span class="st">&quot;Kernel Reg (bw = 15)&quot;</span>), </span>
<span id="cb195-10"><a href="#cb195-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">4</span>),</span>
<span id="cb195-11"><a href="#cb195-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;darkorange&quot;</span>,<span class="st">&quot;dodgerblue4&quot;</span>,<span class="st">&quot;forestgreen&quot;</span>), </span>
<span id="cb195-12"><a href="#cb195-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="dv">32</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p>As shown in Figure <a href="#fig:f9-3">164</a>, the <em>bandwidth</em> parameter determines how smooth are the fitted curves: the larger the bandwidth, the smoother the regression curve<label for="tufte-sn-233" class="margin-toggle sidenote-number">233</label><input type="checkbox" id="tufte-sn-233" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">233</span> Revisit Figure <a href="#fig:f9-gauss">163</a> and connect the observations made in both figures, i.e., which one in Figure <a href="#fig:f9-gauss">163</a> leads to the smoothest curve in Figure <a href="#fig:f9-3">164</a> and why?</span>.</p>
<p>Similarly, we can use the same simulation experiment to study the KNN smoother model. We build three KNN smoother models with <span class="math inline">\(k=3\)</span>, <span class="math inline">\(k=10\)</span>, and <span class="math inline">\(k=50\)</span>, respectively.</p>
<p></p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;FNN&quot;)</span></span>
<span id="cb196-2"><a href="#cb196-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(FNN)</span>
<span id="cb196-3"><a href="#cb196-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Loading required package: FNN</span></span>
<span id="cb196-4"><a href="#cb196-4" aria-hidden="true" tabindex="-1"></a>xy.knn3<span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train =</span> x, <span class="at">y =</span> y, <span class="at">k=</span><span class="dv">3</span>)</span>
<span id="cb196-5"><a href="#cb196-5" aria-hidden="true" tabindex="-1"></a>xy.knn10<span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train =</span> x, <span class="at">y =</span> y, <span class="at">k=</span><span class="dv">10</span>)</span>
<span id="cb196-6"><a href="#cb196-6" aria-hidden="true" tabindex="-1"></a>xy.knn50<span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train =</span> x, <span class="at">y =</span> y, <span class="at">k=</span><span class="dv">50</span>)</span></code></pre></div>
<p></p>
<p>Similar to Figure <a href="#fig:f9-3">164</a>, we use the following R code to draw Figure <a href="#fig:f9-2">165</a> that contains the true model, the sampled data points, and the three fitted models.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9-2"></span>
<p class="caption marginnote shownote">
Figure 165: KNN regression models with different choices on the number of nearest neighbors
</p>
<img src="graphics/9_2.png" alt="KNN regression models with different choices on the number of nearest neighbors" width="80%"  />
</div>
<p></p>
<p></p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data</span></span>
<span id="cb197-2"><a href="#cb197-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb197-3"><a href="#cb197-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, X <span class="sc">%*%</span> coef, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb197-4"><a href="#cb197-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, xy.knn3<span class="sc">$</span>pred, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb197-5"><a href="#cb197-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, xy.knn10<span class="sc">$</span>pred, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue4&quot;</span>)</span>
<span id="cb197-6"><a href="#cb197-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, xy.knn50<span class="sc">$</span>pred, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb197-7"><a href="#cb197-7" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb197-8"><a href="#cb197-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>, <span class="st">&quot;KNN (k = 3)&quot;</span>,</span>
<span id="cb197-9"><a href="#cb197-9" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;KNN (k = 10)&quot;</span>, <span class="st">&quot;KNN (k = 50)&quot;</span>), </span>
<span id="cb197-10"><a href="#cb197-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">4</span>),</span>
<span id="cb197-11"><a href="#cb197-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;dodgerblue4&quot;</span>,</span>
<span id="cb197-12"><a href="#cb197-12" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;forestgreen&quot;</span>), </span>
<span id="cb197-13"><a href="#cb197-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">text.width =</span> <span class="dv">32</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p>Comparing Figures <a href="#fig:f9-3">164</a> and <a href="#fig:f9-2">165</a>, it seems that the curve of the kernel regression model is generally <em>smoother</em> than the curve of a KNN model. This observation relates to the <em>discrete</em> manner the KNN model employs, while the kernel model uses smooth kernel functions that introduce smoothness and continuity into the definition of the neighbors of a data point (thus no hard thresholding is needed to classify whether or not a data point is a neighbor of another data point).</p>
<p>With a smaller <span class="math inline">\(k\)</span>, the fitted curve by the KNN smoother model is less smooth. This is because a KNN smoother model with a smaller <span class="math inline">\(k\)</span> predicts on a data point by relying on fewer data points in the training dataset, ignoring information provided by the other data points that are considered far away<label for="tufte-sn-234" class="margin-toggle sidenote-number">234</label><input type="checkbox" id="tufte-sn-234" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">234</span> What about a linear regression model? When it predicts on a given data point, does it use all the data points in the training data, or just a few local data points?</span>.</p>
<p>In terms of model complexity, the smaller the parameter <span class="math inline">\(k\)</span> in the KNN model, the larger the complexity of the model. Most beginners think of the opposite when they first encounter this question.</p>
</div>
</div>
<div id="conditional-variance-regression-model" class="section level2 unnumbered">
<h2>Conditional variance regression model</h2>
<div id="rationale-and-formulation-15" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p>Another common complication when applying linear regression model in real-world applications is that the variance of the response variable may also change. This phenomenon is called <strong>heteroscedasticity</strong> in regression analysis. This complication can be taken care of by a <em>conditional variance regression</em> model that allows the variance of the response variable to be a (usually implicit) function of the input variables. This leads to the following model</p>
<p><span class="math display" id="eq:9-cvr">\[\begin{equation}
    y=\boldsymbol{\beta}^T\boldsymbol{x}+\epsilon_{\boldsymbol{x}}, \epsilon_{\boldsymbol{x}} \sim N(0, \sigma^2_{\boldsymbol{x}}),
\tag{99}
\end{equation}\]</span></p>
<p>with <span class="math inline">\(\epsilon_{\boldsymbol{x}}\)</span> modeled as a normal distribution with <em>varying</em> variance as a function of <span class="math inline">\(\boldsymbol{x}\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-cvr-datamodel"></span>
<img src="graphics/9_cvr_datamodel.png" alt="The *data-generating mechanism* of a conditional variance regression model" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 166: The <em>data-generating mechanism</em> of a conditional variance regression model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-condsigma"></span>
<img src="graphics/9_condsigma.png" alt="$\sigma_{\boldsymbol{x}}$ is a function of $\boldsymbol{x}$" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 167: <span class="math inline">\(\sigma_{\boldsymbol{x}}\)</span> is a function of <span class="math inline">\(\boldsymbol{x}\)</span><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The conditional variance regression model differs from the regular linear regression model in terms of how it models <span class="math inline">\(\sigma_{\boldsymbol{x}}\)</span>, as illustrated in Figure <a href="#fig:f9-cvr-datamodel">166</a>. In other words, the linear regression model is a special type of the conditional variance regression model, with <span class="math inline">\(\sigma_{\boldsymbol{x}}\)</span> being a fixed constant as the horizontal line shown in Figure <a href="#fig:f9-condsigma">167</a>. The conditional variance regression model is a stacked model with one regression model on <span class="math inline">\(y\)</span> and another model on <span class="math inline">\(\sigma_{\boldsymbol{x}}\)</span>. The model stacking is a common strategy in statistics to handle multi-layered problems.</p>
</div>
<div id="theory-and-method-10" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>Given a dataset with <span class="math inline">\(N\)</span> data points and <span class="math inline">\(p\)</span> variables</p>
<p><span class="math display">\[ 
\boldsymbol{y}=\left[ \begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{N}}\end{array}\right], \boldsymbol{X}=\left[ \begin{array}{ccccc}{1} &amp; {x_{11}} &amp; {x_{21}} &amp; {\cdots} &amp; {x_{p 1}} \\ {1} &amp; {x_{12}} &amp; {x_{22}} &amp; {\cdots} &amp; {x_{p 2}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {1} &amp; {x_{1 N}} &amp; {x_{2 N}} &amp; {\cdots} &amp; {x_{p N}}\end{array}\right].
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol y \in R^{N \times 1}\)</span> denotes the <span class="math inline">\(N\)</span> measurements of the outcome variable, and <span class="math inline">\(\boldsymbol{X} \in R^{N \times(p+1)}\)</span> denotes the data matrix that includes the <span class="math inline">\(N\)</span> measurements of the <span class="math inline">\(p\)</span> input variables and one dummy variable that corresponds to the intercept term <span class="math inline">\(\beta_0\)</span>. The remaining issue is how to estimate the regression parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>. There are two situations: <span class="math inline">\(\sigma_{\boldsymbol{x}}^2\)</span> is known and <span class="math inline">\(\sigma_{\boldsymbol{x}}^2\)</span> is unknown.</p>
<p>The likelihood function is<label for="tufte-sn-235" class="margin-toggle sidenote-number">235</label><input type="checkbox" id="tufte-sn-235" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">235</span> Readers can use Eq. <a href="#eq:9-cvr">(99)</a> to derive this likelihood function.</span>
<span class="math display">\[-\frac{\pi}{2} \ln 2 \pi-\frac{1}{2} \sum_{n=1}^{N} \log \sigma_{\boldsymbol x_{n}}^{2}-\frac{1}{2} \sum_{n=1}^{N} \frac{\left(y_{n}-\boldsymbol{\beta}^{T} \boldsymbol{x}_{n}\right)^{2}}{\sigma_{\boldsymbol x_{n}}^{2}}.\]</span></p>
<p>As we have known <span class="math inline">\(\sigma_{\boldsymbol x}^2\)</span>, the parameters to be estimated only involve the last part of the likelihood function. Thus, we estimate the parameters that minimize:
<span class="math display">\[\frac{1}{2} \sum_{n=1}^{N} \frac{\left(y_{n}-\boldsymbol{\beta}^{T} \boldsymbol{x}_{n}\right)^{2}}{\sigma_{\boldsymbol x_{n}}^{2}}.\]</span>
This could be written in the matrix form as<label for="tufte-sn-236" class="margin-toggle sidenote-number">236</label><input type="checkbox" id="tufte-sn-236" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">236</span> <span class="math inline">\(\boldsymbol W\)</span> is a diagonal matrix with its diagonal elements as <span class="math inline">\(\boldsymbol W_{nn}=\frac{1}{\sigma_{\boldsymbol x_n}^2}\)</span>.</span></p>
<p><span class="math display" id="eq:9-condivar-gls">\[\begin{equation}
    \min_\beta (\boldsymbol{y}-\boldsymbol{X \beta})^T\boldsymbol{W}(\boldsymbol{y}-\boldsymbol{X\beta}).
\tag{100}
\end{equation}\]</span></p>
<p>This has the same structure as the <em>generalized least squares (GLS)</em> problem we have mentioned in <strong>Chapter 3</strong>. To solve this optimization problem, we take the gradient of the objective function in Eq. <a href="#eq:9-condivar-gls">(100)</a> and set it to be zero</p>
<p><span class="math display">\[\frac{\partial (\boldsymbol{y}-\boldsymbol{\mathrm{X} \beta})^T\boldsymbol{W}(\boldsymbol{y}-\boldsymbol{ X\beta})}{\partial \boldsymbol \beta}=0,\]</span></p>
<p>which gives rise to the equation</p>
<p><span class="math display">\[\boldsymbol{X}^T \boldsymbol W (\boldsymbol y - \boldsymbol{X} \boldsymbol \beta) = 0.\]</span></p>
<p>This leads to the GLS estimator of <span class="math inline">\(\boldsymbol \beta\)</span></p>
<p><span class="math display" id="eq:9-gls">\[\begin{equation}
    \hat{\boldsymbol{\beta}} = (\boldsymbol{X}^T\boldsymbol{WX} )^{-1}\boldsymbol{X}^T \boldsymbol{W} \boldsymbol y.
\tag{101}
\end{equation}\]</span></p>
<p>A more complicated situation, also a more realistic one, is that we don’t know <span class="math inline">\(\sigma_{\boldsymbol x}^2\)</span>. If we can estimate <span class="math inline">\(\sigma_{\boldsymbol x}^2\)</span>, we can reuse the procedure we have developed for the case when we have known <span class="math inline">\(\sigma_{\boldsymbol x}^2\)</span>.</p>
<p>As shown in Figure <a href="#fig:f9-condsigma">167</a>, <span class="math inline">\(\sigma_{\boldsymbol x}^2\)</span> is a function of <span class="math inline">\(\boldsymbol x\)</span>. In other words, it uses the input variables <span class="math inline">\(\boldsymbol x\)</span> to predict a new outcome variable, <span class="math inline">\(\sigma_{\boldsymbol x}^2\)</span>. Isn’t this a regression problem? The problem here is we don’t have the “measurements” of the outcome variable, i.e., the outcome variable <span class="math inline">\(\sigma_{\boldsymbol x}^2\)</span> is not directly measurable<label for="tufte-sn-237" class="margin-toggle sidenote-number">237</label><input type="checkbox" id="tufte-sn-237" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">237</span> Another example of a <strong>latent variable</strong>.</span>.</p>
<p>To overcome this problem, we estimate the <em>measurements</em> of the latent variable<label for="tufte-sn-238" class="margin-toggle sidenote-number">238</label><input type="checkbox" id="tufte-sn-238" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">238</span> I.e., just like what we did in the EM algorithm to estimate <span class="math inline">\(z_{nm}\)</span>. See <strong>Chapter 6</strong>.</span>, denoted as <span class="math inline">\(\hat{\sigma}_{\boldsymbol{x}_n}^2\)</span> for <span class="math inline">\(n=1, 2, \dots, N\)</span>. We propose the following steps:</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Initialize <span class="math inline">\(\hat{\sigma}_{\boldsymbol{x}_n}^2\)</span> for <span class="math inline">\(n=1, 2, \dots, N\)</span> by any reasonable approach (i.e., a trivial but popular approach, randomization).</p></li>
<li><p> Estimate <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> using the GLS estimator shown in Eq. <a href="#eq:9-gls">(101)</a>, and get <span class="math inline">\(\hat{y}_n=\hat{\boldsymbol \beta}^T \boldsymbol{x}_n\)</span> for <span class="math inline">\(n=1, 2, \dots, N\)</span>.</p></li>
<li><p> Derive the residuals <span class="math inline">\(\hat{\epsilon}_n = y_n-\hat{y}_n\)</span> for <span class="math inline">\(n=1, 2, \dots, N\)</span>.</p></li>
<li><p> Build a regression model, e.g., using the kernel regression model, to fit <span class="math inline">\(\hat \epsilon\)</span> using <span class="math inline">\(\boldsymbol x\)</span>.<label for="tufte-sn-239" class="margin-toggle sidenote-number">239</label><input type="checkbox" id="tufte-sn-239" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">239</span> I.e., the training dataset is <span class="math inline">\(\{\boldsymbol x_n, \hat{\epsilon}_n, n=1, 2, \dots, N\}\)</span>.</span></p></li>
<li><p> Predict <span class="math inline">\(\hat{\sigma}^2_{\boldsymbol x_n}\)</span> for <span class="math inline">\(n=1,2,\dots,N\)</span> using the fitted model in Step 4.</p></li>
<li><p> Repeat Steps 2 – 5 until convergence or satisfaction of a stopping criteria<label for="tufte-sn-240" class="margin-toggle sidenote-number">240</label><input type="checkbox" id="tufte-sn-240" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">240</span> E.g., fix the number of iterations, or set a threshold for changes in the parameter estimation.</span>.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>This approach of taking some variables as latent variables and further using statistical estimation/inference to fill in the unseen measurements has been useful in statistics and used in many models, such as the latent factor models, structural equation models, missing values imputation, EM algorithm, Gaussian mixture model, graphical models with latent variables, etc.</p>
</div>
<div id="r-lab-14" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>Simulation Experiment.</em> We simulate a dataset to see how well the proposed iterative procedure works for the parameter estimation when <span class="math inline">\(\sigma_{\boldsymbol x}^2\)</span> is unknown. The simulated data has one predictor and one outcome variable. The true model is</p>
<p><span class="math display">\[
  y = 1 + 0.5 x + \epsilon_x, \quad \sigma^2_x = 0.5 + 0.8 x^2.
\]</span></p>
<p>We stimulate <span class="math inline">\(100\)</span> data points from this model using the R script shown below.</p>
<p></p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditional variance function</span></span>
<span id="cb198-2"><a href="#cb198-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a regression model with heterogeneous variance</span></span>
<span id="cb198-3"><a href="#cb198-3" aria-hidden="true" tabindex="-1"></a>gen_data <span class="ot">&lt;-</span> <span class="cf">function</span>(n, coef) {</span>
<span id="cb198-4"><a href="#cb198-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">2</span>)</span>
<span id="cb198-5"><a href="#cb198-5" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="fu">sapply</span>(x,<span class="cf">function</span>(x){<span class="fl">0.5+0.8</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span>}))</span>
<span id="cb198-6"><a href="#cb198-6" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x)</span>
<span id="cb198-7"><a href="#cb198-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(X <span class="sc">%*%</span> coef <span class="sc">+</span> eps)</span>
<span id="cb198-8"><a href="#cb198-8" aria-hidden="true" tabindex="-1"></a><span class="fu">return</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb198-9"><a href="#cb198-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb198-10"><a href="#cb198-10" aria-hidden="true" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb198-11"><a href="#cb198-11" aria-hidden="true" tabindex="-1"></a>coef <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>)</span>
<span id="cb198-12"><a href="#cb198-12" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_train, coef)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-4"></span>
<img src="graphics/9_4.png" alt="Linear regression model to fit a heteroscedastic dataset" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 168: Linear regression model to fit a heteroscedastic dataset<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The simulated data points are shown in Figure <a href="#fig:f9-4">168</a>, together with the true model (the black line).</p>
<p>To initialize the iterative procedure of parameter estimation for the conditional variance regression model, we fit a regular linear regression model using the following R code.</p>
<p></p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the data using linear regression model (OLS)</span></span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> tempData[, <span class="st">&quot;x&quot;</span>]</span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> tempData[, <span class="st">&quot;y&quot;</span>]</span>
<span id="cb199-4"><a href="#cb199-4" aria-hidden="true" tabindex="-1"></a>fit.ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x,<span class="at">data=</span>tempData)</span>
<span id="cb199-5"><a href="#cb199-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data and the models</span></span>
<span id="cb199-6"><a href="#cb199-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> tempData<span class="sc">$</span>x</span>
<span id="cb199-7"><a href="#cb199-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb199-8"><a href="#cb199-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> tempData<span class="sc">$</span>y</span>
<span id="cb199-9"><a href="#cb199-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb199-10"><a href="#cb199-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the true model</span></span>
<span id="cb199-11"><a href="#cb199-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, X <span class="sc">%*%</span> coef, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb199-12"><a href="#cb199-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the linear regression model (OLS)</span></span>
<span id="cb199-13"><a href="#cb199-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit.ols), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb199-14"><a href="#cb199-14" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>,</span>
<span id="cb199-15"><a href="#cb199-15" aria-hidden="true" tabindex="-1"></a>                                 <span class="st">&quot;Linear model (OLS)&quot;</span>), </span>
<span id="cb199-16"><a href="#cb199-16" aria-hidden="true" tabindex="-1"></a><span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb199-17"><a href="#cb199-17" aria-hidden="true" tabindex="-1"></a>      <span class="at">text.width =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p>The fitted line is shown in Figure <a href="#fig:f9-4">168</a>, which has a significant deviation from the true regression model. We use the fitted line as a starting point, i.e., so that we can estimate the residuals<label for="tufte-sn-241" class="margin-toggle sidenote-number">241</label><input type="checkbox" id="tufte-sn-241" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">241</span>  Residuals <span class="math inline">\(\hat{\epsilon}_n = y_n-\hat{y}_n\)</span> for <span class="math inline">\(n=1, 2, \dots, N\)</span>.</span> based on the fitted linear regression model. The estimated residuals are plotted in Figure <a href="#fig:f9-5">169</a> as grey dots. A nonlinear regression model, the kernel regression model implemented by <code>npreg()</code>, is fitted on these residuals and shown in Figure <a href="#fig:f9-5">169</a> as the orange curve. The true function of the variance, i.e., <span class="math inline">\(\sigma^2_x = 0.5 + 0.8 x^2\)</span>, is also shown in Figure <a href="#fig:f9-5">169</a> as the black curve.</p>
<p>It can be seen that the residuals provide a good starting point for us to approximate the underlying true variance function. To reproduce Figure <a href="#fig:f9-5">169</a>, use the following R script.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-5"></span>
<img src="graphics/9_5.png" alt="Nonlinear regression model to fit the residuals (i.e., the grey dots)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 169: Nonlinear regression model to fit the residuals (i.e., the grey dots)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the residual estimated from the linear regression model (OLS)</span></span>
<span id="cb200-2"><a href="#cb200-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,<span class="fu">residuals</span>(fit.ols)<span class="sc">^</span><span class="dv">2</span>,<span class="at">ylab=</span><span class="st">&quot;squared residuals&quot;</span>,</span>
<span id="cb200-3"><a href="#cb200-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb200-4"><a href="#cb200-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the true model underlying the variance of the</span></span>
<span id="cb200-5"><a href="#cb200-5" aria-hidden="true" tabindex="-1"></a><span class="co"># error term</span></span>
<span id="cb200-6"><a href="#cb200-6" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>((<span class="dv">1</span><span class="fl">+0.8</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span>,<span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb200-7"><a href="#cb200-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a nonlinear regression model for residuals</span></span>
<span id="cb200-8"><a href="#cb200-8" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;np&quot;)</span></span>
<span id="cb200-9"><a href="#cb200-9" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(np)</span>
<span id="cb200-10"><a href="#cb200-10" aria-hidden="true" tabindex="-1"></a>var1 <span class="ot">&lt;-</span> <span class="fu">npreg</span>(<span class="fu">residuals</span>(fit.ols)<span class="sc">^</span><span class="dv">2</span> <span class="sc">~</span> x)</span>
<span id="cb200-11"><a href="#cb200-11" aria-hidden="true" tabindex="-1"></a>grid.x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="fu">min</span>(x),<span class="at">to=</span><span class="fu">max</span>(x),<span class="at">length.out=</span><span class="dv">300</span>)</span>
<span id="cb200-12"><a href="#cb200-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(grid.x,<span class="fu">predict</span>(var1,<span class="at">exdat=</span>grid.x), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb200-13"><a href="#cb200-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb200-14"><a href="#cb200-14" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb200-15"><a href="#cb200-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>,</span>
<span id="cb200-16"><a href="#cb200-16" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Fitted nonlinear model (1st iter)&quot;</span>), </span>
<span id="cb200-17"><a href="#cb200-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb200-18"><a href="#cb200-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="dv">5</span>, <span class="at">cex =</span> <span class="fl">1.2</span>)</span></code></pre></div>
<p></p>
<p>The orange curve shown in Figure <a href="#fig:f9-5">169</a> provides an approach to initialize the iterative procedure of parameter estimation for the conditional variance regression model: to estimate the <span class="math inline">\(\hat{\sigma}_{\boldsymbol{x}_n}^2\)</span> for <span class="math inline">\(n=1, 2, \dots, N\)</span> in Step 1.<label for="tufte-sn-242" class="margin-toggle sidenote-number">242</label><input type="checkbox" id="tufte-sn-242" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">242</span> In R, this is done by <code>fitted(var1)</code>.</span> Then, for Step 2, we fit a linear regression model according to Eq. <a href="#eq:9-gls">(101)</a>. This is done by the following R code.</p>
<p></p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a linear regression model (WLS) with the weights specified </span></span>
<span id="cb201-2"><a href="#cb201-2" aria-hidden="true" tabindex="-1"></a><span class="co"># by the fitted nonlinear model of the residuals</span></span>
<span id="cb201-3"><a href="#cb201-3" aria-hidden="true" tabindex="-1"></a>fit.wls <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x,<span class="at">weights=</span><span class="dv">1</span><span class="sc">/</span><span class="fu">fitted</span>(var1))</span>
<span id="cb201-4"><a href="#cb201-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb201-5"><a href="#cb201-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the true model</span></span>
<span id="cb201-6"><a href="#cb201-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, X <span class="sc">%*%</span> coef, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb201-7"><a href="#cb201-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the linear regression model (OLS)</span></span>
<span id="cb201-8"><a href="#cb201-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit.ols), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb201-9"><a href="#cb201-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the linear regression model (WLS) with estimated </span></span>
<span id="cb201-10"><a href="#cb201-10" aria-hidden="true" tabindex="-1"></a><span class="co"># variance function</span></span>
<span id="cb201-11"><a href="#cb201-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit.wls), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb201-12"><a href="#cb201-12" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>, </span>
<span id="cb201-13"><a href="#cb201-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>, <span class="st">&quot;Linear (OLS)&quot;</span>, </span>
<span id="cb201-14"><a href="#cb201-14" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Linear (WLS) + estimated variance&quot;</span>), </span>
<span id="cb201-15"><a href="#cb201-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">4</span>, <span class="dv">4</span>), </span>
<span id="cb201-16"><a href="#cb201-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;darkorange&quot;</span>,<span class="st">&quot;forestgreen&quot;</span>),</span>
<span id="cb201-17"><a href="#cb201-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="dv">5</span>, <span class="at">cex =</span> <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-6"></span>
<img src="graphics/9_6.png" alt="Fit the heteroscedastic dataset with two linear regression models using OLS and GLS (that accounts for the heteroscedastic effects with a nonlinear regression model to model the variance regression)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 170: Fit the heteroscedastic dataset with two linear regression models using OLS and GLS (that accounts for the heteroscedastic effects with a nonlinear regression model to model the variance regression)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The new regression model is added to Figure <a href="#fig:f9-4">168</a> as the green line, which generates Figure <a href="#fig:f9-6">170</a>. The new regression model is closer to the true model.</p>
<p>And we could continue this iterative procedure until a convergence criterion is met.</p>
<p><em>Real data.</em> Now let’s apply the conditional variance regression model on the AD dataset. Like what we did in the simulation experiment, we first fit a regular linear regression model, then, use the kernel regression model to fit the residuals, then obtain the estimates of the variances, then estimate the regression parameters using Eq. <a href="#eq:9-gls">(101)</a>. The R code is shown below. Results are shown in Figure <a href="#fig:f9-8">171</a>.</p>
<p></p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb202-3"><a href="#cb202-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb202-4"><a href="#cb202-4" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb202-5"><a href="#cb202-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-6"><a href="#cb202-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span>
<span id="cb202-7"><a href="#cb202-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the data using linear regression model (OLS)</span></span>
<span id="cb202-8"><a href="#cb202-8" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> AD<span class="sc">$</span>HippoNV</span>
<span id="cb202-9"><a href="#cb202-9" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> AD<span class="sc">$</span>MMSCORE</span>
<span id="cb202-10"><a href="#cb202-10" aria-hidden="true" tabindex="-1"></a>fit.ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x,<span class="at">data=</span>AD)</span>
<span id="cb202-11"><a href="#cb202-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a linear regression model (WLS) with the weights specified </span></span>
<span id="cb202-12"><a href="#cb202-12" aria-hidden="true" tabindex="-1"></a><span class="co"># by the fitted nonlinear model of the residuals</span></span>
<span id="cb202-13"><a href="#cb202-13" aria-hidden="true" tabindex="-1"></a>var1 <span class="ot">&lt;-</span> <span class="fu">npreg</span>(<span class="fu">residuals</span>(fit.ols)<span class="sc">^</span><span class="dv">2</span> <span class="sc">~</span> HippoNV, <span class="at">data =</span> AD)         </span>
<span id="cb202-14"><a href="#cb202-14" aria-hidden="true" tabindex="-1"></a>fit.wls <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x,<span class="at">weights=</span><span class="dv">1</span><span class="sc">/</span><span class="fu">fitted</span>(var1))</span>
<span id="cb202-15"><a href="#cb202-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb202-16"><a href="#cb202-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the linear regression model (OLS)</span></span>
<span id="cb202-17"><a href="#cb202-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit.ols), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb202-18"><a href="#cb202-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the linear regression model (WLS) with estimated variance</span></span>
<span id="cb202-19"><a href="#cb202-19" aria-hidden="true" tabindex="-1"></a><span class="co"># function</span></span>
<span id="cb202-20"><a href="#cb202-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit.wls), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb202-21"><a href="#cb202-21" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb202-22"><a href="#cb202-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Linear (OLS)&quot;</span>,</span>
<span id="cb202-23"><a href="#cb202-23" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Linear (WLS) + estimated variance&quot;</span>), </span>
<span id="cb202-24"><a href="#cb202-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>,<span class="st">&quot;forestgreen&quot;</span>),</span>
<span id="cb202-25"><a href="#cb202-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="fl">0.2</span>, <span class="at">cex =</span> <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-8"></span>
<img src="graphics/9_8.png" alt="Fit the AD dataset with two linear regression models using OLS and GLS (that accounts for the heteroscedastic effects with a nonlinear regression model to model the variance regression)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 171: Fit the AD dataset with two linear regression models using OLS and GLS (that accounts for the heteroscedastic effects with a nonlinear regression model to model the variance regression)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We also visualize the fitted variance functions in Figure <a href="#fig:f9-9">172</a> via the following R code.</p>
<p></p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the residual estimated from the linear regression </span></span>
<span id="cb203-2"><a href="#cb203-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model (OLS)</span></span>
<span id="cb203-3"><a href="#cb203-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,<span class="fu">residuals</span>(fit.ols)<span class="sc">^</span><span class="dv">2</span>,<span class="at">ylab=</span><span class="st">&quot;squared residuals&quot;</span>,</span>
<span id="cb203-4"><a href="#cb203-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb203-5"><a href="#cb203-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a nonlinear regression model for residuals</span></span>
<span id="cb203-6"><a href="#cb203-6" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;np&quot;)</span></span>
<span id="cb203-7"><a href="#cb203-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(np)</span>
<span id="cb203-8"><a href="#cb203-8" aria-hidden="true" tabindex="-1"></a>var2 <span class="ot">&lt;-</span> <span class="fu">npreg</span>(<span class="fu">residuals</span>(fit.wls)<span class="sc">^</span><span class="dv">2</span> <span class="sc">~</span> x)</span>
<span id="cb203-9"><a href="#cb203-9" aria-hidden="true" tabindex="-1"></a>grid.x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="fu">min</span>(x),<span class="at">to=</span><span class="fu">max</span>(x),<span class="at">length.out=</span><span class="dv">300</span>)</span>
<span id="cb203-10"><a href="#cb203-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(grid.x,<span class="fu">predict</span>(var1,<span class="at">exdat=</span>grid.x), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb203-11"><a href="#cb203-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb203-12"><a href="#cb203-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(grid.x,<span class="fu">predict</span>(var2,<span class="at">exdat=</span>grid.x), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb203-13"><a href="#cb203-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;forestgreen&quot;</span>)</span>
<span id="cb203-14"><a href="#cb203-14" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb203-15"><a href="#cb203-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Fitted nonlinear model (1st iter)&quot;</span>,</span>
<span id="cb203-16"><a href="#cb203-16" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;Fitted nonlinear model (2nd iter)&quot;</span>), </span>
<span id="cb203-17"><a href="#cb203-17" aria-hidden="true" tabindex="-1"></a>                <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb203-18"><a href="#cb203-18" aria-hidden="true" tabindex="-1"></a>                <span class="at">col =</span> <span class="fu">c</span>( <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;forestgreen&quot;</span>),</span>
<span id="cb203-19"><a href="#cb203-19" aria-hidden="true" tabindex="-1"></a>                <span class="at">text.width =</span> <span class="fl">0.25</span>, <span class="at">cex =</span> <span class="fl">1.2</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="#fig:f9-9">172</a> shows that in the data the heteroscedasticity is significant. Learning the variance function is helpful in this context. First, in terms of the statistical aspect, it improves the fitting of the regression line. Second, knowing the variance function itself is important knowledge in healthcare, e.g., variance often implies unpredictability or low quality in healthcare operations, pointing out root causes of quality problems or areas of improvement.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-9"></span>
<img src="graphics/9_9.png" alt="Nonlinear regression model to fit the residuals in the 2nd iteration for the AD data" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 172: Nonlinear regression model to fit the residuals in the 2nd iteration for the AD data<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="remarks-7" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="experiment" class="section level3 unnumbered">
<h3>Experiment</h3>
<p>The following R code conducts the experiment in Figure <a href="#fig:f9-1">161</a> (left).</p>
<p></p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Write a nice simulator to generate dataset with one</span></span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predictor and one outcome from a polynomial regression</span></span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb204-4"><a href="#cb204-4" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(splines)</span>
<span id="cb204-5"><a href="#cb204-5" aria-hidden="true" tabindex="-1"></a>seed <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>)</span>
<span id="cb204-6"><a href="#cb204-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(seed)</span>
<span id="cb204-7"><a href="#cb204-7" aria-hidden="true" tabindex="-1"></a>gen_data <span class="ot">&lt;-</span> <span class="cf">function</span>(n, coef, v_noise) {</span>
<span id="cb204-8"><a href="#cb204-8" aria-hidden="true" tabindex="-1"></a>  eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, v_noise)</span>
<span id="cb204-9"><a href="#cb204-9" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">100</span>))</span>
<span id="cb204-10"><a href="#cb204-10" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,<span class="fu">ns</span>(x, <span class="at">df =</span> (<span class="fu">length</span>(coef) <span class="sc">-</span> <span class="dv">1</span>)))</span>
<span id="cb204-11"><a href="#cb204-11" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(X <span class="sc">%*%</span> coef <span class="sc">+</span> eps)</span>
<span id="cb204-12"><a href="#cb204-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y))</span>
<span id="cb204-13"><a href="#cb204-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb204-14"><a href="#cb204-14" aria-hidden="true" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb204-15"><a href="#cb204-15" aria-hidden="true" tabindex="-1"></a>coef <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>)</span>
<span id="cb204-16"><a href="#cb204-16" aria-hidden="true" tabindex="-1"></a>v_noise <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb204-17"><a href="#cb204-17" aria-hidden="true" tabindex="-1"></a>tempData <span class="ot">&lt;-</span> <span class="fu">gen_data</span>(n_train, coef, v_noise)</span>
<span id="cb204-18"><a href="#cb204-18" aria-hidden="true" tabindex="-1"></a>tempData[<span class="dv">31</span>,] <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">200</span>,<span class="dv">200</span>)</span>
<span id="cb204-19"><a href="#cb204-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the data using linear regression model</span></span>
<span id="cb204-20"><a href="#cb204-20" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> tempData[, <span class="st">&quot;x&quot;</span>]</span>
<span id="cb204-21"><a href="#cb204-21" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> tempData[, <span class="st">&quot;y&quot;</span>]</span>
<span id="cb204-22"><a href="#cb204-22" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x,<span class="at">data=</span>tempData)</span>
<span id="cb204-23"><a href="#cb204-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data</span></span>
<span id="cb204-24"><a href="#cb204-24" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> tempData<span class="sc">$</span>x</span>
<span id="cb204-25"><a href="#cb204-25" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb204-26"><a href="#cb204-26" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> tempData<span class="sc">$</span>y</span>
<span id="cb204-27"><a href="#cb204-27" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb204-28"><a href="#cb204-28" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, X <span class="sc">%*%</span> coef, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb204-29"><a href="#cb204-29" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">fitted</span>(fit), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb204-30"><a href="#cb204-30" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topleft&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;True function&quot;</span>,</span>
<span id="cb204-31"><a href="#cb204-31" aria-hidden="true" tabindex="-1"></a>       <span class="st">&quot;Fitted linear model&quot;</span>), <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb204-32"><a href="#cb204-32" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb204-33"><a href="#cb204-33" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="dv">100</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p></p>
</div>
<div id="linear-regression-as-a-kernel-regression-model" class="section level3 unnumbered">
<h3>Linear regression as a kernel regression model</h3>
<p>Let’s consider a simple linear regression problem that has one predictor, <span class="math inline">\(x\)</span>, and no intercept</p>
<p><span class="math display">\[y=\beta x + \epsilon.\]</span></p>
<p>Given a dataset with <span class="math inline">\(N\)</span> samples, i.e., <span class="math inline">\(\{x_n, y_n, n = 1, 2, \ldots, N. \}\)</span>, the least squares estimator of <span class="math inline">\(\beta\)</span> is</p>
<p><span class="math display">\[\hat{\beta}=\frac{\left(\sum_{i=1}^{N} x_{n} y_{n}\right)}{\sum_{n=1}^{N} x_{n}^{2}}.\]</span></p>
<p>Now comes a new data point, <span class="math inline">\(x^*\)</span>. To derive the prediction <span class="math inline">\(y^*\)</span>,</p>
<p><span class="math display">\[y^{*} = \hat{\beta}x^{*} =x^{*} \frac{\left(\sum_{n=1}^{N} x_{n} y_{n}\right)}{\sum_{n=1}^{N} x_{n}^{2}}.\]</span></p>
<p>This could be further reformed as</p>
<p><span class="math display">\[y^{*}=\sum_{n=1}^{N} y_{n} \frac{x_{n}x^{*}}{\sum_{n=1}^{N} x_{n}^{2}}.\]</span></p>
<p>This fits the form of the kernel regression as defined in Eq. <a href="#eq:9-kr">(98)</a>.<label for="tufte-sn-243" class="margin-toggle sidenote-number">243</label><input type="checkbox" id="tufte-sn-243" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">243</span> I.e., <span class="math inline">\(w(x_{n},x^{*}) = \sum_{n=1}^{N} \frac{x_{n}x^{*}}{\sum_{n=1}^{N} x_{n}^{2}}.\)</span></span></p>
<!-- % Now if we look closely at this formula, we can draw interesting observations how linear regression model works in prediction on a new location using its knowledge on other locations (e.g., the historical data points $(x_i,y_i )$ for $i=1,2,\dots,n$). It first evaluates the similarity between the new location with each of the knowing locations, as reflected in $\frac{x_i x^*}{nS_x^2}$, where $x_i x^*$ calculates the similarity and $nS_x^2$ is a normalization factor. Then, the prediction $y^*$ is a weighted sum of $y_i$ for $i=1,2,\dots,n$ while the weight of $y_i$ is proportional to the similarity between $x_i$ and $x^*$. From this perspective, we see linear regression model as a very empirical prediction model that bears the same idea with those lazy learning methods such as k-nearest-neighbor regression model or local regression models. The difference here, in the linear regression model, is that a special similarity measure (i.e., $\frac{x_i x^*}{nS_x^2}$) is used, that means the weight of a data point depends on how far it is from the center of the data, not how far it is from the point at which we are trying to predict. Thus, for this similarity measure to work we need to hope that the underlying model is globally linear. -->
</div>
<div id="more-about-heteroscedasticity" class="section level3 unnumbered">
<h3>More about heteroscedasticity</h3>
<p>For regression problems, the interest is usually in the modeling of the relationship between the <em>mean</em><label for="tufte-sn-244" class="margin-toggle sidenote-number">244</label><input type="checkbox" id="tufte-sn-244" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">244</span> See sidenote 11 and Figure <a href="#fig:f2-lrpred">5</a>.</span> of the outcome variable with the input variables. Thus, when there is heteroscedasticity in the data, a nonparametric regression method is recommended to estimate the latent variance, more from a curve-fitting perspective which is to smooth and estimate, rather than a modeling perspective which is to study the relationship between the outcome variable with input variables. But, of course, we can still study how the input variables affect the variance of the response variable explicitly. Specifically, we can use a linear regression model to link the variance of <span class="math inline">\(y\)</span> with the input variables. The iterative procedure developed for the case when <span class="math inline">\(\sigma_{\boldsymbol{x}}^2\)</span> is unknown is still applicable here for parameter estimation.</p>
</div>
</div>
<div id="exercises-7" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-hw-kr">Table 51: </span>Dataset for building a kernel regression model</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-0.32\)</span></td>
<td align="left"><span class="math inline">\(0.66\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-0.1\)</span></td>
<td align="left"><span class="math inline">\(0.82\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0.74\)</span></td>
<td align="left"><span class="math inline">\(-0.37\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(1.21\)</span></td>
<td align="left"><span class="math inline">\(-0.8\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(0.44\)</span></td>
<td align="left"><span class="math inline">\(0.52\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(-0.68\)</span></td>
<td align="left"><span class="math inline">\(0.97\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li> Manually build a kernel regression model with Gaussian kernel with bandwidth parameter <span class="math inline">\(\gamma=1\)</span> using the data shown in Table <a href="#tab:t9-hw-kr">51</a>, and predict on the data points shown in Table <a href="#tab:t9-hw-kr2">52</a>.</li>
</ul>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t9-hw-kr2">Table 52: </span>Testing dataset for the kernel regression model</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li><p> Follow up on the dataset in Q1. Manually build a KNN regression model with <span class="math inline">\(K = 2\)</span>. Predict on the testing data in Table <a href="#tab:t9-hw-kr2">52</a>.</p></li>
<li><p> Follow up on the dataset in Q1. Use the R pipeline for KNN regression on this data. Compare the result from R and the result by your manual calculation.</p></li>
<li><p> Follow up on the dataset in Q1. Use the <code>gausskernel()</code> function from the R package <code>KRLS</code> to calculate the similarity between the data points (including the <span class="math inline">\(6\)</span> training data points and the <span class="math inline">\(3\)</span> testing data points in Tables <a href="#tab:t9-hw-kr">51</a> and <a href="#tab:t9-hw-kr2">52</a>).</p></li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-sampledcurve"></span>
<img src="graphics/9_sampledcurve.png" alt="The true model and its sampled data points" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 173: The true model and its sampled data points<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<ul>
<li> Use the <code>BostonHousing</code> dataset from the R package <code>mlbench</code>, select the variable <code>medv</code> as the outcome, and use other numeric variables as the predictors. Run the R pipeline for KNN regression on it. Use cross-validation to select the best number of nearest neighbor, and summarize your findings.</li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f9-linetobesampled"></span>
<img src="graphics/9_linetobesampled.png" alt="The true model and its sampled data points" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 174: The true model and its sampled data points<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<ul>
<li><p> Use the <code>BostonHousing</code> dataset from the R package <code>mlbench</code> and select the variable <code>lstat</code> as the predictor and <code>medv</code> as the outcome, and run the R pipeline for kernel regression on it. Try the Gaussian kernel function with its bandwidth parameter taking values as <span class="math inline">\(5, 10, 30, 100\)</span>.</p></li>
<li><p> Figure <a href="#fig:f9-sampledcurve">173</a> shows a nonlinear model (i.e., the curve) and its sampled points. Suppose that the curve is unknown to us, and our task is to build a KNN regression model with <span class="math inline">\(K=2\)</span> based on the samples. Draw the fitted curve of this KNN regression model.</p></li>
<li><p> Suppose that the underlying model is a linear model, as shown in Figure <a href="#fig:f9-linetobesampled">174</a>. To use KNN model to approximate the underlying model, we need samples. Suppose that we could afford sampling <span class="math inline">\(8\)</span> data points. Which locations would you like to acquire samples in order to achieve best approximation of the underlying model using your later fitted KNN model?</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<div id="chapter-10.-synthesis-architecture-pipeline" class="section level1 unnumbered">
<h1>Chapter 10. Synthesis: Architecture &amp; Pipeline</h1>
<div id="overview-8" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 10 is about <em>synthesis</em>. Synthesis is not a rigorous term but refers to a type of common pratice that integrates, consolidates, or streamlines many otherwise stand-alone models into a mega-model or pipeline. Two styles of synthesis will be introduced here, one represented by deep learning, and another takes the form as pipelines.</p>
</div>
<div id="deep-learning" class="section level2 unnumbered">
<h2>Deep learning</h2>
<p>To know more about “deep learning,” we need to start with its name. The word “deep” is ambiguous but expressive, undetermined but significant. This inviting gesture may have a dazzling effect, but it is based on a specific reason: a deep neural network model is truly deep in terms of its architecture—from input variables to output variables there are many layers in between. Other than that, it is not different from other models in this book. The basic framework of learning as shown in Figure <a href="#fig:f2-1">2</a> and Eq. <a href="#eq:ch2-genericmodel">(1)</a> in <strong>Chapter 2</strong> still holds true for deep learning.</p>
<p>The word “deep” doesn’t imply that other models we have learned so far are not deep. Many models have been studied in great depth, such as the linear models<label for="tufte-sn-245" class="margin-toggle sidenote-number">245</label><input type="checkbox" id="tufte-sn-245" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">245</span> Anderson, T. W., <em>An Introduction to Multivariate Statistical Analysis</em>, Wiley, 3rd edition, 2003.</span> and the learning theory developed for the support vector machine<label for="tufte-sn-246" class="margin-toggle sidenote-number">246</label><input type="checkbox" id="tufte-sn-246" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">246</span> Vapnik, V., <em>The Nature of Statistical Learning Theory</em>, Springer, 2000.</span>. In this chapter, we will refer to deep learning, specifically to those neural network (NN) models that have many hidden layers, because for NN models we could take the word “deep” at face value—if a model looks deep, it is a deep model. This superficiality, however, builds on a solid foundation<label for="tufte-sn-247" class="margin-toggle sidenote-number">247</label><input type="checkbox" id="tufte-sn-247" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">247</span> E.g., the Universal approximation theorem; please refer to Hornik, K., Approximation Capabilities of Multilayer Feedforward Networks, <em>Neural Networks</em>, Volume 4, Issue 2, Pages 251-257, 1991.</span>: a neural network with a more complex architecture means a more complex form for <span class="math inline">\(f(x)\)</span> in Eq. <a href="#eq:ch2-genericmodel">(1)</a>. In other words, this is an attractive proposal, since it suggests we can easily build up depth and capacity of the model by merely increasing its visual complexity. And there have been tools that allow users to drag ready-made modules and piece them together to create the architecture of the NN model they’d like to build, and automatically translate the architecture into its mathematical form and carry out the computational tasks for model training and prediction<label for="tufte-sn-248" class="margin-toggle sidenote-number">248</label><input type="checkbox" id="tufte-sn-248" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">248</span> E.g., TensorFlow <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>.</span>.</p>
<div id="rationale-and-formulation-16" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><em>An architecture means a function.</em> We have mentioned in <strong>Chapter 2</strong> that the data modeling methods seek explicit forms of <span class="math inline">\(f(x)\)</span> in Eq. <a href="#eq:ch2-genericmodel">(1)</a>, while algorithmic modeling methods seek implicit forms. Deep models bend the two. It is like an algorithmic modeling method that you don’t need to write up the specific form of <span class="math inline">\(f(x)\)</span>, while on the other hand, in theory you could write up <span class="math inline">\(f(x)\)</span> after you have had the architecture<label for="tufte-sn-249" class="margin-toggle sidenote-number">249</label><input type="checkbox" id="tufte-sn-249" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">249</span> In this sense, it is also like the kernel trick used in the SVM model. Remember that in <strong>Chapter 7</strong> we have seen that by using the kernel function in SVM, an implicit transformation of the variables is achieved, and we usually do not know what is the explicit form of <span class="math inline">\(\phi(x)\)</span> the SVM model encodes, but in theory there is such a form of <span class="math inline">\(\phi(x)\)</span>.</span>.</p>
<p>The architecture of a NN model could be quite expressive, i.e., Figure <a href="#fig:f10-nn-architecture">175</a> shows an architecture of a neural network model with one layer that is flexible enough to include existing models such as the linear regression model, logistic regression model, and SVM, as shown in Table <a href="#tab:t10-NNexample">53</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-nn-architecture"></span>
<p class="caption marginnote shownote">
Figure 175: Architecture of a simple neural network model. The figure is drawn using Alex LeNail’s online tool: <a href="http://alexlenail.me/NN-SVG/index.html">http://alexlenail.me/NN-SVG/index.html</a>.
</p>
<img src="graphics/10_nn_architecture.png" alt="Architecture of a simple neural network model. The figure is drawn using Alex LeNail's online tool: [http://alexlenail.me/NN-SVG/index.html](http://alexlenail.me/NN-SVG/index.html)." width="80%"  />
</div>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-NNexample">Table 53: </span>Expression of some models using the architecture of a one-layer neural network in Figure <a href="#fig:f10-nn-architecture">175</a></span><!--</caption>--></p>
<table>
<colgroup>
<col width="17%" />
<col width="28%" />
<col width="54%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Model</strong></th>
<th align="left"><strong>Activation Function <span class="math inline">\(\Phi\)</span></strong></th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear regression</td>
<td align="left">Linear: <span class="math inline">\(\Phi(z)=z\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\left(y-\sum_{i=1}^{p} w_{i} x_{i}\right)^2\)</span></td>
</tr>
<tr class="even">
<td align="left">Logistic regression</td>
<td align="left">Sigmoid: <span class="math inline">\(\Phi(z)=\frac{1}{1-e^{-z}}\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\log(1+\exp[-y\sum_{i=1}^{p} w_{i} x_{i}])\)</span></td>
</tr>
<tr class="odd">
<td align="left">Support vector machine</td>
<td align="left">Null: <span class="math inline">\(\Phi(z)=z\)</span></td>
<td align="left"><span class="math inline">\(\mathcal{L}(\boldsymbol{w})=\max(0,1-y\sum_{i=1}^{p} w_{i} x_{i})\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>The NN structure shown in Figure <a href="#fig:f10-nn-architecture">175</a> is a basic form of NN architecture that is called the <strong>perceptron</strong>. As a basic form, it is a module that could be repeatedly used in different kinds of composition, e.g., in parallel, concatenation, or in a sequence. The basic forms are also called architectural primitives or foundational building blocks. Most deep architectures are built by combining these architectural primitives. Figures <a href="#fig:f10-nn-composition">176</a> and <a href="#fig:f10-nn-composition2">177</a> show two examples. There have been many of those basic forms developed. Softwares such as TensorFlow build on this concept by allowing users to use graphic user interface (GUI) to compose the architecture of their deep networks using these building blocks<label for="tufte-sn-250" class="margin-toggle sidenote-number">250</label><input type="checkbox" id="tufte-sn-250" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">250</span> For introduction of TensorFlow, readers may check out this book: Ramsundar, B. and Zadeh, R. <em>TensorFlow for Deep Learning: from Linear Regression to Reinforcement Learning</em>, O’Reilly Media, 2017.</span>.</p>
<p>As we have mentioned, for NN models there are theories showing that if a model looks deep, it is a deep model. The universal approximation theorem has shown that a NN model with one hidden layer could characterize all smooth functions. While there is no guarantee that in practice adding more layers will always be better, the theoretical results did imply that is the right direction.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-nn-composition"></span>
<img src="graphics/10_nn_composition.png" alt="Build more complicated NN models with a basic form" width="80%"  />
<p class="caption marginnote shownote">
Figure 176: Build more complicated NN models with a basic form
</p>
</div>
<p></p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-nn-composition2"></span>
<img src="graphics/10_nn_composition2.png" alt="Build deeper NN models with basic forms and activation functions" width="80%"  />
<p class="caption marginnote shownote">
Figure 177: Build deeper NN models with basic forms and activation functions
</p>
</div>
<p></p>
<p>Recall the XOR problem introduced in <strong>Chapter 7</strong> as shown in Figure <a href="#fig:f7-8">121</a>. With a slight modification of the problem to facilitate the presentation here, the dataset has <span class="math inline">\(4\)</span> data points</p>
<p><span class="math display">\[ 
\begin{array}{l}{\boldsymbol{x}_{1}=(0,0), y_{1}=0}; \\ {\boldsymbol{x}_{2}=(0,1), y_{2}=1}; \\ {\boldsymbol{x}_{3}=(1,0), y_{3}=1} ;\\ {\boldsymbol{x}_{4}=(1,1), y_{4}=0.}\end{array}
\]</span></p>
<p>This is a typical nonlinear problem. A NN model with one hidden layer as shown in Figure <a href="#fig:f10-xor-nn">178</a> could solve this problem.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-xor-nn"></span>
<img src="graphics/10_xor_nn.png" alt="Architecture of a neural network with a hidden layer" width="80%"  />
<p class="caption marginnote shownote">
Figure 178: Architecture of a neural network with a hidden layer
</p>
</div>
<p></p>
<p>For instance, for <span class="math inline">\(\boldsymbol{x}_{1}=(0,0)\)</span>, from the input layer to the first node (i.e., the upper one) in the hidden layer, we have</p>
<p><span class="math display">\[
0 \times 1  + 0 \times 1 + 1 \times 0 = 0.
\]</span></p>
<p>The value <span class="math inline">\(0\)</span> provides the input for the activation function at the hidden node, and we have <span class="math inline">\(\Phi(0) = \max (0,0) = 0\)</span>.</p>
<p>From the input layer to the second node (i.e., the lower one) in the hidden layer, we have</p>
<p><span class="math display">\[
0 \times 1  + 0 \times 1 + 1 \times -1 = -1.
\]</span></p>
<p>The value <span class="math inline">\(-1\)</span> provides the input for the activation function at the hidden node, and we have <span class="math inline">\(\Phi(-1) = \max (0,-1) = 0\)</span>.</p>
<p>Then, from the hidden layer to the output layer, we have</p>
<p><span class="math display">\[
1 \times 0 - 2 \times 0 = 0.
\]</span></p>
<p>Using the activation function at the output layer, <span class="math inline">\(\Phi(z) = z\)</span>, the final prediction correctly predicts</p>
<p><span class="math display">\[
y = 0.
\]</span></p>
<p>We can follow the same process and see that the two-layer NN as shown in Figure <a href="#fig:f10-xor-nn">178</a> could solve the XOR problem.</p>
<p><em>How to read a deep net.</em> Roughly speaking, there are three major efforts in developing deep learning models: to create basic forms, to design architectural principles or composition rules, and to design learning algorithms that can robustly and efficiently learn the parameters of the deep model using data<label for="tufte-sn-251" class="margin-toggle sidenote-number">251</label><input type="checkbox" id="tufte-sn-251" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">251</span> A deep NN model has massive parameters, so learning these parameters from data had been a challenge in the past. Some contributed the recent revitalization of deep learning—as the neural network model had its “rise and fall” in the past decades—to a range of optimization tricks such as pretraining and dropout, the growth of computing power, and the availability of Big Data, all enabled the data-driven learning of a giant collection of parameters of a deep NN model.</span>. Practical application of deep models is to make the network deeper by stacking these basic forms following some composition rules. From this perspective, it is not a surprise to see why it was quoted, “For reason in this sense is nothing but reckoning, that is adding and subtracting …”<label for="tufte-sn-252" class="margin-toggle sidenote-number">252</label><input type="checkbox" id="tufte-sn-252" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">252</span> Hobbes, T., Leviathan. 1651.</span>, to explain the logic of designing neural networks in Raul Rojas’s book<label for="tufte-sn-253" class="margin-toggle sidenote-number">253</label><input type="checkbox" id="tufte-sn-253" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">253</span> Rojas. R., <em>Neural Networks: a Systematic Introduction</em>. Springer, 1996.</span>.</p>
<p>We can take a look at the convolutional neural networks (CNN) as an example. The CNN is one popular deep NN model and is often used for learning from image data. Its architecture consists of a few basic forms and composition rules that are particularly developed for images.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f10-cnn-image"></span>
<img src="graphics/10_cnn_structure.png" alt="Architecture of a CNN model" width="80%"  />
<p class="caption marginnote shownote">
Figure 179: Architecture of a CNN model
</p>
</div>
<p></p>
<p>The CNN architecture shown in Figure <a href="#fig:f10-cnn-image">179</a> has two parts. The first part (i.e., everything before the last <span class="math inline">\(3\)</span> layers) is to translate the image data into vectorized form and provides the input for the second part (i.e., the last <span class="math inline">\(3\)</span> layers) that is a NN as we have discussed earlier. One basic form of CNN is the convolutional layer. The basic purpose of a convolutional layer is to transform the image into a feature map, as shown in Figure <a href="#fig:f10-conv-op">180</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-conv-op"></span>
<p class="caption marginnote shownote">
Figure 180: A convolutional layer aggregates spatially correlated information as a feature extraction process
</p>
<img src="graphics/10_conv_op.png" alt="A convolutional layer aggregates spatially correlated information as a feature extraction process" width="80%"  />
</div>
<p></p>
<p>Suppose that <span class="math inline">\(w_1=1\)</span>, <span class="math inline">\(w_2=2\)</span>, <span class="math inline">\(w_3=2\)</span>, <span class="math inline">\(w_4=1\)</span> in Figure <a href="#fig:f10-conv-op">180</a>; Figure <a href="#fig:f10-conv-layer">181</a> further shows the computational details of how the convolutional layer works.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-conv-layer"></span>
<p class="caption marginnote shownote">
Figure 181: How the convolutional layer works.
</p>
<img src="graphics/10_conv_layer.png" alt="How the convolutional layer works." width="80%"  />
</div>
<p></p>
<p>The convolutional layer is good at exploiting the spatial structure<label for="tufte-sn-254" class="margin-toggle sidenote-number">254</label><input type="checkbox" id="tufte-sn-254" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">254</span> I.e., if the entities that are close to each other are semantically related, it is a spatial structure.</span> in its input data. Because of this, CNN is particularly useful for learning from image data, since for images the pixels close to one another are usually semantically related.</p>
<p>The max pooling layer is another basic form of CNN. Figure <a href="#fig:f10-max-pool">182</a> shows how it works. The max pooling looks too simple an idea, but it works remarkably well. The real mystery when we look at a “simple” idea like this is why it was the max pooling that stood out among many other “simple” ideas. But there has been no conclusive theory to explain it<label for="tufte-sn-255" class="margin-toggle sidenote-number">255</label><input type="checkbox" id="tufte-sn-255" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">255</span> To quote Andrew Ng in his online course for convolutional neural networks (<a href="https://www.coursera.org/learn/convolutional-neural-networks">https://www.coursera.org/learn/convolutional-neural-networks</a>): <em>“… the main reason people use max pooling is because it’s been found in a lot of experiments to work well … I don’t know of anyone who fully knows if that is the real underlying reason.”</em></span>.</p>
<p>But one can compare the max pooling with the convolutional layer. One difference is that the parameters of a convolutional layer is learned from data, making it an adaptive and flexible form to a particular problem. The max pooling, however, is a fixed nonlinear transformation without parameters to learn. In other words, it has no computational cost. No wonder it is believed that one main function of the max pooling is to reduce the number of parameters of the deep NN model and to alleviate the computational cost. This would relieve some computational burden since a deep NN model has a massive number of parameters to be learned from data. Another aspect we should think of is that max pooling is good for image data. It may help increase the robustness of the model against translation invariance, i.e., to recognize an object, say, a cat, in an image, we need the algorithm to be resilient to the potential variation on angle or distance or any other factors that cause scale issues. Max pooling only keeps the “max” and discards the rest.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-max-pool"></span>
<p class="caption marginnote shownote">
Figure 182: How the max pooling layer works
</p>
<img src="graphics/10_max_pool.png" alt="How the max pooling layer works" width="80%"  />
</div>
<p></p>
<p>One can add as many convolutional layers or max pooling layers as needed when designing a CNN model, and the convolutional layers and the max pooling layer could be alternatively arranged as a pipeline to extract features from the image data, e.g., in Figure <a href="#fig:f10-cnn-image">179</a>, there are <span class="math inline">\(2\)</span> convolutional layers and <span class="math inline">\(1\)</span> max pooling layer. It has been found in many cases that for the CNN to be successful, it needs to be made quite deep. For this reason, some consider the deep NN models a different species from NN models.</p>
</div>
<div id="r-lab-15" class="section level3 unnumbered">
<h3>R Lab</h3>
<p><em>The 6-Step R Pipeline for NN.</em> <strong>Step 1</strong> and <strong>Step 2</strong> get the dataset into R and organize it in required format.</p>
<p></p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read data into R workstation</span></span>
<span id="cb205-2"><a href="#cb205-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-3"><a href="#cb205-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb205-4"><a href="#cb205-4" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb205-5"><a href="#cb205-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/KR.csv&quot;</span>)</span>
<span id="cb205-6"><a href="#cb205-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb205-7"><a href="#cb205-7" aria-hidden="true" tabindex="-1"></a><span class="co"># str(data)</span></span>
<span id="cb205-8"><a href="#cb205-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-9"><a href="#cb205-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb205-10"><a href="#cb205-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X matrix (predictors) and Y vector (outcome variable)</span></span>
<span id="cb205-11"><a href="#cb205-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data<span class="sc">$</span>x</span>
<span id="cb205-12"><a href="#cb205-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> data<span class="sc">$</span>y</span>
<span id="cb205-13"><a href="#cb205-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-14"><a href="#cb205-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a training data </span></span>
<span id="cb205-15"><a href="#cb205-15" aria-hidden="true" tabindex="-1"></a>train.ix <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data),<span class="fu">floor</span>( <span class="fu">nrow</span>(data) <span class="sc">*</span> <span class="dv">4</span><span class="sc">/</span><span class="dv">5</span>) )</span>
<span id="cb205-16"><a href="#cb205-16" aria-hidden="true" tabindex="-1"></a>data.train <span class="ot">&lt;-</span> data[train.ix,]</span>
<span id="cb205-17"><a href="#cb205-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a testing data </span></span>
<span id="cb205-18"><a href="#cb205-18" aria-hidden="true" tabindex="-1"></a>data.test <span class="ot">&lt;-</span> data[<span class="sc">-</span>train.ix,]</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates a list of models. For a NN model, important decisions are made on the design of the architecture, e.g., how many hidden layers and how many nodes in each hidden layer. For example, here, we create three NN models, all have one hidden layer but a different number of hidden nodes.</p>
<p></p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="#cb206-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb206-2"><a href="#cb206-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb206-3"><a href="#cb206-3" aria-hidden="true" tabindex="-1"></a><span class="co"># NN model with one hidden layer and different # of nodes</span></span>
<span id="cb206-4"><a href="#cb206-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb206-5"><a href="#cb206-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model1: neuralnet(y~x, data=data, hidden=c(3)) </span></span>
<span id="cb206-6"><a href="#cb206-6" aria-hidden="true" tabindex="-1"></a><span class="co"># model2: neuralnet(y~x, data=data, hidden=c(5)) </span></span>
<span id="cb206-7"><a href="#cb206-7" aria-hidden="true" tabindex="-1"></a><span class="co"># model3: neuralnet(y~x, data=data, hidden=c(8)) </span></span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses cross-validation to evaluate the candidate models to identify the best model.</p>
<p></p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; cross-validation for model evaluation </span></span>
<span id="cb207-2"><a href="#cb207-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-3"><a href="#cb207-3" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">=</span> <span class="dv">10</span> <span class="co"># number of folds</span></span>
<span id="cb207-4"><a href="#cb207-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the sample size, N, of the dataset</span></span>
<span id="cb207-5"><a href="#cb207-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(data.train)[<span class="dv">1</span>] </span>
<span id="cb207-6"><a href="#cb207-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-7"><a href="#cb207-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-8"><a href="#cb207-8" aria-hidden="true" tabindex="-1"></a>folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb207-9"><a href="#cb207-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(neuralnet)</span>
<span id="cb207-10"><a href="#cb207-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-11"><a href="#cb207-11" aria-hidden="true" tabindex="-1"></a><span class="co"># cv_mse records the prediction error for each fold</span></span>
<span id="cb207-12"><a href="#cb207-12" aria-hidden="true" tabindex="-1"></a>cv_mse <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb207-13"><a href="#cb207-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb207-14"><a href="#cb207-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># In each iteration of the n_folds iterations</span></span>
<span id="cb207-15"><a href="#cb207-15" aria-hidden="true" tabindex="-1"></a>  test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k) </span>
<span id="cb207-16"><a href="#cb207-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is the testing data, from the ith fold</span></span>
<span id="cb207-17"><a href="#cb207-17" aria-hidden="true" tabindex="-1"></a>  data.test.cv <span class="ot">&lt;-</span> data.train[test_i, ]  </span>
<span id="cb207-18"><a href="#cb207-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Then, the remaining data form the training data</span></span>
<span id="cb207-19"><a href="#cb207-19" aria-hidden="true" tabindex="-1"></a>  data.train.cv <span class="ot">&lt;-</span> data.train[<span class="sc">-</span>test_i, ] </span>
<span id="cb207-20"><a href="#cb207-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the neural network model with one hidden layer of 3</span></span>
<span id="cb207-21"><a href="#cb207-21" aria-hidden="true" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(y<span class="sc">~</span>x, <span class="at">data=</span>data, <span class="at">hidden=</span><span class="fu">c</span>(<span class="dv">3</span>)) </span>
<span id="cb207-22"><a href="#cb207-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb207-23"><a href="#cb207-23" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">compute</span> (model1, data.test.cv)  </span>
<span id="cb207-24"><a href="#cb207-24" aria-hidden="true" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb207-25"><a href="#cb207-25" aria-hidden="true" tabindex="-1"></a>  model1<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb207-26"><a href="#cb207-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get the true y values for the testing data</span></span>
<span id="cb207-27"><a href="#cb207-27" aria-hidden="true" tabindex="-1"></a>  true_y <span class="ot">&lt;-</span> data.test.cv<span class="sc">$</span>y </span>
<span id="cb207-28"><a href="#cb207-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mean((true_y - y_hat)^2): mean squared error (MSE). </span></span>
<span id="cb207-29"><a href="#cb207-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The smaller this error, the better your model is</span></span>
<span id="cb207-30"><a href="#cb207-30" aria-hidden="true" tabindex="-1"></a>  cv_mse[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb207-31"><a href="#cb207-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb207-32"><a href="#cb207-32" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cv_mse)</span></code></pre></div>
<p></p>
<p>The result is shown below</p>
<p></p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.09439574 # Model1</span></span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.04433521 # Model2</span></span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.1142009  # Model3</span></span></code></pre></div>
<p></p>
<p>Obviously, <code>model2</code> achieves the lowest prediction error.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-visual-3nn"></span>
<p class="caption marginnote shownote">
Figure 183: Visualization of the three fitted models and the data
</p>
<img src="graphics/10_visual_3nn.png" alt="Visualization of the three fitted models and the data" width="80%"  />
</div>
<p></p>
<p>We can also visually examine the fitness of the three models in Figure <a href="#fig:f10-visual-3nn">183</a> to see how well the three models fit the data.</p>
<p></p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use visual inspection to assist the model selection. </span></span>
<span id="cb209-2"><a href="#cb209-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-3"><a href="#cb209-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb209-4"><a href="#cb209-4" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model1, data.train)  </span>
<span id="cb209-5"><a href="#cb209-5" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb209-6"><a href="#cb209-6" aria-hidden="true" tabindex="-1"></a>model1<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb209-7"><a href="#cb209-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb209-8"><a href="#cb209-8" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model2, data.train)  </span>
<span id="cb209-9"><a href="#cb209-9" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb209-10"><a href="#cb209-10" aria-hidden="true" tabindex="-1"></a>model2<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb209-11"><a href="#cb209-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb209-12"><a href="#cb209-12" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(model3, data.train) </span>
<span id="cb209-13"><a href="#cb209-13" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result</span>
<span id="cb209-14"><a href="#cb209-14" aria-hidden="true" tabindex="-1"></a>model3<span class="sc">$</span>y_hat <span class="ot">&lt;-</span> y_hat</span>
<span id="cb209-15"><a href="#cb209-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-16"><a href="#cb209-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-17"><a href="#cb209-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data.train, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb209-18"><a href="#cb209-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model1<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb209-19"><a href="#cb209-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model2<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb209-20"><a href="#cb209-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data.train<span class="sc">$</span>x, model3<span class="sc">$</span>y_hat,<span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb209-21"><a href="#cb209-21" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;NN (3 hidden nodes)&quot;</span>, </span>
<span id="cb209-22"><a href="#cb209-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;NN (5 hidden nodes)&quot;</span>, <span class="st">&quot;NN (8 hidden nodes)&quot;</span>), </span>
<span id="cb209-23"><a href="#cb209-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;black&quot;</span>), </span>
<span id="cb209-24"><a href="#cb209-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">text.width =</span> <span class="dv">32</span>, <span class="at">cex =</span> <span class="fl">0.85</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-visual-finalnn"></span>
<p class="caption marginnote shownote">
Figure 184: Visualization of the architecture of the final model
</p>
<img src="graphics/10_visual_finalnn.png" alt="Visualization of the architecture of the final model" width="80%"  />
</div>
<p></p>
<p><strong>Step 5</strong> builds the final model. Figure <a href="#fig:f10-visual-finalnn">184</a> shows the architecture of the final model.</p>
<p></p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, build your final model</span></span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a>nn.final <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(y<span class="sc">~</span>x, <span class="at">data=</span>data.train, <span class="at">hidden=</span><span class="fu">c</span>(<span class="dv">5</span>)) <span class="co"># </span></span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nn.final) <span class="co"># Draw the architecture of the NN model</span></span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the final model for prediction.</p>
<p></p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Evaluate the prediction performance of your model</span></span>
<span id="cb211-2"><a href="#cb211-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on the testing data using the trained model</span></span>
<span id="cb211-3"><a href="#cb211-3" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">compute</span>(nn.final, data.test)  </span>
<span id="cb211-4"><a href="#cb211-4" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> pred<span class="sc">$</span>net.result </span>
<span id="cb211-5"><a href="#cb211-5" aria-hidden="true" tabindex="-1"></a><span class="co"># get the true y values for the testing data</span></span>
<span id="cb211-6"><a href="#cb211-6" aria-hidden="true" tabindex="-1"></a>true_y <span class="ot">&lt;-</span> data.test<span class="sc">$</span>y  </span>
<span id="cb211-7"><a href="#cb211-7" aria-hidden="true" tabindex="-1"></a><span class="co"># mean((true_y - y_hat)^2): mean squared error (MSE). </span></span>
<span id="cb211-8"><a href="#cb211-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The smaller this error, the better your model is</span></span>
<span id="cb211-9"><a href="#cb211-9" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((true_y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)    </span>
<span id="cb211-10"><a href="#cb211-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(mse)</span></code></pre></div>
<p></p>
<p><em>The 6-Step R Pipeline for CNN.</em> Before starting the pipeline, let’s first install the Keras package.</p>
<p></p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;devtools&quot;</span>) <span class="co"># install devtools</span></span>
<span id="cb212-2"><a href="#cb212-2" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;rstudio/keras&quot;</span>) <span class="co"># install Keras</span></span></code></pre></div>
<p></p>
<p><strong>Step 1</strong> and <strong>Step 2</strong> get the MNIST handwritten digit dataset into R and process the data in required format. The goal is to classify a handwritten number into one of the <span class="math inline">\(10\)</span> classes (from <span class="math inline">\(0\)</span> to <span class="math inline">\(9\)</span>).</p>
<p></p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1 -&gt; Read digits classification data </span></span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb213-3"><a href="#cb213-3" aria-hidden="true" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb213-4"><a href="#cb213-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-5"><a href="#cb213-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2 -&gt; Data preprocessing</span></span>
<span id="cb213-6"><a href="#cb213-6" aria-hidden="true" tabindex="-1"></a><span class="co"># code adapted from </span></span>
<span id="cb213-7"><a href="#cb213-7" aria-hidden="true" tabindex="-1"></a><span class="co"># keras.rstudio.com/articles/examples/mnist_cnn.html</span></span>
<span id="cb213-8"><a href="#cb213-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Input image dimensions</span></span>
<span id="cb213-9"><a href="#cb213-9" aria-hidden="true" tabindex="-1"></a>img_rows <span class="ot">&lt;-</span> <span class="dv">28</span></span>
<span id="cb213-10"><a href="#cb213-10" aria-hidden="true" tabindex="-1"></a>img_cols <span class="ot">&lt;-</span> <span class="dv">28</span></span>
<span id="cb213-11"><a href="#cb213-11" aria-hidden="true" tabindex="-1"></a>num_classes <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb213-12"><a href="#cb213-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-13"><a href="#cb213-13" aria-hidden="true" tabindex="-1"></a><span class="co"># The data, shuffled and split between training and testing sets</span></span>
<span id="cb213-14"><a href="#cb213-14" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb213-15"><a href="#cb213-15" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb213-16"><a href="#cb213-16" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb213-17"><a href="#cb213-17" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y</span>
<span id="cb213-18"><a href="#cb213-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-19"><a href="#cb213-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Redefine  dimension of train/test inputs</span></span>
<span id="cb213-20"><a href="#cb213-20" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_train, </span>
<span id="cb213-21"><a href="#cb213-21" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(<span class="fu">nrow</span>(x_train), img_rows, img_cols, <span class="dv">1</span>))</span>
<span id="cb213-22"><a href="#cb213-22" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_test, </span>
<span id="cb213-23"><a href="#cb213-23" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(<span class="fu">nrow</span>(x_test), img_rows, img_cols, <span class="dv">1</span>))</span>
<span id="cb213-24"><a href="#cb213-24" aria-hidden="true" tabindex="-1"></a>input_shape <span class="ot">&lt;-</span> <span class="fu">c</span>(img_rows, img_cols, <span class="dv">1</span>)</span>
<span id="cb213-25"><a href="#cb213-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-26"><a href="#cb213-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform RGB values into [0,1] range</span></span>
<span id="cb213-27"><a href="#cb213-27" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> x_train <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb213-28"><a href="#cb213-28" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> x_test <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb213-29"><a href="#cb213-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-30"><a href="#cb213-30" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&#39;x_train_shape:&#39;</span>, <span class="fu">dim</span>(x_train), <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb213-31"><a href="#cb213-31" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">nrow</span>(x_train), <span class="st">&#39;train samples</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb213-32"><a href="#cb213-32" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">nrow</span>(x_test), <span class="st">&#39;test samples</span><span class="sc">\n</span><span class="st">&#39;</span>)</span>
<span id="cb213-33"><a href="#cb213-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-34"><a href="#cb213-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert class vectors to binary class matrices</span></span>
<span id="cb213-35"><a href="#cb213-35" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_train, num_classes)</span>
<span id="cb213-36"><a href="#cb213-36" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_test, num_classes)</span></code></pre></div>
<p></p>
<p><strong>Step 3</strong> creates different models. In deep learning, parameters that are determined before training a model are called <strong>hyperparameters</strong>. Hyperparameters for a CNN include number of layers, number of nodes for a layer, kernel size of a convolution layer<label for="tufte-sn-256" class="margin-toggle sidenote-number">256</label><input type="checkbox" id="tufte-sn-256" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">256</span> E.g., in Figure <a href="#fig:f10-conv-layer">181</a> the kernel size is <span class="math inline">\(2\)</span>.</span>, etc. Here we create three models with different kernel sizes for the convolution layers.</p>
<p></p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3 -&gt; gather a list of candidate models</span></span>
<span id="cb214-2"><a href="#cb214-2" aria-hidden="true" tabindex="-1"></a>define_model <span class="ot">&lt;-</span> <span class="cf">function</span>(kernel_size){</span>
<span id="cb214-3"><a href="#cb214-3" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb214-4"><a href="#cb214-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convolution layer 1</span></span>
<span id="cb214-5"><a href="#cb214-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">8</span>, </span>
<span id="cb214-6"><a href="#cb214-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">kernel_size =</span> <span class="fu">c</span>(kernel_size,kernel_size), </span>
<span id="cb214-7"><a href="#cb214-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb214-8"><a href="#cb214-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">input_shape =</span> input_shape) <span class="sc">%&gt;%</span> </span>
<span id="cb214-9"><a href="#cb214-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pooling layer 1</span></span>
<span id="cb214-10"><a href="#cb214-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb214-11"><a href="#cb214-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convolution layer 2</span></span>
<span id="cb214-12"><a href="#cb214-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">16</span>, </span>
<span id="cb214-13"><a href="#cb214-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">kernel_size =</span> <span class="fu">c</span>(kernel_size,kernel_size), </span>
<span id="cb214-14"><a href="#cb214-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb214-15"><a href="#cb214-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pooling layer 2</span></span>
<span id="cb214-16"><a href="#cb214-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_max_pooling_2d</span>(<span class="at">pool_size =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb214-17"><a href="#cb214-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dense layers</span></span>
<span id="cb214-18"><a href="#cb214-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb214-19"><a href="#cb214-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb214-20"><a href="#cb214-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> num_classes, <span class="at">activation =</span> <span class="st">&#39;softmax&#39;</span>)</span>
<span id="cb214-21"><a href="#cb214-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb214-22"><a href="#cb214-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compile model</span></span>
<span id="cb214-23"><a href="#cb214-23" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb214-24"><a href="#cb214-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> loss_categorical_crossentropy,</span>
<span id="cb214-25"><a href="#cb214-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="fu">optimizer_adadelta</span>(),</span>
<span id="cb214-26"><a href="#cb214-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb214-27"><a href="#cb214-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb214-28"><a href="#cb214-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(model)</span>
<span id="cb214-29"><a href="#cb214-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb214-30"><a href="#cb214-30" aria-hidden="true" tabindex="-1"></a><span class="co"># define three models</span></span>
<span id="cb214-31"><a href="#cb214-31" aria-hidden="true" tabindex="-1"></a>model_kernel_1 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">2</span>)</span>
<span id="cb214-32"><a href="#cb214-32" aria-hidden="true" tabindex="-1"></a>model_kernel_2 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">3</span>)</span>
<span id="cb214-33"><a href="#cb214-33" aria-hidden="true" tabindex="-1"></a>model_kernel_3 <span class="ot">=</span> <span class="fu">define_model</span>(<span class="at">kernel_size=</span><span class="dv">5</span>)</span></code></pre></div>
<p></p>
<p><strong>Step 4</strong> uses cross-validation to evaluate the candidate models to identify the best model.</p>
<p></p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4 -&gt; Use cross-validation for model evaluation</span></span>
<span id="cb215-2"><a href="#cb215-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-3"><a href="#cb215-3" aria-hidden="true" tabindex="-1"></a><span class="co"># set upfunction for evaluating accuracy</span></span>
<span id="cb215-4"><a href="#cb215-4" aria-hidden="true" tabindex="-1"></a>cv_accuracy <span class="ot">&lt;-</span> <span class="cf">function</span>(n_folds, kernel_size,x_train,y_train){</span>
<span id="cb215-5"><a href="#cb215-5" aria-hidden="true" tabindex="-1"></a>  N <span class="ot">&lt;-</span> <span class="fu">dim</span>(x_train)[<span class="dv">1</span>] <span class="co"># the sample size, N, of the dataset</span></span>
<span id="cb215-6"><a href="#cb215-6" aria-hidden="true" tabindex="-1"></a>  folds_i <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> N)) </span>
<span id="cb215-7"><a href="#cb215-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb215-8"><a href="#cb215-8" aria-hidden="true" tabindex="-1"></a>  accuracy_v <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb215-9"><a href="#cb215-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb215-10"><a href="#cb215-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set up training and testing data</span></span>
<span id="cb215-11"><a href="#cb215-11" aria-hidden="true" tabindex="-1"></a>    test_i <span class="ot">&lt;-</span> <span class="fu">which</span>(folds_i <span class="sc">==</span> k)</span>
<span id="cb215-12"><a href="#cb215-12" aria-hidden="true" tabindex="-1"></a>    x.train.cv <span class="ot">&lt;-</span> x_train[<span class="sc">-</span>test_i,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>] </span>
<span id="cb215-13"><a href="#cb215-13" aria-hidden="true" tabindex="-1"></a>    x.test.cv <span class="ot">&lt;-</span> x_train[test_i,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>]   </span>
<span id="cb215-14"><a href="#cb215-14" aria-hidden="true" tabindex="-1"></a>    y.train.cv <span class="ot">&lt;-</span> y_train[<span class="sc">-</span>test_i,,drop<span class="ot">=</span><span class="cn">FALSE</span> ] </span>
<span id="cb215-15"><a href="#cb215-15" aria-hidden="true" tabindex="-1"></a>    y.test.cv <span class="ot">&lt;-</span> y_train[test_i,,drop<span class="ot">=</span><span class="cn">FALSE</span> ]</span>
<span id="cb215-16"><a href="#cb215-16" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb215-17"><a href="#cb215-17" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">define_model</span>(kernel_size)</span>
<span id="cb215-18"><a href="#cb215-18" aria-hidden="true" tabindex="-1"></a>    model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb215-19"><a href="#cb215-19" aria-hidden="true" tabindex="-1"></a>      x_train, y_train, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb215-20"><a href="#cb215-20" aria-hidden="true" tabindex="-1"></a>      <span class="at">epochs =</span> <span class="dv">2</span>,<span class="at">validation_split =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> <span class="dv">0</span></span>
<span id="cb215-21"><a href="#cb215-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb215-22"><a href="#cb215-22" aria-hidden="true" tabindex="-1"></a>    scores <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(</span>
<span id="cb215-23"><a href="#cb215-23" aria-hidden="true" tabindex="-1"></a>    x.test.cv, y.test.cv, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb215-24"><a href="#cb215-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb215-25"><a href="#cb215-25" aria-hidden="true" tabindex="-1"></a>    accuracy_v <span class="ot">&lt;-</span> <span class="fu">c</span>(accuracy_v, scores[<span class="dv">2</span>])</span>
<span id="cb215-26"><a href="#cb215-26" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb215-27"><a href="#cb215-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(accuracy_v)</span>
<span id="cb215-28"><a href="#cb215-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb215-29"><a href="#cb215-29" aria-hidden="true" tabindex="-1"></a><span class="co"># get average accuracy for each model</span></span>
<span id="cb215-30"><a href="#cb215-30" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_1 <span class="ot">&lt;-</span> </span>
<span id="cb215-31"><a href="#cb215-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">2</span>,x_train,y_train)</span>
<span id="cb215-32"><a href="#cb215-32" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_1))</span>
<span id="cb215-33"><a href="#cb215-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-34"><a href="#cb215-34" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_2 <span class="ot">&lt;-</span> </span>
<span id="cb215-35"><a href="#cb215-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">3</span>,x_train,y_train)</span>
<span id="cb215-36"><a href="#cb215-36" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_2))</span>
<span id="cb215-37"><a href="#cb215-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-38"><a href="#cb215-38" aria-hidden="true" tabindex="-1"></a>accuracy_v_kernel_3 <span class="ot">&lt;-</span> </span>
<span id="cb215-39"><a href="#cb215-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cv_accuracy</span>(<span class="at">n_folds=</span><span class="dv">2</span>,<span class="at">kernel_size=</span><span class="dv">5</span>,x_train,y_train)</span>
<span id="cb215-40"><a href="#cb215-40" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(accuracy_v_kernel_3))</span></code></pre></div>
<p></p>
<p>The result is shown below.</p>
<p></p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9680667 # Model1</span></span>
<span id="cb216-2"><a href="#cb216-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9742167 # Model2</span></span>
<span id="cb216-3"><a href="#cb216-3" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] 0.9760833  # Model3</span></span></code></pre></div>
<p></p>
<p><strong>Step 5</strong> builds the final model based on all the training data.</p>
<p></p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5 -&gt; After model selection, build your final model</span></span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-3"><a href="#cb217-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">define_model</span>(<span class="dv">5</span>)</span>
<span id="cb217-4"><a href="#cb217-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb217-5"><a href="#cb217-5" aria-hidden="true" tabindex="-1"></a>      x_train, y_train, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb217-6"><a href="#cb217-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">epochs =</span> <span class="dv">2</span>,<span class="at">validation_split =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> <span class="dv">0</span></span>
<span id="cb217-7"><a href="#cb217-7" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p></p>
<p><strong>Step 6</strong> uses the final model for prediction.</p>
<p></p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6 -&gt; Evaluate the prediction performance of your model</span></span>
<span id="cb218-2"><a href="#cb218-2" aria-hidden="true" tabindex="-1"></a>scores <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(</span>
<span id="cb218-3"><a href="#cb218-3" aria-hidden="true" tabindex="-1"></a>    x_test, y_test, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb218-4"><a href="#cb218-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(scores[<span class="dv">2</span>])</span></code></pre></div>
<p></p>
<p>To visualize the process of how this CNN model works, the following R code is used to visualize the output from each layer, shown in Figure <a href="#fig:f10-cnn-activations">185</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-cnn-activations"></span>
<p class="caption marginnote shownote">
Figure 185: Visualize the outputs from all layers of the CNN model
</p>
<img src="graphics/10_visual_7_activations.png" alt="Visualize the outputs from all layers of the CNN model" width="80%"  />
</div>
<p></p>
<p></p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize output for a layer</span></span>
<span id="cb219-2"><a href="#cb219-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-3"><a href="#cb219-3" aria-hidden="true" tabindex="-1"></a><span class="co"># use the first image from testing data</span></span>
<span id="cb219-4"><a href="#cb219-4" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> x_test[<span class="dv">1</span>,,,]</span>
<span id="cb219-5"><a href="#cb219-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.raster</span>(img))</span>
<span id="cb219-6"><a href="#cb219-6" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> x_test[<span class="dv">1</span>,,,,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb219-7"><a href="#cb219-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-8"><a href="#cb219-8" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to plot an image</span></span>
<span id="cb219-9"><a href="#cb219-9" aria-hidden="true" tabindex="-1"></a>plot_image <span class="ot">&lt;-</span> <span class="cf">function</span>(channel) {</span>
<span id="cb219-10"><a href="#cb219-10" aria-hidden="true" tabindex="-1"></a>    rotate <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">t</span>(<span class="fu">apply</span>(x, <span class="dv">2</span>, rev))</span>
<span id="cb219-11"><a href="#cb219-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">image</span>(<span class="fu">rotate</span>(channel), <span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">asp =</span> <span class="dv">1</span>, </span>
<span id="cb219-12"><a href="#cb219-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">col =</span> <span class="fu">gray.colors</span>(<span class="dv">12</span>))</span>
<span id="cb219-13"><a href="#cb219-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb219-14"><a href="#cb219-14" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the testing image</span></span>
<span id="cb219-15"><a href="#cb219-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_image</span>(  <span class="dv">1</span> <span class="sc">-</span> img[<span class="dv">1</span>,,,]   )</span>
<span id="cb219-16"><a href="#cb219-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-17"><a href="#cb219-17" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the output from the second layer </span></span>
<span id="cb219-18"><a href="#cb219-18" aria-hidden="true" tabindex="-1"></a>layer_number <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb219-19"><a href="#cb219-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-20"><a href="#cb219-20" aria-hidden="true" tabindex="-1"></a><span class="co"># print layer name</span></span>
<span id="cb219-21"><a href="#cb219-21" aria-hidden="true" tabindex="-1"></a>layer_name <span class="ot">&lt;-</span> model<span class="sc">$</span>layers[[layer_number]]<span class="sc">$</span>name</span>
<span id="cb219-22"><a href="#cb219-22" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(layer_name)</span>
<span id="cb219-23"><a href="#cb219-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-24"><a href="#cb219-24" aria-hidden="true" tabindex="-1"></a>layer_outputs <span class="ot">&lt;-</span> <span class="fu">lapply</span>(model<span class="sc">$</span>layers[layer_number], </span>
<span id="cb219-25"><a href="#cb219-25" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">function</span>(layer) layer<span class="sc">$</span>output)</span>
<span id="cb219-26"><a href="#cb219-26" aria-hidden="true" tabindex="-1"></a>activation_model <span class="ot">&lt;-</span> <span class="fu">keras_model</span>(<span class="at">inputs =</span> model<span class="sc">$</span>input, </span>
<span id="cb219-27"><a href="#cb219-27" aria-hidden="true" tabindex="-1"></a>                                <span class="at">outputs =</span> layer_outputs)</span>
<span id="cb219-28"><a href="#cb219-28" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the outputs from the layer for the image</span></span>
<span id="cb219-29"><a href="#cb219-29" aria-hidden="true" tabindex="-1"></a>layer_activation <span class="ot">&lt;-</span> activation_model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(img)</span>
<span id="cb219-30"><a href="#cb219-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-31"><a href="#cb219-31" aria-hidden="true" tabindex="-1"></a><span class="co"># check dimension</span></span>
<span id="cb219-32"><a href="#cb219-32" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">dim</span>(layer_activation))</span>
<span id="cb219-33"><a href="#cb219-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-34"><a href="#cb219-34" aria-hidden="true" tabindex="-1"></a><span class="co"># number of features</span></span>
<span id="cb219-35"><a href="#cb219-35" aria-hidden="true" tabindex="-1"></a>n_features <span class="ot">&lt;-</span> <span class="fu">dim</span>(layer_activation)[[<span class="dv">4</span>]] </span>
<span id="cb219-36"><a href="#cb219-36" aria-hidden="true" tabindex="-1"></a><span class="co"># image width</span></span>
<span id="cb219-37"><a href="#cb219-37" aria-hidden="true" tabindex="-1"></a>image_size <span class="ot">&lt;-</span> <span class="fu">dim</span>(layer_activation)[[<span class="dv">2</span>]] </span>
<span id="cb219-38"><a href="#cb219-38" aria-hidden="true" tabindex="-1"></a><span class="co"># number of columns and images per column </span></span>
<span id="cb219-39"><a href="#cb219-39" aria-hidden="true" tabindex="-1"></a><span class="co"># (each column plots an image)</span></span>
<span id="cb219-40"><a href="#cb219-40" aria-hidden="true" tabindex="-1"></a>n_cols <span class="ot">&lt;-</span> n_features </span>
<span id="cb219-41"><a href="#cb219-41" aria-hidden="true" tabindex="-1"></a>images_per_col <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co">#</span></span>
<span id="cb219-42"><a href="#cb219-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-43"><a href="#cb219-43" aria-hidden="true" tabindex="-1"></a><span class="co"># plot n_cols of images</span></span>
<span id="cb219-44"><a href="#cb219-44" aria-hidden="true" tabindex="-1"></a>op <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(n_cols, images_per_col), </span>
<span id="cb219-45"><a href="#cb219-45" aria-hidden="true" tabindex="-1"></a>            <span class="at">mai =</span> <span class="fu">rep_len</span>(<span class="dv">0</span>, <span class="dv">4</span>)) </span>
<span id="cb219-46"><a href="#cb219-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-47"><a href="#cb219-47" aria-hidden="true" tabindex="-1"></a><span class="co"># plot each image</span></span>
<span id="cb219-48"><a href="#cb219-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (col <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span>(n_cols<span class="dv">-1</span>)) {</span>
<span id="cb219-49"><a href="#cb219-49" aria-hidden="true" tabindex="-1"></a>        col_ix <span class="ot">&lt;-</span> col <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb219-50"><a href="#cb219-50" aria-hidden="true" tabindex="-1"></a>        channel_image <span class="ot">&lt;-</span> layer_activation[<span class="dv">1</span>,,,col_ix]</span>
<span id="cb219-51"><a href="#cb219-51" aria-hidden="true" tabindex="-1"></a>      <span class="fu">plot_image</span>(<span class="dv">1</span><span class="sc">-</span>channel_image)</span>
<span id="cb219-52"><a href="#cb219-52" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
</div>
</div>
<div id="intrees" class="section level2 unnumbered">
<h2>inTrees</h2>
<div id="rationale-and-formulation-17" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><em>What is a variable?</em> Given an Excel file, we call the column that is titled as the name <em>Age</em> as a variable. And in fact, as a convention, in an Excel file or a data table in some common formats, we usually do not doubt that each column implies a variable. These “variables,” or entities, may have definitions in the domain of common sense (i.e., where we take things for granted), but they may not be the best candidates to characterize the system under study. Recall that in <strong>Chapter 2</strong> we mentioned that the goal of modeling begins with abstraction—“identification of a few main entities from the problem,” and continues to “characterize their relationships.” If we comfortably play the data using the variables that have been defined without examination, we lose sight of a large territory of data analytics—identification of a few main entities from the problem that can sufficiently characterize the problem.</p>
<p>Now let’s switch to the domain of linear regression. A variable is an abstract entity in the equation of the linear regression model<label for="tufte-sn-257" class="margin-toggle sidenote-number">257</label><input type="checkbox" id="tufte-sn-257" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">257</span> I.e., denoted as <span class="math inline">\(x_i\)</span>.</span>, multiplied by a regression coefficient<label for="tufte-sn-258" class="margin-toggle sidenote-number">258</label><input type="checkbox" id="tufte-sn-258" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">258</span> I.e., denoted as <span class="math inline">\(\beta_i\)</span>.</span>. It stands in the equation in parallel with other variables, which jointly determines an outcome variable. This form implies that the difference between the variables are only numeric, characterized by the differences in signs and magnitudes (i.e., encoded in the regression coefficients), but not in semantics. Now comes a reflection: in order for a linear regression model to work out in an application, shouldn’t we ensure that the variables <em>could be</em> lined up in this manner of apposition<label for="tufte-sn-259" class="margin-toggle sidenote-number">259</label><input type="checkbox" id="tufte-sn-259" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">259</span> <em>Apposition</em>—with a little abuse of the term—the grammar used in the linear regression to line up the variables in a parallel and additive manner.</span>?</p>
<p><span class="math display" id="eq:10-apposition">\[\begin{equation}
        y = \ldots + \underbrace{\beta_1 x_1}_{\text{A}} + \underbrace{\beta_2 x_2}_{\text{p}} + \underbrace{\beta_3 x_3}_{\text{p}} + \underbrace{\beta_4 x_4}_{\text{o}} + \underbrace{\beta_5 x_5}_{\text{s}} +
        \underbrace{\beta_6 x_6}_{\text{i}} +
        \underbrace{\beta_7 x_7}_{\text{t}} + \underbrace{\beta_8 x_8}_{\text{i}} + \underbrace{\beta_9 x_9}_{\text{o}} + \underbrace{\beta_{10} x_{10}}_{\text{n}} + \ldots
\tag{102}
\end{equation}\]</span></p>
<p>In other words, if the variables in the domain of common sense are not semantic equals, the application of linear regression on them is questionable. For example, it is probably common sense to line up a few genetic factors in Eq. <a href="#eq:10-apposition">(102)</a>, but could we also put <em>Age</em> as another variable that stands among the genetic factors in a line? In many contexts, we need to work out a better definition of the variable, i.e., it is not uncommon to define two new variables such as <em><span class="math inline">\(\text{Age} \leq 65\)</span></em> and <em><span class="math inline">\(\text{Age} &gt; 65\)</span></em> instead of using the variable <em>Age</em> directly. Sometimes we use the variable <em>Age</em> in a model because it is named as <em>Age</em>. But what is <em>Age</em>? When we put a variable in a model, it is destined to be <em>re</em>defined, either before the analysis, or after, or along the way.</p>
<p>This effort to redefine variables could be automated by tree models. Recall that the tree models use rule-based semantics. Rules like <em><span class="math inline">\(\text{Age} \leq 65\)</span></em> and <em><span class="math inline">\(\text{Age} &gt; 65\)</span></em> sometimes yield statistically significant <em>and</em> semantically meaningful entities, perfect candidates for variable redefinition purposes. And a tree is essentially a collection of multiple rules. If we could run tree models on a dataset first, we could extract those rules, and each rule is a new variable.</p>
<p>This is the starting point of <code>inTrees</code>. It uses the random forest to collect potentially useful rules<label for="tufte-sn-260" class="margin-toggle sidenote-number">260</label><input type="checkbox" id="tufte-sn-260" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">260</span> This step automates the variable redefinition process.</span>, then puts the rules as the variables into a model<label for="tufte-sn-261" class="margin-toggle sidenote-number">261</label><input type="checkbox" id="tufte-sn-261" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">261</span> I.e., a classification/regression model. This step automates the integration of the variable redefinition with modeling.</span> and employs a computational process to select the final variables. In this way, we have the best parts of both methods: the rules capture the variable-level patterns in the data, and the model (i.e., a regression model) captures the synthetic effects of these patterns in predicting an outcome variable. Note that <code>inTrees</code> is not the first of its kind. It follows a few pioneers such as the <code>rulefit</code> and modifies existing efforts according to some in-field experiences.</p>
</div>
<div id="theory-and-method-11" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>The <code>inTrees</code> uses a framework that is shown in Figure <a href="#fig:f10-inTrees">186</a>. In the following text, we introduce each functionality of the <code>inTrees</code> framework.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-inTrees"></span>
<p class="caption marginnote shownote">
Figure 186: The pipeline of <code>inTrees</code>
</p>
<img src="graphics/10_intrees.png" alt="The pipeline of `inTrees`" width="80%"  />
</div>
<p></p>
<p>Consider the dataset that has <span class="math inline">\(2\)</span> predictors and <span class="math inline">\(7\)</span> instances as shown in Table <a href="#tab:t10-1">54</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-1">Table 54: </span>Example of a dataset with <span class="math inline">\(7\)</span> instances</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<!-- % It can be seen that the tree was built based on the resampled dataset that includes the instances $\{1,1,2,2,7,7,7\}$. In other words, in this resampled dataset, the instance (ID: $1$) was resampled twice, the instance (ID: $2$) was resampled twice, and the instance (ID: $7$) was resampled three times. In the tree, the root and inner nodes are labeled with the data point IDs and leaf nodes are labeled with the data point IDs and the decisions (i.e., which class to predict).   -->
<p><em>Extract rules.</em> The <code>inTrees</code> uses a tree emsemble learning method to grow many trees. A decision tree can be dissembled into a set of rules. For example, suppose that a random forest model has been built on the dataset shown in Table <a href="#tab:t10-1">54</a>. One tree of this random forest model is shown in Figure <a href="#fig:f10-3">187</a>. Three rules (each rule corresponds to a leaf node) are extracted and shown in Table <a href="#tab:t10-2">55</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-3"></span>
<img src="graphics/10_3.png" alt="Example of a decision tree; leaf nodes (a.k.a., decision nodes) are shadowed in gray." width="250px"  />
<!--
<p class="caption marginnote">-->Figure 187: Example of a decision tree; leaf nodes (a.k.a., decision nodes) are shadowed in gray.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<!-- % \[ -->
<!-- % \begin{array}{l}{\text{Rule 1: } \{ X_1=0\to Class=C_1\}}; \\ { \text{Rule 2: } \{ X_1 \neq 0,X_2=0\to Class=C_1\}}; \\ { \text{Rule 3: } \{X_1 \neq 0,X_2 \neq 0 \to Class=C_0\}.}\end{array} -->
<!-- % \] -->
<p>Each rule is evaluated by three criteria: the <strong>length</strong> of a rule that is defined as the number of variables in the rule; the <strong>frequency</strong> of a rule that is the proportion of data points in the dataset that meet the condition of the rule, and the <strong>error rate</strong> of a rule. For classification problems, the error rate of a rule is the number of data points incorrectly identified by the rule divided by the number of data points that meet the condition of the rule.</p>
<p>For regression problems, the error rate of a rule is the <em>mean squared error (MSE)</em>, that is defined as</p>
<p><span class="math display">\[MSE = \frac{1}{N}\sum_{i=1}^N \left(y_i - \bar{y}\right)^2,\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the number of data points in the leaf node that corresponds to the rule, <span class="math inline">\(y_i\)</span> is the value of the outcome variable of the <span class="math inline">\(i^{th}\)</span> data point, and <span class="math inline">\(\bar y\)</span> is the average of the outcome variable (i.e., as the prediction at the leaf node).</p>
<p>Based on these three criteria, the evaluation of the three rules is shown in Table <a href="#tab:t10-2">55</a>.<label for="tufte-sn-262" class="margin-toggle sidenote-number">262</label><input type="checkbox" id="tufte-sn-262" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">262</span> Apply each rule on the data points in Table <a href="#tab:t10-1">54</a>.</span></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-2">Table 55: </span>Evaluation of the three rules extracted from the tree in Figure <a href="#fig:f10-3">187</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Rule</th>
<th align="left">Length</th>
<th align="left">Frequency</th>
<th align="left">Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 = 0 \to Class = C_0\}\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(5/7\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1/7\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(1/7\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>Prune rules.</em> A lengthy rule, i.e., a rule with many variables, is hard to interpret. For example, consider a rule</p>
<p><span class="math display">\[
\text{Rule: } \{ \text{Age} \leq 65, \text{Gene A } = \text{Type 1}, \text{Gene B } = \text{Type 3} \to Class= \text{No risk}\}.
\]</span></p>
<p>It is unknown if the three variables are equally important in making the prediction. And, because the way the random forests grow the trees, it is possible that some variables in a rule are not significant at all<label for="tufte-sn-263" class="margin-toggle sidenote-number">263</label><input type="checkbox" id="tufte-sn-263" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">263</span> I.e., these variables are selected because the random forest model purposely <em>randomizes</em> the learning process.</span>. Therefore, it is beneficial to prune the rules and remove irrelevant variables from the rules.</p>
<p>Take Rule <span class="math inline">\(2\)</span> <span class="math inline">\(\{x_1 \neq 0, x_2=0\to Class=C_1\}\)</span> for example. The error rate for this rule is <span class="math inline">\(0\)</span>.<label for="tufte-sn-264" class="margin-toggle sidenote-number">264</label><input type="checkbox" id="tufte-sn-264" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">264</span> See Table <a href="#tab:t10-2">55</a>.</span> Now remove <span class="math inline">\(x_1 \neq 0\)</span> from the rule, and the new rule becomes <span class="math display">\[\{x_2=0\to Class=C_1\},\]</span> which has an error of <span class="math inline">\(3/5\)</span>.<label for="tufte-sn-265" class="margin-toggle sidenote-number">265</label><input type="checkbox" id="tufte-sn-265" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">265</span> Use Table <a href="#tab:t10-1">54</a>.</span> Therefore, the error rate increases by <span class="math inline">\(3/5\)</span>. This increase of error rate is named <em>decay</em> in the terminology of <code>inTrees</code>. A threshold is set by the user, i.e., here, if the threshold is set to be <span class="math inline">\(0.05\)</span>, we should not remove <span class="math inline">\(x_1\)</span> from Rule <span class="math inline">\(2\)</span> since <span class="math inline">\(3/5&gt;0.05\)</span>.</p>
<p>Now let’s remove <span class="math inline">\(x_2=0\)</span>. The resulting rule is <span class="math display">\[ \{x_1 \neq 0\to Class=C_1\}, \]</span> which has an error of <span class="math inline">\(1/2\)</span>. Therefore, <span class="math inline">\(x_2\)</span> should not be pruned either.</p>
<p><em>Rules are variables.</em> Each rule leads to a redefined variable. For example, consider the dataset in Table <a href="#tab:t10-1">54</a>. We name the three rules shown in Table <a href="#tab:t10-2">55</a> as variables <span class="math inline">\(z_1\)</span>, <span class="math inline">\(z_2\)</span>, and <span class="math inline">\(z_3\)</span>, respectively. We only use the <strong>condition of a rule</strong> to define the variable. For instance, the condition of a rule is illustrated below</p>
<p><span class="math display">\[ \{\underbrace{x_1 \neq 0}_{condition}\to \underbrace{Class=C_1}_{outcome}\}. \]</span></p>
<p>Consider <span class="math inline">\(z_1\)</span> first<label for="tufte-sn-266" class="margin-toggle sidenote-number">266</label><input type="checkbox" id="tufte-sn-266" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">266</span> I.e., <span class="math inline">\(\{x_1=0\}\)</span>.</span>. The data points in Table <a href="#tab:t10-1">54</a> that meet the condition <span class="math inline">\(\{x_1=0\}\)</span> include <span class="math inline">\(\text{ID} = \{3,4,5,6,7\}\)</span>. Thus, the values of <span class="math inline">\(z_1\)</span> are <span class="math inline">\(\{0,0,1,1,1,1,1\}\)</span>. For <span class="math inline">\(z_2\)</span>,<label for="tufte-sn-267" class="margin-toggle sidenote-number">267</label><input type="checkbox" id="tufte-sn-267" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">267</span> I.e., <span class="math inline">\(\{x_1 \neq 0,x_2=0\}\)</span>.</span> only data point <span class="math inline">\(\text{ID} = \{2\}\)</span> meets the condition, and therefore, the values of <span class="math inline">\(z_2\)</span> are <span class="math inline">\(\{0,1,0,0,0,0,0\}\)</span>. Similarly, the values of <span class="math inline">\(z_3\)</span> are <span class="math inline">\(\{1,0,0,0,0,0,0\}\)</span>.<label for="tufte-sn-268" class="margin-toggle sidenote-number">268</label><input type="checkbox" id="tufte-sn-268" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">268</span> I.e., <span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0\}\)</span>.</span></p>
<p>The new dataset is shown in Table <a href="#tab:t10-5">56</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-5">Table 56: </span>The <em>binarized</em> dataset of Table <a href="#tab:t10-1">54</a> by the rules in Table <a href="#tab:t10-2">55</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(z_1\)</span></th>
<th align="left"><span class="math inline">\(z_2\)</span></th>
<th align="left"><span class="math inline">\(z_3\)</span></th>
<th align="left">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(C_0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_1\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(6\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(7\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(C_0\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>Select rules.</em> A feature selection method could be applied on the new dataset to select the significant variables. Methods such as the <span class="math inline">\(L_1\)</span> regularized logistics regression (i.e., the equivalent of LASSO for logistic regression model) and regularized random forests are used in the <code>inTrees</code>.</p>
<p>Note that most existing methods don’t concern the length of the rules. But, given two rules with the same predictive power, the rule with a shorter length should be preferred<label for="tufte-sn-269" class="margin-toggle sidenote-number">269</label><input type="checkbox" id="tufte-sn-269" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">269</span> A shorter rule means a simpler model, better interpretability, etc.</span>. In <code>inTrees</code>, the <strong>Guided Regularized Random Forest</strong> (<strong>GRRF</strong>) is also an option for feature selection: the GRRF can assign a weight to each variable, so that when two variables have similar predictive power, the variable with higher weight is more likely to be selected. In our case, we could set higher weight<label for="tufte-sn-270" class="margin-toggle sidenote-number">270</label><input type="checkbox" id="tufte-sn-270" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">270</span> What is the optimal weight? We could use cross-validation to decide.</span> for shorter rules in GRRF.</p>
<p><em>Rule ensemble.</em> As the rules are taken as new variables, a new dataset such as the one shown in Table <a href="#tab:t10-5">56</a> is created. So theoretically, any model could be applied on the new dataset to build a prediction model. There are preferences in different packages. For example, in <code>RuleFit</code>, a linear regression model is used that takes the rules as predictors. In <code>inTrees</code>, a simple rule ensemble method summarizes the rules into an <em>ordered rule set</em> for prediction.</p>
<p>It takes a few iterations to develop the ordered rule set. First, we create a default rule, denoted as <span class="math inline">\(r_0\)</span>, that has a null <em>condition</em> and classifies all the data points to be the most frequent class (if it is a regression model, then <span class="math inline">\(r_0\)</span> predicts all the data points to be the population average).</p>
<p>Denote the ordered rule set as <em>R</em>, which is set to be empty at the beginning. Then, the algorithm searches through the available rules and identifies the best rule and adds it into <em>R</em>. The best rule is defined as the rule with the minimum error evaluated by the training data. If there are ties, the rule with higher frequency and smaller length is selected. Then, the data points that meet the condition of the best rule are removed, and the default rule <span class="math inline">\(r_0\)</span> is re-calculated with the data points left. The algorithm iterates to search for the next best rule and update <span class="math inline">\(r_0\)</span> after each iteration. This iterative process continues until no data point is left in the training dataset, or the default rule <span class="math inline">\(r_0\)</span> beats all other available rules that have not been added into the rule ensemble set <span class="math inline">\(R\)</span>. Note that, the selected rules in <em>R</em> are ordered according to the sequential order of their inclusion.</p>
<p>Consider the dataset shown in Table <a href="#tab:t10-1">54</a> and the rules shown in Table <a href="#tab:t10-2">55</a>. The error rate and frequency of each rule is shown in Table <a href="#tab:t10-9">57</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-9">Table 57: </span>Error rates and frequencies of the rules in Table <a href="#tab:t10-2">55</a> using the dataset in Table <a href="#tab:t10-1">54</a></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Rule</th>
<th align="left">Error</th>
<th align="left">Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(\{Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(3/7\)</span></td>
<td align="left"><span class="math inline">\(7/7\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 = 0 \to Class = C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
<td align="left"><span class="math inline">\(5/7\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
<td align="left"><span class="math inline">\(1/7\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
<td align="left"><span class="math inline">\(1/7\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>At the beginning, the default rule is <span class="math inline">\(\{Class=C_0\}\)</span>, and its error rate is <span class="math inline">\(3/7\)</span>. The algorithm then searches for the best rule in the available rules (i.e., shown in Table <a href="#tab:t10-2">55</a>). Rule 2 and Rule 3 have the least errors, and their frequency and length are also the same. Thus, we can add either of them into <span class="math inline">\(R\)</span>. Assume that Rule 2 is selected: <span class="math inline">\(R=\{x_1 \neq 0,x_2=0\to Class=C_1\}\)</span>. Then, the data point (ID:<span class="math inline">\(2\)</span>) classified by this rule is removed from Table <a href="#tab:t10-1">54</a>. The default rule <span class="math inline">\(r_0\)</span> is still <span class="math inline">\(\{Class=C_0\}\)</span>, and the error and frequency of each rule on the updated dataset<label for="tufte-sn-271" class="margin-toggle sidenote-number">271</label><input type="checkbox" id="tufte-sn-271" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">271</span> The data point (ID:<span class="math inline">\(2\)</span>) is removed.</span> is updated, as shown in Table <a href="#tab:t10-10">58</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-10">Table 58: </span>Updated error rates and frequencies of the rules in Table <a href="#tab:t10-2">55</a> using the reduced dataset, i.e., data point (ID:<span class="math inline">\(2\)</span>) in Table <a href="#tab:t10-1">54</a> is removed</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Rule</th>
<th align="left">Error</th>
<th align="left">Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(\{Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/6\)</span></td>
<td align="left"><span class="math inline">\(6/6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 = 0 \to Class = C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
<td align="left"><span class="math inline">\(5/6\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
<td align="left">NA</td>
<td align="left"><span class="math inline">\(0/6\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(0/1\)</span></td>
<td align="left"><span class="math inline">\(1/6\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>A new iteration begins and <span class="math inline">\(\{x_1 \neq 0,x_2\neq 0\to Class=C_0\}\)</span> is added to <em>R</em>, and the data point (ID:<span class="math inline">\(1\)</span>) is removed. The default rule remains unchanged and the error and frequency of each rule on the updated dataset<label for="tufte-sn-272" class="margin-toggle sidenote-number">272</label><input type="checkbox" id="tufte-sn-272" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">272</span> The data points (ID:<span class="math inline">\(1\)</span> and ID:<span class="math inline">\(2\)</span>) in Table <a href="#tab:t10-1">54</a> are removed.</span> is updated in Table <a href="#tab:t10-11">59</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-11">Table 59: </span>Updated error rates and frequencies of the rules in Table <a href="#tab:t10-2">55</a> using the reduced dataset, i.e., data points (ID:<span class="math inline">\(1\)</span> and ID:<span class="math inline">\(2\)</span>) in Table <a href="#tab:t10-1">54</a> are removed</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left">Rule</th>
<th align="left">Error</th>
<th align="left">Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(\{Class=C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
<td align="left"><span class="math inline">\(5/5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 = 0 \to Class = C_0\}\)</span></td>
<td align="left"><span class="math inline">\(2/5\)</span></td>
<td align="left"><span class="math inline">\(5/5\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
<td align="left">NA</td>
<td align="left"><span class="math inline">\(0/5\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_0\}\)</span></td>
<td align="left">NA</td>
<td align="left"><span class="math inline">\(0/5\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>Now the default rule <span class="math inline">\(Class=C_0\)</span> has the minimum error <span class="math inline">\(2/5\)</span>, the same as <span class="math inline">\(\{x_1=0\to Class=C_0\}\)</span>. Therefore, the default rule is added to <em>R</em> and the process stops. The final ordered rule set <em>R</em> is summarized in Table <a href="#tab:t10-12">60</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-12">Table 60: </span>Final results of <em>R</em></span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Order</th>
<th align="left">Rule</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(\{Class=C_0\}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 = 0 \to Class=C_1\}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(\{x_1 \neq 0, x_2 \neq 0 \to Class=C_1\}\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>When predicting on an instance, the first rule in <em>R</em> that <em>hits</em> the data point is used for prediction. For example, for a data point <span class="math inline">\(\{x_1 \neq 0,x_2=1\}\)</span>, it meets the condition of Rule 2 in <em>R</em>. The prediction on this data point is <span class="math inline">\(C_1\)</span>. For data point <span class="math inline">\(\{x_1=0,x_2=1\}\)</span>, it does not meet the condition of either Rule <span class="math inline">\(1\)</span> or Rule <span class="math inline">\(2\)</span> in <em>R</em>. Therefore, the default rule is used, and the prediction is <span class="math inline">\(C_0\)</span>.</p>
</div>
<div id="r-lab-16" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We use <code>inTrees</code> on the AD datasedt. Based on the random forest model, <span class="math inline">\(4555\)</span> rules are extracted.</p>
<p></p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>(<span class="at">all =</span> <span class="cn">TRUE</span>))</span>
<span id="cb220-2"><a href="#cb220-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;arules&quot;</span>)</span>
<span id="cb220-3"><a href="#cb220-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;randomForest&quot;</span>)</span>
<span id="cb220-4"><a href="#cb220-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;RRF&quot;</span>)</span>
<span id="cb220-5"><a href="#cb220-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;inTrees&quot;</span>)</span>
<span id="cb220-6"><a href="#cb220-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;reshape&quot;</span>)</span>
<span id="cb220-7"><a href="#cb220-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb220-8"><a href="#cb220-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb220-9"><a href="#cb220-9" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb220-10"><a href="#cb220-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb220-11"><a href="#cb220-11" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb220-12"><a href="#cb220-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-13"><a href="#cb220-13" aria-hidden="true" tabindex="-1"></a>target_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">==</span> <span class="st">&quot;DX_bl&quot;</span>)</span>
<span id="cb220-14"><a href="#cb220-14" aria-hidden="true" tabindex="-1"></a>target <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;class_&quot;</span>, <span class="fu">as.character</span>(data[, target_indx]))</span>
<span id="cb220-15"><a href="#cb220-15" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> </span>
<span id="cb220-16"><a href="#cb220-16" aria-hidden="true" tabindex="-1"></a>            <span class="fu">c</span>(<span class="st">&quot;DX_bl&quot;</span>, <span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>, <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb220-17"><a href="#cb220-17" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data</span>
<span id="cb220-18"><a href="#cb220-18" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> X[, <span class="sc">-</span>rm_indx]</span>
<span id="cb220-19"><a href="#cb220-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(X)) X[, i] <span class="ot">&lt;-</span> </span>
<span id="cb220-20"><a href="#cb220-20" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as.factor</span>(<span class="fu">dicretizeVector</span>(X[, i], <span class="at">K =</span> <span class="dv">3</span>))</span>
<span id="cb220-21"><a href="#cb220-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-22"><a href="#cb220-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Use random forests to grow the trees</span></span>
<span id="cb220-23"><a href="#cb220-23" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(target))</span>
<span id="cb220-24"><a href="#cb220-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-25"><a href="#cb220-25" aria-hidden="true" tabindex="-1"></a><span class="co"># transform rf object to an inTrees&#39; format</span></span>
<span id="cb220-26"><a href="#cb220-26" aria-hidden="true" tabindex="-1"></a>treeList <span class="ot">&lt;-</span> <span class="fu">RF2List</span>(rf)  </span>
<span id="cb220-27"><a href="#cb220-27" aria-hidden="true" tabindex="-1"></a>exec <span class="ot">&lt;-</span> <span class="fu">extractRules</span>(treeList, X)  <span class="co"># Extract the rules</span></span>
<span id="cb220-28"><a href="#cb220-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-29"><a href="#cb220-29" aria-hidden="true" tabindex="-1"></a><span class="do">## The rules are measured by length, error and frequency.</span></span>
<span id="cb220-30"><a href="#cb220-30" aria-hidden="true" tabindex="-1"></a>class <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;class_&quot;</span>, <span class="fu">as.character</span>(target))</span>
<span id="cb220-31"><a href="#cb220-31" aria-hidden="true" tabindex="-1"></a>rules <span class="ot">&lt;-</span> <span class="fu">getRuleMetric</span>(exec, X, target)</span></code></pre></div>
<p></p>
<p>The statistics of the rules could be extracted, e.g., <span class="math inline">\(5\)</span> rules are shown below.</p>
<p></p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="#cb221-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rules[<span class="fu">order</span>(<span class="fu">as.numeric</span>(rules[, <span class="st">&quot;len&quot;</span>])), ][<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ])</span>
<span id="cb221-2"><a href="#cb221-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb221-3"><a href="#cb221-3" aria-hidden="true" tabindex="-1"></a><span class="co">#      len freq    err     </span></span>
<span id="cb221-4"><a href="#cb221-4" aria-hidden="true" tabindex="-1"></a><span class="co"># [1,] &quot;2&quot; &quot;0.118&quot; &quot;0.098&quot; </span></span>
<span id="cb221-5"><a href="#cb221-5" aria-hidden="true" tabindex="-1"></a><span class="co"># [2,] &quot;2&quot; &quot;0.182&quot; &quot;0&quot;     </span></span>
<span id="cb221-6"><a href="#cb221-6" aria-hidden="true" tabindex="-1"></a><span class="co"># [3,] &quot;2&quot; &quot;0.182&quot; &quot;0&quot;     </span></span>
<span id="cb221-7"><a href="#cb221-7" aria-hidden="true" tabindex="-1"></a><span class="co"># [4,] &quot;2&quot; &quot;0.081&quot; &quot;0.024&quot; </span></span>
<span id="cb221-8"><a href="#cb221-8" aria-hidden="true" tabindex="-1"></a><span class="co"># [5,] &quot;2&quot; &quot;0.043&quot; &quot;0.136&quot; </span></span>
<span id="cb221-9"><a href="#cb221-9" aria-hidden="true" tabindex="-1"></a><span class="co">#      condition                                    pred</span></span>
<span id="cb221-10"><a href="#cb221-10" aria-hidden="true" tabindex="-1"></a><span class="co"># [1,] &quot;X[,6] %in% c(&#39;L1&#39;) &amp; X[,11] %in% c(&#39;L1&#39;)&quot;  &quot;class_1&quot;</span></span>
<span id="cb221-11"><a href="#cb221-11" aria-hidden="true" tabindex="-1"></a><span class="co"># [2,] &quot;X[,4] %in% c(&#39;L1&#39;) &amp; X[,6] %in% c(&#39;L1&#39;)&quot;   &quot;class_1&quot;</span></span>
<span id="cb221-12"><a href="#cb221-12" aria-hidden="true" tabindex="-1"></a><span class="co"># [3,] &quot;X[,4] %in% c(&#39;L1&#39;) &amp; X[,6] %in% c(&#39;L1&#39;)&quot;   &quot;class_1&quot;</span></span>
<span id="cb221-13"><a href="#cb221-13" aria-hidden="true" tabindex="-1"></a><span class="co"># [4,] &quot;X[,3] %in% c(&#39;L3&#39;) &amp; X[,4] %in% c(&#39;L3&#39;)&quot;    &quot;class_0&quot;</span></span>
<span id="cb221-14"><a href="#cb221-14" aria-hidden="true" tabindex="-1"></a><span class="co"># [5,] &quot;X[,6] %in% c(&#39;L3&#39;) &amp; X[,7] %in% c(&#39;L3&#39;)&quot;    &quot;class_0&quot;</span></span></code></pre></div>
<p></p>
<p>We then prune the rules. Recall that we need to specify the threshold of <em>decay</em>. This could be done in R by setting the value of the parameter <code>maxDecay</code>. The statistics of the rules before and after pruning are shown in Figures <a href="#fig:f10-4">188</a>—<a href="#fig:f10-6">190</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-4"></span>
<img src="graphics/10_4.png" alt="Histogram of *lengths of the rules* before and after the pruning " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 188: Histogram of <em>lengths of the rules</em> before and after the pruning <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The R code below generates Figure <a href="#fig:f10-4">188</a>.</p>
<p></p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a>rules.pruned <span class="ot">&lt;-</span> <span class="fu">pruneRule</span>(rules, X, target, <span class="at">maxDecay =</span> <span class="fl">0.005</span>,</span>
<span id="cb222-2"><a href="#cb222-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">typeDecay =</span> <span class="dv">2</span>)</span>
<span id="cb222-3"><a href="#cb222-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-4"><a href="#cb222-4" aria-hidden="true" tabindex="-1"></a>length <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">original =</span> <span class="fu">as.numeric</span>(rules[, <span class="st">&quot;len&quot;</span>]),</span>
<span id="cb222-5"><a href="#cb222-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">pruned =</span> <span class="fu">as.numeric</span>(rules.pruned[,<span class="st">&quot;len&quot;</span>]))</span>
<span id="cb222-6"><a href="#cb222-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-7"><a href="#cb222-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Visualize the result</span></span>
<span id="cb222-8"><a href="#cb222-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">melt</span>(length), <span class="fu">aes</span>(value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb222-9"><a href="#cb222-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>,<span class="at">binwidth =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb222-10"><a href="#cb222-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Histogram of Lengths&quot;</span>) <span class="sc">+</span></span>
<span id="cb222-11"><a href="#cb222-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-5"></span>
<img src="graphics/10_5.png" alt="Histogram of *frequencies of the rules* before and after the pruning" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 189: Histogram of <em>frequencies of the rules</em> before and after the pruning<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The R code below generates Figure <a href="#fig:f10-5">189</a>.</p>
<p></p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="#cb223-1" aria-hidden="true" tabindex="-1"></a>frequency <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb223-2"><a href="#cb223-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">original =</span> <span class="fu">as.numeric</span>(rules[, <span class="st">&quot;freq&quot;</span>]),</span>
<span id="cb223-3"><a href="#cb223-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">pruned =</span> <span class="fu">as.numeric</span>(rules.pruned[,<span class="st">&quot;freq&quot;</span>]))</span>
<span id="cb223-4"><a href="#cb223-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">melt</span>(frequency), <span class="fu">aes</span>(value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb223-5"><a href="#cb223-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>,<span class="at">binwidth =</span> <span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb223-6"><a href="#cb223-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Histogram of Frequencies&quot;</span>) <span class="sc">+</span></span>
<span id="cb223-7"><a href="#cb223-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p></p>
<p>The R code below generates Figure <a href="#fig:f10-6">190</a>.</p>
<p></p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="#cb224-1" aria-hidden="true" tabindex="-1"></a>error <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">original =</span> <span class="fu">as.numeric</span>(rules[, <span class="st">&quot;err&quot;</span>]),</span>
<span id="cb224-2"><a href="#cb224-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">pruned =</span> <span class="fu">as.numeric</span>(rules.pruned[,<span class="st">&quot;err&quot;</span>]))</span>
<span id="cb224-3"><a href="#cb224-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb224-4"><a href="#cb224-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Visualize the result</span></span>
<span id="cb224-5"><a href="#cb224-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">melt</span>(error), <span class="fu">aes</span>(value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb224-6"><a href="#cb224-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>,<span class="at">binwidth =</span> <span class="fl">0.01</span>) <span class="sc">+</span></span>
<span id="cb224-7"><a href="#cb224-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Histogram of Errors&quot;</span>) <span class="sc">+</span></span>
<span id="cb224-8"><a href="#cb224-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-6"></span>
<img src="graphics/10_6.png" alt="Histogram of *errors of the rules* before and after the pruning" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 190: Histogram of <em>errors of the rules</em> before and after the pruning<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Figure <a href="#fig:f10-4">188</a> shows that the lengths of the rules are substantially reduced. For example, a majority of the original rules have a length of <span class="math inline">\(6\)</span>, while after pruning, only a slight percentage of the rules have a length of <span class="math inline">\(6\)</span>. Also, since rules are shortened, reduction of frequencies of the rules is also significant, as shown in Figure <a href="#fig:f10-5">189</a>. The errors are also reduced; e.g., Figure <a href="#fig:f10-6">190</a> shows the distribution of errors shifted to the left after pruning. Overall, the quality of the rules is improved with a proper choice of the pruning parameters.</p>
<p>The following R code prunes the rule set.</p>
<p></p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="#cb225-1" aria-hidden="true" tabindex="-1"></a>rules.selected <span class="ot">&lt;-</span> <span class="fu">selectRuleRRF</span>(rules.pruned, X, target)</span>
<span id="cb225-2"><a href="#cb225-2" aria-hidden="true" tabindex="-1"></a>rules.present <span class="ot">&lt;-</span> <span class="fu">presentRules</span>(rules.selected, <span class="fu">colnames</span>(X))</span>
<span id="cb225-3"><a href="#cb225-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb225-4"><a href="#cb225-4" aria-hidden="true" tabindex="-1"></a><span class="do">## See the specific contents of the selected rules</span></span>
<span id="cb225-5"><a href="#cb225-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cbind</span>(<span class="at">ID =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(rules.present), </span>
<span id="cb225-6"><a href="#cb225-6" aria-hidden="true" tabindex="-1"></a>            rules.present[, <span class="fu">c</span>(<span class="st">&quot;condition&quot;</span>, <span class="st">&quot;pred&quot;</span>)]))</span></code></pre></div>
<p></p>
<p>Finally, <span class="math inline">\(16\)</span> rules are selected. Their performances are shown below<label for="tufte-sn-273" class="margin-toggle sidenote-number">273</label><input type="checkbox" id="tufte-sn-273" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">273</span> Details of the rules could also be printed out in R.</span>.</p>
<p></p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">cbind</span>(<span class="at">ID =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(rules.present),</span>
<span id="cb226-2"><a href="#cb226-2" aria-hidden="true" tabindex="-1"></a>            rules.present[, <span class="fu">c</span>(<span class="st">&quot;len&quot;</span>, <span class="st">&quot;freq&quot;</span>, <span class="st">&quot;err&quot;</span>)]))</span>
<span id="cb226-3"><a href="#cb226-3" aria-hidden="true" tabindex="-1"></a><span class="do">##       ID   len freq    err</span></span>
<span id="cb226-4"><a href="#cb226-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1,] &quot;1&quot;  &quot;2&quot; &quot;0.279&quot; &quot;0.083&quot;</span></span>
<span id="cb226-5"><a href="#cb226-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  [2,] &quot;2&quot;  &quot;2&quot; &quot;0.279&quot; &quot;0.09&quot;</span></span>
<span id="cb226-6"><a href="#cb226-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  [3,] &quot;3&quot;  &quot;5&quot; &quot;0.029&quot; &quot;0.133&quot;</span></span>
<span id="cb226-7"><a href="#cb226-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [4,] &quot;4&quot;  &quot;3&quot; &quot;0.122&quot; &quot;0.016&quot;</span></span>
<span id="cb226-8"><a href="#cb226-8" aria-hidden="true" tabindex="-1"></a><span class="do">##  [5,] &quot;5&quot;  &quot;4&quot; &quot;0.031&quot; &quot;0.312&quot;</span></span>
<span id="cb226-9"><a href="#cb226-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  [6,] &quot;6&quot;  &quot;2&quot; &quot;0.207&quot; &quot;0.121&quot;</span></span>
<span id="cb226-10"><a href="#cb226-10" aria-hidden="true" tabindex="-1"></a><span class="do">##  [7,] &quot;7&quot;  &quot;3&quot; &quot;0.172&quot; &quot;0.124&quot;</span></span>
<span id="cb226-11"><a href="#cb226-11" aria-hidden="true" tabindex="-1"></a><span class="do">##  [8,] &quot;8&quot;  &quot;4&quot; &quot;0.06&quot;  &quot;0.194&quot;</span></span>
<span id="cb226-12"><a href="#cb226-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  [9,] &quot;9&quot;  &quot;5&quot; &quot;0.006&quot; &quot;0&quot;</span></span>
<span id="cb226-13"><a href="#cb226-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [10,] &quot;10&quot; &quot;4&quot; &quot;0.044&quot; &quot;0.13&quot;</span></span>
<span id="cb226-14"><a href="#cb226-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [11,] &quot;11&quot; &quot;5&quot; &quot;0.019&quot; &quot;0.2&quot;</span></span>
<span id="cb226-15"><a href="#cb226-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [12,] &quot;12&quot; &quot;3&quot; &quot;0.043&quot; &quot;0.182&quot;</span></span>
<span id="cb226-16"><a href="#cb226-16" aria-hidden="true" tabindex="-1"></a><span class="do">## [13,] &quot;13&quot; &quot;4&quot; &quot;0.037&quot; &quot;0.158&quot;</span></span>
<span id="cb226-17"><a href="#cb226-17" aria-hidden="true" tabindex="-1"></a><span class="do">## [14,] &quot;14&quot; &quot;3&quot; &quot;0.114&quot; &quot;0.203&quot;</span></span>
<span id="cb226-18"><a href="#cb226-18" aria-hidden="true" tabindex="-1"></a><span class="do">## [15,] &quot;15&quot; &quot;2&quot; &quot;0.234&quot; &quot;0.215&quot;</span></span>
<span id="cb226-19"><a href="#cb226-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [16,] &quot;16&quot; &quot;3&quot; &quot;0.282&quot; &quot;0.144&quot;</span></span></code></pre></div>
<p></p>
</div>
</div>
<div id="remarks-8" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="images-text-and-audio" class="section level3 unnumbered">
<h3>Images, text, and audio</h3>
<p>To learn more about deep learning we recommend readers to start with this book<label for="tufte-sn-274" class="margin-toggle sidenote-number">274</label><input type="checkbox" id="tufte-sn-274" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">274</span> Goodfellow, I., Bengio, Y., and Courville, A., <em>Deep Learning</em>. The MIT Press, 2016.</span>. There are many online lecture notes and tutorials that are informative. Here, it is worth mentioning that there has not been a unified theory about deep learning, and even the definition of what is a deep model is up to debate. This is good. If we look back at the developmental processes of many statistics and machine learning models, we may observe that some models were developed based on inspiration from theory and we often call these models too theoretical. These models usually wobble and stumble in their early years, gradually become mature and a proven approach, and eventually establish themselves as effective models in practice. Some other models, however, were developed ahead of theory, and theory only comes later to explain the model’s success. For deep learning, it is hard to say if it was theory that inspired the models, or it was the models that inspired theory. Many efforts are committed to give an overarching theory to explain the success of deep learning, at least in some special cases. Yet there has not been such an overarching theory about deep learning, only competing narratives<label for="tufte-sn-275" class="margin-toggle sidenote-number">275</label><input type="checkbox" id="tufte-sn-275" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">275</span> Interested readers may read this article by Colah, C., <em>Neural Networks, Types, and Functional Programming</em>, <a href="https://colah.github.io/posts/2015-09-NN-Types-FP/">https://colah.github.io/posts/2015-09-NN-Types-FP/</a>.</span>.</p>
<p>It is natural to wonder why we use a deep model. Can’t we just use a nondeep model? Readers may have been using nondeep models and found those nondeep models sufficient to solve problems in practice. Certainly we can just use nondeep models. There have been plenty of examples in practice that nondeep models were the best<label for="tufte-sn-276" class="margin-toggle sidenote-number">276</label><input type="checkbox" id="tufte-sn-276" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">276</span> A recent example: The Math of March Madness, <em>New York Times</em>, <a href="https://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy.html">https://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy.html</a>.</span>, only if we have the best variables (e.g., the <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_p\)</span>) that are sufficient to explain the “movement” of our target <span class="math inline">\(y\)</span>. The availability of high-quality and ready-to-use <span class="math inline">\(\boldsymbol{x}\)</span> is a precondition for the success of nondeep models. This precondition, however, is not always held in practice.</p>
<p>This is one reason why deep models can make a difference. Most nondeep models deal with a data structure that is Excel-sheet-like, i.e., they are stored or could be stored in an Excel spreadsheet. In many applications, particularly in recent years, the raw data is in free-form (sometimes it is also called unstructured data) such as images, text, and audio data. That means, to use the nondeep models for these applications, there should be a preprocessing/translational step that could extract the variables <span class="math inline">\(\boldsymbol{x}\)</span> from the raw data. It is notable that the translational process itself takes up a larger portion of effort of a data scientist in practice, and since the process involves multiple steps and layers, it naturally adopts a deep form, and further includes the nondeep model as its last layer to be part of its architecture.</p>
<p>It is no surpise then that mature practices of deep learning have been mainly found on unstructured data such as images, text, and audio data. From the raw data such as an X-ray image to the final outcome such as diagnosis of a disease, there are plenty of steps to transform the raw data into interpretable information. These steps put together creates a deep model. In other words, deep learning automates and optimizes this translational process.</p>
</div>
<div id="a-key-is-made-to-unlock-but-what-is-the-lock" class="section level3 unnumbered">
<h3>A key is made to unlock, but what is the lock?</h3>
<p>There is another magic dimension to deep learning. Researchers in different disciplines have created many basic forms of functions that could be used as building blocks to build larger functions. On the other hand, as model validation has been made automatic and data-driven<label for="tufte-sn-277" class="margin-toggle sidenote-number">277</label><input type="checkbox" id="tufte-sn-277" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">277</span> I.e., given a few candidate models, we no longer need to validate the models by their scientific implication but only check how well they fit the data. See Figure <a href="#fig:f5-flowchart">92</a> in <strong>Chapter 5</strong>.</span>, a deep model doesn’t demand interpretability or validity to be useful. In practice, your task is empirical: put together the architecture of a deep model, and if it obtains superior performance on data, it is a superior model. Now, if a complex problem in the real world is a sophisticated lock, deep learning’s real appeal is that we only need to spend effort in guessing at the key (i.e., the architecture of the deep neural network), but not necessarily in understanding the lock. And what makes it more convenient is that we can try every key we made (i.e., by fitting it with data) until the lock is opened. Is this a rational practice? There have been discussions around this topic and readers may be interested to look into this<label for="tufte-sn-278" class="margin-toggle sidenote-number">278</label><input type="checkbox" id="tufte-sn-278" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">278</span> See, Hutson, M., Has artifical intelligence become alchemy?, <em>Science</em>, 2018.</span>.</p>
</div>
<div id="decay-and-relative-decay" class="section level3 unnumbered">
<h3>Decay and relative decay</h3>
<p>To prune a rule, <code>inTrees</code> uses <em>leave-one-out</em> pruning, i.e., at each round, it removes one variable and checks how much error this removal will induce. We have introduced the concept <em>decay</em>. For the <span class="math inline">\(i^{th}\)</span> variable in the condition of a rule, its decay is defined as</p>
<p><span class="math display">\[decay_i = Err_{-i}-Err,\]</span></p>
<p>where <span class="math inline">\(Err\)</span> is the error of the original rule, <span class="math inline">\(Err_{-i}\)</span> is the error of the rule with the <span class="math inline">\(i^{th}\)</span> variable removed.</p>
<p>There is another definition of decay in <code>inTrees</code>, called <em>relative decay</em>, which is defined as</p>
<p><span class="math display">\[decay_i = \frac{Err_{-i}-Err}{\max(Err,s)},\]</span></p>
<p>where <span class="math inline">\(s\)</span> is a small positive constant (e.g., <span class="math inline">\(0.001\)</span>) that bounds the value of decay when <span class="math inline">\(Err\)</span> is zero or close to zero. An advantage of using relative decay is that one does not need to know the level of error of a dataset<label for="tufte-sn-279" class="margin-toggle sidenote-number">279</label><input type="checkbox" id="tufte-sn-279" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">279</span> For instance, for one dataset the error rate <span class="math inline">\(0.01\)</span> is probably insignificant, but for another <span class="math inline">\(0.01\)</span> is a big difference.</span>.</p>
</div>
</div>
<div id="exercises-8" class="section level2 unnumbered">
<h2>Exercises</h2>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-convolution-hw"></span>
<img src="graphics/10_convolution_hw.png" alt="A NN model with its parameters" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 191: A NN model with its parameters<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Complete the convolution operation as shown in Figure <a href="#fig:f10-convolution-hw">191</a>.</p></li>
<li><p> Use the <code>convolution()</code> function in R package <code>OpenImageR</code> to run the data in Q1.</p></li>
<li><p> Let’s try applying the convolution operation on a real image. For example, use the following R code to get the image shown in Figure <a href="#fig:f10-parrot">192</a>.</p></li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f10-parrot"></span>
<img src="graphics/10_parrot.png" alt="Data for Q3" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 192: Data for Q3<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- end{enumerate} --></p>
<p></p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="#cb227-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(EBImage)</span>
<span id="cb227-2"><a href="#cb227-2" aria-hidden="true" tabindex="-1"></a><span class="fu">readImage</span>(<span class="fu">system.file</span>(<span class="st">&quot;images&quot;</span>, <span class="st">&quot;sample-color.png&quot;</span>, </span>
<span id="cb227-3"><a href="#cb227-3" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> <span class="fu">readImage</span>(<span class="fu">system.file</span>(<span class="st">&quot;images&quot;</span>, <span class="st">&quot;sample-color.png&quot;</span>, </span>
<span id="cb227-4"><a href="#cb227-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">package=</span><span class="st">&quot;EBImage&quot;</span>))</span>
<span id="cb227-5"><a href="#cb227-5" aria-hidden="true" tabindex="-1"></a>grayimage<span class="ot">&lt;-</span><span class="fu">channel</span>(img,<span class="st">&quot;gray&quot;</span>)</span>
<span id="cb227-6"><a href="#cb227-6" aria-hidden="true" tabindex="-1"></a><span class="fu">display</span>(grayimage)</span></code></pre></div>
<p>
Use the <code>convolution()</code> function in R package <code>OpenImageR</code> to filter this image. You can use the high-pass Laplacian filter, that would be defined in R as</p>
<p></p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="#cb228-1" aria-hidden="true" tabindex="-1"></a>kernel <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">1</span>, <span class="at">nc=</span><span class="dv">3</span>, <span class="at">nr=</span><span class="dv">3</span>)</span>
<span id="cb228-2"><a href="#cb228-2" aria-hidden="true" tabindex="-1"></a>kernel[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">=</span> <span class="sc">-</span><span class="dv">8</span></span></code></pre></div>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<ul>
<li> Figure <a href="#fig:f10-xor-hw">193</a> shows a NN model with its parameters. Use this NN model to predict on the data points shown in Table <a href="#tab:t10-hw-nn">61</a>.</li>
</ul>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f10-xor-hw"></span>
<p class="caption marginnote shownote">
Figure 193: A NN model for Q4
</p>
<img src="graphics/10_xor_hw.png" alt="A NN model for Q4" width="80%"  />
</div>
<p></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t10-hw-nn">Table 61: </span>Test dataset for the NN model in Q4</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<ul>
<li> Use the <code>BostonHousing</code> dataset from the R package <code>mlbench</code> and select the variable <code>medv</code> as the outcome and all other numeric variables as predictors. Run the R pipeline for NN on it. Use <span class="math inline">\(10\)</span>-fold cross-validation to evaluate a NN model with <span class="math inline">\(2\)</span> hidden layers, while each layer has a number of nodes of your choice. Comment on the result.</li>
</ul>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<div id="conclusion" class="section level1 unnumbered">
<h1>Conclusion</h1>
<p>In his book<label for="tufte-sn-280" class="margin-toggle sidenote-number">280</label><input type="checkbox" id="tufte-sn-280" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">280</span> Shewhart, W.A., <em>Statistical Method from the Viewpoint of Quality Control</em>, The Department of Agriculture, 1939.</span> published in 1939—the era when statistics and mass production found each other—Walter A. Shewhart wrote, “<em>In any case … an observed sequence is or is not random can be verified only in the future</em>.” The sequence of observations shown in Figure <a href="#fig:f3-spcintro">36</a> in <strong>Chapter 3</strong> must have puzzled many in his time, and we know it was a time of chaos. The <em>data</em> without being looked at by a model is like a mountain in the Cascade Range that has not been named. Why, according to Ayn Rand, do the machines have their moral code“<em>Every part of the motors was an embodied answer to ‘Why?’ and ‘What for?’</em>”<label for="tufte-sn-281" class="margin-toggle sidenote-number">281</label><input type="checkbox" id="tufte-sn-281" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">281</span> Rand, A., <em>Atlas Shrugged</em>, Random House, 1957.</span>. Not surprisingly, but still conceptually challenging even nowadays for newcomers to accept his idea, Walter A. Shewhart created the concept of a control chart to frame the sequence of observations in Figure <a href="#fig:f3-spcintro">36</a>. His brilliant and bold idea is shown in Figure <a href="#fig:f3-spcintro2">37</a>.</p>
<p>We are still doing the same thing Shewhart had done to create a <em>frame</em> and look at the data <em>within</em> the frame, as Figure <a href="#fig:f9points">194</a> shows.</p>
<p>Sometimes, we even love the frame more than the picture. But any valuable effort in practice must meet a specific goal, and how the frame can help us reach the goal is the first step to exercise our knowledge, willpower, and judgment. So, without further rambling, we invite you to answer this question: how do you connect the <span class="math inline">\(9\)</span> points in Figure <a href="#fig:f9points">194</a> with <span class="math inline">\(4\)</span> lines?</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9points"></span>
<p class="caption marginnote shownote">
Figure 194: (Left) the <em>data</em>; (right) the <em>data</em> being looked at <em>within</em> a frame
</p>
<img src="graphics/9points.png" alt="(Left) the *data*; (right) the *data* being looked at *within* a frame" width="80%"  />
</div>
<p></p>
<p>Readers, congratulations. The answer is illustrated in Figure <a href="#fig:f9points-4lines">195</a>.</p>
<div style="page-break-after: always;"></div>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f9points-4lines"></span>
<p class="caption marginnote shownote">
Figure 195: The answer to connecting <span class="math inline">\(9\)</span> points with <span class="math inline">\(4\)</span> lines
</p>
<img src="graphics/9points_4lines.png" alt="The answer to connecting $9$ points with $4$ lines" width="80%"  />
</div>
<p></p>

</div>
<div id="appendix-a-brief-review-of-background-knowledge" class="section level1 unnumbered">
<h1>Appendix: A Brief Review of Background Knowledge</h1>
<p>Recall that in this book, we use lower case letters, e.g., <span class="math inline">\(x\)</span>, to represent scalars; bold face, lower case letters, e.g., <span class="math inline">\(\boldsymbol{x}\)</span>, to represent vectors; and bold face, upper case letters, e.g., <span class="math inline">\(\boldsymbol{X}\)</span>, to represent matrices.</p>
<div id="the-normal-distribution" class="section level2 unnumbered">
<h2>The Normal Distribution</h2>
<p>A distribution model characterizes the random behavior of a random variable. A random variable takes value from a predefined set, range, or a continuum, but not all values are taken with equal probabilities. How these probabilities are distributed is characterized by the distribution model. Before the computer age, for a distribution model to acquire a status of natural law it usually has an elegant geometric shape that comes with a delicate mathematical form, as many examples shown in Figure <a href="#fig:f2-errorlaws">4</a>. As we have computers now doing a lot of computation, a distribution could be just an empirical histogram that has not yet found its explicit mathematical form. Whether or not this empirical form of distribution would repeat itself as a natural law remains to be seen. In practice, a competitive edge could be gained before you find scientific explanation, as long as it works.</p>
<p>In this book we will not have extensive coverage of distribution models. We will focus on normal distribution; but other than that, everything we learn about the normal distribution is also to help us extend beyond it and establish the concept of distribution as an abstract one.</p>
<p>A random variable <span class="math inline">\(x\)</span> distributed as a normal distribution is denoted as</p>
<p><span class="math display">\[
x \sim N\left(\mu, \sigma^{2}\right).
\]</span></p>
<p>The normal distribution has the mathematical form</p>
<p><span class="math display">\[
N\left(\mu, \sigma^{2}\right) = \frac{1}{\sqrt{2\pi} \sigma}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2 }.
\]</span></p>
<p>If we multiply <span class="math inline">\(x\)</span> with a constant <span class="math inline">\(a\)</span>, then</p>
<p><span class="math display" id="eq:apx-normal">\[
ax \sim N\left(a\mu, a^2\sigma^{2}\right).
\tag{103}
\]</span></p>
<p>Extending the concept of distribution to <span class="math inline">\(p\)</span>-dimensional space, we have the multivariate normal distribution (MVN) of vector <span class="math inline">\(\boldsymbol{x}\)</span></p>
<p><span class="math display">\[
\boldsymbol{x} \sim MVN\left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right),
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{\mu}=\left[ \begin{array}{c}{\mu_{1}} \\ {\mu_{2}} \\ {\vdots} \\ {\mu_{p}}\end{array}\right], \text {         }  \boldsymbol{\Sigma}=\left[ \begin{array}{cccc} {\sigma^2_{1}} &amp; {\sigma_{12}} &amp; {\cdots} &amp; {\sigma_{1p}} \\ {\sigma_{21}} &amp; {\sigma^2_{2}} &amp; {\cdots} &amp; {\sigma_{2p}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {\sigma_{p1}} &amp; {\sigma_{p2}} &amp; {\cdots} &amp; {\sigma^2_{p }}\end{array}\right],
\]</span></p>
<p>and</p>
<p><span class="math display">\[
MVN\left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right) = \frac{1}{\sqrt{(2\pi)^p\det{\boldsymbol{\Sigma}}}}\exp\left({-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}}(\boldsymbol{x}-\boldsymbol{\mu})\right).
\]</span></p>
<p>To interpret the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, let’s look at an example where <span class="math inline">\(p=2\)</span>. Its covariance matrix is</p>
<p><span class="math display">\[
\boldsymbol{\Sigma_1} = \left[ \begin{array}{cc} {\sigma^2_1} &amp; {\sigma_{12}} \\ {\sigma_{21}} &amp; {\sigma_2^2}\end{array}\right].  
\]</span></p>
<p>The element <span class="math inline">\(\sigma^2_1\)</span> is the marginal variance of variable <span class="math inline">\(x_1\)</span>, <span class="math inline">\(\sigma^2_2\)</span> is the marginal variance of variable <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(\sigma_{12}\)</span> that equals to <span class="math inline">\(\sigma_{21}\)</span> is the covariance between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.<label for="tufte-sn-282" class="margin-toggle sidenote-number">282</label><input type="checkbox" id="tufte-sn-282" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">282</span> Covariance is closely related to the concept of correlation. For instance, denote the correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> as <span class="math inline">\(r\)</span>, which is defined as <span class="math inline">\(r = \frac{\sigma_{12}}{\sigma_1\sigma_2}\)</span>. It could be shown that <span class="math inline">\(r\)</span> takes value from <span class="math inline">\(-1\)</span> (i.e., perfect negative correlation) to <span class="math inline">\(1\)</span> (i.e., perfect positive correlation). Note that this correlation concept is built on the normal distribution, and the correlation <span class="math inline">\(0\)</span> doesn’t imply the two variables have no relationship in any possible form. Rather, it only implies that there is no <em>linear</em> relationship between the two.</span></p>
<p>Three examples of the covariance matrix are shown below</p>
<p><span class="math display">\[
\boldsymbol{\Sigma_1} = \left[ \begin{array}{cc} {1} &amp; {0} \\ {0} &amp; {1}\end{array}\right], \text {         }  \boldsymbol{\Sigma_2} = \left[ \begin{array}{cc} {1} &amp; {0.8} \\ {0.8} &amp; {1}\end{array}\right], \text {         }  \boldsymbol{\Sigma_3} = \left[ \begin{array}{cc} {1} &amp; {1} \\ {1} &amp; {1}\end{array}\right].  
\]</span></p>
<p>The corresponding contour plots of the three bivariate normal distributions are shown in Figure <a href="#fig:fapx-binormal">196</a>.</p>
<p>If we add <span class="math inline">\(\boldsymbol{x}\)</span> (i.e., <span class="math inline">\(\boldsymbol{x} \in R^{p \times 1}\)</span>) with a constant vector <span class="math inline">\(\boldsymbol{a}\)</span> (i.e., <span class="math inline">\(\boldsymbol{a} \in R^{p \times 1}\)</span>), then</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:fapx-binormal"></span>
<p class="caption marginnote shownote">
Figure 196: The contour plots of the three bivariate normal distributions
</p>
<img src="graphics/apx_binormal.png" alt="The contour plots of the three bivariate normal distributions" width="80%"  />
</div>
<p></p>
<p><span class="math display">\[
\boldsymbol{a + x} \sim MVN\left(\boldsymbol{a + \mu}, \boldsymbol{\Sigma}\right).
\]</span></p>
<p>If we multiply <span class="math inline">\(\boldsymbol{x}\)</span> (i.e., <span class="math inline">\(\boldsymbol{x} \in R^{p \times 1}\)</span>) with a constant <span class="math inline">\(\boldsymbol{a}\)</span> (i.e., <span class="math inline">\(\boldsymbol{a} \in R^{p \times 1}\)</span>), then</p>
<p><span class="math display">\[
\boldsymbol{a^Tx} \sim MVN\left(\boldsymbol{a^T\mu}, \boldsymbol{a^T\Sigma a}\right).
\]</span></p>
</div>
<div id="matrix-operations" class="section level2 unnumbered">
<h2>Matrix Operations</h2>
<p>A matrix is a basic structure in data analytics that organizes data in a rectangular array, e.g., a matrix <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span> with <span class="math inline">\(p\)</span> rows and <span class="math inline">\(q\)</span> columns is</p>
<p><span class="math display">\[
\boldsymbol{X}=\left[ \begin{array}{cccc} {x_{11}} &amp; {x_{12}} &amp; {\cdots} &amp; {x_{1q}} \\ {x_{21}} &amp; {x_{22}} &amp; {\cdots} &amp; {x_{2q}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {x_{p1}} &amp; {x_{p2}} &amp; {\cdots} &amp; {x_{pq}}\end{array}\right].
\]</span></p>
<p><em>Matrix transposition.</em> A matrix <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span> could be transposed into a matrix <span class="math inline">\(\boldsymbol{X}^T \in R^{q \times p}\)</span>, i.e.,</p>
<p><span class="math display">\[
\boldsymbol{X}^T=\left[ \begin{array}{cccc} {x_{11}} &amp; {x_{21}} &amp; {\cdots} &amp; {x_{q1}} \\ {x_{12}} &amp; {x_{22}} &amp; {\cdots} &amp; {x_{q2}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {x_{1p}} &amp; {x_{2p}} &amp; {\cdots} &amp; {x_{qp}}\end{array}\right].
\]</span></p>
<p><em>Matrix addition.</em> Two matrices of the same dimensions could be added together entrywise, i.e., <span class="math inline">\(\boldsymbol{X} + \boldsymbol{Y}\)</span> is defined as</p>
<p><span class="math display">\[
\boldsymbol{X + Y}=\left[ \begin{array}{cccc} {x_{11}+y_{11}} &amp; {x_{12}+y_{12}} &amp; {\cdots} &amp; {x_{1q}+y_{1q}} \\ {x_{21}+y_{21}} &amp; {x_{22}+y_{22}} &amp; {\cdots} &amp; {x_{2q}+y_{2q}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {x_{p1}+y_{p1}} &amp; {x_{p2}+y_{p2}} &amp; {\cdots} &amp; {x_{pq}+y_{pq}}\end{array}\right].
\]</span></p>
<p><em>Scalar multiplication:</em> The product of a constant <span class="math inline">\(c\)</span> and a matrix <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span> is computed by multiplying every entry of <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span> by <span class="math inline">\(c\)</span>, i.e.,</p>
<p><span class="math display">\[
c\boldsymbol{X}=\left[ \begin{array}{cccc} {cx_{11}} &amp; {cx_{12}} &amp; {\cdots} &amp; {cx_{1q}} \\ {cx_{21}} &amp; {cx_{22}} &amp; {\cdots} &amp; {cx_{2q}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {cx_{p1}} &amp; {cx_{p2}} &amp; {\cdots} &amp; {cx_{pq}}\end{array}\right].
\]</span></p>
<p><em>Matrix multiplication.</em> Two matrices could be multiplied if the number of columns of the left matrix is the same as the number of rows of the right matrix, i.e., for <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span>, it could be multiplied with any matrix that has <span class="math inline">\(q\)</span> rows. Let’s say we have two matrices, <span class="math inline">\(\boldsymbol{X} \in R^{2 \times 3}\)</span> and <span class="math inline">\(\boldsymbol{Y} \in R^{3 \times 2}\)</span>, the multiplication <span class="math inline">\(\boldsymbol{XY}\)</span> is a matrix <span class="math inline">\(\in R^{2 \times 2}\)</span>, where</p>
<p><span class="math display">\[
\boldsymbol{XY}=\left[ \begin{array}{cccc} {x_{11}y_{11}+ x_{12}y_{21}+ x_{13}y_{31}}  &amp; {x_{11}y_{12}+ x_{12}y_{22}+ x_{13}y_{32}} \\ {x_{21}y_{11}+ x_{22}y_{21}+ x_{23}y_{31}} &amp; {x_{21}y_{12}+ x_{22}y_{22}+ x_{23}y_{32}}\end{array}\right].
\]</span></p>
<p><em>Matrix derivative.</em> Matrix derivative is a rich category that includes many situations. Readers may find a comprehensive coverage in a few books<label for="tufte-sn-283" class="margin-toggle sidenote-number">283</label><input type="checkbox" id="tufte-sn-283" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">283</span> Harville, D.A., <em>Matrix Algebra From a Statistician’s Perspective</em>, Springer, 2000.</span> or find a quick reference in online resources<label for="tufte-sn-284" class="margin-toggle sidenote-number">284</label><input type="checkbox" id="tufte-sn-284" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">284</span> Petersen, K.B. and
Pedersen, M.S., <em>The Matrix Cookbook</em>, online document (<a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf</a>).</span>. Here, we mention a few examples that are related topics in this book.</p>
<p>Denote that <span class="math inline">\(y=f(\boldsymbol{X})\)</span> is a scalar function of the matrix <span class="math inline">\(\boldsymbol{X} \in R^{p \times q}\)</span>. Then derivative of <span class="math inline">\(y\)</span> with respect to the matrix <span class="math inline">\(\boldsymbol{X}\)</span> is given by</p>
<p><span class="math display">\[
\frac{\partial y }{\partial \boldsymbol{X}} = \left[ \begin{array}{cccc} \frac{\partial y }{\partial x_{11}} &amp; \frac{\partial y }{\partial x_{12}} &amp; {\cdots} &amp; \frac{\partial y }{\partial x_{1q}} \\ \frac{\partial y }{\partial x_{21}} &amp; \frac{\partial y }{\partial x_{22}} &amp; {\cdots} &amp; \frac{\partial y }{\partial x_{2q}} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ \frac{\partial y }{\partial x_{p1}} &amp; \frac{\partial y }{\partial x_{p2}} &amp; {\cdots} &amp; \frac{\partial y }{\partial x_{pq}}\end{array}\right].
\]</span></p>
<p>Based on this definition, we can derive that</p>
<p><span class="math display">\[
\frac{\partial \boldsymbol{a}^T\boldsymbol{x} }{\partial \boldsymbol{x}} = \boldsymbol{a};
\]</span></p>
<p><span class="math display">\[
\frac{\partial \boldsymbol{x}^T\boldsymbol{B}\boldsymbol{x} }{\partial \boldsymbol{x}} = (\boldsymbol{B} + \boldsymbol{B}^T)\boldsymbol{x};
\]</span></p>
<p><span class="math display">\[
\frac{\partial \boldsymbol{(x-a)}^T\boldsymbol{B}\boldsymbol{(x-a)} }{\partial \boldsymbol{x}} = 2\boldsymbol{B}\boldsymbol{(x-a)};
\]</span></p>
<p><span class="math display">\[
\frac{\partial \boldsymbol{(Ax+b)}^T\boldsymbol{W}\boldsymbol{(Cx+d)} }{\partial \boldsymbol{x}} = \boldsymbol{A}^T\boldsymbol{W}\boldsymbol{(Cx+d)} + \boldsymbol{C}^T\boldsymbol{W}\boldsymbol{(Ax+b)}.
\]</span></p>
<p><em>Matrix norm.</em> The <span class="math inline">\(L_1\)</span> norm of a vector <span class="math inline">\(\boldsymbol{x}\)</span> is defined as</p>
<p><span class="math display">\[
\lVert \boldsymbol{x} \rVert_1 = \sum_{i=1}^p \lvert x_i \rvert.
\]</span></p>
<p>The <span class="math inline">\(L_2\)</span> norm of a vector <span class="math inline">\(\boldsymbol{x}\)</span> is defined as</p>
<p><span class="math display">\[
\lVert \boldsymbol{x} \rVert^2_2 = \sum_{i=1}^p x_i^2.
\]</span></p>
</div>
<div id="optimization" class="section level2 unnumbered">
<h2>Optimization</h2>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:fapx-fdt-normal"></span>
<img src="graphics/apx_fdt_normal.png" alt="Illustration of the application of the **First Derivative Test** on the density function of a normal distribution to identify the location where the probability density is maximal." width="250px"  />
<!--
<p class="caption marginnote">-->Figure 197: Illustration of the application of the <strong>First Derivative Test</strong> on the density function of a normal distribution to identify the location where the probability density is maximal.<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The <strong>First Derivative Test</strong>, illustrated in Figure <a href="#fig:f2-1stderivativetest">8</a> in <strong>Chapter 2</strong>, is widely used in statistics and machine learning to find optimal solutions of a model formulation. Given a function <span class="math inline">\(f(x)\)</span>, the First Derivative Test finds the location <span class="math inline">\(x^*\)</span> that leads to <span class="math inline">\(\frac{\partial f(x) }{\partial x} = 0\)</span>, i.e., denoted as <span class="math inline">\(f&#39;(x^*)=0\)</span>. For instance, we can apply the First Derivative Test on the density function of a normal distribution to identify the location where the probability density is maximal<label for="tufte-sn-285" class="margin-toggle sidenote-number">285</label><input type="checkbox" id="tufte-sn-285" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">285</span> That is, the mean <span class="math inline">\(\mu\)</span>.</span>. An illustration of this application is shown in Figure <a href="#fig:fapx-fdt-normal">197</a>.</p>
<p>The locations that are identified by the First Derivative Test may not be the global optimal points, as shown in Figure <a href="#fig:f2-localoptimal">19</a> in <strong>Chapter 2</strong>. On the other hand, it is relatively easy to use and is found to be quite effective in practice. Gradient-based optimization algorithms have been built on this concept to iteratively search for the locations where the first derivative could be set to zero. One such example is shown in Figure <a href="#fig:f3-RWalgor">29</a> in <strong>Chapter 3</strong>.</p>

</div>
</div>
<p style="text-align: center;">
</p>
</div>
</div>



</body>
</html>
