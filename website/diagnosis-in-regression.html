<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Diagnosis in regression | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Diagnosis in regression | Data Analytics">

<title>Diagnosis in regression | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="diagnosis-in-regression" class="section level2 unnumbered">
<h2>Diagnosis in regression</h2>
<div id="residual-analysis" class="section level3 unnumbered">
<h3>Residual analysis</h3>
<p>The R package <code>ggfortify</code> provides a nice bundle that includes the <strong>residual analysis, cook’s distance, leverage</strong>, and <strong>Q-Q plot</strong>.</p>
<p>Let’s use the final regression model we identified in <strong>Chapter 2</strong> for an example.</p>
<p></p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="diagnosis-in-regression.html#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb127-2"><a href="diagnosis-in-regression.html#cb127-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb127-3"><a href="diagnosis-in-regression.html#cb127-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb127-4"><a href="diagnosis-in-regression.html#cb127-4" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb127-5"><a href="diagnosis-in-regression.html#cb127-5" aria-hidden="true" tabindex="-1"></a>AD<span class="sc">$</span>ID <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(AD)[<span class="dv">1</span>])</span>
<span id="cb127-6"><a href="diagnosis-in-regression.html#cb127-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span>
<span id="cb127-7"><a href="diagnosis-in-regression.html#cb127-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a full-scale model</span></span>
<span id="cb127-8"><a href="diagnosis-in-regression.html#cb127-8" aria-hidden="true" tabindex="-1"></a>AD_full <span class="ot">&lt;-</span> AD[,<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">17</span>)]</span>
<span id="cb127-9"><a href="diagnosis-in-regression.html#cb127-9" aria-hidden="true" tabindex="-1"></a>lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> ., <span class="at">data =</span> AD_full)</span>
<span id="cb127-10"><a href="diagnosis-in-regression.html#cb127-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD)</span>
<span id="cb127-11"><a href="diagnosis-in-regression.html#cb127-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic model selection</span></span>
<span id="cb127-12"><a href="diagnosis-in-regression.html#cb127-12" aria-hidden="true" tabindex="-1"></a>lm.AD.F <span class="ot">&lt;-</span> <span class="fu">step</span>(lm.AD, <span class="at">direction=</span><span class="st">&quot;backward&quot;</span>, <span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<p></p>
<p>Details of the model are shown below.</p>
<p></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="diagnosis-in-regression.html#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="do">## MMSCORE ~ PTEDUCAT + FDG + AV45 + HippoNV + rs744373 + rs610932</span></span>
<span id="cb128-2"><a href="diagnosis-in-regression.html#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="do">##     + rs3764650 + rs3865444</span></span>
<span id="cb128-3"><a href="diagnosis-in-regression.html#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb128-4"><a href="diagnosis-in-regression.html#cb128-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    </span></span>
<span id="cb128-5"><a href="diagnosis-in-regression.html#cb128-5" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;                   1537.5 581.47                      </span></span>
<span id="cb128-6"><a href="diagnosis-in-regression.html#cb128-6" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3764650  1     7.513 1545.0 581.99  2.4824  0.115750    </span></span>
<span id="cb128-7"><a href="diagnosis-in-regression.html#cb128-7" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs744373   1    12.119 1549.6 583.53  4.0040  0.045924 *  </span></span>
<span id="cb128-8"><a href="diagnosis-in-regression.html#cb128-8" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs610932   1    14.052 1551.6 584.17  4.6429  0.031652 *  </span></span>
<span id="cb128-9"><a href="diagnosis-in-regression.html#cb128-9" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3865444  1    21.371 1558.9 586.61  7.0612  0.008125 ** </span></span>
<span id="cb128-10"><a href="diagnosis-in-regression.html#cb128-10" aria-hidden="true" tabindex="-1"></a><span class="do">## - AV45       1    50.118 1587.6 596.05 16.5591 5.467e-05 ***</span></span>
<span id="cb128-11"><a href="diagnosis-in-regression.html#cb128-11" aria-hidden="true" tabindex="-1"></a><span class="do">## - PTEDUCAT   1    82.478 1620.0 606.49 27.2507 2.610e-07 ***</span></span>
<span id="cb128-12"><a href="diagnosis-in-regression.html#cb128-12" aria-hidden="true" tabindex="-1"></a><span class="do">## - HippoNV    1   118.599 1656.1 617.89 39.1854 8.206e-10 ***</span></span>
<span id="cb128-13"><a href="diagnosis-in-regression.html#cb128-13" aria-hidden="true" tabindex="-1"></a><span class="do">## - FDG        1   143.852 1681.4 625.71 47.5288 1.614e-11 ***</span></span>
<span id="cb128-14"><a href="diagnosis-in-regression.html#cb128-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb128-15"><a href="diagnosis-in-regression.html#cb128-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span></code></pre></div>
<p></p>
<p>We use the <code>ggfortify</code> to produce <span class="math inline">\(6\)</span> diagnostic figures as shown in Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a>.</p>
<p></p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="diagnosis-in-regression.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct diagnostics of the model</span></span>
<span id="cb129-2"><a href="diagnosis-in-regression.html#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;ggfortify&quot;)</span></span>
<span id="cb129-3"><a href="diagnosis-in-regression.html#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggfortify&quot;</span>)</span>
<span id="cb129-4"><a href="diagnosis-in-regression.html#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.AD.F, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure"><span id="fig:f6-1"></span>
<p class="caption marginnote shownote">
Figure 99: Diagnostic figures of regression model on the AD dataset
</p>
<img src="graphics/6_1.png" alt="Diagnostic figures of regression model on the AD dataset" width="100%"  />
</div>
<p></p>
<p>The following is what we observe from Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a>.</p>
<p><!-- begin{enumerate} --></p>
<ul>
<li><p> Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a> (upper left). This is the scatterplot of the residuals versus fitted values of the outcome variable. As we have discussed in Figure <a href="overview-4.html#fig:f6-3residuals">97</a>, this scatterplot is supposed to show purely random distributions of the dots. Here, we notice two abnormalities: (1) there is a relationship between the residuals and fitted values; and (2) there are unusual parallel lines<label for="tufte-sn-138" class="margin-toggle sidenote-number">138</label><input type="checkbox" id="tufte-sn-138" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">138</span> This is often observed if the outcome variable takes integer values.</span>. These abnormalities have a few implications: (1) the linear model <em>underfits</em> the data, so a nonlinear model is needed; (2) we have assumed that the data points are independent with each other, now this assumption needs to be checked; and (3) we have assumed <em>homoscedasticity</em><label for="tufte-sn-139" class="margin-toggle sidenote-number">139</label><input type="checkbox" id="tufte-sn-139" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">139</span> In <strong>Chapter 2</strong>, we assume that <span class="math inline">\(\epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right)\)</span>. It assumes the errors have the same variance, <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>, for all data points.</span> of the variance of the errors. This is another assumption that needs to be checked<label for="tufte-sn-140" class="margin-toggle sidenote-number">140</label><input type="checkbox" id="tufte-sn-140" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">140</span> To build nonlinear regression model or conditional variance regression model, see <strong>Chapter 9</strong>.</span>.</p></li>
<li><p> Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a> (upper right). The <strong>Q-Q plot</strong> checks the normality assumption of the errors. The <span class="math inline">\(45^{\circ}\)</span> line is a fixed <em>baseline</em>, while the dots correspond to the data points. If the normality assumption is met, the dots should align with the line. Here, we see mild departure of the data from the normality assumption. And some particular data points such as the data points <span class="math inline">\(282\)</span> and <span class="math inline">\(256\)</span> are labelled since they are outstanding<label for="tufte-sn-141" class="margin-toggle sidenote-number">141</label><input type="checkbox" id="tufte-sn-141" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">141</span> Are those points outliers? The Q-Q plot provides no conclusive evidence. It only suggests.</span>.</p></li>
<li><p> Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a> (middle left). This is a transformation of Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a> (upper left). Diagnostic tools are usually <em>opportunistic</em> approaches, i.e., what you see is what you get; if nothing particular is observed, it doesn’t mean there is no anomaly in the data. Changing perspectives is a common practice in model diagnosis.</p></li>
<li><p> Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a> (middle right). The <strong>Cook’s distance</strong> identifies influential data points that have <em>larger than average</em> influence on the parameter estimation of the model. For a data point <span class="math inline">\(\boldsymbol{x}_i\)</span>, its Cook’s distance <span class="math inline">\(D_{i}\)</span> is defined as the sum of all the changes in the regression model when <span class="math inline">\(\boldsymbol{x}_i\)</span> is removed from the training data. There is a closed-form formula<label for="tufte-sn-142" class="margin-toggle sidenote-number">142</label><input type="checkbox" id="tufte-sn-142" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">142</span> Cook, R.D., <em>Influential Observations in Linear Regression</em>, Journal of the American Statistical Association, Volume 74, Number 365, Pages 169-174, 1979.</span> to compute <span class="math inline">\({D_{i}, \text{ for } j=1,2,\dots,p}\)</span>, based on the <em>least squares estimator</em> of the regression parameters.</p></li>
<li><p> Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a> (lower left). The <strong>leverage</strong> of a data point, on the other hand, shows the influence of the data point in another way. The leverage of a data point is defined as <span class="math inline">\(\frac{\partial \hat{y}_{i}}{\partial y_{i}}\)</span>. This reflects how sensitively the prediction <span class="math inline">\(\hat{y}_{i}\)</span> is influenced by <span class="math inline">\(y_{i}\)</span>. What data point will have a larger leverage value? For those surrounded by many closeby data points, their leverages won’t be large: the impact of a data point’s removal in a dense neighborhood is limited, given many other similar data points nearby. It is the data points in sparsely occupied neighborhoods that have large leverages. These data points could either be outliers that severely deviate from the linear trend represented by the majority of the data points, or could be valuable data points that align with the linear trend but lack neighbor data points. Thus, a data point that is influential doesn’t necessarily imply it is an outlier, as shown in Figure <a href="diagnosis-in-regression.html#fig:f6-outlier-infl">100</a>. When a data point has a larger leverage value, in-depth examination of the data point is needed to determine which case it is.</p></li>
<li><p> Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a> (lower right). This is another form of showing the information that is presented in Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a> (middle right) and (lower left).</p></li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-outlier-infl"></span>
<img src="graphics/6_outlier_influ.png" alt="Outliers v.s. influential data points" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 100: Outliers v.s. influential data points<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- end{enumerate} --></p>
<p><em>A Simulation Experiment.</em> We simulate a dataset while all the assumptions of the linear regression model are met. The model is</p>
<p><span class="math display">\[ 
y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\varepsilon, \varepsilon \sim N(0,1).
\]</span></p>
<p>We simulate <span class="math inline">\(100\)</span> samples from this model.</p>
<p></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="diagnosis-in-regression.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For comparison, let&#39;s simulate data </span></span>
<span id="cb130-2"><a href="diagnosis-in-regression.html#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="co"># from a model that fits the assumptions</span></span>
<span id="cb130-3"><a href="diagnosis-in-regression.html#cb130-3" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb130-4"><a href="diagnosis-in-regression.html#cb130-4" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb130-5"><a href="diagnosis-in-regression.html#cb130-5" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb130-6"><a href="diagnosis-in-regression.html#cb130-6" aria-hidden="true" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb130-7"><a href="diagnosis-in-regression.html#cb130-7" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> beta1 <span class="sc">*</span> x1 <span class="sc">+</span> beta2 <span class="sc">*</span> x2</span>
<span id="cb130-8"><a href="diagnosis-in-regression.html#cb130-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, mu, <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p>We fit the data using linear regression model.</p>
<p></p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="diagnosis-in-regression.html#cb131-1" aria-hidden="true" tabindex="-1"></a>lm.XY <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(y,x1,x2))</span>
<span id="cb131-2"><a href="diagnosis-in-regression.html#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.XY)</span></code></pre></div>
<p></p>
<p>The fitted model fairly reflects the underlying model.</p>
<p></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="diagnosis-in-regression.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-2"><a href="diagnosis-in-regression.html#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb132-3"><a href="diagnosis-in-regression.html#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data.frame(y, x1, x2))</span></span>
<span id="cb132-4"><a href="diagnosis-in-regression.html#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-5"><a href="diagnosis-in-regression.html#cb132-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb132-6"><a href="diagnosis-in-regression.html#cb132-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb132-7"><a href="diagnosis-in-regression.html#cb132-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.6475 -0.6630 -0.1171  0.7986  2.5074 </span></span>
<span id="cb132-8"><a href="diagnosis-in-regression.html#cb132-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-9"><a href="diagnosis-in-regression.html#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb132-10"><a href="diagnosis-in-regression.html#cb132-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb132-11"><a href="diagnosis-in-regression.html#cb132-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)   0.0366     0.1089   0.336    0.738    </span></span>
<span id="cb132-12"><a href="diagnosis-in-regression.html#cb132-12" aria-hidden="true" tabindex="-1"></a><span class="do">## x1            0.9923     0.1124   8.825 4.60e-14 ***</span></span>
<span id="cb132-13"><a href="diagnosis-in-regression.html#cb132-13" aria-hidden="true" tabindex="-1"></a><span class="do">## x2            0.9284     0.1159   8.011 2.55e-12 ***</span></span>
<span id="cb132-14"><a href="diagnosis-in-regression.html#cb132-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb132-15"><a href="diagnosis-in-regression.html#cb132-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb132-16"><a href="diagnosis-in-regression.html#cb132-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-17"><a href="diagnosis-in-regression.html#cb132-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.088 on 97 degrees of freedom</span></span>
<span id="cb132-18"><a href="diagnosis-in-regression.html#cb132-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6225, Adjusted R-squared:  0.6147 </span></span>
<span id="cb132-19"><a href="diagnosis-in-regression.html#cb132-19" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 79.98 on 2 and 97 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="diagnosis-in-regression.html#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.XY, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure"><span id="fig:f6-2"></span>
<p class="caption marginnote shownote">
Figure 101: Diagnostic figures of regression model on a simulation dataset
</p>
<img src="graphics/6_2.png" alt="Diagnostic figures of regression model on a simulation dataset" width="100%"  />
</div>
<p></p>
<p>Then, we generate the diagnostic figures as shown in Figure <a href="diagnosis-in-regression.html#fig:f6-2">101</a>. Now Figure <a href="diagnosis-in-regression.html#fig:f6-2">101</a> provides a contrast of Figure <a href="diagnosis-in-regression.html#fig:f6-1">99</a>. For example, in Figure <a href="diagnosis-in-regression.html#fig:f6-2">101</a> (upper left), we don’t see a nonrandom statistical pattern. The relationship between the residual and fitted values seems to be null. From the <em>QQ-plot</em>, we see that the normality assumption is held well. On the other hand, from the <em>Cook’s distance</em> and the <em>leverage</em>, some data points are observed to be outstanding, which are labeled. As we simulated the data following the assumptions of the linear regression model, this experiment shows that it is normal to expect a few data points to show outstanding <em>Cook’s distance</em> and <em>leverage</em> values.</p>
<p></p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="diagnosis-in-regression.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct diagnostics of the model</span></span>
<span id="cb134-2"><a href="diagnosis-in-regression.html#cb134-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggfortify&quot;</span>)</span>
<span id="cb134-3"><a href="diagnosis-in-regression.html#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.XY, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
</div>
<div id="multicollinearity" class="section level3 unnumbered">
<h3>Multicollinearity</h3>
<p>Multicollinearity refers to the phenomenon that there is <em>a high correlation among the predictor variables</em>. This causes a serious problem for linear regression models. We can do a simple analysis. Consider a linear system shown below</p>
<p><span class="math display">\[ 
\begin{array}{c}{y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\cdots+\beta_{p} x_{p}+\varepsilon_y}, \\ {\varepsilon_y \sim N\left(0, \sigma_{\varepsilon_y}^{2}\right)}. \end{array}
\]</span></p>
<p>This looks like a regular linear regression model. However, here we further have</p>
<p><span class="math display">\[ 
\begin{array}{c}{x_{1}=2 x_{2}+\epsilon_x}, \\ {\epsilon_x \sim N\left(0, \sigma_{\varepsilon_x}^{2}\right).}\end{array}
\]</span></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-multilinear"></span>
<img src="graphics/6_multilinear.png" alt="The *data-generating mechanism* of a system that suffers from *multicollinearity*" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 102: The <em>data-generating mechanism</em> of a system that suffers from <em>multicollinearity</em><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>This <em>data-generating mechanism</em> is shown in Figure <a href="diagnosis-in-regression.html#fig:f6-multilinear">102</a>. It is a system that suffers from <em>multicollinearity</em>, i.e., if we apply a linear regression model on this system, the following models are <em>both</em> true models</p>
<p><span class="math display">\[ 
\begin{array}{c}{y=\beta_{0}+\left(2 \beta_{1}+\beta_{2}\right) x_{2}+\beta_{3} x_{3} \ldots+\beta_{p} x_{p}} \\ {y=\beta_{0}+\left(\beta_{1}+0.5 \beta_{2}\right) x_{1}+\beta_{3} x_{3}+\cdots+\beta_{p} x_{p}}\end{array}.
\]</span></p>
<p>The problem of multicollinearity results from an inherent ambiguity of the models that could be taken as faithful representation of the <em>data-generating mechanism</em>. If the <em>true</em> model is ambiguous, it is expected that an estimated model suffers from this problem as well.</p>
<p>There are some methods that we can use to diagnose <em>multicollinearity</em>. For instance, we may visualize the correlations among the predictor variables using the R package <code>corrplot</code>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-3"></span>
<img src="graphics/6_3.png" alt="Correlations of the predictors in the regression model of  `MMSCORE` " width="250px"  />
<!--
<p class="caption marginnote">-->Figure 103: Correlations of the predictors in the regression model of <code>MMSCORE</code> <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="diagnosis-in-regression.html#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the covariance matrix of the regression parameters</span></span>
<span id="cb135-2"><a href="diagnosis-in-regression.html#cb135-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> <span class="fu">vcov</span>(lm.AD.F)</span>
<span id="cb135-3"><a href="diagnosis-in-regression.html#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the correlation matrix of the estimated regression </span></span>
<span id="cb135-4"><a href="diagnosis-in-regression.html#cb135-4" aria-hidden="true" tabindex="-1"></a><span class="co"># parameters</span></span>
<span id="cb135-5"><a href="diagnosis-in-regression.html#cb135-5" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;corrplot&quot;)</span></span>
<span id="cb135-6"><a href="diagnosis-in-regression.html#cb135-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb135-7"><a href="diagnosis-in-regression.html#cb135-7" aria-hidden="true" tabindex="-1"></a><span class="fu">corrplot</span>(<span class="fu">cov2cor</span>(Sigma), <span class="at">method=</span><span class="st">&quot;ellipse&quot;</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="diagnosis-in-regression.html#fig:f6-3">103</a> shows that there are significant correlations between the variables, <code>FDG</code>, <code>AV45</code>, and <code>HippoNV</code>, indicating a concern for multicollinearity. On the other hand, it seems that the correlations are moderate, and not all the variables are strongly correlated with each other.</p>
<p>It is of interest to see why the strong correlations among predictor variables cause problems in the <em>least squares</em> estimator of the regression coefficients. Recall that <span class="math inline">\(\widehat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y}\)</span>. If there are strong correlations among predictor variables, the matrix <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> is ill-conditioned, i.e., small changes on <span class="math inline">\(\boldsymbol{X}\)</span> result in large and unpredictable changes on the inverse matrix <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span>, which further causes instability of the parameter estimation in <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.<label for="tufte-sn-143" class="margin-toggle sidenote-number">143</label><input type="checkbox" id="tufte-sn-143" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">143</span> To overcome multicollinearity in linear regression, the <em>Principal Component Analysis</em> discussed in <strong>Chapter 8</strong> is useful.</span></p>
</div>
</div>
<p style="text-align: center;">
<a href="overview-4.html"><button class="btn btn-default">Previous</button></a>
<a href="diagnosis-in-random-forests.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
