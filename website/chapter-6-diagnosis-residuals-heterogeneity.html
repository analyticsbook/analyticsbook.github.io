<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Chapter 6. Diagnosis: Residuals &amp; Heterogeneity | book_migrate.utf8" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Book for analalytics" />




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Book for analalytics">

<title>Chapter 6. Diagnosis: Residuals &amp; Heterogeneity | book_migrate.utf8</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#cover">Cover</a></li>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="preface.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="chapter-6.-diagnosis-residuals-heterogeneity" class="section level1 unnumbered">
<h1>Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</h1>
<div id="overview-4" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 6 is about <em>Diagnosis</em>. Diagnosis, in one sense, is to see if the assumptions of the model match the empirical characteristics of the data. For example, the t-test of linear regression model builds on the normality assumption of the errors. If this assumption is not met by the data, the result of the t-test is concerned. Departure from assumptions doesn’t always mean that the model is not useful<label for="tufte-sn-138" class="margin-toggle sidenote-number">138</label><input type="checkbox" id="tufte-sn-138" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">138</span> <em>“All models are wrong, some are useful.”—</em> George Box.</span>. The gap between the theoretical assumptions of the model and the empirical data characteristics, together with the model itself, should be taken as a whole when we evaluate the strength of the conclusion. This wholesome idea is what diagnosis is about. It also helps us to identify opportunities to improve the model. Models are representations/approximations of reality, so we have to be critical about them, yet being critical is different from being dismissive<label for="tufte-sn-139" class="margin-toggle sidenote-number">139</label><input type="checkbox" id="tufte-sn-139" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">139</span> A model that doesn’t fit the data also generates knowledge—revealed not by the failed model but by the fact that this model actually misfits. See Jaynes, E.T., <em>Probability Theory: the Logic of Science. </em> Cambridge Press, 2003.</span>. There are many diagnostic tools that we can use to strengthen our critical evaluation.</p>
<p>Many diagnostic tools focus on the <strong>residual analysis</strong>. Residuals provide a numerical evaluation of the difference between the model and the data. Recall that <span class="math inline">\(y\)</span> denotes the observed value of the outcome variable, <span class="math inline">\(f(\boldsymbol{x})\)</span> denotes the model, and <span class="math inline">\(\hat{y}\)</span> denotes the prediction (i.e., <span class="math inline">\(\hat{y} = f(\boldsymbol{x})\)</span> is the prediction made by the model on the data point <span class="math inline">\(\boldsymbol{x}\)</span>). The residual, denoted as <span class="math inline">\(\hat{\epsilon}\)</span>, is defined as <span class="math inline">\(\hat{\epsilon} = \hat{y} - y\)</span>. For any model that is trained on <span class="math inline">\(N\)</span> data points, we could obtain <span class="math inline">\(N\)</span> residuals, and draw the residuals as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-3residuals">100</a>:</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-3residuals"></span>
<p class="caption marginnote shownote">
Figure 100: Suppose that three models are built on a dataset, and their residual plots are drawn: (left) decision tree; (middle) RF; (right) linear regression
</p>
<img src="graphics/6_residualplots.png" alt="Suppose that three models are built on a dataset, and their residual plots are drawn: (left) decision tree; (middle) RF; (right) linear regression" width="80%"  />
</div>
<p></p>
<p><!-- begin{enumerate} --></p>
<p>1. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-3residuals">100</a> (left). There is a linear relationship between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span>, which suggests an absurd fact: <span class="math inline">\(\hat{y}\)</span> could be used as a predictor to predict the <span class="math inline">\(\hat{\epsilon}\)</span>, the <em>error</em>. For instance, when <span class="math inline">\(\hat{y} = -1\)</span>, the error is between <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. If we adjust the prediction to be <span class="math inline">\(\hat{y} + 1\)</span>, wouldn’t that make the error to be between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>? A reduced error means a better prediction model.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-newmodel"></span>
<img src="graphics/6_newmodel.png" alt="A new model, inspired by the pattern seen in Figure \@ref(fig:f6-3residuals) (left)" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 101: A new model, inspired by the pattern seen in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-3residuals">100</a> (left)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>To generalize this, let’s build another model <span class="math inline">\(g[f(\boldsymbol{x})]\)</span> that takes <span class="math inline">\(f(\boldsymbol{x})\)</span> as the predictor to predict <span class="math inline">\(\hat{\epsilon}\)</span>. Then, we can combine the two models, <span class="math inline">\(f(\boldsymbol{x})\)</span> and <span class="math inline">\(g[f(\boldsymbol{x})]\)</span>, and obtain an improved prediction <span class="math inline">\(\hat{y}\)</span> as <span class="math inline">\(f(\boldsymbol{x}) + g[f(\boldsymbol{x})]\)</span>. This is shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-newmodel">101</a>.</p>
<p>2. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-3residuals">100</a> (middle). No correlation between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span> is observed. In other words, knowing <span class="math inline">\(\hat{y}\)</span> offers no help to predict <span class="math inline">\(\hat{\epsilon}\)</span>. This is what a good model would behave like.</p>
<p>3. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-3residuals">100</a> (right). There is a piece-wise linear relationship between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span>. If we segment the figure by a vertical line at zero, we could apply the same argument made in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-3residuals">100</a> (left) for each piece here: the model could be further improved following the same strategy outlined in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-newmodel">101</a>.</p>
<p><!-- end{enumerate} --></p>
<p>As each data point contributes a residual, the <em>residual analysis</em> offers us opportunities to examine some collective phenomena to improve the overall quality of the model. It also helps us check local patterns where we may find areas of improvement of the model or particularities of the data that the model could not synthesize. The beauty of checking out the residuals is that there is always something that is beyond our experience and expectation.</p>
</div>
<div id="diagnosis-in-regression" class="section level2 unnumbered">
<h2>Diagnosis in regression</h2>
<div id="residual-analysis" class="section level3 unnumbered">
<h3>Residual analysis</h3>
<p>The R package <code>ggfortify</code> provides a nice bundle that includes the <strong>residual analysis, cook’s distance, leverage</strong>, and <strong>Q-Q plot</strong>.</p>
<p>Let’s use the final regression model we identified in <strong>Chapter 2</strong> for an example.</p>
<p></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb128-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb128-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb128-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-4" aria-hidden="true" tabindex="-1"></a>AD <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb128-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-5" aria-hidden="true" tabindex="-1"></a>AD<span class="sc">$</span>ID <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(AD)[<span class="dv">1</span>])</span>
<span id="cb128-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(AD)</span>
<span id="cb128-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a full-scale model</span></span>
<span id="cb128-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-8" aria-hidden="true" tabindex="-1"></a>AD_full <span class="ot">&lt;-</span> AD[,<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">17</span>)]</span>
<span id="cb128-9"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-9" aria-hidden="true" tabindex="-1"></a>lm.AD <span class="ot">&lt;-</span> <span class="fu">lm</span>(MMSCORE <span class="sc">~</span> ., <span class="at">data =</span> AD_full)</span>
<span id="cb128-10"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.AD)</span>
<span id="cb128-11"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic model selection</span></span>
<span id="cb128-12"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb128-12" aria-hidden="true" tabindex="-1"></a>lm.AD.F <span class="ot">&lt;-</span> <span class="fu">step</span>(lm.AD, <span class="at">direction=</span><span class="st">&quot;backward&quot;</span>, <span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<p></p>
<p>Details of the model are shown below.</p>
<p></p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="do">## MMSCORE ~ PTEDUCAT + FDG + AV45 + HippoNV + rs744373 + rs610932</span></span>
<span id="cb129-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="do">##     + rs3764650 + rs3865444</span></span>
<span id="cb129-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb129-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="do">##             Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    </span></span>
<span id="cb129-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-5" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;                   1537.5 581.47                      </span></span>
<span id="cb129-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-6" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3764650  1     7.513 1545.0 581.99  2.4824  0.115750    </span></span>
<span id="cb129-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-7" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs744373   1    12.119 1549.6 583.53  4.0040  0.045924 *  </span></span>
<span id="cb129-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-8" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs610932   1    14.052 1551.6 584.17  4.6429  0.031652 *  </span></span>
<span id="cb129-9"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-9" aria-hidden="true" tabindex="-1"></a><span class="do">## - rs3865444  1    21.371 1558.9 586.61  7.0612  0.008125 ** </span></span>
<span id="cb129-10"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-10" aria-hidden="true" tabindex="-1"></a><span class="do">## - AV45       1    50.118 1587.6 596.05 16.5591 5.467e-05 ***</span></span>
<span id="cb129-11"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-11" aria-hidden="true" tabindex="-1"></a><span class="do">## - PTEDUCAT   1    82.478 1620.0 606.49 27.2507 2.610e-07 ***</span></span>
<span id="cb129-12"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-12" aria-hidden="true" tabindex="-1"></a><span class="do">## - HippoNV    1   118.599 1656.1 617.89 39.1854 8.206e-10 ***</span></span>
<span id="cb129-13"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-13" aria-hidden="true" tabindex="-1"></a><span class="do">## - FDG        1   143.852 1681.4 625.71 47.5288 1.614e-11 ***</span></span>
<span id="cb129-14"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb129-15"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb129-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span></code></pre></div>
<p></p>
<p>We use the <code>ggfortify</code> to produce <span class="math inline">\(6\)</span> diagnostic figures as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a>.</p>
<p></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct diagnostics of the model</span></span>
<span id="cb130-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;ggfortify&quot;)</span></span>
<span id="cb130-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggfortify&quot;</span>)</span>
<span id="cb130-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb130-4" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.AD.F, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-1"></span>
<p class="caption marginnote shownote">
Figure 102: Diagnostic figures of regression model on the AD dataset
</p>
<img src="graphics/6_1.png" alt="Diagnostic figures of regression model on the AD dataset" width="80%"  />
</div>
<p></p>
<p>The following is what we observe from Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a>.</p>
<p><!-- begin{enumerate} --></p>
<p>1. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a> (upper left). This is the scatterplot of the residuals versus fitted values of the outcome variable. As we have discussed in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-3residuals">100</a>, this scatterplot is supposed to show purely random distributions of the dots. Here, we notice two abnormalities: (1) there is a relationship between the residuals and fitted values; and (2) there are unusual parallel lines<label for="tufte-sn-140" class="margin-toggle sidenote-number">140</label><input type="checkbox" id="tufte-sn-140" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">140</span> This is often observed if the outcome variable takes integer values.</span>. These abnormalities have a few implications: (1) the linear model <em>underfits</em> the data, so a nonlinear model is needed; (2) we have assumed that the data points are independent with each other, now this assumption needs to be checked; and (3) we have assumed <em>homoscedasticity</em><label for="tufte-sn-141" class="margin-toggle sidenote-number">141</label><input type="checkbox" id="tufte-sn-141" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">141</span> In <strong>Chapter 2</strong>, we assume that <span class="math inline">\(\epsilon \sim N\left(0, \sigma_{\varepsilon}^{2}\right)\)</span>. It assumes the errors have the same variance, <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span>, for all data points.</span> of the variance of the errors. This is another assumption that needs to be checked<label for="tufte-sn-142" class="margin-toggle sidenote-number">142</label><input type="checkbox" id="tufte-sn-142" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">142</span> To build nonlinear regression model or conditional variance regression model, see <strong>Chapter 9</strong>.</span>.</p>
<p>2. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a> (upper right). The <strong>Q-Q plot</strong> checks the normality assumption of the errors. The <span class="math inline">\(45^{\circ}\)</span> line is a fixed <em>baseline</em>, while the dots correspond to the data points. If the normality assumption is met, the dots should align with the line. Here, we see mild departure of the data from the normality assumption. And some particular data points such as the data points <span class="math inline">\(282\)</span> and <span class="math inline">\(256\)</span> are labelled since they are outstanding<label for="tufte-sn-143" class="margin-toggle sidenote-number">143</label><input type="checkbox" id="tufte-sn-143" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">143</span> Are those points outliers? The Q-Q plot provides no conclusive evidence. It only suggests.</span>.</p>
<p>3. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a> (middle left). This is a transformation of Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a> (upper left). Diagnostic tools are usually <em>opportunistic</em> approaches, i.e., what you see is what you get; if nothing particular is observed, it doesn’t mean there is no anomaly in the data. Changing perspectives is a common practice in model diagnosis.</p>
<p>4. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a> (middle right). The <strong>Cook’s distance</strong> identifies influential data points that have <em>larger than average</em> influence on the parameter estimation of the model. For a data point <span class="math inline">\(\boldsymbol{x}_i\)</span>, its Cook’s distance <span class="math inline">\(D_{i}\)</span> is defined as the sum of all the changes in the regression model when <span class="math inline">\(\boldsymbol{x}_i\)</span> is removed from the training data. There is a closed-form formula<label for="tufte-sn-144" class="margin-toggle sidenote-number">144</label><input type="checkbox" id="tufte-sn-144" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">144</span> Cook, R.D., <em>Influential Observations in Linear Regression</em>, Journal of the American Statistical Association, Volume 74, Number 365, Pages 169-174, 1979.</span> to compute <span class="math inline">\({D_{i}, \text{ for } j=1,2,\dots,p}\)</span>, based on the <em>least squares estimator</em> of the regression parameters.</p>
<p>5. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a> (lower left). The <strong>leverage</strong> of a data point, on the other hand, shows the influence of the data point in another way. The leverage of a data point is defined as <span class="math inline">\(\frac{\partial \hat{y}_{i}}{\partial y_{i}}\)</span>. This reflects how sensitively the prediction <span class="math inline">\(\hat{y}_{i}\)</span> is influenced by <span class="math inline">\(y_{i}\)</span>. What data point will have a larger leverage value? For those surrounded by many closeby data points, their leverages won’t be large: the impact of a data point’s removal in a dense neighborhood is limited, given many other similar data points nearby. It is the data points in sparsely occupied neighborhoods that have large leverages. These data points could either be outliers that severely deviate from the linear trend represented by the majority of the data points, or could be valuable data points that align with the linear trend but lack neighbor data points. Thus, a data point that is influential doesn’t necessarily imply it is an outlier, as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-outlier-infl">103</a>. When a data point has a larger leverage value, in-depth examination of the data point is needed to determine which case it is.</p>
<p>6. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a> (lower right). This is another form of showing the information that is presented in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a> (middle right) and (lower left).</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-outlier-infl"></span>
<img src="graphics/6_outlier_influ.png" alt="Outliers v.s. influential data points" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 103: Outliers v.s. influential data points<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><!-- end{enumerate} --></p>
<p><em>A Simulation Experiment.</em> We simulate a dataset while all the assumptions of the linear regression model are met. The model is</p>
<p><span class="math display">\[\begin{equation*}
\small
   
y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\varepsilon, \varepsilon \sim N(0,1).
 
\end{equation*}\]</span></p>
<p>We simulate <span class="math inline">\(100\)</span> samples from this model.</p>
<p></p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For comparison, let&#39;s simulate data </span></span>
<span id="cb131-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="co"># from a model that fits the assumptions</span></span>
<span id="cb131-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb131-3" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb131-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb131-4" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb131-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb131-5" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb131-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb131-6" aria-hidden="true" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb131-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb131-7" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> beta1 <span class="sc">*</span> x1 <span class="sc">+</span> beta2 <span class="sc">*</span> x2</span>
<span id="cb131-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb131-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, mu, <span class="dv">1</span>)</span></code></pre></div>
<p></p>
<p>We fit the data using linear regression model.</p>
<p></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb132-1" aria-hidden="true" tabindex="-1"></a>lm.XY <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(y,x1,x2))</span>
<span id="cb132-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.XY)</span></code></pre></div>
<p></p>
<p>The fitted model fairly reflects the underlying model.</p>
<p></p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb133-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb133-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-3" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ ., data = data.frame(y, x1, x2))</span></span>
<span id="cb133-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb133-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb133-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb133-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.6475 -0.6630 -0.1171  0.7986  2.5074 </span></span>
<span id="cb133-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb133-9"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb133-10"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-10" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb133-11"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-11" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)   0.0366     0.1089   0.336    0.738    </span></span>
<span id="cb133-12"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-12" aria-hidden="true" tabindex="-1"></a><span class="do">## x1            0.9923     0.1124   8.825 4.60e-14 ***</span></span>
<span id="cb133-13"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-13" aria-hidden="true" tabindex="-1"></a><span class="do">## x2            0.9284     0.1159   8.011 2.55e-12 ***</span></span>
<span id="cb133-14"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb133-15"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb133-16"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb133-17"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 1.088 on 97 degrees of freedom</span></span>
<span id="cb133-18"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.6225, Adjusted R-squared:  0.6147 </span></span>
<span id="cb133-19"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb133-19" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 79.98 on 2 and 97 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.XY, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-2"></span>
<p class="caption marginnote shownote">
Figure 104: Diagnostic figures of regression model on a simulation dataset
</p>
<img src="graphics/6_2.png" alt="Diagnostic figures of regression model on a simulation dataset" width="80%"  />
</div>
<p></p>
<p>Then, we generate the diagnostic figures as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-2">104</a>. Now Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-2">104</a> provides a contrast of Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-1">102</a>. For example, in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-2">104</a> (upper left), we don’t see a nonrandom statistical pattern. The relationship between the residual and fitted values seems to be null. From the <em>QQ-plot</em>, we see that the normality assumption is held well. On the other hand, from the <em>Cook’s distance</em> and the <em>leverage</em>, some data points are observed to be outstanding, which are labeled. As we simulated the data following the assumptions of the linear regression model, this experiment shows that it is normal to expect a few data points to show outstanding <em>Cook’s distance</em> and <em>leverage</em> values.</p>
<p></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct diagnostics of the model</span></span>
<span id="cb135-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb135-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggfortify&quot;</span>)</span>
<span id="cb135-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(lm.XY, <span class="at">which =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">label.size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
</div>
<div id="multicollinearity" class="section level3 unnumbered">
<h3>Multicollinearity</h3>
<p>Multicollinearity refers to the phenomenon that there is <em>a high correlation among the predictor variables</em>. This causes a serious problem for linear regression models. We can do a simple analysis. Consider a linear system shown below</p>
<p><span class="math display">\[\begin{equation*}
\small
   
\begin{array}{c}{y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\cdots+\beta_{p} x_{p}+\varepsilon_y}, \\ {\varepsilon_y \sim N\left(0, \sigma_{\varepsilon_y}^{2}\right)}. \end{array}
 
\end{equation*}\]</span></p>
<p>This looks like a regular linear regression model. However, here we further have</p>
<p><span class="math display">\[\begin{equation*}
\small
   
\begin{array}{c}{x_{1}=2 x_{2}+\epsilon_x}, \\ {\epsilon_x \sim N\left(0, \sigma_{\varepsilon_x}^{2}\right).}\end{array}
 
\end{equation*}\]</span></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-multilinear"></span>
<img src="graphics/6_multilinear.png" alt="The *data-generating mechanism* of a system that suffers from *multicollinearity*" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 105: The <em>data-generating mechanism</em> of a system that suffers from <em>multicollinearity</em><!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>This <em>data-generating mechanism</em> is shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-multilinear">105</a>. It is a system that suffers from <em>multicollinearity</em>, i.e., if we apply a linear regression model on this system, the following models are <em>both</em> true models</p>
<p><span class="math display">\[\begin{equation*}
\small
   
\begin{array}{c}{y=\beta_{0}+\left(2 \beta_{1}+\beta_{2}\right) x_{2}+\beta_{3} x_{3} \ldots+\beta_{p} x_{p}} \\ {y=\beta_{0}+\left(\beta_{1}+0.5 \beta_{2}\right) x_{1}+\beta_{3} x_{3}+\cdots+\beta_{p} x_{p}}\end{array}.
 
\end{equation*}\]</span></p>
<p>The problem of multicollinearity results from an inherent ambiguity of the models that could be taken as faithful representation of the <em>data-generating mechanism</em>. If the <em>true</em> model is ambiguous, it is expected that an estimated model suffers from this problem as well.</p>
<p>There are some methods that we can use to diagnose <em>multicollinearity</em>. For instance, we may visualize the correlations among the predictor variables using the R package <code>corrplot</code>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-3"></span>
<img src="graphics/6_3.png" alt="Correlations of the predictors in the regression model of  `MMSCORE` " width="100%"  />
<!--
<p class="caption marginnote">-->Figure 106: Correlations of the predictors in the regression model of <code>MMSCORE</code> <!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the covariance matrix of the regression parameters</span></span>
<span id="cb136-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb136-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> <span class="fu">vcov</span>(lm.AD.F)</span>
<span id="cb136-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the correlation matrix of the estimated regression </span></span>
<span id="cb136-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb136-4" aria-hidden="true" tabindex="-1"></a><span class="co"># parameters</span></span>
<span id="cb136-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb136-5" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;corrplot&quot;)</span></span>
<span id="cb136-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb136-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb136-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb136-7" aria-hidden="true" tabindex="-1"></a><span class="fu">corrplot</span>(<span class="fu">cov2cor</span>(Sigma), <span class="at">method=</span><span class="st">&quot;ellipse&quot;</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-3">106</a> shows that there are significant correlations between the variables, <code>FDG</code>, <code>AV45</code>, and <code>HippoNV</code>, indicating a concern for multicollinearity. On the other hand, it seems that the correlations are moderate, and not all the variables are strongly correlated with each other.</p>
<p>It is of interest to see why the strong correlations among predictor variables cause problems in the <em>least squares</em> estimator of the regression coefficients. Recall that <span class="math inline">\(\widehat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y}\)</span>. If there are strong correlations among predictor variables, the matrix <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span> is ill-conditioned, i.e., small changes on <span class="math inline">\(\boldsymbol{X}\)</span> result in large and unpredictable changes on the inverse matrix <span class="math inline">\(\boldsymbol{X}^{T} \boldsymbol{X}\)</span>, which further causes instability of the parameter estimation in <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.<label for="tufte-sn-145" class="margin-toggle sidenote-number">145</label><input type="checkbox" id="tufte-sn-145" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">145</span> To overcome multicollinearity in linear regression, the <em>Principal Component Analysis</em> discussed in <strong>Chapter 8</strong> is useful.</span></p>
</div>
</div>
<div id="diagnosis-in-random-forests" class="section level2 unnumbered">
<h2>Diagnosis in random forests</h2>
<div id="residual-analysis-1" class="section level3 unnumbered">
<h3>Residual analysis</h3>
<p>We can use the <code>plotmo</code> package to perform residual analysis for a random forest model. For instance, we build a random forest model to predict the variable <code>AGE</code> in the AD dataset. We plot the residual versus the fitted values as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-8">107</a> which shows there is a linear pattern between the fitted values and residuals. This indicates that this random forest model missed some linear relationship in the AD dataset.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-8"></span>
<img src="graphics/6_8.png" alt="Residuals versus fitted in the random forest model" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 107: Residuals versus fitted in the random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(randomForest)</span>
<span id="cb137-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(plotmo)</span>
<span id="cb137-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb137-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb137-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-5" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb137-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-6" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD_hd.csv&quot;</span>)</span>
<span id="cb137-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb137-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-9"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-9" aria-hidden="true" tabindex="-1"></a>target <span class="ot">&lt;-</span> data<span class="sc">$</span>AGE</span>
<span id="cb137-10"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-10" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> </span>
<span id="cb137-11"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-11" aria-hidden="true" tabindex="-1"></a>                   <span class="fu">c</span>(<span class="st">&quot;AGE&quot;</span>, <span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>, <span class="st">&quot;MMSCORE&quot;</span>,<span class="st">&quot;DX_bl&quot;</span>))</span>
<span id="cb137-12"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-12" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb137-13"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-13" aria-hidden="true" tabindex="-1"></a>rf.mod <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(X, target)</span>
<span id="cb137-14"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb137-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plotres</span>(rf.mod, <span class="at">which =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>The random forest model doesn’t assume normality of its residuals. To make a comparison with the linear regression model, we draw the Q-Q plot of the random forest model in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-9">108</a>. It can be seen that the residuals deviate from the straight line.</p>
<p></p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plotres</span>(rf.mod, <span class="at">which =</span> <span class="dv">4</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-9"></span>
<img src="graphics/6_9.png" alt="The Q-Q plot of residuals of the random forest model" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 108: The Q-Q plot of residuals of the random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>As the random forest model is an algorithmic modeling approach that imposes no analytic assumption, diagnosis could still be done but interpretations are not as strong as in a linear regression model. There is still value to do so, to find area of improvement of the model, e.g., as Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-8">107</a> suggests the random forest model could be further improved to incorporate the linear pattern in the data.</p>
</div>
</div>
<div id="clustering" class="section level2 unnumbered">
<h2>Clustering</h2>
<div id="rationale-and-formulation-9" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><em>Clustering</em> takes the idea of <em>diagnosis</em> to a different level. If the <em>residual analysis</em> is like a tailor working out the perfect outfit for a client, clustering is … well, it is better to see Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-nd">109</a> first.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-twocluster-nd"></span>
<p class="caption marginnote shownote">
Figure 109: A tailor tries to (left) make an outfit (i.e., the normal curve) for a client (i.e., the data, represented as a histogram) vs. (right) then the tailor realizes the form of the outfit should be two normal curves
</p>
<img src="graphics/6_twocluster_nd.png" alt="A tailor tries to (left) make an outfit (i.e., the normal curve) for a client (i.e., the data, represented as a histogram) vs. (right) then the tailor realizes the form of the outfit should be two normal curves" width="80%"  />
</div>
<p></p>
<p>Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-nd">109</a> demonstrates one meaning of clustering: a dataset is heterogeneous and is probably collected from a few different populations (sometimes we call them <em>sub</em>populations). Understanding the clustering structure of a dataset not only benefits the statistical modeling, as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-nd">109</a> where we will use two normal distributions to model the data, but also reveals insights about the problem under study. For example, the dataset shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-nd">109</a> was collected from a disease study of young children. It suggests that there are two disease mechanisms (we often call them two <em>phenotypes</em>). Phenotypes discovery is important for disease treatment, since patients with different disease mechanisms respond to treatments differently. A typical approach for phenotypes discovery is to collect an abundance of data from many patients. Then, we employ a range of algorithms to discover clusters of the data points. These clustering algorithms, differ from each other in their premises of what a cluster looks like, more or less bear the same conceptual framework as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-nd">109</a>.</p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-twocluster-lr"></span>
<p class="caption marginnote shownote">
Figure 110: Another example of clustering: if the clustering structure is ignored, the fitted model (left) may show the opposite direction of the true model (right)
</p>
<img src="graphics/6_twocluster_lr.png" alt="Another example of clustering: if the clustering structure is ignored, the fitted model (left) may show the opposite direction of the true model (right) " width="80%"  />
</div>
<p></p>
<p>Clustering is a flexible concept that could be applied in other scenarios as well. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-lr">110</a> demonstrates another meaning of clustering. It is less commonly perceived, but in practice it is not uncommon. The “moral of the story” shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-lr">110</a> tells us that, when you have a dataset, you may want to conduct EDA and check the clustering structure first before imposing a model that may only fit the <em>data format</em> but not the <em>statistical structure</em><label for="tufte-sn-146" class="margin-toggle sidenote-number">146</label><input type="checkbox" id="tufte-sn-146" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">146</span> E.g., in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-lr">110</a>: <em>data format</em>: we have predictors and outcome, so it seems natural to fit a linear regression model; <em>statistical structure</em>: however, it is a mix of two subpopulations that demand two models.</span>.</p>
</div>
<div id="theory-and-method-6" class="section level3 unnumbered">
<h3>Theory and method</h3>
<p>Given a dataset, how do we know there is a clustering structure? Consider the dataset shown in Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example">25</a>. Are there <em>sub</em>populations as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-nd">109</a>?</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example">Table 25: </span>Example of a dataset</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">$ $</th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Value</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Cluster</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p></p>
<p>A visual check of the <span class="math inline">\(7\)</span> data points suggests there are probably two clusters. If each cluster can be modeled as a Gaussian distribution, this would be a two-component <strong>Gaussian Mixture Model</strong> (<strong>GMM</strong>)<label for="tufte-sn-147" class="margin-toggle sidenote-number">147</label><input type="checkbox" id="tufte-sn-147" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">147</span> A GMM consists of multiple Gaussian distributions. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-twocluster-nd">109</a> shows one example of two univariate Gaussian distributions mixed together. Generally, the parameters of a GMM are denoted as <span class="math inline">\(\boldsymbol{\Theta}\)</span>, which include the parameters of each Gaussian distribution: <span class="math inline">\(\mu_{i}\)</span> and <span class="math inline">\(\sigma_{i}\)</span> are the mean and variance of the <span class="math inline">\(i^{th}\)</span> Gaussian distribution, respectively, and <span class="math inline">\(\pi_{i}\)</span> is the proportion of the data points that were sampled from the <span class="math inline">\(i^{th}\)</span> Gaussian distribution. </span>.</p>
<p>In this particular dataset, clustering could be done by learning the parameters of the two-component (<strong>GMM</strong>), (i.e., to address the question marks in the last row of Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example">25</a>). If we have known the parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span>, we could probabilistically infer which cluster each data point belongs to (i.e., to address the question marks in the second row of Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example">25</a>). On the other hand, if we have known which cluster each data point belongs to, we can collect the data points of each cluster to estimate the parameters of the Gaussian distribution that characterizes each cluster. This “locked” relation between the two tasks is shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-cluster-cycle">111</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-cluster-cycle"></span>
<img src="graphics/6_cluster_cycle.png" alt="The locked relation between parameter estimation (M-step, i.e., last row of Table \@ref(tab:t6-example)) and data point inference (E-step, i.e., second row of Table \@ref(tab:t6-example)) in GMM" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 111: The locked relation between parameter estimation (M-step, i.e., last row of Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example">25</a>) and data point inference (E-step, i.e., second row of Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example">25</a>) in GMM<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>The two interdependent tasks hold the key for each other. What is needed is <em>initialization</em>. As there are two blocks in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-cluster-cycle">111</a>, we have two locations to initialize the process of unlocking.</p>
<p><em>Initialization.</em> Let’s initialize the values in the second row of Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example">25</a> for an example. We assign (i.e., <em>randomly</em>) labels on the data points as shown in Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example-init">26</a>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-init">Table 26: </span>Initialization on the dataset example</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>M-step.</em> Then, we estimate <span class="math inline">\(\mu_{1}=1.75\)</span> and <span class="math inline">\(\sigma_{1}^{2}=2.83\)</span> based on the data points <span class="math inline">\(\{1.13, 4.76, 0.87, 1.03, 0.98\}\)</span>.<label for="tufte-sn-148" class="margin-toggle sidenote-number">148</label><input type="checkbox" id="tufte-sn-148" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">148</span> These <span class="math inline">\(5\)</span> data instances are initially assigned to <span class="math inline">\(C1\)</span>. Note that <span class="math inline">\(4.76\)</span> is different from the rest of the data points in the same cluster. This is an error introduced by the initialization. Later we will see that this error could be automatically fixed by the algorithm.</span></p>
<p>Similarly, we could estimate <span class="math inline">\(\mu_{2}=3.81\)</span> and <span class="math inline">\(\sigma_{2}^{2}=0.47\)</span> based on the data points <span class="math inline">\(\{3.32, 4.29\}\)</span>.</p>
<p>It is straightforward to estimate <span class="math inline">\(\pi_{1}=5/7 = 0.714\)</span> and <span class="math inline">\(\pi_{2}=2/7 = 0.286\)</span>.</p>
<p>Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example-init">26</a> is updated.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-thetaupdated">Table 27: </span><span class="math inline">\(\boldsymbol{\Theta}\)</span> updated</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p><em>E-step.</em> Since the labels of the data points were randomly initialized, they need to be updated given the latest estimation of <span class="math inline">\(\boldsymbol{\Theta}\)</span>. We continue to update the labels of the data points. To facilitate the presentation, we invent a binary indicator variable, denoted as <span class="math inline">\(z_{n m}\)</span>: <span class="math inline">\(z_{n m}=1\)</span> indicates that the data point <span class="math inline">\(x_{n}\)</span> was <em>assumed to be</em> sampled from the <span class="math inline">\(m^{th}\)</span> cluster; otherwise, <span class="math inline">\(z_{n m}=0\)</span>.</p>
<p>For example, if the first data point was sampled from the first cluster, the probability that <span class="math inline">\(x_1 = 1.13\)</span> is<label for="tufte-sn-149" class="margin-toggle sidenote-number">149</label><input type="checkbox" id="tufte-sn-149" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">149</span> In R, we could use the function <code>dnorm</code> to calculate it. For example, for <span class="math inline">\(p\left(x_{1} = 1.13| z_{11}=1 \right)\)</span>, we use <code>dnorm(1.13, mean = 1.75, sd = sqrt(2.83))</code> since <span class="math inline">\(\mu_{1}=1.75, \sigma_{1}^{2}=2.83\)</span>.</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
    p\left(x_{1} = 1.13| z_{11}=1 \right)=0.22.
 
\end{equation*}\]</span></p>
<p>And if the first data point was sampled from the second cluster, the probability that <span class="math inline">\(x_1 = 1.13\)</span> is</p>
<p><span class="math display">\[\begin{equation*}
\small
  
    p\left(x_{1} = 1.13 | z_{12}=1 \right)=0.0003.
 
\end{equation*}\]</span></p>
<p>Repeat it for all the other data points, we have:</p>
<p><!-- % \setlength{\belowdisplayskip}{0pt} % %%\setlength{\belowdisplayshortskip}{0pt} -->
<!-- % \setlength{\abovedisplayskip}{0pt} % %%\setlength{\abovedisplayshortskip}{0pt} --></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(x_{2}=4.76 | z_{21}=1 \right)=0.05, p\left(x_{2}=4.76 | z_{22}=1 \right)=0.22;
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(x_{3}=0.87 | z_{31}=1 \right)=0.02, p\left(x_{3}=0.87 | z_{32}=1 \right)=0;
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(x_{4}=3.32 | z_{41}=1 \right)=0.15, p\left(x_{4}=3.32 | z_{42}=1 \right)=0.45;
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(x_{5}=4.29 | z_{51}=1 \right)=0.08, p\left(x_{5}=4.29 | z_{52}=1 \right)=0.45;
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(x_{6}=1.03 | z_{61}=1 \right)=0.22, p\left(x_{6}=1.03 | z_{62}=1 \right)=0.0001;
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(x_{7}=0.98 | z_{71}=1 \right)=0.21, p\left(x_{7}=0.98 | z_{72}=1 \right)=0.0001.
 
\end{equation*}\]</span></p>
<!-- % \vspace{6pt} -->
<p>Note that we need to calculate “the probability of <em>which cluster</em> a data point was sampled from”<label for="tufte-sn-150" class="margin-toggle sidenote-number">150</label><input type="checkbox" id="tufte-sn-150" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">150</span> I.e., <span class="math inline">\(p\left(z_{11}=1 | x_1 = 1.13\right)\)</span>.</span>. This is different from the probabilities we have calculated as shown above, which concerns“if a data point was sampled from a cluster, then the probability of the <em>specific value</em> the data point took on”<label for="tufte-sn-151" class="margin-toggle sidenote-number">151</label><input type="checkbox" id="tufte-sn-151" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">151</span> I.e., <span class="math inline">\(p\left(x_{1} = 1.13| z_{11}=1 \right)\)</span>.</span>.</p>
<p>Thus, we further calculate the conditional probabilities of <span class="math inline">\(p\left(z_{i1} | x_i\right)\)</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(z_{11}=1 | x_1 = 1.13 \right)=\frac{0.22 \times 0.714}{0.22 \times 0.714+0.0003 \times 0.286}=0.99; \text{ thus } x_1 \in C_1.
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(z_{21}=1 | x_2 = 4.76 \right)=\frac{0.05 \times 0.714}{0.05 \times 0.714+0.22 \times 0.286}=0.37; \text{ thus } x_2 \in C_2.
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(z_{31}=1 | x_3 = 0.87 \right)=\frac{0.02 \times 0.714}{0.02 \times 0.714+0.00 \times 0.286}=1; \text{ thus } x_3 \in C_1.
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(z_{41}=1 | x_4 = 3.32 \right)=\frac{0.15 \times 0.714}{0.15 \times 0.714+0.45 \times 0.286}=0.44; \text{ thus } x_4 \in C_2.
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(z_{51}=1 | x_5 = 4.29 \right)=\frac{0.08 \times 0.714}{0.08 \times 0.714+0.45 \times 0.286}=0.29; \text{ thus } x_5 \in C_2.
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(z_{61}=1 | x_6 = 1.03 \right)=\frac{0.22 \times 0.714}{0.22 \times 0.714+0.0001 \times 0.286}=0.99; \text{ thus } x_6 \in C_1.
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
p\left(z_{71}=1 | x_7 = 0.98 \right)=\frac{0.21 \times 0.714}{0.21 \times 0.714+0.0001 \times 0.286}=0.99; \text{ thus } x_7 \in C_1.
 
\end{equation*}\]</span></p>
<!-- %\vspace{6pt} -->
<p>Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example-thetaupdated">27</a> can be updated to Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example-final">28</a>.</p>
<p>We can repeat this process and cycle through the two steps as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-cluster-cycle">111</a>, until the process converges, i.e., <span class="math inline">\(\boldsymbol{\Theta}\)</span> remains the same (or its change is very small), or the labels of the data points remain the same. In this example, we actually only need one more iteration to reach convergence. This algorithm is a basic version of the so-called <strong>EM algorithm</strong>. Interested readers could find a complete derivation process in the <strong>Remarks</strong> section.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-example-final">Table 28: </span>Cluster labels updated</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(x_1\)</span></th>
<th align="left"><span class="math inline">\(x_2\)</span></th>
<th align="left"><span class="math inline">\(x_3\)</span></th>
<th align="left"><span class="math inline">\(x_4\)</span></th>
<th align="left"><span class="math inline">\(x_5\)</span></th>
<th align="left"><span class="math inline">\(x_6\)</span></th>
<th align="left"><span class="math inline">\(x_7\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ID</td>
<td align="left"><span class="math inline">\(1.13\)</span></td>
<td align="left"><span class="math inline">\(4.76\)</span></td>
<td align="left"><span class="math inline">\(0.87\)</span></td>
<td align="left"><span class="math inline">\(3.32\)</span></td>
<td align="left"><span class="math inline">\(4.29\)</span></td>
<td align="left"><span class="math inline">\(1.03\)</span></td>
<td align="left"><span class="math inline">\(0.98\)</span></td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
</tr>
</tbody>
</table>
<p></p>
</div>
<div id="formal-definition-of-the-gmm" class="section level3 unnumbered">
<h3>Formal definition of the GMM</h3>
<p>As a <em>data modeling</em> approach, the GMM implies a <em>data-generating mechanism</em>, that is summarized in below.</p>
<p><!-- begin{enumerate} --></p>
<p>1. [1.] Suppose that there are <span class="math inline">\(M\)</span> distributions mixed together.</p>
<p>2. [2.] In GMM, we assume that all the distributions are Gaussian distributions, i.e., the parameters of the <span class="math inline">\(m^{\text{th}}\)</span> distribution are <span class="math inline">\(\left\{\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m}\right\}\)</span>, and <span class="math inline">\(m=1,2, \ldots, M\)</span>.<label for="tufte-sn-152" class="margin-toggle sidenote-number">152</label><input type="checkbox" id="tufte-sn-152" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">152</span> <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span> is the mean vector; <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> is the covariance matrix.</span></p>
<p>3. [3.] For any data point <span class="math inline">\(\boldsymbol{x}\)</span>, without knowing its specific value, the prior probability that it comes from the <span class="math inline">\(m^{\text{th}}\)</span> distribution is denoted as <span class="math inline">\(\pi_m\)</span>.<label for="tufte-sn-153" class="margin-toggle sidenote-number">153</label><input type="checkbox" id="tufte-sn-153" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">153</span> In other words, this is the percentage of the data points in the whole mix that come from the <span class="math inline">\(m^{th}\)</span> distribution.</span> Note that <span class="math inline">\(\sum_{m=1}^{M} \pi_m=1\)</span>.</p>
<p><!-- end{enumerate} --></p>
<p>The final distribution form of <span class="math inline">\(\boldsymbol{x}\)</span> is a mixed distribution with <span class="math inline">\(m\)</span> components</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{x} \sim \pi_{1} N\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1}\right) + \pi_{2} N\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2}\right) + {\ldots} + \pi_{m} N\left(\boldsymbol{\mu}_{m}, \boldsymbol{\Sigma}_{m}\right).
 
\end{equation*}\]</span></p>
<p>To learn the parameters from data, the EM algorithm is used. A basic walk-through of the EM algorithm has been given, i.e., see the example using Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example">25</a>.</p>
</div>
<div id="r-lab-8" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We simulate a dataset with <span class="math inline">\(4\)</span> clusters as shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-4clusters">112</a>.</p>
<p></p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a clustering structure</span></span>
<span id="cb139-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb139-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">10</span>,<span class="dv">2</span>), </span>
<span id="cb139-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb139-3" aria-hidden="true" tabindex="-1"></a>       <span class="fu">rnorm</span>(<span class="dv">200</span>,<span class="dv">20</span>,<span class="dv">1</span>), <span class="fu">rnorm</span>(<span class="dv">200</span>,<span class="dv">40</span>, <span class="dv">2</span>))</span>
<span id="cb139-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb139-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(<span class="dv">800</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb139-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X,Y, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;gray25&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-4clusters"></span>
<img src="graphics/6_13.png" alt="A mixture of $4$ Gaussian distributions" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 112: A mixture of <span class="math inline">\(4\)</span> Gaussian distributions<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We use the R package <code>Mclust</code> to implement the GMM model using the EM algorithm.</p>
<p></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use GMM to identify the clusters</span></span>
<span id="cb140-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(mclust)</span>
<span id="cb140-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb140-3" aria-hidden="true" tabindex="-1"></a>XY.clust <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(<span class="fu">data.frame</span>(X,Y))</span>
<span id="cb140-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb140-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(XY.clust)</span>
<span id="cb140-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb140-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(XY.clust)</span></code></pre></div>
<p></p>
<p>We obtain the following result. Visualization of the identified clusters is shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-14">113</a>. Note that we didn’t specify the number of clusters in the analysis. <code>Mclust</code> used BIC and correctly identified the <span class="math inline">\(4\)</span> clusters. For each cluster, the data points are about <span class="math inline">\(200\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb141-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Gaussian finite mixture model fitted by EM algorithm </span></span>
<span id="cb141-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-3" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb141-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Mclust VVI (diagonal, varying volume and shape) model with</span></span>
<span id="cb141-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 components:</span></span>
<span id="cb141-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-8" aria-hidden="true" tabindex="-1"></a><span class="do">##  log.likelihood   n df       BIC       ICL</span></span>
<span id="cb141-9"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-9" aria-hidden="true" tabindex="-1"></a><span class="do">##        -3666.07 800 19 -7459.147 -7459.539</span></span>
<span id="cb141-10"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb141-11"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Clustering table:</span></span>
<span id="cb141-12"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-12" aria-hidden="true" tabindex="-1"></a><span class="do">##   1   2   3   4 </span></span>
<span id="cb141-13"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb141-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 199 201 200 200</span></span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-14"></span>
<img src="graphics/6_14.png" alt="Clustering results of the simulated data" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 113: Clustering results of the simulated data<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>Now let’s implement GMM on the AD data. Result is shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-15">114</a>.</p>
<p></p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;mclust&quot;)</span></span>
<span id="cb142-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(mclust)</span>
<span id="cb142-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-3" aria-hidden="true" tabindex="-1"></a>AD.Mclust <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(AD[,<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">10</span>,<span class="dv">12</span>,<span class="dv">14</span>,<span class="dv">15</span>)])</span>
<span id="cb142-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(AD.Mclust)</span>
<span id="cb142-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-5" aria-hidden="true" tabindex="-1"></a>AD.Mclust<span class="sc">$</span>data <span class="ot">=</span> AD.Mclust<span class="sc">$</span>data[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)]</span>
<span id="cb142-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(AD.Mclust)</span></span>
<span id="cb142-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-7" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb142-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Gaussian finite mixture model fitted by EM algorithm </span></span>
<span id="cb142-9"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-9" aria-hidden="true" tabindex="-1"></a><span class="do">## ----------------------------------------------------</span></span>
<span id="cb142-10"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb142-11"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Mclust EEI (diagonal, equal volume and shape) model </span></span>
<span id="cb142-12"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-12" aria-hidden="true" tabindex="-1"></a><span class="do">## with 4 components:</span></span>
<span id="cb142-13"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb142-14"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  log.likelihood   n df       BIC       ICL</span></span>
<span id="cb142-15"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       -3235.874 517 43 -6740.414 -6899.077</span></span>
<span id="cb142-16"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb142-17"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Clustering table:</span></span>
<span id="cb142-18"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   1   2   3   4 </span></span>
<span id="cb142-19"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb142-19" aria-hidden="true" tabindex="-1"></a><span class="do">##  43 253  92 129</span></span></code></pre></div>
<p></p>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:f6-15"></span>
<p class="caption marginnote shownote">
Figure 114: Clustering results of the AD dataset
</p>
<img src="graphics/6_15.png" alt="Clustering results of the AD dataset" width="80%"  />
</div>
<p></p>
<p>Four clusters are identified as well. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-15">114</a> shows the boundaries between clusters are not as distinct as the boundaries in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-14">113</a>. In real applications, particularly for those applications of which we haven’t known enough, clustering is an exploration tool that could generate suggestive results but may not provide confirmatory conclusions.</p>
</div>
</div>
<div id="remarks-4" class="section level2 unnumbered">
<h2>Remarks</h2>
<div id="derivation-of-the-em-algorithm" class="section level3 unnumbered">
<h3>Derivation of the EM algorithm</h3>
<p>The aforementioned two-step iterative algorithm (i.e., as outlined in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-cluster-cycle">111</a>) illustrates how the <strong>EM Algorithm</strong> works. We have assumed that the two-step iterative algorithm would converge. Luckily, it had been proved that the EM Algorithm generally would converge<label for="tufte-sn-154" class="margin-toggle sidenote-number">154</label><input type="checkbox" id="tufte-sn-154" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">154</span> Wu, J., <em>On the Convergence Properties of the EM Algorithm</em>, The Annals of Statistics, Volume 11, Number 1, Pages 95-103, 1983.</span>.</p>
<!-- % Let's start with the Gaussian mixture model (GMM), that has been one of the most popular clustering model. GMM assumes that the data come from not just one distribution but a few. As shown in Figure \@ref(fig:f6-13) , the data is sampled from a mix of 4 distributions.  -->
<!-- % ```{r eval=FALSE,tidy=FALSE} -->
<!-- % # Simulate a clustering structure -->
<!-- % X <- c(rnorm(200, 0, 1), rnorm(200, 10,2), rnorm(200,20,1), rnorm(200,40, 2)) -->
<!-- % Y <- c(rnorm(800, 0, 1)) -->
<!-- % plot(X,Y, ylim = c(-5, 5), pch = 19, col = "gray25") -->
<!-- % ``` -->
<!-- % \begin{marginfigure} -->
<!-- %  \centering -->
<!-- %  \includegraphics{6_13.png} -->
<!-- %  \caption{A mixture of four Gaussian distributions} -->
<!-- %  \label{fig:6-13} -->
<!-- % \end{marginfigure} -->
<p>The task of the EM algorithm is to learn the unknown parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span> from a given dataset. The <span class="math inline">\(\boldsymbol{\Theta}\)</span> includes</p>
<p><!-- begin{enumerate} --></p>
<p>1. [1.] The parameters of the <span class="math inline">\(M\)</span> Gaussian distributions: <span class="math inline">\(\left\{\boldsymbol{\mu}_{m}, \boldsymbol{\Sigma}_{m}, m=1,2, \ldots, M\right\}\)</span>.</p>
<p>2. [2.] The probability vector <span class="math inline">\(\boldsymbol{\pi}\)</span> that includes the elements <span class="math inline">\(\left\{\pi_{m}, m=1,2, \ldots, M\right\}\)</span>.</p>
<p><!-- end{enumerate} --></p>
<p>Don’t forget the binary indicator variable for each data point, denoted as <span class="math inline">\(z_{n m}\)</span>: <span class="math inline">\(z_{n m}=1\)</span> indicates that the data point <span class="math inline">\(x_{n}\)</span> was sampled from the <span class="math inline">\(m^{th}\)</span> cluster<label for="tufte-sn-155" class="margin-toggle sidenote-number">155</label><input type="checkbox" id="tufte-sn-155" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">155</span> The reason that <span class="math inline">\(z_{n m}\)</span> is not included in <span class="math inline">\(\boldsymbol{\Theta}\)</span>, as it could be seen later, after the presentation of the EM algorithm, is that <span class="math inline">\(z_{n m}\)</span> provides a bridge to facilitate the learning of <span class="math inline">\(\boldsymbol{\Theta}\)</span>. They are not essential parameters of the model, although they are useful to facilitate the estimation of the parameters of the model. Entities like <span class="math inline">\(z_{n m}\)</span> are often called <strong>latent variables</strong> instead of <em>parameters</em>.</span>.</p>
<p><em>The Likelihood Function.</em> To learn these parameters from data, like in the logistic regression model, we derive a likelihood function to connect the data and parameters. For GMM, we cannot write <span class="math inline">\(p\left(\boldsymbol{x}_{n} | \boldsymbol{\Theta}\right)\)</span> directly. But it is possible to write <span class="math inline">\(p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right)\)</span> directly<label for="tufte-sn-156" class="margin-toggle sidenote-number">156</label><input type="checkbox" id="tufte-sn-156" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">156</span> That is what <span class="math inline">\(z_{n m}\)</span> is needed for.</span></p>
<p><span class="math display" id="eq:6-likelihood-xn">\[\begin{equation}
\small
    p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) = \prod_{m=1}^{M}\left[p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right) p\left(z_{n m}=1\right)\right]^{z_{n m}}.
\tag{35}
\end{equation}\]</span></p>
<p>We apply <em>log</em> on Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-likelihood-xn">(35)</a> and get the log-likelihood function in Eq <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-loglike-xn">(36)</a><label for="tufte-sn-157" class="margin-toggle sidenote-number">157</label><input type="checkbox" id="tufte-sn-157" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">157</span> Note that, by definition, <span class="math inline">\(\pi_m = p\left(z_{n m}=1\right)\)</span>.</span></p>
<p><span class="math display" id="eq:6-loglike-xn">\[\begin{equation}
\small
    \log p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) = \sum_{m=1}^{M}\left[z_{n m} \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right)+z_{n m} \log \pi_{m}\right]. 
\tag{36}
\end{equation}\]</span></p>
<p>It is known that<label for="tufte-sn-158" class="margin-toggle sidenote-number">158</label><input type="checkbox" id="tufte-sn-158" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">158</span> I.e., by the definition of multivariate normal distribution; interested readers may see the <strong>Appendix</strong> of this book for a brief review. Here, the constant term <span class="math inline">\((2 \pi)^{-p / 2}\)</span> in the density function of the multivariate normal distribution is ignored, so “<span class="math inline">\(\propto\)</span>” is used instead of “<span class="math inline">\(=\)</span>.”</span></p>
<p><span class="math display" id="eq:6-like-mnv-xn">\[\begin{equation}
\small
    p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta} \right) \propto \left|\boldsymbol{\Sigma}_{m}\right|^{-1 / 2} \exp \left\{-\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\right\}.
\tag{37}
\end{equation}\]</span></p>
<p>Plug Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-like-mnv-xn">(37)</a> into Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-loglike-xn">(36)</a>, we get</p>
<p><span class="math display" id="eq:6-loglike-xn2">\[\begin{equation}
\small
\begin{gathered}
    \log p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right) \propto \\
    \sum_{m=1}^{M}\left[z_{n m} \left( - \frac{1}{2}\log \left|\boldsymbol{\Sigma}_{m}\right|  -\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)+\right. z_{n m} \log \pi_{m} \right].
\end{gathered}
\tag{38}
\end{equation}\]</span></p>
<p>As there are <span class="math inline">\(N\)</span> data points, the complete log-likelihood function is defined as</p>
<p><span class="math display" id="eq:6-complete-loglike">\[\begin{equation}
\small
    l(\boldsymbol{\Theta}) = \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\Theta}) = \log \prod_{n=1}^{N} p\left(\boldsymbol{x}_{n}, z_{n m} | \boldsymbol{\Theta}\right).
\tag{39}
\end{equation}\]</span></p>
<p>With Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-loglike-xn2">(38)</a>, Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-complete-loglike">(39)</a> can be rewritten as</p>
<p><span class="math display" id="eq:6-complete-loglike2">\[\begin{equation}
\small
\begin{gathered}
    l(\boldsymbol{\Theta}) \propto \\
   \sum_{n=1}^{N} \sum_{m=1}^{M}\left[z_{n m} \left( - \frac{1}{2}\log \left|\boldsymbol{\Sigma}_{m}\right|  -\frac{1}{2}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)+\right. z_{n m} \log \pi_{m} \right].
\end{gathered}
\tag{40}
\end{equation}\]</span></p>
<p>Now we have an <em>explicit</em> form of <span class="math inline">\(l(\boldsymbol{\Theta})\)</span>, based on which we use an optimization algorithm to search for the best estimate of <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<p>Recall that <span class="math inline">\(z_{n m}\)</span> is unknown. Here comes the <em>initialization</em> again. Following the idea we have implemented in the data example shown in Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-example">25</a>, we propose the following strategy:</p>
<p><!-- begin{enumerate} --></p>
<p>1. Initialization. Either initialize <span class="math inline">\(\left\{z_{nm}, n=1,2, \ldots, N; m=1,2, \ldots, M\right\}\)</span> or <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<p>2. E-step. We can estimate <span class="math inline">\(z_{n m}\)</span> if we have known <span class="math inline">\(\boldsymbol{\Theta}\)</span> (i.e., given <span class="math inline">\(\boldsymbol{\Theta}\)</span>), the best estimate of <span class="math inline">\(z_{n m}\)</span> is the expectation of <span class="math inline">\(z_{n m}\)</span> where the expectation is taken regarding the distribution <span class="math inline">\(p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)\)</span> (i.e., denoted as <span class="math inline">\(\left\langle Z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}\)</span>). By definition, we have</p>
<p><span class="math display" id="eq:6-Eznm">\[\begin{equation}
\small
\begin{gathered}
    \left\langle z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}=1\cdot p\left(z_{n m}=1 | \boldsymbol{x}_{n},{\boldsymbol{\Theta}}\right)+0 \cdot p\left(z_{n m}=0 | \boldsymbol{x}_{n}, {\boldsymbol{\Theta}}\right).
\end{gathered}
\tag{41}
\end{equation}\]</span></p>
<p>It is known that</p>
<p><span class="math display" id="eq:6-znm">\[\begin{equation}
\small
    p\left(z_{n m}=1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)=\frac{p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right) \pi_{m}}{\sum_{k=1}^{M} p\left(\boldsymbol{x}_{n} | z_{n k}=1, \boldsymbol{\Theta}\right) \pi_{k}}.
\tag{42}
\end{equation}\]</span></p>
<p>Thus,</p>
<p><span class="math display" id="eq:6-Eznm2">\[\begin{equation}
\small
\begin{gathered}
    \left\langle z_{n m}\right\rangle_{p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}= \frac{p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right) \pi_{m}}{\sum_{k=1}^{M}  p\left(\boldsymbol{x}_{n} | z_{n k}=1, \boldsymbol{\Theta}\right) \pi_{k}}.
\end{gathered}
\tag{43}
\end{equation}\]</span></p>
<p>3. M-step. Then, we derive the expectation of <span class="math inline">\(l(\boldsymbol{\Theta})\)</span> regarding the distribution <span class="math inline">\(p\left(z_{n m} | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)\)</span></p>
<p><span class="math display" id="eq:6-likehihood-eznm">\[\begin{equation}
\small
\begin{gathered}
    \langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}=\sum_{n=1}^{N} \sum_{m=1}^{M}\left[\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)+\right. \\      \left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \log \pi_{m} ].
\end{gathered}   
\tag{44}
\end{equation}\]</span></p>
<p>And we optimize Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-likehihood-eznm">(44)</a> for <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<p>4. Repeat the E-step and M-step. With the updated <span class="math inline">\(\boldsymbol{\Theta}\)</span>, we go back to the estimate of <span class="math inline">\(z_{n m}\)</span> using Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-Eznm2">(43)</a>, and then, feed the new estimate of <span class="math inline">\(z_{n m}\)</span> into Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-likehihood-eznm">(44)</a>, and solve for <span class="math inline">\(\boldsymbol{\Theta}\)</span> again. Repeat these iterations, until all the parameters in the iterations don’t change significantly<label for="tufte-sn-159" class="margin-toggle sidenote-number">159</label><input type="checkbox" id="tufte-sn-159" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">159</span> Usually, we define a tolerance, e.g., the difference between two consecutive estimates of <span class="math inline">\(\boldsymbol{\Theta}\)</span> is numerically bounded, such as <span class="math inline">\(10^{-4}\)</span>.</span>.</p>
<p><!-- end{enumerate} --></p>
<p><em>More about the M-step.</em> To estimate the parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span>, in the M-step we use the First Derivative Test again and take derivatives of <span class="math inline">\(\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> (i.e., as shown in Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-likehihood-eznm">(44)</a>) regarding <span class="math inline">\(\boldsymbol{\Theta}\)</span> and put the derivatives equal to zero.</p>
<p>For <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span>, we have</p>
<p><span class="math display" id="eq:6-diff-mu">\[\begin{equation}
\small
    \frac{\partial\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}}{\partial \boldsymbol{\mu}_{m}}=\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\mu}_{m}}=\boldsymbol{0}.
\tag{45}
\end{equation}\]</span></p>
<p>Based on Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-like-mnv-xn">(37)</a>, we can derive</p>
<p><span class="math display" id="eq:6-diff-mu2">\[\begin{equation}
\small
    \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\mu}_{m}}=
    -\frac{1}{2} \frac{\partial\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)}{\partial \boldsymbol{\mu}_{m}}=\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}.
\tag{46}
\end{equation}\]</span></p>
<p>Putting the result of Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-diff-mu2">(46)</a> into Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-diff-mu">(45)</a>, we can estimate <span class="math inline">\(\boldsymbol{\mu}_{m}\)</span> by solving Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-diff-mu">(45)</a></p>
<p><span class="math display" id="eq:6-solution-mu">\[\begin{equation}
\small
    \boldsymbol{\mu}_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \boldsymbol{x}_{n}}{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}.
\tag{47}
\end{equation}\]</span></p>
<p>Similarly, we take derivatives of <span class="math inline">\(\langle l({\boldsymbol{\Theta}})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> regarding <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> and put the derivatives equal to zero</p>
<p><span class="math display" id="eq:6-diff-sigma">\[\begin{equation}
\small
    \frac{\partial\langle l(\boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}}{\partial \boldsymbol{\Sigma}_{m}}=\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m}  =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\Sigma}_{m}}=\boldsymbol{O}.
\tag{48}
\end{equation}\]</span></p>
<p>Based on Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-like-mnv-xn">(37)</a>, we can derive</p>
<p><span class="math display" id="eq:6-diff-sigma2">\[\begin{equation}
\small
\begin{gathered}
    \frac{\partial \log p\left(\boldsymbol{x}_{n} | z_{n m}=1, \boldsymbol{\Theta}\right)}{\partial \boldsymbol{\Sigma}_{m}} =  \\
    \frac{1}{2} \frac{\partial\left\{\left|\boldsymbol{\Sigma}_{m}\right|^{-1 / 2}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T} \boldsymbol{\Sigma}_{m}^{-1}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\right\}}{\partial \boldsymbol{\Sigma}_{m}}=\frac{1}{2}\left[\boldsymbol{\Sigma}_{m}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right].
\end{gathered}
\tag{49}
\end{equation}\]</span></p>
<p>Plug Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-diff-sigma2">(49)</a> into Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-diff-sigma">(48)</a>, we have</p>
<p><span class="math display" id="eq:6-diff-sigma3">\[\begin{equation}
\small
    \sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1 | \boldsymbol{X}, \boldsymbol{\Theta}\right)}\left[\boldsymbol{\Sigma}_{m}-\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right]=\boldsymbol{O}.
\tag{50}
\end{equation}\]</span></p>
<p>Solving Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-diff-sigma3">(50)</a>, we estimate <span class="math inline">\(\boldsymbol{\Sigma}_{m}\)</span> as</p>
<p><span class="math display" id="eq:6-solution-sigma">\[\begin{equation}
\small
    \boldsymbol{\Sigma}_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1| \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)} \left[\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{m}\right)^{T}\right]}{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1| \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}.
\tag{51}
\end{equation}\]</span></p>
<p>Lastly, to estimate <span class="math inline">\(\pi_{m}\)</span>, recall that <span class="math inline">\(\pi_m\)</span> is the percentage of the data points in the whole mix that come from the <span class="math inline">\(m^{th}\)</span> distribution, and <span class="math inline">\(\pi_m = p\left(z_{n m}=1\right)\)</span>, we can estimate <span class="math inline">\(\pi_{m}\)</span> as</p>
<p><span class="math display" id="eq:6-solution-pi">\[\begin{equation}
\small
    \pi_{m}=\frac{\sum_{n=1}^{N}\left\langle z_{n m}\right\rangle_{p\left(z_{n m} =1 | \boldsymbol{x}_{n}, \boldsymbol{\Theta}\right)}}{N}.
\tag{52}
\end{equation}\]</span></p>
</div>
<div id="convergence-of-the-em-algorithm" class="section level3 unnumbered">
<h3>Convergence of the EM Algorithm</h3>
<p>Readers may have found that Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-complete-loglike2">(40)</a> gives us the form of <span class="math inline">\(\log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\)</span>, that is what is denoted as <span class="math inline">\(l(\boldsymbol{\Theta})\)</span>. But, since <span class="math inline">\(\boldsymbol{Z}\)</span> is the latent variable and not part of the parameters, the objective function of the GMM model should be</p>
<p><span class="math display" id="eq:6-EMobj1">\[\begin{equation}
\small
    \log p(\boldsymbol{X}| \boldsymbol{\Theta})=\log \int p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\Theta}) d \boldsymbol{Z}.
\tag{53}
\end{equation}\]</span></p>
<p>But this is not what has been done in the EM algorithm. Instead, the EM algorithm solves for Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-likehihood-eznm">(44)</a>, that is essentially</p>
<p><span class="math display" id="eq:6-EMobj2">\[\begin{equation}
\small
    \langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})} =
    \int \log p(\boldsymbol{X}, \boldsymbol{Z} ; \boldsymbol{\Theta}) p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta}) d \boldsymbol{Z}.
\tag{54}
\end{equation}\]</span></p>
<p>How does the solving of Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-EMobj2">(54)</a> help the solving of Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-EMobj1">(53)</a>?</p>
<p>The power of the EM algorithm draws on <strong>Jensen’s inequality</strong>. Let <span class="math inline">\(f\)</span> be a convex function defined on an interval <span class="math inline">\(I\)</span>. If <span class="math inline">\(x_{1}, x_{2}, \ldots x_{n} \in I \text { and } \gamma_{1}, \gamma_{2}, \ldots \gamma_{n} \geq0\)</span> with <span class="math inline">\(\sum_{i=1}^{n} \gamma_{i}=1\)</span>, then based on Jensen’s inequality, it is known that <span class="math inline">\(f\left(\sum_{i=1}^{n} \gamma_{i} x_{i}\right) \leq \sum_{i=1}^{n} \gamma_{i} f\left(x_{i}\right)\)</span>. Let’s apply this result to analyze the EM algorithm.</p>
<p>First, notice that</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\log p(\boldsymbol{X} | \boldsymbol \Theta)=\log \int p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
=\log \int Q(\boldsymbol{Z}) \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
 
\end{equation*}\]</span></p>
<p>Here, <span class="math inline">\(Q(\boldsymbol{Z})\)</span> is any distribution of <span class="math inline">\(\boldsymbol{Z}\)</span>. In the EM algorithm</p>
<p><span class="math display">\[\begin{equation*}
\small
  Q(\boldsymbol{Z})=p(\boldsymbol{Z} | \boldsymbol{X}, \Theta). 
\end{equation*}\]</span></p>
<p>Using Jensen’s inequality here, we have</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\log \int Q(\boldsymbol{Z}) \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\geq \int Q(\boldsymbol{Z}) \log \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
 
\end{equation*}\]</span></p>
<p>Since</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\int Q(\boldsymbol{Z}) \log \frac{p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta)}{Q(\boldsymbol{Z})} d \boldsymbol{Z}.
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
=\int Q(\boldsymbol{Z}) \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}-\int Q(\boldsymbol{Z}) Q(\boldsymbol{Z}) d \boldsymbol{Z},
 
\end{equation*}\]</span></p>
<p>and <span class="math inline">\(\int Q(\boldsymbol{Z}) Q(\boldsymbol{Z}) d \boldsymbol{Z}\)</span> is quadratic and thus non-negative,</p>
<p>our final result is</p>
<p><span class="math display" id="eq:6-JI">\[\begin{equation}
\small
    \log p(\boldsymbol{X} | \boldsymbol \Theta) \geq \int Q(\boldsymbol{Z}) \log p(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol \Theta) d \boldsymbol{Z}.
\tag{55}
\end{equation}\]</span></p>
<p>When we set <span class="math inline">\(Q(\boldsymbol{Z}) = p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-JI">(55)</a> is rewritten as</p>
<p><span class="math display" id="eq:6-JI2">\[\begin{equation}
\small
    \log p(\boldsymbol{X} | \boldsymbol \Theta) \geq \langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}.
\tag{56}
\end{equation}\]</span></p>
<p>Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-JI2">(56)</a> reveals that <span class="math inline">\(\langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> is the <strong>lower bound</strong> of <span class="math inline">\(\log p(\boldsymbol{X} | \boldsymbol \Theta)\)</span>. Thus, maximization of <span class="math inline">\(\langle \log p(\boldsymbol{X}, \boldsymbol{Z}| \boldsymbol{\Theta})\rangle_{p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})}\)</span> can only increase the value of <span class="math inline">\(\log p(\boldsymbol{X} | \boldsymbol \Theta)\)</span>. This is why solving Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-EMobj2">(54)</a> helps the solving of Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-EMobj1">(53)</a>. This is the foundation of the effectiveness of the EM algorithm. The EM algorithm is often used to solve for problems that involve latent variables. Note that, <span class="math inline">\(Q(\boldsymbol{Z})\)</span> could be any distribution rather than <span class="math inline">\(p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, and Eq. <a href="chapter-6-diagnosis-residuals-heterogeneity.html#eq:6-JI">(55)</a> still holds. In applications where we could not explicitly derive <span class="math inline">\(p(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\Theta})\)</span>, a surrogate distribution is used for <span class="math inline">\(Q(\boldsymbol{Z})\)</span>. This variant of the EM algorithm is called the <em>variational inference</em><label for="tufte-sn-160" class="margin-toggle sidenote-number">160</label><input type="checkbox" id="tufte-sn-160" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">160</span> A good starting point to know more about variational inference within a context of GMM, see: David, B., Kucukelbir, A. and McAuliffe, J., <em>Variational Inference: A Review for Statisticians</em>, Journal of the American Statistical Association, Volume 112, Number 518, Pages 859-877, 2017.</span>.</p>
</div>
<div id="clustering-by-random-forest" class="section level3 unnumbered">
<h3>Clustering by random forest</h3>
<p>Many clustering algorithms have been developed. The random forest model can be used for clustering as well. This is a byproduct utility of a random forest model. One advantage of using random forest for clustering is that it can cluster data points with mixed types of variables. To conduct clustering in random forests is to extract the distance information between data points that have been learned by the random forest model. There are multiple ways to do so. For example, one approach<label for="tufte-sn-161" class="margin-toggle sidenote-number">161</label><input type="checkbox" id="tufte-sn-161" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">161</span> Shi, T. and Horvath, S., <em>Unsupervised learning with random forest predictors.</em> Journal of Computational and Graphical Statistics, Volume 15, Issue 1, Pages 118-138, 2006.</span> that has been implemented in the R package <code>randomForests</code> is to generate a synthetic dataset with the same size as the original dataset, e.g., randomly generate the measurements of each variable using its empirical marginal distribution. The original dataset is taken as one class, while the synthetic dataset is taken as another class. Since the random forest model is used to classify the two classes, it will stress on the difference between the two datasets, which is, the variable dependency that is embedded in the original dataset but lost in the synthetic dataset because of the way the synthetic dataset is generated. Hence, each tree will be enriched with splitting variables that are dependent on other variables. After the random forest model is built, a distance between any pair of two data points can be calculated based on the frequency of this pair of data points existing in the same nodes of the random forest model. With this distance information, distance-based clustering algorithms such as the <em>hierarchical clustering</em> or <em>K-means clustering</em><label for="tufte-sn-162" class="margin-toggle sidenote-number">162</label><input type="checkbox" id="tufte-sn-162" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">162</span> E.g., both could be implemented using the R package <code>cluster</code>.</span> algorithms can be applied to detect the clusters.</p>
<p>In the following example, we generate a dataset with two clusters. The clusters produced from the random forest model are shown in Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-16">115</a>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-16"></span>
<img src="graphics/6_16.png" alt="Clusters produced by the random forest model" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 115: Clusters produced by the random forest model<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We then use the following R code to apply a random forest model on this dataset to find the clusters. It can be seen that the clusters are reasonably recovered by the random forest model.</p>
<p></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>(<span class="at">all =</span> <span class="cn">TRUE</span>))</span>
<span id="cb143-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb143-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb143-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb143-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb143-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb143-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb143-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-8" aria-hidden="true" tabindex="-1"></a>ndata <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb143-9"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-10"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-10" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb143-11"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-11" aria-hidden="true" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>), sigma)</span>
<span id="cb143-12"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-12" aria-hidden="true" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">2</span>), sigma)</span>
<span id="cb143-13"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data1, data2)</span>
<span id="cb143-14"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-14" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(data)</span>
<span id="cb143-15"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-15" aria-hidden="true" tabindex="-1"></a>prox <span class="ot">&lt;-</span> rf<span class="sc">$</span>proximity</span>
<span id="cb143-16"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-16" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">pam</span>(prox, <span class="dv">2</span>)</span>
<span id="cb143-17"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-17" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(data)</span>
<span id="cb143-18"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-18" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>cluster <span class="ot">&lt;-</span> <span class="fu">as.character</span>(clusters<span class="sc">$</span>clustering)</span>
<span id="cb143-19"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-19" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> V1, <span class="at">y =</span> V2, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb143-20"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb143-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Data points&#39;</span>)</span></code></pre></div>
<p></p>
</div>
<div id="clustering-based-prediction-models" class="section level3 unnumbered">
<h3>Clustering-based prediction models</h3>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-clusterwisepred"></span>
<img src="graphics/6_clusterwisepred.png" alt="Clustering-based prediction models" width="100%"  />
<!--
<p class="caption marginnote">-->Figure 116: Clustering-based prediction models<!--</p>-->
<!--</div>--></span>
</p>
<p>
<!-- %[width=0.6\textwidth] --></p>
<p>As we have mentioned, clustering is a flexible concept. And it could be used in a combination of methods. Figure <a href="chapter-6-diagnosis-residuals-heterogeneity.html#fig:f6-clusterwisepred">116</a> illustrates the basic idea of <em>clustering-based prediction models</em>. It applies a clustering algorithm first on the data and then builds a model for each cluster. As a data analytics strategy, we could combine different clustering algorithms and prediction models that are appropriate for an application context. There are also integrated algorithms that have articulated this strategy on the formulation level. For example, the <em>Treed Regression</em> method<label for="tufte-sn-163" class="margin-toggle sidenote-number">163</label><input type="checkbox" id="tufte-sn-163" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">163</span> Alexander, W. and Grimshaw, S., <em>Treed regression.</em> Journal of Computational and Graphical Statistics, Volume 5, Issue 2, Pages 156-175, 1996.</span>
is one example that proposed to build a tree to stratify the dataset first, and then, create regression models on the leaf nodes—here, each leaf node is a cluster. Similarly, the <em>logistic model trees</em><label for="tufte-sn-164" class="margin-toggle sidenote-number">164</label><input type="checkbox" id="tufte-sn-164" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">164</span> Landwehr, N., Hall, M. and Frank, E. <em>Logistic model trees.</em> Machine Learning, Volume 59, Issue 1, Pages 161–205, 2004.</span> also use a tree model to cluster data points into different leaf nodes and build different logistic regression model for each leaf node. Motivated by this line of thought, more models have been developed with different combination of tree models and prediction models (or other types of statistical models) on the leaf nodes<label for="tufte-sn-165" class="margin-toggle sidenote-number">165</label><input type="checkbox" id="tufte-sn-165" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">165</span> Gramacy, R. and Lee, H. <em>Bayesian treed Gaussian process models with an application to computer modeling.</em> Journal of American Statistical Association, Volume 103, Issue 483, Pages 1119-1130, 2008.</span>.<label for="tufte-sn-166" class="margin-toggle sidenote-number">166</label><input type="checkbox" id="tufte-sn-166" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">166</span> Liu, H., Chen, X., Lafferty, J. and Wasserman, L. <em>Graph-valued regression.</em> In the Proceeding of Advances in Neural Information Processing Systems 23 (NIPS), 2010.</span></p>
</div>
</div>
<div id="exercises-4" class="section level2 unnumbered">
<h2>Exercises</h2>
<p><!-- begin{enumerate} --></p>
<p>1. In what follows is a summary of the clustering result on a dataset by using the R package <code>mclust</code>.</p>
<p><!-- end{enumerate} --></p>
<p></p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ---------------------------------------------------- </span></span>
<span id="cb144-2"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Gaussian finite mixture model fitted by EM algorithm </span></span>
<span id="cb144-3"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-3" aria-hidden="true" tabindex="-1"></a><span class="do">## ---------------------------------------------------- </span></span>
<span id="cb144-4"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb144-5"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Mclust VVV (ellipsoidal, varying volume, shape, </span></span>
<span id="cb144-6"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-6" aria-hidden="true" tabindex="-1"></a><span class="do">## and orientation) model with 3</span></span>
<span id="cb144-7"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-7" aria-hidden="true" tabindex="-1"></a><span class="do">## components: </span></span>
<span id="cb144-8"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb144-9"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  log-likelihood   n df       BIC       ICL</span></span>
<span id="cb144-10"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-10" aria-hidden="true" tabindex="-1"></a><span class="do">##       -2303.496 145 29 -4751.316 -4770.169</span></span>
<span id="cb144-11"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb144-12"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Clustering table:</span></span>
<span id="cb144-13"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-13" aria-hidden="true" tabindex="-1"></a><span class="do">##  1  2  3 </span></span>
<span id="cb144-14"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 81 36 28 </span></span>
<span id="cb144-15"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-15" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb144-16"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Mixing probabilities:</span></span>
<span id="cb144-17"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-17" aria-hidden="true" tabindex="-1"></a><span class="do">##         1         2         3 </span></span>
<span id="cb144-18"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.5368974 0.2650129 0.1980897 </span></span>
<span id="cb144-19"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-19" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb144-20"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Means:</span></span>
<span id="cb144-21"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-21" aria-hidden="true" tabindex="-1"></a><span class="do">##              [,1]     [,2]       [,3]</span></span>
<span id="cb144-22"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-22" aria-hidden="true" tabindex="-1"></a><span class="do">## glucose  90.96239 104.5335  229.42136</span></span>
<span id="cb144-23"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-23" aria-hidden="true" tabindex="-1"></a><span class="do">## insulin 357.79083 494.8259 1098.25990</span></span>
<span id="cb144-24"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-24" aria-hidden="true" tabindex="-1"></a><span class="do">## sspg    163.74858 309.5583   81.60001</span></span>
<span id="cb144-25"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-25" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb144-26"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-26" aria-hidden="true" tabindex="-1"></a><span class="do">## Variances:</span></span>
<span id="cb144-27"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-27" aria-hidden="true" tabindex="-1"></a><span class="do">## [,,1]</span></span>
<span id="cb144-28"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-28" aria-hidden="true" tabindex="-1"></a><span class="do">##          glucose    insulin       sspg</span></span>
<span id="cb144-29"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-29" aria-hidden="true" tabindex="-1"></a><span class="do">## glucose 57.18044   75.83206   14.73199</span></span>
<span id="cb144-30"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-30" aria-hidden="true" tabindex="-1"></a><span class="do">## insulin 75.83206 2101.76553  322.82294</span></span>
<span id="cb144-31"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-31" aria-hidden="true" tabindex="-1"></a><span class="do">## sspg    14.73199  322.82294 2416.99074</span></span>
<span id="cb144-32"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-32" aria-hidden="true" tabindex="-1"></a><span class="do">## [,,2]</span></span>
<span id="cb144-33"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-33" aria-hidden="true" tabindex="-1"></a><span class="do">##           glucose   insulin       sspg</span></span>
<span id="cb144-34"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-34" aria-hidden="true" tabindex="-1"></a><span class="do">## glucose  185.0290  1282.340  -509.7313</span></span>
<span id="cb144-35"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-35" aria-hidden="true" tabindex="-1"></a><span class="do">## insulin 1282.3398 14039.283 -2559.0251</span></span>
<span id="cb144-36"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-36" aria-hidden="true" tabindex="-1"></a><span class="do">## sspg    -509.7313 -2559.025 23835.7278</span></span>
<span id="cb144-37"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-37" aria-hidden="true" tabindex="-1"></a><span class="do">## [,,3]</span></span>
<span id="cb144-38"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-38" aria-hidden="true" tabindex="-1"></a><span class="do">##           glucose   insulin       sspg</span></span>
<span id="cb144-39"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-39" aria-hidden="true" tabindex="-1"></a><span class="do">## glucose  5529.250  20389.09  -2486.208</span></span>
<span id="cb144-40"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-40" aria-hidden="true" tabindex="-1"></a><span class="do">## insulin 20389.088  83132.48 -10393.004</span></span>
<span id="cb144-41"><a href="chapter-6-diagnosis-residuals-heterogeneity.html#cb144-41" aria-hidden="true" tabindex="-1"></a><span class="do">## sspg    -2486.208 -10393.00   2217.533</span></span></code></pre></div>
<p></p>
<p><!-- begin{enumerate}[resume] --></p>
<p>2. (a) How many samples in total are in this dataset? How many variables? (b) How many clusters are found? What are the sizes of the clusters? (c) What is the fitted GMM model? Please write its mathematical form.</p>
<p>3. Consider the dataset in Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-hw-q2">29</a> that has <span class="math inline">\(9\)</span> data points. Let’s use it to estimate a GMM model with <span class="math inline">\(3\)</span> clusters. The initial values are shown in Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-hw-q2">29</a></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-hw-q2">Table 29: </span>Initial values for a GMM model with <span class="math inline">\(3\)</span> clusters</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(1.53\)</span></th>
<th align="left"><span class="math inline">\(0.57\)</span></th>
<th align="left"><span class="math inline">\(2.56\)</span></th>
<th align="left"><span class="math inline">\(1.22\)</span></th>
<th align="left"><span class="math inline">\(4.13\)</span></th>
<th align="left"><span class="math inline">\(6.03\)</span></th>
<th align="left"><span class="math inline">\(0.98\)</span></th>
<th align="left"><span class="math inline">\(5.21\)</span></th>
<th align="left"><span class="math inline">\(-0.37\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C3\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C3\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>(a) Write the Gaussian mixture model (GMM) that you want to estimate. (b) Estimate the parameters of your GMM model. (c) Update the labels with your estimated parameters. (d) Estimate the parameters again.</p>
<p>4. Follow up on the dataset in Q3. Use the R pipeline for clustering on this data. Compare the result from R and the result from your manual calculation.</p>
<p>5. Consider the dataset in Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-hw-q4">30</a> that has <span class="math inline">\(10\)</span> data points. Let’s use it to estimate a GMM model with <span class="math inline">\(3\)</span> clusters. The initial values are shown in Table <a href="chapter-6-diagnosis-residuals-heterogeneity.html#tab:t6-hw-q4">30</a></p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t6-hw-q4">Table 30: </span>Initial values for a GMM model with <span class="math inline">\(3\)</span> clusters</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">ID</th>
<th align="left"><span class="math inline">\(2.22\)</span></th>
<th align="left"><span class="math inline">\(6.33\)</span></th>
<th align="left"><span class="math inline">\(3.15\)</span></th>
<th align="left"><span class="math inline">\(-0.89\)</span></th>
<th align="left"><span class="math inline">\(3.21\)</span></th>
<th align="left"><span class="math inline">\(1.10\)</span></th>
<th align="left"><span class="math inline">\(1.58\)</span></th>
<th align="left"><span class="math inline">\(0.03\)</span></th>
<th align="left"><span class="math inline">\(8.05\)</span></th>
<th align="left"><span class="math inline">\(0.26\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Label</td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C3\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C1\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
<td align="left"><span class="math inline">\(C3\)</span></td>
<td align="left"><span class="math inline">\(C2\)</span></td>
</tr>
</tbody>
</table>
<p></p>
<p>(a) Write the Gaussian mixture model (GMM) that you want to estimate. (b) Estimate the parameters of your GMM model. (c) Update the labels with your estimated parameters. (d) Estimate the parameters again.</p>
<p>6. Design a simulation experiment to test the effectiveness of the R package <code>mclust</code>. For instance, simulate a three-cluster structure in your dataset by this GMM model
<span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{x} \sim \pi_{1} N\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1}\right) + \pi_{2} N\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2}\right) + \pi_{3} N\left(\boldsymbol{\mu}_{3}, \boldsymbol{\Sigma}_{3}\right),
 
\end{equation*}\]</span>
where <span class="math inline">\(\pi_{1} = 0.5\)</span>, <span class="math inline">\(\pi_{2} = 0.25\)</span>, and <span class="math inline">\(\pi_{3} = 0.25\)</span>, and</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\mu}_{1} = \left[ \begin{array}{c}{5} \\ {3} \\ {3}\end{array}\right],  \text {     } \boldsymbol{\mu}_{2} = \left[ \begin{array}{c}{10} \\ {5} \\ {1}\end{array}\right], \text {     } \boldsymbol{\mu}_{3} = \left[ \begin{array}{c}{-5} \\ {10} \\ {-2}\end{array}\right];
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\Sigma}_{1}=\left[\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right], \text {     } \boldsymbol{\Sigma}_{2}=\left[\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right], \text {     } 
\boldsymbol{\Sigma}_{3}=\left[\begin{array}{ccc}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{array}\right].
 
\end{equation*}\]</span>
Then, use the <code>mclust</code> package on this dataset and see if the true clustering structure could be recovered.</p>
<p>7. Follow up on the simulation experiment in Q6. Let’s consider a GMM model with larger variance
<span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{x} \sim \pi_{1} N\left(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1}\right) + \pi_{2} N\left(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2}\right) + \pi_{3} N\left(\boldsymbol{\mu}_{3}, \boldsymbol{\Sigma}_{3}\right),
 
\end{equation*}\]</span>
where <span class="math inline">\(\pi_{1} = 0.5\)</span>, <span class="math inline">\(\pi_{2} = 0.25\)</span>, and <span class="math inline">\(\pi_{3} = 0.25\)</span>, and</p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\mu}_{1} = \left[ \begin{array}{c}{5} \\ {3} \\ {3}\end{array}\right],  \text {     } \boldsymbol{\mu}_{2} = \left[ \begin{array}{c}{10} \\ {5} \\ {1}\end{array}\right], \text {     } \boldsymbol{\mu}_{3} = \left[ \begin{array}{c}{-5} \\ {10} \\ {-2}\end{array}\right];
 
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\small
  
\boldsymbol{\Sigma}_{1}=\left[\begin{array}{ccc}3 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{array}\right], \text {     } \boldsymbol{\Sigma}_{2}=\left[\begin{array}{ccc}3 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{array}\right], \text {     } 
\boldsymbol{\Sigma}_{3}=\left[\begin{array}{ccc}3 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{array}\right].
 
\end{equation*}\]</span>
Then, use the R package <code>mclust</code> on this dataset and see if the true clustering structure could be recovered.</p>
<p>8. Design a simulation experiment to test the effectiveness of the diagnostic tools in the <code>ggfortify</code> R package. For instance, use the same simulation procedure that has been used in Q9 of <strong>Chapter 2</strong> to design a linear regression model with two variables, simulate <span class="math inline">\(100\)</span> samples from this model, fit the model, and draw the diagnostic figures.</p>
<p>9. Follow up on the simulation experiment in Q8. Add a few outliers into your dataset and see if the diagnostic tools in the <code>ggfortify</code> R package can detect them.</p>
<p><!-- end{enumerate} --></p>
<!-- \begin{figure*} -->
<!--    \centering -->
<!--    \checkoddpage \ifoddpage \forcerectofloat \else \forceversofloat \fi -->
<!--    \includegraphics[width = 0.05\textwidth]{graphics/9points_4lines2.png} -->
<!-- \end{figure*} -->

</div>
</div>
<p style="text-align: center;">
<a href="chapter-5-learning-i-cross-validation-oob.html"><button class="btn btn-default">Previous</button></a>
<a href="chapter-7-learning-ii-svm-ensemble-learning.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
