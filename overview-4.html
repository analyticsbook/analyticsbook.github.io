<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Overview | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Overview | Data Analytics">

<title>Overview | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="overview-4" class="section level2 unnumbered">
<h2>Overview</h2>
<p>Chapter 6 is about <em>Diagnosis</em>. Diagnosis, in one sense, is to see if the assumptions of the model match the empirical characteristics of the data. For example, the t-test of linear regression model builds on the normality assumption of the errors. If this assumption is not met by the data, the result of the t-test is concerned. Departure from assumptions doesn’t always mean that the model is not useful<label for="tufte-sn-136" class="margin-toggle sidenote-number">136</label><input type="checkbox" id="tufte-sn-136" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">136</span> <em>“All models are wrong, some are useful.”—</em> George Box.</span>. The gap between the theoretical assumptions of the model and the empirical data characteristics, together with the model itself, should be taken as a whole when we evaluate the strength of the conclusion. This wholesome idea is what diagnosis is about. It also helps us to identify opportunities to improve the model. Models are representations/approximations of reality, so we have to be critical about them, yet being critical is different from being dismissive<label for="tufte-sn-137" class="margin-toggle sidenote-number">137</label><input type="checkbox" id="tufte-sn-137" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">137</span> A model that doesn’t fit the data also generates knowledge—revealed not by the failed model but by the fact that this model actually misfits. See Jaynes, E.T., <em>Probability Theory: the Logic of Science. </em> Cambridge Press, 2003.</span>. There are many diagnostic tools that we can use to strengthen our critical evaluation.</p>
<p>Many diagnostic tools focus on the <strong>residual analysis</strong> . Residuals provide a numerical evaluation of the difference between the model and the data. Recall that <span class="math inline">\(y\)</span> denotes the observed value of the outcome variable, <span class="math inline">\(f(\boldsymbol{x})\)</span> denotes the model, and <span class="math inline">\(\hat{y}\)</span> denotes the prediction (i.e., <span class="math inline">\(\hat{y} = f(\boldsymbol{x})\)</span> is the prediction made by the model on the data point <span class="math inline">\(\boldsymbol{x}\)</span>). The residual, denoted as <span class="math inline">\(\hat{\epsilon}\)</span>, is defined as <span class="math inline">\(\hat{\epsilon} = \hat{y} - y\)</span>. For any model that is trained on <span class="math inline">\(N\)</span> data points, we could obtain <span class="math inline">\(N\)</span> residuals, and draw the residuals as shown in Figure <a href="overview-4.html#fig:f6-3residuals">97</a>:</p>
<p></p>
<div class="figure"><span id="fig:f6-3residuals"></span>
<p class="caption marginnote shownote">
Figure 97: Suppose that three models are built on a dataset, and their residual plots are drawn: (left) decision tree; (middle) RF; (right) linear regression
</p>
<img src="graphics/6_residualplots.png" alt="Suppose that three models are built on a dataset, and their residual plots are drawn: (left) decision tree; (middle) RF; (right) linear regression" width="100%"  />
</div>
<p></p>
<p><!-- begin{enumerate} --></p>
<ul>
<li> Figure <a href="overview-4.html#fig:f6-3residuals">97</a> (left). There is a linear relationship between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span>, which suggests an absurd fact: <span class="math inline">\(\hat{y}\)</span> could be used as a predictor to predict the <span class="math inline">\(\hat{\epsilon}\)</span>, the <em>error</em>. For instance, when <span class="math inline">\(\hat{y} = -1\)</span>, the error is between <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. If we adjust the prediction to be <span class="math inline">\(\hat{y} + 1\)</span>, wouldn’t that make the error to be between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>? A reduced error means a better prediction model.</li>
</ul>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f6-newmodel"></span>
<img src="graphics/6_newmodel.png" alt="A new model, inspired by the pattern seen in Figure \@ref(fig:f6-3residuals) (left)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 98: A new model, inspired by the pattern seen in Figure <a href="overview-4.html#fig:f6-3residuals">97</a> (left)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>To generalize this, let’s build another model <span class="math inline">\(g[f(\boldsymbol{x})]\)</span> that takes <span class="math inline">\(f(\boldsymbol{x})\)</span> as the predictor to predict <span class="math inline">\(\hat{\epsilon}\)</span>. Then, we can combine the two models, <span class="math inline">\(f(\boldsymbol{x})\)</span> and <span class="math inline">\(g[f(\boldsymbol{x})]\)</span>, and obtain an improved prediction <span class="math inline">\(\hat{y}\)</span> as <span class="math inline">\(f(\boldsymbol{x}) + g[f(\boldsymbol{x})]\)</span>. This is shown in Figure <a href="overview-4.html#fig:f6-newmodel">98</a>.</p>
<ul>
<li><p> Figure <a href="overview-4.html#fig:f6-3residuals">97</a> (middle). No correlation between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span> is observed. In other words, knowing <span class="math inline">\(\hat{y}\)</span> offers no help to predict <span class="math inline">\(\hat{\epsilon}\)</span>. This is what a good model would behave like.</p></li>
<li><p> Figure <a href="overview-4.html#fig:f6-3residuals">97</a> (right). There is a piece-wise linear relationship between <span class="math inline">\(\hat{\epsilon}\)</span> and <span class="math inline">\(\hat{y}\)</span>. If we segment the figure by a vertical line at zero, we could apply the same argument made in Figure <a href="overview-4.html#fig:f6-3residuals">97</a> (left) for each piece here: the model could be further improved following the same strategy outlined in Figure <a href="overview-4.html#fig:f6-newmodel">98</a>.</p></li>
</ul>
<p><!-- end{enumerate} --></p>
<p>As each data point contributes a residual, the <em>residual analysis</em> offers us opportunities to examine some collective phenomena to improve the overall quality of the model. It also helps us check local patterns where we may find areas of improvement of the model or particularities of the data that the model could not synthesize. The beauty of checking out the residuals is that there is always something that is beyond our experience and expectation.</p>
</div>
<p style="text-align: center;">
<a href="chapter-6-diagnosis-residuals-heterogeneity.html"><button class="btn btn-default">Previous</button></a>
<a href="diagnosis-in-regression.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
