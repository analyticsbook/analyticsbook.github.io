<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Ensemble learning | Data Analytics" />
<meta property="og:type" content="book" />





<meta name="author" content="Shuai Huang &amp; Houtao Deng" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Ensemble learning | Data Analytics">

<title>Ensemble learning | Data Analytics</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="https://use.typekit.net/ajy6rnl.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>
<!-- <link rel="stylesheet" href="css/normalize.css"> -->
<!-- <link rel="stylesheet" href="css/envisioned.css"/> -->
<link rel="stylesheet" href="css/tablesaw-stackonly.css"/>
<link rel="stylesheet" href="css/nudge.css"/>
<link rel="stylesheet" href="css/sourcesans.css"/>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>




</head>

<body>

<!--bookdown:toc:start-->

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

<!--bookdown:toc:end-->

<div class="menu-btn"><h3>☰ Menu</h3></div>

<div class="site-overlay"></div>


<div class="row">
<div class="col-sm-12">

<nav class="pushy pushy-left" id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li><a href="chapter-1-introduction.html#chapter-1.-introduction">Chapter 1. Introduction</a></li>
<li><a href="chapter-2-abstraction-regression-tree-models.html#chapter-2.-abstraction-regression-tree-models">Chapter 2. Abstraction: Regression &amp; Tree Models</a></li>
<li><a href="chapter-3-recognition-logistic-regression-ranking.html#chapter-3.-recognition-logistic-regression-ranking">Chapter 3. Recognition: Logistic Regression &amp; Ranking</a></li>
<li><a href="chapter-4-resonance-bootstrap-random-forests.html#chapter-4.-resonance-bootstrap-random-forests">Chapter 4. Resonance: Bootstrap &amp; Random Forests</a></li>
<li><a href="chapter-5-learning-i-cross-validation-oob.html#chapter-5.-learning-i-cross-validation-oob">Chapter 5. Learning (I): Cross-validation &amp; OOB</a></li>
<li><a href="chapter-6-diagnosis-residuals-heterogeneity.html#chapter-6.-diagnosis-residuals-heterogeneity">Chapter 6. Diagnosis: Residuals &amp; Heterogeneity</a></li>
<li><a href="chapter-7-learning-ii-svm-ensemble-learning.html#chapter-7.-learning-ii-svm-ensemble-learning">Chapter 7. Learning (II): SVM &amp; Ensemble Learning</a></li>
<li><a href="chapter-8-scalability-lasso-pca.html#chapter-8.-scalability-lasso-pca">Chapter 8. Scalability: LASSO &amp; PCA</a></li>
<li><a href="chapter-9-pragmatism-experience-experimental.html#chapter-9.-pragmatism-experience-experimental">Chapter 9. Pragmatism: Experience &amp; Experimental</a></li>
<li><a href="chapter-10-synthesis-architecture-pipeline.html#chapter-10.-synthesis-architecture-pipeline">Chapter 10. Synthesis: Architecture &amp; Pipeline</a></li>
<li><a href="conclusion.html#conclusion">Conclusion</a></li>
<li><a href="appendix-a-brief-review-of-background-knowledge.html#appendix-a-brief-review-of-background-knowledge">Appendix: A Brief Review of Background Knowledge</a></li>
</ul>
</nav>

</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="ensemble-learning" class="section level2 unnumbered">
<h2>Ensemble learning</h2>
<div id="rationale-and-formulation-11" class="section level3 unnumbered">
<h3>Rationale and formulation</h3>
<p><strong>Ensemble learning</strong> is another example of how we design better learning algorithms. The random forest model is a particular case of <strong>ensemble models</strong>. An ensemble model consists of <span class="math inline">\(K\)</span> <em>base models</em>, denoted as, <span class="math inline">\(h_{1}, h_{2}, \ldots, h_{K}\)</span>. The algorithms to create ensemble models differ from each other in terms of the types of the base models, the way to create diversity in the base models, etc.</p>
<p>We have known the random forest model uses Bootstrap to create many datasets and builds a set of decision tree models. Some other ensemble learning methods, such as the <strong>AdaBoost</strong> model, also use decision tree as the base model. The two differ in the way to build a <em>diverse</em> set of base models. The framework of AdaBoost is illustrated in Figure <a href="ensemble-learning.html#fig:f7-AdaBoost">123</a>. AdaBoost employs a sequential process to build its base models: it uses the original dataset (when the weights for the data points are equal) to build a decision tree; then it uses the decision tree to predict on the dataset, obtains the errors, and updates the weights of the data points<label for="tufte-sn-188" class="margin-toggle sidenote-number">188</label><input type="checkbox" id="tufte-sn-188" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">188</span> I.e., those data points that are wrongly classified will gain higher weights.</span>; then it builds another decision tree on the same dataset with the new weights, obtains the errors, and updates the weights of the data points again. The sequential process continues, until a given number of decision trees are built. This sequential process is designed for adaptability: later models focus more on the <em>hard</em> data points that present challenges for previous base models to achieve good prediction performance. Interested readers may find a formal presentation of the AdaBoost algorithm in the <strong>Remarks</strong> section.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f7-AdaBoost"></span>
<img src="graphics/adaboost.png" alt="A general framework of AdaBoost" width="100%"  />
<p class="caption marginnote shownote">
Figure 123: A general framework of AdaBoost
</p>
</div>
<p></p>
<p>The ensemble learning is flexible, given that any model could be a base model. And there are a variety of ways to resample or perturb a dataset to create a diverse set of base models. Like SVM, the ensemble learning is another approach to have a built-in mechanism to reduce the risk of overfitting. Here, we provide a discussion of this built-in mechanism using the framework proposed by Dietterich, where three perspectives (statistical, computational, and representational) were used to explain why ensemble methods could lead to robust performance. Each perspective is described in details below.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-23"></span>
<img src="graphics/7_EL_stat.png" alt="Ensemble learning approximates the true model with a combination of good models (statistical perspective)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 124: Ensemble learning approximates the true model with a combination of good models (statistical perspective)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Statistical perspective.</em> The statistical reason is illustrated in Figure <a href="ensemble-learning.html#fig:f7-23">124</a>. <span class="math inline">\(\mathcal{H}\)</span> is the model space where a learning algorithm searches for the best model guided by the training data. A model corresponds to a <em>point</em> in Figure <a href="ensemble-learning.html#fig:f7-23">124</a>, e.g., the point labelled as <span class="math inline">\(f\)</span> is the true model. When the data is limited and the best models are multiple, the problem is a statistical one and we need to make an optimal decision despite the uncertainty. This is illustrated by the inner circle in Figure <a href="ensemble-learning.html#fig:f7-23">124</a>. By building an ensemble of multiple base models, e.g., the <span class="math inline">\(h_{1}, h_{2}, \text { and } h_{3}\)</span> in Figure <a href="ensemble-learning.html#fig:f7-23">124</a>, the average of the models is a good approximation to the true model <span class="math inline">\(f\)</span>. This combined solution, comparing with other models that only identify one best model, has less variance, and therefore, could be more robust.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-24"></span>
<img src="graphics/7_EL_comp.png" alt=" Ensemble learning provides a robust coverage of the true model (computational perspective)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 125:  Ensemble learning provides a robust coverage of the true model (computational perspective)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Computational perspective.</em> A computational perspective is shown in Figure <a href="ensemble-learning.html#fig:f7-24">125</a>. This perspective concerns the way we build base models. Often greedy approaches such as the recursive splitting procedure are used to solve optimization problems in training machine learning models. This is optimal only in a <em>local</em> sense<label for="tufte-sn-189" class="margin-toggle sidenote-number">189</label><input type="checkbox" id="tufte-sn-189" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">189</span> E.g., to grow a decision tree, at each node, the node is split according to the maximum information gain <em>at this particular node</em>. To grow a decision tree model, a sequence of splits is needed. Optimization of all the splits <em>simultaneously</em> leads to a <em>global</em> optimal solution, but it is a <em>NP-hard</em> problem that is not solved yet. Optimization of each split is more practical, only we know that the local optimal solution may result in suboptimal situations for further splitting of descendant nodes.</span>. As a remedy to this problem, the ensemble learning initializes the learning algorithm (that is greedy and heuristic) from multiple locations in <span class="math inline">\(\mathcal{H}\)</span>, i.e., as shown in Figure <a href="ensemble-learning.html#fig:f7-24">125</a>, three models are identified by the same algorithm that starts from different initial points. Exploring multiple trajectories help us find a robust coverage of the true model <span class="math inline">\(f\)</span>.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-25"></span>
<img src="graphics/7_EL_rep.png" alt=" Ensemble learning approximates the true model with a combination of good models (representational perspective)" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 126:  Ensemble learning approximates the true model with a combination of good models (representational perspective)<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>Representational perspective.</em> Due to the size of the dataset or the limitations of a model, sometimes the model space <span class="math inline">\(\mathcal{H}\)</span> does not cover the true model, i.e., in Figure <a href="ensemble-learning.html#fig:f7-25">126</a> the true model is outside the region of <span class="math inline">\(\mathcal{H}\)</span>. This is not uncommon in real-world problems, for example, linear models cannot learn nonlinear patterns, or decision trees have difficulty in learning linear patterns. Using multiple base models may provide an approximation of the true model that is outside <span class="math inline">\(\mathcal{H}\)</span>, as shown in Figure <a href="ensemble-learning.html#fig:f7-25">126</a>.</p>
</div>
<div id="analysis-of-the-decision-tree-random-forests-and-adaboost" class="section level3 unnumbered">
<h3>Analysis of the decision tree, random forests, and AdaBoost</h3>
<p>The three models are analyzed using the three perspectives. Results are shown in Table <a href="ensemble-learning.html#tab:t8-threemodels">31</a>. In-depth discussions are provided in the following.</p>
<p><em>Single decision tree.</em> A single decision tree lacks the capability to overcome overfitting in terms of each of the three perspectives. From the statistical perspective, a decision tree algorithm constructs each node using the maximum information gain <em>at that particular node only</em>; thus, random errors in data may mislead subsequent splits. On the other hand, when the training dataset is limited, many models may perform equally well, since there are not enough data to distinguish these models. This results in a large <em>inner circle</em> as shown in Figure <a href="ensemble-learning.html#fig:f7-23">124</a>. With the true model <span class="math inline">\(f\)</span> hidden in a large area in <span class="math inline">\(\mathcal{H}\)</span>, and the sensitivity of the learning algorithm to random noises in data (an issue from the computational perspective), the learning algorithm may end up with a model far away from the true model <span class="math inline">\(f\)</span>.</p>
<p></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:t8-threemodels">Table 31: </span>Analysis of the decision tree (DT), random forests (RF), and AdaBoost using the three perspectives</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left"><em>Perspectives</em></th>
<th align="left">DT</th>
<th align="left">RF</th>
<th align="left">AdaBoost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Statistical</td>
<td align="left">No</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Computational</td>
<td align="left">No</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Representational</td>
<td align="left">No</td>
<td align="left">No</td>
<td align="left">Yes</td>
</tr>
</tbody>
</table>
<p></p>
<p>From the representational perspective, there are also limitations of the decision tree model; i.e., in <strong>Chapter 2</strong> we have shown that the decision tree model has difficulty in modeling linear patterns in the data.</p>
<p></p>
<div class="figure fullwidth"><span id="fig:f7-RF-analysis"></span>
<img src="graphics/7_EL_rf.png" alt="Analysis of the random forest in terms of the statistical (left), computational (middle), and representational (right) perspectives" width="100%"  />
<p class="caption marginnote shownote">
Figure 127: Analysis of the random forest in terms of the statistical (left), computational (middle), and representational (right) perspectives
</p>
</div>
<p></p>
<p><em>Random forests.</em> From the statistical perspective, the random forest model is a good ensemble learning model. As shown in Figure <a href="ensemble-learning.html#fig:f7-RF-analysis">127</a> (left), the way the random forest model grows the base models is to construct the <em>circle</em> of dotted line. Models located in this circle of dotted line have reasonably good accuracy. These models may not be the best models with great accuracy, they do provide a good coverage/approximation of the true model.</p>
<p>Note that, if we could directly build a model that is close to <span class="math inline">\(f\)</span>, or build many best models that are located in the circle of dotted line, that would be ideal. However, both tasks are challenging. Comparing with these ideal goals, the random forest model is more pragmatic. It cleverly uses <em>simple</em><label for="tufte-sn-190" class="margin-toggle sidenote-number">190</label><input type="checkbox" id="tufte-sn-190" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">190</span> As we have seen, <em>Simple</em> is a complex word.</span> techniques of <em>randomness</em>, i.e., the Bootstrap and the random selection of variables, that are robust, effective, and easy to implement. It grows a set of models that are not the best, but good models. Most importantly, these good models complement each other<label for="tufte-sn-191" class="margin-toggle sidenote-number">191</label><input type="checkbox" id="tufte-sn-191" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">191</span> In practice, the challenge to grow a set of <em>best</em> models is that it usually ends up with these <em>best models</em> more or less being the same.</span>.</p>
<p>Random forest model can also address the computational issue. As shown in Figure <a href="ensemble-learning.html#fig:f7-RF-analysis">127</a> (middle), while the circle of solid line (i.e., that represents the space of best models) is computationally difficult to reach, averaging multiple models could provide a good approximation.</p>
<p>It seems that the random forest models do not actively solve the representational issue. If the true model <span class="math inline">\(f\)</span> lies outside <span class="math inline">\(\mathcal{H}\)</span>, as shown in Figure <a href="ensemble-learning.html#fig:f7-RF-analysis">127</a> (right), averaging multiple models won’t necessarily approximate the true model.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-29"></span>
<img src="graphics/7_EL_adaBoost.png" alt="Analysis of the AdaBoost in terms of the representational perspective" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 128: Analysis of the AdaBoost in terms of the representational perspective<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p><em>AdaBoost.</em> Similar to random forest, AdaBoost solves the computational issue by generating many base models. The difference is that, AdaBoost actively solves the representational issue, i.e., it tries to do better on the <em>hard</em> data points where the previous base models fail to predict correctly. For each base model in AdaBoost, the training dataset is not resampled by Bootstrap, but weighted based on the error rates from previous base models, i.e., data points that are difficult to be correctly predicted by the previous models are given more weights in the new training dataset for the subsequent base model. Figure <a href="ensemble-learning.html#fig:f7-29">128</a> shows this sequential learning process helps AdaBoost identify more models around the true model, and put more weight to the models that are closer to the true model.</p>
<p>But AdaBoost is not as good as random forest in terms of addressing the statistical issue. As AdaBoost aggressively solves the representational issue and allows its base models to be impacted by some <em>hard</em> data points<label for="tufte-sn-192" class="margin-toggle sidenote-number">192</label><input type="checkbox" id="tufte-sn-192" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">192</span> This is a common root cause for a model to overfit the training data, if the model tries <em>too hard</em> on a particular training data.</span>, it is more likely to overfit, and may be less stable than the random forest models that place more emphasis on addressing the statistical issue.</p>
</div>
<div id="r-lab-10" class="section level3 unnumbered">
<h3>R Lab</h3>
<p>We use the AD dataset to study decision tree (<code>rpart</code> package), random forests (<code>randomForest</code> package), and AdaBoost (<code>gbm</code> package).</p>
<p>First, we evaluate the overall performance of the three models. Results are shown in Figure <a href="ensemble-learning.html#fig:f7-30">129</a>, produced by the following R code.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-30"></span>
<img src="graphics/7_30.png" alt="Boxplots of the classification error rates for single decision tree, random forest, and AdaBoost" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 129: Boxplots of the classification error rates for single decision tree, random forest, and AdaBoost<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="ensemble-learning.html#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_gray</span>(<span class="at">base_size =</span> <span class="dv">15</span>))</span>
<span id="cb157-2"><a href="ensemble-learning.html#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb157-3"><a href="ensemble-learning.html#cb157-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb157-4"><a href="ensemble-learning.html#cb157-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb157-5"><a href="ensemble-learning.html#cb157-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb157-6"><a href="ensemble-learning.html#cb157-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb157-7"><a href="ensemble-learning.html#cb157-7" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com&quot;</span>,</span>
<span id="cb157-8"><a href="ensemble-learning.html#cb157-8" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;/analyticsbook/book/main/data/AD.csv&quot;</span>)</span>
<span id="cb157-9"><a href="ensemble-learning.html#cb157-9" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">text=</span><span class="fu">getURL</span>(url))</span>
<span id="cb157-10"><a href="ensemble-learning.html#cb157-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-11"><a href="ensemble-learning.html#cb157-11" aria-hidden="true" tabindex="-1"></a>rm_indx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(data) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;TOTAL13&quot;</span>,</span>
<span id="cb157-12"><a href="ensemble-learning.html#cb157-12" aria-hidden="true" tabindex="-1"></a>                                       <span class="st">&quot;MMSCORE&quot;</span>))</span>
<span id="cb157-13"><a href="ensemble-learning.html#cb157-13" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[, <span class="sc">-</span>rm_indx]</span>
<span id="cb157-14"><a href="ensemble-learning.html#cb157-14" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>DX_bl <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>DX_bl)</span>
<span id="cb157-15"><a href="ensemble-learning.html#cb157-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-16"><a href="ensemble-learning.html#cb157-16" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb157-17"><a href="ensemble-learning.html#cb157-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-18"><a href="ensemble-learning.html#cb157-18" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb157-19"><a href="ensemble-learning.html#cb157-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (K <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>)) {</span>
<span id="cb157-20"><a href="ensemble-learning.html#cb157-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-21"><a href="ensemble-learning.html#cb157-21" aria-hidden="true" tabindex="-1"></a>testing.indices <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb157-22"><a href="ensemble-learning.html#cb157-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb157-23"><a href="ensemble-learning.html#cb157-23" aria-hidden="true" tabindex="-1"></a>testing.indices <span class="ot">&lt;-</span> <span class="fu">rbind</span>(testing.indices, <span class="fu">sample</span>(<span class="fu">nrow</span>(data),</span>
<span id="cb157-24"><a href="ensemble-learning.html#cb157-24" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">floor</span>((<span class="dv">1</span> <span class="sc">-</span> K) <span class="sc">*</span> <span class="fu">nrow</span>(data))))</span>
<span id="cb157-25"><a href="ensemble-learning.html#cb157-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb157-26"><a href="ensemble-learning.html#cb157-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-27"><a href="ensemble-learning.html#cb157-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb157-28"><a href="ensemble-learning.html#cb157-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-29"><a href="ensemble-learning.html#cb157-29" aria-hidden="true" tabindex="-1"></a>  testing.ix <span class="ot">&lt;-</span> testing.indices[i, ]</span>
<span id="cb157-30"><a href="ensemble-learning.html#cb157-30" aria-hidden="true" tabindex="-1"></a>  target.testing <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl[testing.ix]</span>
<span id="cb157-31"><a href="ensemble-learning.html#cb157-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb157-32"><a href="ensemble-learning.html#cb157-32" aria-hidden="true" tabindex="-1"></a>  tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ])</span>
<span id="cb157-33"><a href="ensemble-learning.html#cb157-33" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree, data[testing.ix, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb157-34"><a href="ensemble-learning.html#cb157-34" aria-hidden="true" tabindex="-1"></a>  error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb157-35"><a href="ensemble-learning.html#cb157-35" aria-hidden="true" tabindex="-1"></a>                  target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb157-36"><a href="ensemble-learning.html#cb157-36" aria-hidden="true" tabindex="-1"></a>  err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;tree&quot;</span>, K, error))</span>
<span id="cb157-37"><a href="ensemble-learning.html#cb157-37" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb157-38"><a href="ensemble-learning.html#cb157-38" aria-hidden="true" tabindex="-1"></a>  rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ])</span>
<span id="cb157-39"><a href="ensemble-learning.html#cb157-39" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[testing.ix, ])</span>
<span id="cb157-40"><a href="ensemble-learning.html#cb157-40" aria-hidden="true" tabindex="-1"></a>  error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span> </span>
<span id="cb157-41"><a href="ensemble-learning.html#cb157-41" aria-hidden="true" tabindex="-1"></a>                  target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb157-42"><a href="ensemble-learning.html#cb157-42" aria-hidden="true" tabindex="-1"></a>  err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;RF&quot;</span>, K, error))</span>
<span id="cb157-43"><a href="ensemble-learning.html#cb157-43" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb157-44"><a href="ensemble-learning.html#cb157-44" aria-hidden="true" tabindex="-1"></a>  data1 <span class="ot">&lt;-</span> data</span>
<span id="cb157-45"><a href="ensemble-learning.html#cb157-45" aria-hidden="true" tabindex="-1"></a>  data1<span class="sc">$</span>DX_bl <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(data1<span class="sc">$</span>DX_bl))</span>
<span id="cb157-46"><a href="ensemble-learning.html#cb157-46" aria-hidden="true" tabindex="-1"></a>  boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data1[<span class="sc">-</span>testing.ix, ],</span>
<span id="cb157-47"><a href="ensemble-learning.html#cb157-47" aria-hidden="true" tabindex="-1"></a>               <span class="at">dist =</span> <span class="st">&quot;adaboost&quot;</span>,<span class="at">interaction.depth =</span> <span class="dv">6</span>,</span>
<span id="cb157-48"><a href="ensemble-learning.html#cb157-48" aria-hidden="true" tabindex="-1"></a>               <span class="at">n.tree =</span> <span class="dv">2000</span>)  <span class="co">#cv.folds = 5, </span></span>
<span id="cb157-49"><a href="ensemble-learning.html#cb157-49" aria-hidden="true" tabindex="-1"></a>  <span class="co"># best.iter &lt;- gbm.perf(boost,method=&#39;cv&#39;)</span></span>
<span id="cb157-50"><a href="ensemble-learning.html#cb157-50" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost, data1[testing.ix, ], <span class="at">n.tree =</span> <span class="dv">2000</span>,</span>
<span id="cb157-51"><a href="ensemble-learning.html#cb157-51" aria-hidden="true" tabindex="-1"></a>                  <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)  <span class="co"># best.iter n.tree = 400, </span></span>
<span id="cb157-52"><a href="ensemble-learning.html#cb157-52" aria-hidden="true" tabindex="-1"></a>  pred[pred <span class="sc">&gt;</span> <span class="fl">0.5</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb157-53"><a href="ensemble-learning.html#cb157-53" aria-hidden="true" tabindex="-1"></a>  pred[pred <span class="sc">&lt;=</span> <span class="fl">0.5</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb157-54"><a href="ensemble-learning.html#cb157-54" aria-hidden="true" tabindex="-1"></a>  error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb157-55"><a href="ensemble-learning.html#cb157-55" aria-hidden="true" tabindex="-1"></a>                        target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb157-56"><a href="ensemble-learning.html#cb157-56" aria-hidden="true" tabindex="-1"></a>  err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;AdaBoost&quot;</span>, K, error))</span>
<span id="cb157-57"><a href="ensemble-learning.html#cb157-57" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb157-58"><a href="ensemble-learning.html#cb157-58" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb157-59"><a href="ensemble-learning.html#cb157-59" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb157-60"><a href="ensemble-learning.html#cb157-60" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;training_percent&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb157-61"><a href="ensemble-learning.html#cb157-61" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">training_percent =</span></span>
<span id="cb157-62"><a href="ensemble-learning.html#cb157-62" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(training_percent)), <span class="at">error =</span></span>
<span id="cb157-63"><a href="ensemble-learning.html#cb157-63" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb157-64"><a href="ensemble-learning.html#cb157-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-65"><a href="ensemble-learning.html#cb157-65" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> err.mat <span class="sc">%&gt;%</span></span>
<span id="cb157-66"><a href="ensemble-learning.html#cb157-66" aria-hidden="true" tabindex="-1"></a>       <span class="fu">mutate</span>(<span class="at">training_percent =</span> <span class="fu">as.factor</span>(training_percent)), </span>
<span id="cb157-67"><a href="ensemble-learning.html#cb157-67" aria-hidden="true" tabindex="-1"></a>         <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> training_percent,</span>
<span id="cb157-68"><a href="ensemble-learning.html#cb157-68" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> method)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>Figure <a href="ensemble-learning.html#fig:f7-30">129</a> shows that the decision tree is less accurate than the other two ensemble methods. The random forest has lower error rates than AdaBoost in general. As the training data size increases, the gap between random forest and AdaBoost decreases. This may indicate that when the training data size is small, the random forest is more stable due to its advantage of addressing the statistical issue. Overall, all models become better as the percentage of the training data increases.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-32"></span>
<img src="graphics/7_32.png" alt=" Boxplots of the classification error rates for AdaBoost with a different number of trees" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 130:  Boxplots of the classification error rates for AdaBoost with a different number of trees<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We adjust the number of trees in AdaBoost and show the results in Figure <a href="ensemble-learning.html#fig:f7-32">130</a>. It can be seen that the error rates first go down as the number of trees increases to <span class="math inline">\(400\)</span>. Then the error rates increase, and decrease again. The unstable relationship between the error rates with the number of trees of AdaBoost indicates that AdaBoost is impacted by some particularity of the dataset and seems less robust than random forest.</p>
<p></p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="ensemble-learning.html#cb158-1" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb158-2"><a href="ensemble-learning.html#cb158-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb158-3"><a href="ensemble-learning.html#cb158-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb158-4"><a href="ensemble-learning.html#cb158-4" aria-hidden="true" tabindex="-1"></a>  data1 <span class="ot">&lt;-</span> data</span>
<span id="cb158-5"><a href="ensemble-learning.html#cb158-5" aria-hidden="true" tabindex="-1"></a>  data1<span class="sc">$</span>DX_bl <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(data1<span class="sc">$</span>DX_bl))</span>
<span id="cb158-6"><a href="ensemble-learning.html#cb158-6" aria-hidden="true" tabindex="-1"></a>  ntree.v <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">200</span>, <span class="dv">300</span>, <span class="dv">400</span>, <span class="dv">500</span>, <span class="dv">600</span>, <span class="dv">800</span>, <span class="dv">1000</span>, <span class="dv">1200</span>,</span>
<span id="cb158-7"><a href="ensemble-learning.html#cb158-7" aria-hidden="true" tabindex="-1"></a>               <span class="dv">1400</span>, <span class="dv">1600</span>, <span class="dv">1800</span>, <span class="dv">2000</span>)</span>
<span id="cb158-8"><a href="ensemble-learning.html#cb158-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> ntree.v) {</span>
<span id="cb158-9"><a href="ensemble-learning.html#cb158-9" aria-hidden="true" tabindex="-1"></a>    boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(DX_bl <span class="sc">~</span> ., <span class="at">data =</span> data1[<span class="sc">-</span>testing.ix, ],</span>
<span id="cb158-10"><a href="ensemble-learning.html#cb158-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">dist =</span> <span class="st">&quot;adaboost&quot;</span>, <span class="at">interaction.depth =</span> <span class="dv">6</span>,</span>
<span id="cb158-11"><a href="ensemble-learning.html#cb158-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">n.tree =</span> j)</span>
<span id="cb158-12"><a href="ensemble-learning.html#cb158-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># best.iter &lt;- gbm.perf(boost,method=&#39;cv&#39;)</span></span>
<span id="cb158-13"><a href="ensemble-learning.html#cb158-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost, data1[testing.ix, ], <span class="at">n.tree =</span> j,</span>
<span id="cb158-14"><a href="ensemble-learning.html#cb158-14" aria-hidden="true" tabindex="-1"></a>                    <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb158-15"><a href="ensemble-learning.html#cb158-15" aria-hidden="true" tabindex="-1"></a>    pred[pred <span class="sc">&gt;</span> <span class="fl">0.5</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb158-16"><a href="ensemble-learning.html#cb158-16" aria-hidden="true" tabindex="-1"></a>    pred[pred <span class="sc">&lt;=</span> <span class="fl">0.5</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb158-17"><a href="ensemble-learning.html#cb158-17" aria-hidden="true" tabindex="-1"></a>    error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb158-18"><a href="ensemble-learning.html#cb158-18" aria-hidden="true" tabindex="-1"></a>                    target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb158-19"><a href="ensemble-learning.html#cb158-19" aria-hidden="true" tabindex="-1"></a>    err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;AdaBoost&quot;</span>, j, error))</span>
<span id="cb158-20"><a href="ensemble-learning.html#cb158-20" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb158-21"><a href="ensemble-learning.html#cb158-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb158-22"><a href="ensemble-learning.html#cb158-22" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb158-23"><a href="ensemble-learning.html#cb158-23" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;num_trees&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb158-24"><a href="ensemble-learning.html#cb158-24" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span></span>
<span id="cb158-25"><a href="ensemble-learning.html#cb158-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">num_trees =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(num_trees)), </span>
<span id="cb158-26"><a href="ensemble-learning.html#cb158-26" aria-hidden="true" tabindex="-1"></a>         <span class="at">error =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb158-27"><a href="ensemble-learning.html#cb158-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb158-28"><a href="ensemble-learning.html#cb158-28" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> err.mat <span class="sc">%&gt;%</span> </span>
<span id="cb158-29"><a href="ensemble-learning.html#cb158-29" aria-hidden="true" tabindex="-1"></a>          <span class="fu">mutate</span>(<span class="at">num_trees =</span> <span class="fu">as.factor</span>(num_trees)), </span>
<span id="cb158-30"><a href="ensemble-learning.html#cb158-30" aria-hidden="true" tabindex="-1"></a>          <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> num_trees, <span class="at">color =</span> method)) <span class="sc">+</span></span>
<span id="cb158-31"><a href="ensemble-learning.html#cb158-31" aria-hidden="true" tabindex="-1"></a>              <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>We repeat the experiment on random forest and show the result in Figure <a href="ensemble-learning.html#fig:f7-33">131</a>. Similar to AdaBoost, when the number of trees is small, the random forest has higher error rates. Then, the error rates decrease as more trees are added. And the error rates become stable when more trees are added. The random forest handles the statistical issue better than the AdaBoost.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-33"></span>
<img src="graphics/7_33.png" alt=" Boxplots of the classification error rates for random forests with a different number of trees" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 131:  Boxplots of the classification error rates for random forests with a different number of trees<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p></p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="ensemble-learning.html#cb159-1" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb159-2"><a href="ensemble-learning.html#cb159-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb159-3"><a href="ensemble-learning.html#cb159-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb159-4"><a href="ensemble-learning.html#cb159-4" aria-hidden="true" tabindex="-1"></a>testing.ix <span class="ot">&lt;-</span> testing.indices[i, ]</span>
<span id="cb159-5"><a href="ensemble-learning.html#cb159-5" aria-hidden="true" tabindex="-1"></a>target.testing <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl[testing.ix]</span>
<span id="cb159-6"><a href="ensemble-learning.html#cb159-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb159-7"><a href="ensemble-learning.html#cb159-7" aria-hidden="true" tabindex="-1"></a>ntree.v <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">400</span>, <span class="dv">600</span>, <span class="dv">800</span>, <span class="dv">1000</span>)</span>
<span id="cb159-8"><a href="ensemble-learning.html#cb159-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> ntree.v) {</span>
<span id="cb159-9"><a href="ensemble-learning.html#cb159-9" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ], <span class="at">ntree =</span> j)</span>
<span id="cb159-10"><a href="ensemble-learning.html#cb159-10" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[testing.ix, ])</span>
<span id="cb159-11"><a href="ensemble-learning.html#cb159-11" aria-hidden="true" tabindex="-1"></a>error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb159-12"><a href="ensemble-learning.html#cb159-12" aria-hidden="true" tabindex="-1"></a>                        target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb159-13"><a href="ensemble-learning.html#cb159-13" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;RF&quot;</span>, j, error))</span>
<span id="cb159-14"><a href="ensemble-learning.html#cb159-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb159-15"><a href="ensemble-learning.html#cb159-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb159-16"><a href="ensemble-learning.html#cb159-16" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb159-17"><a href="ensemble-learning.html#cb159-17" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;num_trees&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb159-18"><a href="ensemble-learning.html#cb159-18" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">num_trees =</span></span>
<span id="cb159-19"><a href="ensemble-learning.html#cb159-19" aria-hidden="true" tabindex="-1"></a>                          <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(num_trees)), </span>
<span id="cb159-20"><a href="ensemble-learning.html#cb159-20" aria-hidden="true" tabindex="-1"></a><span class="at">error =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb159-21"><a href="ensemble-learning.html#cb159-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb159-22"><a href="ensemble-learning.html#cb159-22" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> </span>
<span id="cb159-23"><a href="ensemble-learning.html#cb159-23" aria-hidden="true" tabindex="-1"></a>            err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">num_trees =</span> <span class="fu">as.factor</span>(num_trees)), </span>
<span id="cb159-24"><a href="ensemble-learning.html#cb159-24" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> num_trees, <span class="at">color =</span> method)) <span class="sc">+</span> </span>
<span id="cb159-25"><a href="ensemble-learning.html#cb159-25" aria-hidden="true" tabindex="-1"></a>            <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p>Building on the result shown in Figure <a href="ensemble-learning.html#fig:f7-33">131</a>, we pursue a further study of the behavior of random forest. Recall that, in random forest, there are two approaches to increase diversity, one is to Bootstrap samples for each tree, while another is to conduct random feature selection for splitting each node.</p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-36"></span>
<img src="graphics/7_36.png" alt=" Boxplots of the classification error rates for random forest with a different sample sizes" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 132:  Boxplots of the classification error rates for random forest with a different sample sizes<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>First, we investigate the effectiveness of the use of Bootstrap. We change the sampling strategy from <em>sampling with replacement</em> to <em>sampling without replacement</em> and change the sampling size<label for="tufte-sn-193" class="margin-toggle sidenote-number">193</label><input type="checkbox" id="tufte-sn-193" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">193</span> The sampling size is the sample size of the Bootstrapped dataset.</span> from <span class="math inline">\(10\%\)</span> to <span class="math inline">\(100\%\)</span>. The number of features tested at each node is kept at the default value, i.e., <span class="math inline">\(\sqrt{p}\)</span>, where <span class="math inline">\(p\)</span> is the number of features. Figure <a href="ensemble-learning.html#fig:f7-36">132</a> shows that the increased sample size has an impact on the error rates.</p>
<p></p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="ensemble-learning.html#cb160-1" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb160-2"><a href="ensemble-learning.html#cb160-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb160-3"><a href="ensemble-learning.html#cb160-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb160-4"><a href="ensemble-learning.html#cb160-4" aria-hidden="true" tabindex="-1"></a>  testing.ix <span class="ot">&lt;-</span> testing.indices[i, ]</span>
<span id="cb160-5"><a href="ensemble-learning.html#cb160-5" aria-hidden="true" tabindex="-1"></a>  target.testing <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl[testing.ix]</span>
<span id="cb160-6"><a href="ensemble-learning.html#cb160-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb160-7"><a href="ensemble-learning.html#cb160-7" aria-hidden="true" tabindex="-1"></a>  sample.size.v <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb160-8"><a href="ensemble-learning.html#cb160-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> sample.size.v) {</span>
<span id="cb160-9"><a href="ensemble-learning.html#cb160-9" aria-hidden="true" tabindex="-1"></a>    sample.size <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fu">nrow</span>(data[<span class="sc">-</span>testing.ix, ]) <span class="sc">*</span> j)</span>
<span id="cb160-10"><a href="ensemble-learning.html#cb160-10" aria-hidden="true" tabindex="-1"></a>    rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ],</span>
<span id="cb160-11"><a href="ensemble-learning.html#cb160-11" aria-hidden="true" tabindex="-1"></a>                       <span class="at">sampsize =</span> sample.size, </span>
<span id="cb160-12"><a href="ensemble-learning.html#cb160-12" aria-hidden="true" tabindex="-1"></a>                       <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb160-13"><a href="ensemble-learning.html#cb160-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[testing.ix, ])</span>
<span id="cb160-14"><a href="ensemble-learning.html#cb160-14" aria-hidden="true" tabindex="-1"></a>    error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb160-15"><a href="ensemble-learning.html#cb160-15" aria-hidden="true" tabindex="-1"></a>                target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb160-16"><a href="ensemble-learning.html#cb160-16" aria-hidden="true" tabindex="-1"></a>    err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;RF&quot;</span>, j, error))</span>
<span id="cb160-17"><a href="ensemble-learning.html#cb160-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb160-18"><a href="ensemble-learning.html#cb160-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb160-19"><a href="ensemble-learning.html#cb160-19" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb160-20"><a href="ensemble-learning.html#cb160-20" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;sample_size&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb160-21"><a href="ensemble-learning.html#cb160-21" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">sample_size =</span></span>
<span id="cb160-22"><a href="ensemble-learning.html#cb160-22" aria-hidden="true" tabindex="-1"></a>                <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(sample_size)), </span>
<span id="cb160-23"><a href="ensemble-learning.html#cb160-23" aria-hidden="true" tabindex="-1"></a>                <span class="at">error =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb160-24"><a href="ensemble-learning.html#cb160-24" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span> err.mat <span class="sc">%&gt;%</span> </span>
<span id="cb160-25"><a href="ensemble-learning.html#cb160-25" aria-hidden="true" tabindex="-1"></a>              <span class="fu">mutate</span>(<span class="at">sample_size =</span> <span class="fu">as.factor</span>(sample_size)), </span>
<span id="cb160-26"><a href="ensemble-learning.html#cb160-26" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> sample_size,<span class="at">color =</span> method)) <span class="sc">+</span> </span>
<span id="cb160-27"><a href="ensemble-learning.html#cb160-27" aria-hidden="true" tabindex="-1"></a>           <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
<p></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:f7-35"></span>
<img src="graphics/7_35.png" alt=" Boxplots of the classification error rates for random forest with a different number of features" width="250px"  />
<!--
<p class="caption marginnote">-->Figure 133:  Boxplots of the classification error rates for random forest with a different number of features<!--</p>-->
<!--</div>--></span>
</p>
<p></p>
<p>We then investigate the effectiveness of using random selection of features for node splitting. We fix the sampling size to be the same size as the original dataset, and change the number of features to be selected. Results are shown in Figure <a href="ensemble-learning.html#fig:f7-35">133</a>. When the number of features reaches <span class="math inline">\(11\)</span>, the error rate starts to increase. This is probably because of the loss of the diversity of the trees, i.e., the more features to be used, the less randomness is introduced into the trees.</p>
<p></p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="ensemble-learning.html#cb161-1" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb161-2"><a href="ensemble-learning.html#cb161-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb161-3"><a href="ensemble-learning.html#cb161-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(testing.indices)) {</span>
<span id="cb161-4"><a href="ensemble-learning.html#cb161-4" aria-hidden="true" tabindex="-1"></a>  testing.ix <span class="ot">&lt;-</span> testing.indices[i, ]</span>
<span id="cb161-5"><a href="ensemble-learning.html#cb161-5" aria-hidden="true" tabindex="-1"></a>  target.testing <span class="ot">&lt;-</span> data<span class="sc">$</span>DX_bl[testing.ix]</span>
<span id="cb161-6"><a href="ensemble-learning.html#cb161-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb161-7"><a href="ensemble-learning.html#cb161-7" aria-hidden="true" tabindex="-1"></a>  num.fea.v <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>(<span class="fu">ncol</span>(data) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb161-8"><a href="ensemble-learning.html#cb161-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> num.fea.v) {</span>
<span id="cb161-9"><a href="ensemble-learning.html#cb161-9" aria-hidden="true" tabindex="-1"></a>    sample.size <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data[<span class="sc">-</span>testing.ix, ])</span>
<span id="cb161-10"><a href="ensemble-learning.html#cb161-10" aria-hidden="true" tabindex="-1"></a>    rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(DX_bl <span class="sc">~</span> ., data[<span class="sc">-</span>testing.ix, ],</span>
<span id="cb161-11"><a href="ensemble-learning.html#cb161-11" aria-hidden="true" tabindex="-1"></a>                       <span class="at">mtry =</span> j, <span class="at">sampsize =</span> sample.size, </span>
<span id="cb161-12"><a href="ensemble-learning.html#cb161-12" aria-hidden="true" tabindex="-1"></a>                       <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb161-13"><a href="ensemble-learning.html#cb161-13" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, data[testing.ix, ])</span>
<span id="cb161-14"><a href="ensemble-learning.html#cb161-14" aria-hidden="true" tabindex="-1"></a>    error <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">which</span>(<span class="fu">as.character</span>(pred) <span class="sc">!=</span></span>
<span id="cb161-15"><a href="ensemble-learning.html#cb161-15" aria-hidden="true" tabindex="-1"></a>                  target.testing))<span class="sc">/</span><span class="fu">length</span>(target.testing)</span>
<span id="cb161-16"><a href="ensemble-learning.html#cb161-16" aria-hidden="true" tabindex="-1"></a>    err.mat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(err.mat, <span class="fu">c</span>(<span class="st">&quot;RF&quot;</span>, j, error))</span>
<span id="cb161-17"><a href="ensemble-learning.html#cb161-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb161-18"><a href="ensemble-learning.html#cb161-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb161-19"><a href="ensemble-learning.html#cb161-19" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(err.mat)</span>
<span id="cb161-20"><a href="ensemble-learning.html#cb161-20" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(err.mat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;method&quot;</span>, <span class="st">&quot;num_fea&quot;</span>, <span class="st">&quot;error&quot;</span>)</span>
<span id="cb161-21"><a href="ensemble-learning.html#cb161-21" aria-hidden="true" tabindex="-1"></a>err.mat <span class="ot">&lt;-</span> err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(num_fea</span>
<span id="cb161-22"><a href="ensemble-learning.html#cb161-22" aria-hidden="true" tabindex="-1"></a>                    <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(num_fea)),</span>
<span id="cb161-23"><a href="ensemble-learning.html#cb161-23" aria-hidden="true" tabindex="-1"></a>                    <span class="at">error =</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(error)))</span>
<span id="cb161-24"><a href="ensemble-learning.html#cb161-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-25"><a href="ensemble-learning.html#cb161-25" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_boxplot</span>(<span class="at">data =</span></span>
<span id="cb161-26"><a href="ensemble-learning.html#cb161-26" aria-hidden="true" tabindex="-1"></a>             err.mat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">num_fea =</span> <span class="fu">as.factor</span>(num_fea)), </span>
<span id="cb161-27"><a href="ensemble-learning.html#cb161-27" aria-hidden="true" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">y =</span> error, <span class="at">x =</span> num_fea, <span class="at">color =</span> method)) <span class="sc">+</span></span>
<span id="cb161-28"><a href="ensemble-learning.html#cb161-28" aria-hidden="true" tabindex="-1"></a>           <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p></p>
</div>
</div>
<p style="text-align: center;">
<a href="support-vector-machine.html"><button class="btn btn-default">Previous</button></a>
<a href="remarks-5.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="js/jquery.js"></script>
<script src="js/tablesaw-stackonly.js"></script>
<script src="js/nudge.min.js"></script>


<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
